{
  "summary_markdown": "# About Oumi\n\nOumi is a company dedicated to creating an open AI ecosystem that encourages collaboration among researchers, developers, and institutions to advance AI technology. The company was founded on January 1, 2024, in collaboration with researchers from 13 leading universities, including Carnegie Mellon, Stanford, and MIT [(Crunchbase, 2025)](https://www.crunchbase.com/organization/oumi). Oumi operates as a Public Benefit Corporation (PBC), which allows it to balance its mission-driven approach with sustainable business operations [(Shah, Obvious, 2025-01-29)](https://obvious.com/ideas/why-we-invested-in-oumi/).\n\nOumi's primary product is an all-in-one open platform for building, evaluating, and deploying AI models. This platform is designed to support a wide range of users, including individual developers, researchers, and enterprises, by providing tools for pre-training, fine-tuning, and evaluating models. The platform is compatible with various model architectures and supports deployment across different environments, including local and cloud settings [(Eadline & Hampton, HPCwire, 2025-01-29)](https://www.hpcwire.com/2025/01/29/ai-needs-its-linux-oumi-comes-out-of-stealth-with-an-open-source-vision/).\n\nThe company has received $10 million in seed funding from investors such as Venrock and Obvious Ventures, with additional contributions from Plug & Play and Ascend [(Eadline & Hampton, HPCwire, 2025-01-29)](https://www.hpcwire.com/2025/01/29/ai-needs-its-linux-oumi-comes-out-of-stealth-with-an-open-source-vision/). Oumi's business model mirrors the Red Hat model for Linux, where the core technology remains open and accessible, but specialized enterprise offerings provide a revenue stream [(Eadline & Hampton, HPCwire, 2025-01-29)](https://www.hpcwire.com/2025/01/29/ai-needs-its-linux-oumi-comes-out-of-stealth-with-an-open-source-vision/).\n\nOumi emphasizes a community-first approach, promoting collaboration, inclusivity, and open-source contributions. The company aims to democratize AI development by making AI research more accessible and collaborative [(Kerner, VentureBeat, 2025-01-29)](https://venturebeat.com/ai/ex-google-apple-engineers-launch-unconditionally-open-source-oumi-ai-platform-that-could-help-to-build-the-next-deepseek/).\n\n# Key Personnel\n\n- **Manos Koukoumidis**: CEO and Co-founder. Previously, he was a Senior Engineering Manager at Google Cloud AI. Koukoumidis is committed to making AI development more transparent and accessible, stating, “We want such open technology to be universal, to be accessible to everyone and present everywhere” [(Eadline & Hampton, HPCwire, 2025-01-29)](https://www.hpcwire.com/2025/01/29/ai-needs-its-linux-oumi-comes-out-of-stealth-with-an-open-source-vision/).\n\n- **Oussama Elachqar**: Co-founder. Formerly a Machine Learning Engineer at Apple. Elachqar has expressed strong commitment to the company's mission and noted the strong investor interest in Oumi, which allowed the team to be selective in choosing backers who aligned with their mission [(Eadline & Hampton, HPCwire, 2025-01-29)](https://www.hpcwire.com/2025/01/29/ai-needs-its-linux-oumi-comes-out-of-stealth-with-an-open-source-vision/).\n\n# News\n\n## Company Launch and Funding\n\nOumi emerged from stealth mode with a mission to create the world's first unconditionally open-source AI platform. The company secured $10 million in seed funding, led by Venrock and Obvious Ventures, with additional contributions from Plug & Play and Ascend [(Eadline & Hampton, HPCwire, 2025-01-29)](https://www.hpcwire.com/2025/01/29/ai-needs-its-linux-oumi-comes-out-of-stealth-with-an-open-source-vision/). The company is structured as a Public Benefit Corporation (PBC), allowing it to balance its mission-driven approach with the ability to operate as a sustainable business [(Shah, Obvious, 2025-01-29)](https://obvious.com/ideas/why-we-invested-in-oumi/).\n\n## Product Development and Features\n\nThe Oumi platform is designed to be a comprehensive solution for AI developers, enabling them to train, fine-tune, and deploy models efficiently across various environments. It supports models ranging from 10 million to 405 billion parameters and incorporates advanced training techniques such as SFT, LoRA, QLoRA, and DPO [(Eadline & Hampton, HPCwire, 2025-01-29)](https://www.hpcwire.com/2025/01/29/ai-needs-its-linux-oumi-comes-out-of-stealth-with-an-open-source-vision/). The platform is compatible with both text and multimodal models, including popular frameworks like Llama and Qwen, and integrates with commercial APIs from OpenAI, Anthropic, and others [(Kerner, VentureBeat, 2025-01-29)](https://venturebeat.com/ai/ex-google-apple-engineers-launch-unconditionally-open-source-oumi-ai-platform-that-could-help-to-build-the-next-deepseek/).\n\n## Collaborations and Partnerships\n\nOumi has developed its platform in collaboration with 13 leading AI universities in the U.S. and the U.K., including prestigious institutions like MIT, Stanford, and Carnegie Mellon [(Eadline & Hampton, HPCwire, 2025-01-29)](https://www.hpcwire.com/2025/01/29/ai-needs-its-linux-oumi-comes-out-of-stealth-with-an-open-source-vision/). The founders have ambitious plans to expand their collaborations with national labs and high-performance computing centers to streamline AI research at scale [(Kerner, VentureBeat, 2025-01-29)](https://venturebeat.com/ai/ex-google-apple-engineers-launch-unconditionally-open-source-oumi-ai-platform-that-could-help-to-build-the-next-deepseek/).\n\n## Market Position and Investor Interest\n\nOumi is positioned to capture a significant market opportunity as enterprise AI spending grows rapidly. Nearly half of enterprises are expressing interest in moving to open-source solutions to reduce costs and vendor lock-in [(Shah, Obvious, 2025-01-29)](https://obvious.com/ideas/why-we-invested-in-oumi/). Oumi's approach mirrors the Red Hat model for Linux, where the core technology remains open and accessible, but specialized enterprise offerings provide a revenue stream [(Eadline & Hampton, HPCwire, 2025-01-29)](https://www.hpcwire.com/2025/01/29/ai-needs-its-linux-oumi-comes-out-of-stealth-with-an-open-source-vision/).\n\nIn summary, Oumi is positioned to become a key player in the AI landscape by providing an open-source platform that democratizes access to AI research and fosters collaboration among researchers and developers. The company's innovative approach and strong backing from investors and academic institutions suggest a promising future in the rapidly evolving AI sector.",
  "target": [
    "Oumi",
    "Oumi",
    "oumi.ai",
    [
      "AI"
    ],
    false,
    false,
    null,
    [
      false,
      false
    ]
  ],
  "webpage_result": {
    "summary_markdown": "# Oumi Company Overview\n\n## Mission\nOumi is dedicated to creating a truly open AI ecosystem, fostering collaboration among researchers, developers, and institutions to advance frontier AI technology. The company believes that AI should be developed collectively and openly, ensuring accessibility and transparency for all.\n\n## Company History\nFounded in collaboration with researchers from 13 leading universities, including Carnegie Mellon, Stanford, and MIT, Oumi aims to democratize AI development. The company has received $10 million in seed funding from notable investors like Venrock and Obvious Ventures.\n\n## Leadership Team\n- **Manos Koukoumidis**: CEO and Co-founder, previously a Senior Engineering Manager at Google Cloud AI.\n- **Oussama Elachqar**: Co-founder, formerly a Machine Learning Engineer at Apple.\n\n## Culture\nOumi promotes a community-first approach, emphasizing collaboration, inclusivity, and open-source contributions. The company encourages participation from individuals and organizations to build a better future for AI.\n\n## Products and Services\nOumi offers an all-in-one open platform for building, evaluating, and deploying AI models. Key features include:\n\n- **Zero Boilerplate**: Quick start with ready-to-use recipes for popular models and workflows.\n- **Enterprise-Grade**: Designed for teams training models at scale.\n- **Research Ready**: Supports reproducible experiments and customizable components.\n- **Broad Model Support**: Compatible with various model architectures, including text and multimodal models.\n- **SOTA Performance**: Optimized for distributed training and inference.\n\n### Key Features\n- Pre-training, fine-tuning, and evaluation of models.\n- Data synthesis and curation.\n- Deployment capabilities across various environments (local, cloud).\n- Open-source with no vendor lock-in.\n\n## Community and Collaboration\nOumi emphasizes the importance of community collaboration in AI development. The platform is designed to support joint research efforts and encourages contributions from users to enhance the ecosystem.\n\n## Customers\nOumi targets a wide range of users, including individual developers, researchers, and enterprises looking to leverage AI for various applications.\n\n## Conclusion\nOumi is positioned as a leader in the open-source AI movement, advocating for collective development and transparency in AI technologies. The company aims to unlock the potential of AI for the benefit of humanity by fostering an inclusive and collaborative environment.\n\nFor more information, visit [Oumi](https://oumi.ai/).",
    "page_markdowns": [
      "# [Oumi](https://oumi.ai/)\nON A MISSION FOR TRULY OPEN AI\n\nOumi is a community of researchers, developers, and institutions working to make frontier AI more open and collaborative.\n\nJoin us in building the platform, models, and tools for the next generation of AI.\n\nGET STARTED TODAY WITH A CUTTING EDGE OPEN PLATFORM\n\nEverything you need to build state-of-the-art AI models, end to end\n\nFrom data preparation to production deployment, Oumi provides an all-in-one production-ready open platform to build, evaluate, and deploy cutting-edge AI models at any scale.\n\nPre-training, fine-tuning, and evaluation with models at any scale\n\nText and multimodal, open and closed models\n\nData synthesis and curation\n\nRun anywhere, from your laptop to the cloud\n\npip install oumi\n\nWHY USE OUMI\n\nThe Oumi platform provides the flexibility and tools you need to build and advance the latest AI.",
      "# [Company](https://oumi.ai/company)\nWe believe that general foundation models and related infrastructure should be developed collectively in the open, not in silos. They need to be fully accessible, not hidden behind an API.\n\nTruly open frontier AI constitutes the best path to developing and offering the advanced, trustworthy, and efficient AI needed to unlock industry applications and scientific breakthroughs.\n\nWe believe that AI will have a transformative impact on humanity. Developing it collectively in the open is the best path forward to ensure that it is done efficiently and responsibly.\n\nWe are committed to enabling the collaborative development of open source frontier AI while making it accessible to all.\n\nBy providing the safest, highest quality, and most flexible AI, we aspire to unlock positive impact for every individual, enterprise, and humanity overall.",
      "# [Oumi](https://oumi.ai/blog)\nBuilt to be truly open for everyone\n\nTruly open and collectively developed AI is the future.\n\nBuild that future.",
      "# [Oumi Jobs](https://oumi.ai/careers)\n",
      "# [🚀 Getting Started — Oumi](https://oumi.ai/docs)\n💻 Why use Oumi?#\n\nIf you need a comprehensive platform for training, evaluating, or deploying models, Oumi is a great choice.\n\nHere are some of the key features that make Oumi stand out:\n\n🔧 Zero Boilerplate: Get started in minutes with ready-to-use recipes for popular models and workflows. No need to write training loops or data pipelines.\n\n🏢 Enterprise-Grade: Built and validated by teams training models at scale\n\n🎯 Research Ready: Perfect for ML research with easily reproducible experiments, and flexible interfaces for customizing each component.\n\n🌐 Broad Model Support: Works with most popular model architectures - from tiny models to the largest ones, text-only to multimodal.\n\n🚀 SOTA Performance: Native support for distributed training techniques (FSDP, DDP) and optimized inference engines (vLLM, SGLang).\n\n🤝 Community First: 100% open source with an active community. No vendor lock-in, no strings attached.\n\n📖 Where to go next?#\n\nWhile you can dive directly into any section that interests you, we recommend following the suggested path below to get the most out of Oumi.\n\nCategory\n\nDescription\n\nLinks",
      "# [Contact Us](https://oumi.ai/contact)\nBuilt to be truly open for everyone\n\nTruly open and collectively developed AI is the future.\n\nBuild that future.",
      "# [Oumi](https://oumi.ai/privacy-policy)\nPrivacy Notice\n\nAt Oumi, PBC, we're committed to protecting the privacy of our users. This notice explains how we collect, use, share, and protect your data.\n\nWhat Is Personal Data?\n\nFor the purposes of this notice, \"personal data\" refers to any information that can be used to identify or contact an individual. This includes:\n\nNames and addresses\n\nEmail addresses and phone numbers\n\nIP addresses (which may reveal a user's location)\n\nOnline identifiers (e.g., cookies, device IDs)\n\nPersonal data also encompasses indirect identifiers like browsing history, search queries, or interactions with our services.\n\nWhat Data Do We Collect?\n\nWhen you visit our website or interact with our services, we may collect certain information from you:\n\nCookies for Analytics: When you browse our site, we use cookies to analyze traffic patterns and improve user experience. These cookies don't contain any personal identifiable information.\n\nContact Information: If you submit your email address through our contact form or explicitly provide it to us (e.g., when requesting a feature), we'll store this information for future communication about Oumi updates, new features, or other relevant topics.\n\nUser Interactions with Our Services: When users engage with our playground by submitting requests or responding to them, we collect data on these interactions to improve the quality and relevance of our generative AI services.\n\nChildren's Personal Data\n\nOur website and services are not intended for, and we do not intend to, or knowingly, collect or solicit personal data from children under the age of 13. If an individual is under the age of 13, they should not use our services or otherwise provide us with any personal data either directly or by other means. If a child under the age of 13 has provided personal data to us, we encourage the child’s parent or guardian to contact us to request that we remove the personal data from our systems. If we learn that any personal data we have collected has been provided by a child under the age of 16, we will promptly delete that personal data.\n\nHow Do We Use Your Data?\n\nWe use your collected data for:\n\nImproving our website's performance and user experience.\n\nSending you relevant information about Oumi updates, new features, or other topics that might interest you (if you've provided us with an email address).\n\nEnhancing the quality of our generative AI services by analyzing user interactions.\n\nEnforcing Our Terms of Service: We may use your personal data to enforce compliance with our Terms of Service (\"ToS\"). This includes, but is not limited to: identifying and taking action against users who violate our ToS, and contacting users regarding any potential breaches of our ToS.\n\nHow Do We Protect Your Data?\n\nWe take data protection seriously:\n\nData Minimization: We only collect and store necessary information for specific purposes.\n\nSecurity Measures: Our systems are designed to protect your data from unauthorized access, use, or disclosure.\n\nCompliance with Laws: We comply with applicable laws and regulations regarding personal data.\n\nControl Over Your Information\n\nAs a user of our services:\n\nYou have the right to request us not to contact you about future updates (opt-out).\n\nIf we've collected your email address through other means, you can ask us to delete it.\n\nIn some jurisdictions, you may also be entitled to access or correct your data.\n\nHow Can I Contact Oumi?\n\nIf you have any questions, concerns, or requests regarding this Privacy Notice or our practices: privacy@oumi.ai\n\nWe're committed to transparency and will respond promptly to your inquiries.\n\nChanges to this Privacy Notice\n\nWe reserve the right to modify this Privacy Notice at any time without prior notice.\n\nIf we update this Privacy Notice:\n\nWe will send an email notification to all registered users with an active email address, informing them of the changes.\n\nWe will post a clear announcement on our website about the updated Privacy Notice and its effective date.\n\nThe revised Privacy Notice will include a \"Last Updated\" footer indicating when it was last modified.\n\nLast Updated: January 27, 2025",
      "# [Oumi Unveils First Unconditionally Open AI Platform](https://oumi.ai/blog/posts/press-release)\nOumi Unveils First Unconditionally Open AI Platform to Ensure AI is Developed Collectively and Responsibly\n\nJanuary 29th, 2025\n\nOumi unites AI researchers and developers globally to collaborate, experiment, and build together the world's largest state-of-the-art open-source laboratory.\n\nSeattle, WA - Oumi, an AI laboratory created in collaboration with researchers from 13 leading research universities, including Carnegie Mellon University, Stanford University, and the Massachusetts Institute of Technology, has unveiled the world's first unconditionally open source AI platform. By enabling open collaboration, Oumi enables thousands of researchers, developers, and AI experts to come together for the first time to advance AI collectively and responsibly.\n\nThe development of multimodal foundation models has largely been limited to incumbent AI platform companies, which release models as closed, proprietary black boxes, or masquerade open-weight models as \"open source.\" Oumi is the first to launch a platform that offers state-of-the-art foundation models with open code, open data, open weights—and open collaboration—uniting a global community of developers and researchers to drive AI forward responsibly and transparently. Oumi is a Public Benefit Corporation and announced $10 million in Seed funding led by Venrock and Obvious Ventures with contributions from Plug & Play and Ascend.\n\n\"AI needs its Linux,\" said Manos Koukoumidis, CEO and Co-founder of Oumi and previously a Senior Engineering Manager at Google Cloud AI who bootstrapped and led the efforts for Cloud PaLM. \"The greatest technology innovations are developed in the open. We must open AI up to the open source community to advance it collectively and responsibly. To make it accessible to everyone.\"\n\nOumi is designed to support all common foundation model workflows in one unified platform to promote accessibility and to foster open collaboration with all AI researchers and developers. With the all-in-one Oumi platform, developers can:\n\nTrain and fine-tune models from 10M to 405B parameters using state-of-the-art techniques (SFT, LoRA, QLoRA, DPO, and more)\n\nWork with both text and multimodal models (Llama, Qwen, Phi, and others)\n\nSynthesize and curate training data with LLM judges, then readily use it to train models\n\nDeploy models efficiently with popular inference engines (vLLM, SGLang)\n\nEvaluate models comprehensively across standard benchmarks\n\nRun anywhere - from laptops to clusters to clouds (AWS, Azure, GCP, Lambda, and more)\n\nIntegrate with both open models and commercial APIs (OpenAI, Anthropic, Vertex AI, Parasail, etc.)\n\nThe current AI landscape creates the illusion of a talent desert that is now starting to impact innovation, as resources focus on individual, rather than collective, AI projects.\n\n\"Our vision with Oumi is to free the AI talent locked in silos and make AI the ultimate team sport,\" said Oussama Elachqar, Co-founder of Oumi and previously a Machine Learning Engineer at Apple. \"Giving AI talent a platform where they can work collectively will accelerate progress and speed of discovery in every area of AI. We have incredibly talented and bright developers and researchers in AI working in silos. If we unite them, we will get further, faster.\"\n\n\"There hasn't been enough focus on open experimentation to collectively push the boundaries of what AI can do,\" said Ganesh Srinivasan, Partner at Venrock. \"With an unconditionally open source platform, we won't cap our talent, we will foster experimentation, and society will reap the benefits.\"\n\nOumi is designed to be fully flexible and easy to use. The end-to-end, all-in-one platform supports the full AI lifecycle with one consistent interface - from pretraining to data curation, data synthesis, fine-tuning (SFT, LoRA, QLoRA, DPO), inference, and evaluation. Users can seamlessly work with both open models (Llama, QWEN, Phi and others) and commercial APIs (OpenAI, Anthropic, Vertex AI), with both text and multimodal models. Oumi will offer prebuilt ready-to-use workflows and recipes for post training and other common operations.\n\nOumi can run anywhere so that developers can train and evaluate models seamlessly across environments, from local machines to remote clusters and clouds (AWS, Azure, GCP, Lambda), with native support for Jupyter notebooks and VS Code debugging. Additionally, the enterprise-grade platform is built for scale, offering first-class support for distributed training with PyTorch DDP and FSDP and ability to efficiently handle models up to 405B parameters.\n\nTo better enable the community to build upon each other's work independently, or to collaborate in research efforts, the recordability and reproducibility of experiments is a key design principle of the Oumi platform. To further promote the collective improvement of foundation models, Oumi plans to coordinate joint research efforts with fully open participation across the world.\n\nOumi is developed in partnership with researchers at the following universities:\n\nUniversity of Illinois Urbana Champaign\n\nCarnegie Mellon University\n\nPrinceton University\n\nStanford University\n\nGeorgia Institute of Technology\n\nCalifornia Institute of Technology\n\nUniversity of California, Berkeley\n\nUniversity of Washington\n\nNew York University\n\nMassachusetts Institute of Technology\n\nUniversity of Waterloo\n\nUniversity of Cambridge\n\nUniversity of Oxford",
      "# [If AI isn't truly open, it will fail us](https://oumi.ai/blog/posts/truly-open-ai)\nJanuary 29th, 2025\n\nImagine a world without electricity, cars, computers, the internet — a truly bleak place. Or one where each of these things are owned and controlled by one company. That's what our world would look like today if the human knowledge that brought about these innovations had been locked away, never shared openly. None of the benefits of modern technology, science or healthcare would be there for us to move society forward — together — at the rate we have. We would be set back hundreds of years.\n\nCurrently, we're faced with the risk of AI, our most powerful invention, failing us. For the sake of short-sighted commercialization and in complete misalignment with humanity's best interests, the development of this highly complex technology has become increasingly closed. Leaders in the industry have become increasingly secretive, moving further and further away from open knowledge sharing and collaboration. Instead, they are feverishly competing against each other to \"win,\" all while wastefully re-inventing each other's work at enormous human, capital, and environmental cost.\n\nAnd despite this high cost, they are still failing us. The technology is too complex for any single company to advance effectively on its own, without openly sharing knowledge and collaborating with the rest of the AI community. Yet, these leading AI companies try to arrogantly dissuade others from pursuing this technology. They want you to believe that the future of AI progress is closed, that it is our destiny to kneel at their feet. They want you to pay tribute and let them freely monetize. They want you to believe that they are the chosen prophets that will lead us all to our AI salvation.\n\nThis pattern is straight out of the Unix playbook. The major tech players developed their proprietary, closed-source versions of Unix, and those seemed to be the only \"possible\" and \"right\" solutions. The argument, or assumption, was that these major tech players were the only ones with the talent, quality and capacity to develop versions of Unix. Then, Linux showed the world how open source was a better approach to developing such a critical and complex technology. Linux gave the crucial flexibility that developers needed to customize it to their needs. It gave them the ownership they needed to avoid getting locked into a closed vendor. It gave them the transparency they needed to fully trust it. All that at a lower cost. As more and more developers adopted Linux, the Linux ecosystem became more mature, safe, versatile and performant than any closed Unix alternative. It became the de facto platform — the platform that is now powering practically all cloud computing and most mobile devices, the internet, and the development of AI itself.\n\nI believe that AI will develop not just in a similar way, but in an even more profound one. If there was ever a technology that should and can thrive in open source, it is AI. Open source AI is not only the best solution for the good of humanity overall, but also for the benefit of every single individual developer and enterprise. At the same time, it is the best path for developing the most competitive version of AI, the one that would dominate the same way Linux did.\n\nHow Open Source Benefits Humanity\n\nAI represents our best opportunity for humanity's advancement. Open source is the best path we have for AI to effectively and safely unlock industry applications and promote scientific discovery.\n\nWithout Open Source, the Bottleneck for Advancement Gets Narrower\n\nFoundation models are used across many different tasks, from improving simple traditional scenarios like classification to diverse, higher-level automation tasks. No single AI solution developed by a single company can solve and optimize for all of them at the same time — there is no AI silver bullet. AI has also become critical to advancing the sciences across the board, from healthcare to weather forecasting, from partial differential equation solving to biology. If frontier AI continues down the path of black-box APIs, scientific progress will be needlessly impeded.\n\nOpen Source is the Safest Option\n\nMaking foundation models safe, unbiased, culturally-inclusive, privacy-preserving, and secure across diverse modalities and tasks is no easy feat, to say the least. No existing big tech company seems to be able to invest enough to solve all these challenges – especially when they are entrenched in one-upping their competitors in the public AI leaderboards, as opposed to focusing on what would benefit the world at large.\n\nOpen source not only helps to collectively invest in AI safety, but even to discover the issues that need fixing in the first place. Per Linus' Law, \"Given enough eyeballs, all bugs are shallow.\" It stands to reason that \"given enough eyeballs, all AI issues are shallow\" too. The open source approach enables the collective review and contributions of a large community, allowing for such issues to be quickly identified and resolved.\n\nThere is also, of course, the significant debate on the hypothetical existential risks posed by superintelligent misaligned AI. Regardless of where you stand on this matter and the probability you assign to this possibility, one thing is certain: such challenges can be better solved in the open with broad collaboration as opposed to in secret by a single organization whose top goal is a sprint to monetization.\n\nOpen Source Provides the Critical Balance of Power\n\nTo put it plainly, the control of such a powerful technology should not be in the hands of a small handful of corporations. This would lead to extreme centralization of power with significant societal, economic, and ethical implications. Negative repercussions have a far greater chance of being mitigated with an open source approach.\n\nHow Open Source Benefits Developers\n\nAs with open source Linux, open source AI has many critical attributes that make it a better choice for developers and organizations:\n\nTrust, Privacy, and Security\n\nOrganizations need AI that they can trust. AI data is often sensitive — it may contain private personal information and IP. Organizations don't want to trust closed vendors' infrastructure with their sensitive data. They don't want their data stored on the vendor's cloud and processed with non-transparent code. As in the case of Linux, open source can not only be more secure, but it also provides organizations with the flexibility to deploy it on the trusted infrastructure of their choosing.\n\nAnd regardless of the infrastructure used, organizations often cannot trust the AI itself when it is developed and offered as a black box solution. For full security and transparency, organizations need to understand how it was developed, so that they can anticipate its behavior and better understand how they can improve what matters for their use case specifically. Ultimately, they need more transparency for auditability and compliance.\n\nFlexibility Fuels Differentiation\n\nAt the end of the day, closed vendor APIs don't provide enough flexibility for much-needed differentiation. Developers are often initially impressed by the quality of the results they get from calling a closed vendor's API with an LLM prompt honed for their particular scenario — AI has come a long way in the last few years. However, what developers sometimes don't realize, especially when misled by \"experts\"1, is that overreliance on such limited, closed APIs, could be setting their organization up for eventual failure. For AI-first companies to succeed, they need AI differentiation. That is, what level of differentiation can an organization really achieve with a bespoke prompt, which a competitor could easily replicate?\n\nTo differentiate, organizations need to leverage their unique and invaluable troves of data and expertise. And to leverage such data, organizations need AI that is easy to use and gives them full flexibility to adjust it to their needs as they build their AI muscle. Unfortunately, some organizations, for the sake of \"convenience,\" may seem content, lingering in AI's shallow waters: customizing prompts and calling APIs that allow for limited tuning. But such organizations are missing key opportunities for differentiation and will likely find themselves on the wrong side of history. Their competitors likely won't make the same mistake.\n\nHigher Quality and Richer Features at a Lower Cost\n\nOpen source solutions that are backed by strong communities can tap on a diverse and deep well of talent. That helps them to often have at the same time richer features, stronger performance and lower cost than any of the closed source alternatives. Linux makes this case perfectly.\n\nKeeping the Keys to the Castle: Staying in Control and Avoiding Lock-in\n\nLock-in greatly decreases an organization's flexibility to innovate, while typically increasing its costs. Organizations know that, and thus, they don't want to get locked into a specific closed vendor's model, cloud, and hardware; they want control and flexibility. They also want predictability. It can be very disruptive for an organization when a closed vendor forces changes in the model, related infrastructure, or terms of use.\n\nHow Community Can Develop the Best AI\n\nAI isn't just a team sport: it's a global effort. AI is very complex, with a lot of issues to fix and capabilities to improve: it's worthy of having a global team tackle it. The innovation and experimentation needed can be easily pursued in parallel across a distributed and loosely-coupled large global team, an open community spreading all across the world.\n\nTo protect their interests, closed vendors openly brag about how many GPUs they have in their attempt to secure their compute \"moat.\" But, I truly believe that an open community can do at least as well as they can. In fact, I think that it can do a lot better. Certainly compute is needed for AI research, but, especially with pre-training scaling laws hitting a wall, a relatively \"small\" siloed group of people doesn't stand a chance against a massive open community, when it comes to innovating on new algorithms. Similarly, with post-training, which does not require the same level of experimentation resources, would the siloed group or the large community deliver higher quality, safer, more secure, more reliable, and more versatile models across all the modalities, tasks, and higher level systems and applications?\n\nImagine there are two horses in the AI race. There is the closed-source horse that tries to do everything by itself in its silo, bearing the full cost for its efforts. And then there is the horse that brings together all the cumulative innovation and compute contributions of a very large global community of academic researchers, developers, and enterprises. Which horse would be faster? Which would be more cost-efficient? Which one would you bet your business on as a developer or decision-maker? Which one would you bet on as an investor?\n\nI would bet on open-source outpacing closed-source AI every time.\n\nThe Future of AI is Bright! Let's Build It Together\n\nWhile there are several examples of complex technologies in modern history that were advanced better in open source than in closed source, the future of AI won't just fix itself — it needs everyone's help. The future of operating systems didn't fix itself: Linus Torvalds released Linux, and then the community joined in.\n\nAI needs its \"Linux\" moment to thrive: a complete open-source platform with all the required development tools propelled forward by the open community. Open-weight models developed in a silo by Meta2, DeepSeek, Alibaba, and others won't cut it. The three components (open data, open code, open weights), as identified by OSI, are not sufficient either. For AI to succeed, it also needs to have \"open collaboration\", and for that, AI needs a platform. Access to the data, code, and trained model is of limited value, if there is a barrier for others in the community to experiment, build upon it, and contribute.\n\nCollaboration must be the lifeblood of this \"Linux\" for AI. The ability to reproduce and build on top of others' contributions, across the full spectrum of frontier foundation model development, is a must. Collaboration needs to be the priority, or open source AI will fail to fulfill its promise.\n\nUntil now, the right AI collaboration didn't exist, so we started building a platform, Open Universal Machine Intelligence (Oumi), alongside academics from 13+ universities. We started building it to support foundation model research and to enable the community to effectively work together. The Oumi platform supports pre-training, tuning, data curation/synthesis, evaluation, and any other common utility, in a fully recordable and reproducible fashion, while being easily customizable to support novel approaches. It supports all common open models (Llama, QWEN, Phi, Mistral) and any platform, with seamless experiment transition from one's laptop to any cloud (AWS, Azure, GCP, Lambda) or on-prem cluster. Regardless of where you get your compute, you can still experiment together with anyone in the community and collaborate.\n\nWith the right AI collaboration platform in place, the community is enabled to develop foundational models collaboratively. We will be starting with the post-training of existing open models, which represent an excellent opportunity for the open community to advance the state of the art: sign up here if you are interested in participating. Next up will be collaboratively pursuing improvements to pre-training.\n\nFor AI to succeed, we can't work in silos. We need to all work together and be on the same team. If we work together, we will succeed, and open source AI will fulfill its promise. Every single organization and every single one of us will then greatly benefit. If not… well, let's just all make sure we build the right future for AI, together. The alternative is simply unacceptable.\n\nWhat future do we want for AI and what will we do about it?\n\nLet's start building it here… 🚀🚀🚀\n\n- Manos\n\nCEO, Oumi\n\n1 An Anthropic representative strongly suggested to me at AWS re:Invent (Dec' 24) that I don't need to train models and that curating a prompt is all I need. I can't overstate how wrong this self-serving argument is — note that Anthropic doesn't currently provide any tuning APIs.",
      "# [Terms of Service](https://oumi.ai/terms-of-service)\nWelcome to Oumi, PBC (\"we,\" \"our,\" or \"us\"). These Terms of Service (\"Terms\") govern your use of our website (the \"Website\"), application programming interface (the \"API\"), and other functionalities offered on or through our services (collectively, the \"Service\") provided by Oumi, PBC. By accessing or using our Service, you agree to comply with these Terms. If you do not agree to these Terms, you may not use the Service.\n\n1) Introduction\n\n1.1) Purpose\n\nThese Terms are designed to ensure responsible use of our Website and API and to define our relationship with you as a user.\n\n1.2) Definitions\n\nAPI: The application programming interface provided by us to enable programmatic interactions with our services.\n\nUser: Any individual or entity accessing or using our Website or API.\n\n2) Using the Website and API\n\n2.1) Acceptable Use\n\nYou agree to:\n\nUse the Website and API only as permitted by applicable law and these Terms.\n\nFollow any technical documentation, usage guidelines, or other instructions provided.\n\n2.2) Prohibited Use\n\nYou may not:\n\nUse the API to violate any law, regulation, or the rights of others.\n\nReverse engineer, decompile, or otherwise attempt to extract source code from the API.\n\nUse the Website or API to create or distribute malicious software or content.\n\nResell, lease, or sublicense the API or its access to third parties without our written consent.\n\n2.3) Open Source Contributions\n\nOur AI tools are open source, governed by the Apache License 2.0. Contributions are welcome under the terms of this license.\n\n3) Access to the API\n\n3.1) Registration\n\nTo access and use the API, you must register and obtain an API key. You agree to provide accurate and up-to-date information during registration.\n\n3.2) API Key Usage\n\nYour API key is personal to you. You are responsible for maintaining its confidentiality and ensuring it is not shared, transferred, or used by others. If your API key is compromised, notify us immediately.\n\n3.3) Rate Limits\n\nWe may impose rate limits or other restrictions on API usage to ensure fair access and optimal performance. You agree not to exceed these limits.\n\n3.4) Termination of Access\n\nWe reserve the right to suspend or terminate your API access if you violate these Terms or if your use of the API creates a risk to our systems or other users.\n\n4) Intellectual Property\n\n4.1) Ownership\n\nWe retain ownership of all intellectual property rights in the Website, API, and associated content, except as provided in our open-source license.\n\n4.2) Feedback\n\nIf you provide suggestions or feedback, you grant us a worldwide, royalty-free, perpetual license to use and incorporate your feedback into our offerings.\n\n5) Privacy\n\n5.1) Data Use\n\nOur Privacy Notice governs the collection, use, and storage of personal data. By using the Website or API, you agree to the terms of our Privacy Notice.\n\n5.2) Your Responsibilities\n\nIf your use of the API involves the collection or processing of personal data, you are solely responsible for ensuring compliance with applicable privacy laws.\n\n6) Disclaimer of Warranties\n\nThe website and api are provided \"as is\" and \"as available.\" we make no warranties, express or implied, including warranties of merchantability, fitness for a particular purpose, or non-infringement.\n\n7) Limitation of Liability\n\nTo the maximum extent permitted by law, we will not be liable for any indirect, incidental, consequential, or special damages arising out of or related to your use of the website or api.\n\n8) Indemnification\n\nYou agree to indemnify and hold harmless Oumi, PBC, its affiliates, employees, and agents from any claims, damages, or expenses arising from your use of the Website or API or your violation of these Terms.\n\n9) Changes to the Terms\n\nWe may update these Terms from time to time. We will post any changes on our Website, and your continued use of the Website or API constitutes acceptance of the updated Terms.\n\n10) Governing Law\n\nThese Terms are governed by the laws of the State of Delaware, excluding its conflict-of-law principles. Any disputes will be resolved exclusively in the courts of the State of Delaware.\n\n11) Contact Information\n\nIf you have any questions about these Terms, please contact us at contact@oumi.ai.\n\nLast Updated: January 27, 2025",
      "# [CLI Reference — Oumi](https://oumi.ai/docs/en/latest/cli/commands.html)\nCLI Reference\n\nContents\n\nCLI Reference#\n\nThis page contains a complete reference of all CLI commands available in Oumi.\n\nFor detailed guides and examples of specific areas (training, inference, evaluation, etc.), please refer to the corresponding user guides in the documentation.\n\nCLI Overrides#\n\nAny Oumi command which takes a config path as an argument (train, evaluate, infer, etc.) can override parameters from the command line. For example:\n\noumi train -c configs/recipes/smollm/sft/135m/quickstart_train.yaml\\ --training.max_steps20\\ --training.learning_rate 1e-4\\ --data.train.datasets[0].shuffletrue\\ --training.output_dir output/smollm-135m-sft\n\nOumi uses OmegaConf to parse the configs from YAML files, and to parse the command line overrides. OmegaConf allows a Pythonic specification of parameters to override with dot-separated syntax, as seen above. Note that for lists (ex. data.train.datasets), you can specify the index either with brackets ([0]) or dot notation (.0).\n\nWith OmegaConf, you can set the value of an entire dictionary or list, in addition to overriding individual primitive values. For example:\n\n# Override one entry in the list. Note that the new dict is merged with the existing # one, so the existing value of `\"dataset_name\": \"yahma/alpaca-cleaned\"` is kept. oumi train -c configs/recipes/smollm/sft/135m/quickstart_train.yaml\\ --data.train.datasets[0]'{\"shuffle\": True, \"sample_count\": 100}' # Override the list, in this case to add a new entry. # Note that we redundantly specify an existing entry in the list here. oumi train -c configs/recipes/smollm/sft/135m/quickstart_train.yaml\\ --data.train.datasets'[{\"dataset_name\": \"yahma/alpaca-cleaned\"}, {\"dataset_name\": \"CohereForAI/aya_dataset\"}]'\n\nWarning\n\nOmegaConf doesn’t readily support adding/deleting entries in a list from command line overrides using index notation. Instead, you need to set the value of the entire list, or modify the YAML config.\n\nWarning\n\nBy default, when you override a dict value with another dict, the items from both will be merged, preferring the value from the overriding dict in case of an existing key. This is equivalent to Python dict merging behavior, ex. new_dict = {**old_dict, **override_dict}\n\nTraining#\n\nFor a detailed guide on training, see Training.\n\noumi train#\n\noumi train Usage: oumi train [OPTIONS]Train a model.╭─ Options ────────────────────────────────────────────────────────────────────╮│*--config-cTEXT Path to the configuration││file for training. ││[default: None] ││[required] ││--log-level-log[DEBUG|INFO|WARNING|ERRThe logging level for the││OR|CRITICAL]specified command. ││--helpShow this message and ││exit. │╰──────────────────────────────────────────────────────────────────────────────╯\n\nEvaluation#\n\nFor a detailed guide on evaluation, see Evaluation.\n\noumi evaluate#\n\noumi evaluate Usage: oumi evaluate [OPTIONS]Evaluate a model.╭─ Options ────────────────────────────────────────────────────────────────────╮│*--config-cTEXT Path to the configuration││file for training. ││[default: None] ││[required] ││--log-level-log[DEBUG|INFO|WARNING|ERRThe logging level for the││OR|CRITICAL]specified command. ││--helpShow this message and ││exit. │╰──────────────────────────────────────────────────────────────────────────────╯\n\nInference#\n\nFor a detailed guide on inference, see Inference.\n\noumi infer#\n\noumi infer Usage: oumi infer [OPTIONS]Run inference on a model.╭─ Options ────────────────────────────────────────────────────────────────────╮│*--config-cTEXT Path to the ││configuration file for ││inference. ││[default: None] ││[required] ││--interactive-iRun in an interactive ││session. ││--imageTEXT File path or URL of an ││input image to be used ││with image+text VLLMs. ││Only used in ││interactive mode. ││[default: None] ││--system-promptTEXT System prompt for ││task-specific ││instructions. Only used││in interactive mode. ││[default: None] ││--log-level-log[DEBUG|INFO|WARNING|EThe logging level for ││RROR|CRITICAL]the specified command. ││--helpShow this message and ││exit. │╰──────────────────────────────────────────────────────────────────────────────╯\n\nJudge#\n\nFor a detailed guide on judging, see LLM Judge.\n\noumi judge#\n\noumi judge Usage: oumi judge [OPTIONS] COMMAND [ARGS]...Judge datasets, models or conversations.╭─ Options ────────────────────────────────────────────────────────────────────╮│--helpShow this message and exit.│╰──────────────────────────────────────────────────────────────────────────────╯╭─ Commands ───────────────────────────────────────────────────────────────────╮│conversations Judge a list of conversations. ││dataset Judge a dataset. ││model Judge the outputs of a model on a dataset. │╰──────────────────────────────────────────────────────────────────────────────╯\n\noumi judge conversations#\n\noumi judge conversations Usage: oumi judge conversations [OPTIONS]Judge a list of conversations.╭─ Options ────────────────────────────────────────────────────────────────────╮│*--config-cTEXT Path to the judge ││config file ││[default: None] ││[required] ││--input-fileTEXT Path to the input file ││(jsonl) ││[default: None] ││--output-fileTEXT Path to the output file││(jsonl) ││[default: None] ││--log-level-log[DEBUG|INFO|WARNING|ERRThe logging level for ││OR|CRITICAL]the specified command. ││--helpShow this message and ││exit. │╰──────────────────────────────────────────────────────────────────────────────╯\n\noumi judge dataset#\n\noumi judge dataset Usage: oumi judge dataset [OPTIONS]Judge a dataset.╭─ Options ────────────────────────────────────────────────────────────────────╮│*--config-cTEXT Path to the judge ││config file ││[default: None] ││[required] ││--dataset-nameTEXT Name of the dataset ││from the registry ││[default: None] ││--dataset-subsetTEXT Subset of the dataset ││to use, if applicable ││[default: None] ││--dataset-splitTEXT Split of the dataset ││to use. ││[default: train] ││--output-fileTEXT Path to the output ││file (jsonl) ││[default: None] ││--log-level-log[DEBUG|INFO|WARNING|EThe logging level for ││RROR|CRITICAL]the specified command.││--helpShow this message and ││exit. │╰──────────────────────────────────────────────────────────────────────────────╯\n\noumi judge model#\n\noumi judge model Usage: oumi judge model [OPTIONS]Judge the outputs of a model on a dataset.╭─ Options ────────────────────────────────────────────────────────────────────╮│*--config-cTEXT Path to the judge ││config file ││[default: None] ││[required] ││*--inference-configTEXT Path to the inference││config file ││[default: None] ││[required] ││--input-fileTEXT Path to the input ││file (jsonl) ││[default: None] ││--output-fileTEXT Path to the output ││file (jsonl) ││[default: None] ││--log-level-log[DEBUG|INFO|WARNING|The logging level for││ERROR|CRITICAL]the specified ││command. ││--helpShow this message and││exit. │╰──────────────────────────────────────────────────────────────────────────────╯\n\nLaunch#\n\nFor a detailed guide on launching jobs, see Running Jobs on Clusters.\n\noumi launch#\n\noumi launch Usage: oumi launch [OPTIONS] COMMAND [ARGS]...Launch jobs remotely.╭─ Options ────────────────────────────────────────────────────────────────────╮│--helpShow this message and exit.│╰──────────────────────────────────────────────────────────────────────────────╯╭─ Commands ───────────────────────────────────────────────────────────────────╮│cancel Cancels a job. ││down Turns down a cluster. ││run Runs a job. ││status Prints the status of jobs launched from Oumi. ││stop Stops a cluster. ││up Launches a job. ││which Prints the available clouds. │╰──────────────────────────────────────────────────────────────────────────────╯\n\noumi launch cancel#\n\noumi launch cancel Usage: oumi launch cancel [OPTIONS]Cancels a job.╭─ Options ────────────────────────────────────────────────────────────────────╮│*--cloudTEXT Filter results by this ││cloud. ││[default: None] ││[required] ││*--clusterTEXT Filter results by ││clusters matching this ││name. ││[default: None] ││[required] ││*--idTEXT Filter results by jobs ││matching this job ID. ││[default: None] ││[required] ││--log-level-log[DEBUG|INFO|WARNING|ERRThe logging level for the││OR|CRITICAL]specified command. ││--helpShow this message and ││exit. │╰──────────────────────────────────────────────────────────────────────────────╯\n\noumi launch down#\n\noumi launch down Usage: oumi launch down [OPTIONS]Turns down a cluster.╭─ Options ────────────────────────────────────────────────────────────────────╮│*--clusterTEXT The cluster to turn down.││[default: None] ││[required] ││--cloudTEXT If specified, only ││clusters on this cloud ││will be affected. ││[default: None] ││--log-level-log[DEBUG|INFO|WARNING|ERRThe logging level for the││OR|CRITICAL]specified command. ││--helpShow this message and ││exit. │╰──────────────────────────────────────────────────────────────────────────────╯\n\noumi launch run#\n\noumi launch run Usage: oumi launch run [OPTIONS]Runs a job.╭─ Options ────────────────────────────────────────────────────────────────────╮│*--config-cTEXT Path to the ││configuration file ││for the job. ││[default: None] ││[required] ││--clusterTEXT The cluster to use ││for this job. If ││unspecified, a new ││cluster will be ││created. ││[default: None] ││--detach--no-detachRun the job in the ││background. ││[default: ││no-detach] ││--log-level-log[DEBUG|INFO|WARNINThe logging level ││G|ERROR|CRITICAL]for the specified ││command. ││--helpShow this message ││and exit. │╰──────────────────────────────────────────────────────────────────────────────╯\n\noumi launch status#\n\noumi launch status Usage: oumi launch status [OPTIONS]Prints the status of jobs launched from Oumi.╭─ Options ────────────────────────────────────────────────────────────────────╮│--cloudTEXT Filter results by this ││cloud. ││[default: None] ││--clusterTEXT Filter results by clusters││matching this name. ││[default: None] ││--idTEXT Filter results by jobs ││matching this job ID. ││[default: None] ││--log-level-log[DEBUG|INFO|WARNING|ERRORThe logging level for the ││|CRITICAL]specified command. ││--helpShow this message and ││exit. │╰──────────────────────────────────────────────────────────────────────────────╯\n\noumi launch stop#\n\noumi launch stop Usage: oumi launch stop [OPTIONS]Stops a cluster.╭─ Options ────────────────────────────────────────────────────────────────────╮│*--clusterTEXT The cluster to stop.││[default: None] ││[required] ││--cloudTEXT If specified, only ││clusters on this cloud ││will be affected. ││[default: None] ││--log-level-log[DEBUG|INFO|WARNING|ERRThe logging level for the││OR|CRITICAL]specified command. ││--helpShow this message and ││exit. │╰──────────────────────────────────────────────────────────────────────────────╯\n\noumi launch up#\n\noumi launch up Usage: oumi launch up [OPTIONS]Launches a job.╭─ Options ────────────────────────────────────────────────────────────────────╮│*--config-cTEXT Path to the ││configuration file ││for the job. ││[default: None] ││[required] ││--clusterTEXT The cluster to use ││for this job. If ││unspecified, a new ││cluster will be ││created. ││[default: None] ││--detach--no-detachRun the job in the ││background. ││[default: ││no-detach] ││--log-level-log[DEBUG|INFO|WARNINThe logging level ││G|ERROR|CRITICAL]for the specified ││command. ││--helpShow this message ││and exit. │╰──────────────────────────────────────────────────────────────────────────────╯\n\noumi launch which#\n\noumi launch which Usage: oumi launch which [OPTIONS]Prints the available clouds.╭─ Options ────────────────────────────────────────────────────────────────────╮│--log-level-log[DEBUG|INFO|WARNING|ERRORThe logging level for the ││|CRITICAL]specified command. ││--helpShow this message and ││exit. │╰──────────────────────────────────────────────────────────────────────────────╯\n\nDistributed#\n\nFor a detailed guide on distributed training, see Training.\n\noumi distributed#\n\noumi distributed Usage: oumi distributed [OPTIONS] COMMAND [ARGS]...A wrapper for torchrun/accelerate with reasonable default values for distributed training.╭─ Options ────────────────────────────────────────────────────────────────────╮│--helpShow this message and exit.│╰──────────────────────────────────────────────────────────────────────────────╯╭─ Commands ───────────────────────────────────────────────────────────────────╮│accelerate Starts `accelerate` sub-process w/ automatically configured ││common params. ││torchrun Starts `torchrun` sub-process w/ automatically configured ││common params. │╰──────────────────────────────────────────────────────────────────────────────╯\n\noumi distributed accelerate#\n\noumi distributed accelerate Usage: oumi distributed accelerate [OPTIONS]Starts `accelerate` sub-process w/ automatically configured common params.Args: ctx: The Typer context object. level: The logging level for the specified command.╭─ Options ────────────────────────────────────────────────────────────────────╮│--log-level-log[DEBUG|INFO|WARNING|ERRORThe logging level for the ││|CRITICAL]specified command. ││--helpShow this message and ││exit. │╰──────────────────────────────────────────────────────────────────────────────╯\n\noumi distributed torchrun#\n\noumi distributed torchrun Usage: oumi distributed torchrun [OPTIONS]Starts `torchrun` sub-process w/ automatically configured common params.Args: ctx: The Typer context object. level: The logging level for the specified command.╭─ Options ────────────────────────────────────────────────────────────────────╮│--log-level-log[DEBUG|INFO|WARNING|ERRORThe logging level for the ││|CRITICAL]specified command. ││--helpShow this message and ││exit. │╰──────────────────────────────────────────────────────────────────────────────╯\n\nEnvironment#\n\nThis command is a great tool for debugging!\n\noumi env will list relevant details of your environment setup, including python version, package versions, and Oumi environment variables.\n\noumi env#\n\noumi env Usage: oumi env [OPTIONS]Prints information about the current environment.╭─ Options ────────────────────────────────────────────────────────────────────╮│--helpShow this message and exit.│╰──────────────────────────────────────────────────────────────────────────────╯",
      "# [oumi — Oumi](https://oumi.ai/docs/en/latest/api/oumi.html)\nA list of evaluation results (one for each task). Each evaluation result is a dictionary of metric names and their corresponding values.\n\nThis is a utility method for running evaluations iteratively over a series of checkpoints. This method can be run in parallel with a training job to compute metrics per checkpoint without wasting valuable time in the main training loop.",
      "# [Customizing Oumi — Oumi](https://oumi.ai/docs/en/latest/user_guides/customization.html)\nCustomizing Oumi#\n\nOften times when using a new framework, you may find that something you’d like to use is missing. We always welcome contributions, but we also understand that sometimes it’s simpler to prototype changes locally. Whether you want to quickly experiment with new features, test out different implementation approaches, or iterate rapidly on your ideas without impacting the main codebase, Oumi provides a simple way to support local customizations without any additional installations.\n\nThe Oumi Registry#\n\nWe support customization via the oumi.core.registry.Registry.\n\nYou can easily register classes that are then loaded as if they’re a native part of the Oumi framework.\n\nSee the diagram below for how we load your custom code:\n\n%%{init: {'theme': 'base', 'themeVariables': { 'background': '#f5f5f5'}}}%% graph LR %% Oumi Framework FR[Oumi] --> |Read OUMI_EXTRA_DEPS_FILE| RQ[requirements.txt] %% Load Custom Files RQ --> |Import File| CF1[Custom Class 1 File] RQ --> |Import File| CF2[Custom Class 2 File] RQ --> |Import File| CF3[ ... ] %% Style for core workflow style FR fill:#1565c0,color:#ffffff style RQ fill:#1565c0,color:#ffffff style CF1 fill:#1565c0,color:#ffffff style CF1 fill:#1565c0,color:#ffffff style CF2 fill:#1565c0,color:#ffffff style CF3 fill:#1565c0,color:#ffffff\n\nYou can register your custom code in two simple steps:\n\nWriting a custom Model, Dataset, Cloud, etc\n\nCreating a requirements.txt file so your code is available via the CLI\n\nWriting Custom Classes#\n\nCustom Models#\n\nYou can easily customize models with oumi if something isn’t supported out of the box. We often hear requests for custom loss and custom model architectures: both are simple to implement via a custom model.\n\nCheck out our guide for an in-depth explanation: Custom Models\n\nNote\n\nDon’t forget to decorate your class with\n\n@registry.register(..., registry.RegistryType.MODEL)!\n\nCustom Datasets#\n\nCustom datasets are a great way to handle unique dataset formats that Oumi may not yet support.\n\nSee the following snippets for examples of custom datasets:\n\nCustom SFT Dataset\n\nCustom Pre-training Dataset\n\nCustom Preference Tuning Dataset\n\nCustom Vision-Language Dataset\n\nCustom Numpy Dataset\n\nNote\n\nDon’t forget to decorate your class with @register_dataset(...)!\n\nCustom Clouds/Clusters#\n\nAdding a custom cloud is perfect for handling local clusters not hosted by major cloud providers.\n\nFor example, we wrote a custom cloud for the Polaris platform. Our research team used this cloud to schedule jobs seamlessly on a remote super computer.\n\nTake a look at our custom cluster tutorial here.\n\nNote\n\nDon’t forget to decorate your class with @register_cloud_builder(...)!\n\nCustom Judge Configs#\n\nFor quick reference, you can register custom judge configs\n\nYou can find examples of custom judge configs here.\n\nNote\n\nDon’t forget to decorate your function with @register_judge(...)!\n\nEnable Your Classes for the CLI#\n\nIf you’re using Oumi as a python library, your custom classes will work out of the box! However, to use your custom classes from the Oumi CLI, you need to tell Oumi which files to load when initializing our Registry.\n\nTo do this, you must first create a requirements.txt file. This file has a simple structure: each line must be an absolute filepath to the file with your custom class / function (that you specified with the @register... decorator).\n\nFor example, if you created two custom classes in files /path/to/custom_cloud.py and /another/path/to/custom_model.py, your requirements.txt files should look like:\n\n/path/to/custom_cloud.py /another/path/to/custom_model.py\n\nWith your requirements.txt file created, you simply need to set the OUMI_EXTRA_DEPS_FILE environment variable to the location of your file, and you’re good to go!\n\nexportOUMI_EXTRA_DEPS_FILE=/another/path/requirements.txt",
      "# [Core Concepts — Oumi](https://oumi.ai/docs/en/latest/get_started/core_concepts.html)\nOumi CLI#\n\nThe CLI is the entry point for all Oumi commands.\n\noumi <command>[options]\n\nFor detailed help on any command, you can use the --help option:\n\noumi --help# for general help oumi <command> --help# for command-specific help\n\nThe available commands are:\n\nCommand\n\nPurpose\n\nAny Oumi command which takes a config path as an argument (train, evaluate, infer, etc.) can override parameters from the command line. For example:\n\noumi train -c configs/recipes/smollm/sft/135m/quickstart_train.yaml\\ --training.max_steps20\\ --training.learning_rate 1e-4\\ --data.train.datasets[0].shuffletrue\\ --training.output_dir output/smollm-135m-sft\n\nSee CLI Reference for full CLI details, including more details about CLI overrides.\n\nPython API#\n\nThe Python API allows you to use Oumi to train, evaluate, infer, judge, and more. You can use it in a notebook, a script, or any custom workflow.\n\nFor example, to train a model, you can use the train function:\n\nfromoumi.trainimport train fromoumi.core.configsimport TrainingConfig config = TrainingConfig.from_yaml(\"path/to/config.yaml\") train(config)\n\nSee oumi for full API details.",
      "# [Quickstart — Oumi](https://oumi.ai/docs/en/latest/get_started/quickstart.html)\nLaunching your first cloud job with Oumi#\n\nOnce the one-time setup is out of the way, launching a new cloud job with Oumi is very simple.\n\nconfigs/recipes/smollm/sft/135m/quickstart_gcp_job.yaml\n\n# Job config to tune smollm 135M on 1 GCP node. # # Usage: # oumi launch up -c configs/recipes/smollm/sft/135m/quickstart_gcp_job.yaml --cluster smollm-135m-fft # # See Also: # - Documentation: https://oumi.ai/docs/en/latest/user_guides/launch/launch.html # - Config class: oumi.core.configs.JobConfig # - Config source: https://github.com/oumi-ai/oumi/blob/main/src/oumi/core/configs/job_config.py # - Other job configs: configs/**/*job.yaml name:smollm-135m-sft resources: cloud:gcp accelerators:\"A100:1\" use_spot:false disk_size:100# Disk size in GBs working_dir:. envs: OUMI_RUN_NAME:smollm135m.train # https://github.com/huggingface/tokenizers/issues/899#issuecomment-1027739758 TOKENIZERS_PARALLELISM:false setup:| set -e pip install uv && uv pip install oumi[gpu] run:| set -e # Exit if any command failed. source ./configs/examples/misc/sky_init.sh set -x oumi train -c configs/recipes/smollm/sft/135m/quickstart_train.yaml echo \"Training complete!\"\n\nTo launch an evaluation job:\n\nconfigs/recipes/smollm/evaluation/135m/quickstart_gcp_job.yaml\n\n# Job config to evaluate smollm 135M on 1 GCP node. # # Usage: # oumi launch up -c configs/recipes/smollm/evaluation/135m/quickstart_gcp_job.yaml --cluster smollm-135m-eval # # See Also: # - Documentation: https://oumi.ai/docs/en/latest/user_guides/launch/launch.html # - Config class: oumi.core.configs.JobConfig # - Config source: https://github.com/oumi-ai/oumi/blob/main/src/oumi/core/configs/job_config.py # - Other job configs: configs/**/*job.yaml name:smollm-135m-eval resources: cloud:gcp accelerators:\"A100:1\" use_spot:false disk_size:100# Disk size in GBs working_dir:. envs: OUMI_RUN_NAME:smollm135m.eval # https://github.com/huggingface/tokenizers/issues/899#issuecomment-1027739758 TOKENIZERS_PARALLELISM:false setup:| set -e pip install uv && uv pip install oumi[gpu,evaluation] run:| set -e # Exit if any command failed. source ./configs/examples/misc/sky_init.sh set -x oumi evaluate -c configs/recipes/smollm/evaluation/135m/quickstart_eval.yaml echo \"Evaluation complete!\"\n\nAfter you run one of the above commands, you should see some console output from Oumi which describes how your job is being provisioned and how the cloud installation is proceeding. In particular, your cluster will be assigned a semi-random name such as sky-7fdd-ab183, which you should take note of.\n\nAfter 15 minutes or so, Oumi should tell you that the run is complete.\n\nIf you want to see the logs from your cloud run, you can pull them down to your local machine –\n\nCloud services can be expensive! Please keep an eye on your costs, and don’t forget to tear down your cluster when you’re done with this tutorial.",
      "# [Recipes — Oumi](https://oumi.ai/docs/en/latest/resources/recipes.html)\nRecipes#\n\nTo help you get started with Oumi, we’ve prepared a set of recipes for common use cases. These recipes are designed to be easy to understand and modify, and should be a good starting point for your own projects. Each recipe is a YAML file that can be used to train, evaluate, or deploy a model. We also have corresponding job configs for most recipes that let you run the job remotely; they’re usually files ending in _job.yaml in the same directory as the recipe config.\n\nOverview#\n\nThe recipes are organized by model family and task type. Each recipe includes:\n\nConfiguration files for different tasks (training, evaluation, inference)\n\nPlatform-specific job configurations (Cloud (e.g. GCP), Polaris, or local)\n\nMultiple training methods (FFT, LoRA, QLoRA, FSDP/DDP)\n\nTo use a recipe, simply download the desired configuration file, modify any parameters as needed, and run the configuration using the Oumi CLI. For example:\n\noumi train --config path/to/config.yaml oumi evaluate --config path/to/config.yaml oumi infer --config path/to/config.yaml\n\nYou can also check out the README.md in each recipe’s directory for more details and examples. You can easily adapt these recipes to use with other supported models, datasets, and cloud providers.\n\nCommon Models#\n\n🐋 DeepSeek R1 Family#\n\nModel\n\nConfiguration\n\nLinks\n\nDeepSeek R1 671B\n\nrecipes/deepseek_r1/inference/671b_together_infer.yaml\n\nDownload oumi-ai/oumi\n\nDistilled Llama 8B\n\nrecipes/deepseek_r1/sft/distill_llama_8b/full_train.yaml\n\nDownload oumi-ai/oumi\n\nrecipes/deepseek_r1/sft/distill_llama_8b/lora_train.yaml\n\nDownload oumi-ai/oumi\n\nrecipes/deepseek_r1/sft/distill_llama_8b/qlora_train.yaml\n\nDownload oumi-ai/oumi\n\nrecipes/deepseek_r1/evaluation/distill_llama_8b/eval.yaml\n\nDownload oumi-ai/oumi\n\nrecipes/deepseek_r1/inference/distill_llama_8b_infer.yaml\n\nDownload oumi-ai/oumi\n\nDistilled Llama 70B\n\nrecipes/deepseek_r1/sft/distill_llama_70b/full_train.yaml\n\nDownload oumi-ai/oumi\n\nrecipes/deepseek_r1/sft/distill_llama_70b/lora_train.yaml\n\nDownload oumi-ai/oumi\n\nrecipes/deepseek_r1/sft/distill_llama_70b/qlora_train.yaml\n\nDownload oumi-ai/oumi\n\nrecipes/deepseek_r1/evaluation/distill_llama_70b/eval.yaml\n\nDownload oumi-ai/oumi\n\nrecipes/deepseek_r1/inference/distill_llama_70b_infer.yaml\n\nDownload oumi-ai/oumi\n\nDistilled Qwen 1.5B\n\nrecipes/deepseek_r1/sft/distill_qwen_1_5b/full_train.yaml\n\nDownload oumi-ai/oumi\n\nrecipes/deepseek_r1/sft/distill_qwen_1_5b/lora_train.yaml\n\nDownload oumi-ai/oumi\n\nrecipes/deepseek_r1/evaluation/distill_qwen_1_5b/eval.yaml\n\nDownload oumi-ai/oumi\n\nrecipes/deepseek_r1/inference/distill_qwen_1_5b_infer.yaml\n\nDownload oumi-ai/oumi\n\nDistilled Qwen 32B\n\nrecipes/deepseek_r1/sft/distill_qwen_32b/lora_train.yaml\n\nDownload oumi-ai/oumi\n\nrecipes/deepseek_r1/evaluation/distill_qwen_32b/eval.yaml\n\nDownload oumi-ai/oumi\n\nrecipes/deepseek_r1/inference/distill_qwen_32b_infer.yaml\n\nDownload oumi-ai/oumi\n\n🦙 Llama Family#\n\nModel\n\nConfiguration\n\nLinks\n\nLlama 3.1 8B\n\nrecipes/llama3_1/sft/8b_full/train.yaml\n\nDownload oumi-ai/oumi\n\nrecipes/llama3_1/sft/8b_lora/train.yaml\n\nDownload oumi-ai/oumi\n\nrecipes/llama3_1/sft/8b_qlora/train.yaml\n\nDownload oumi-ai/oumi\n\nrecipes/llama3_1/pretraining/8b/train.yaml\n\nDownload oumi-ai/oumi\n\nrecipes/llama3_1/evaluation/8b_eval.yaml\n\nDownload oumi-ai/oumi\n\nrecipes/llama3_1/inference/8b_infer.yaml\n\nDownload oumi-ai/oumi\n\nLlama 3.3 70B\n\nrecipes/llama3_3/sft/70b_full/train.yaml\n\nDownload oumi-ai/oumi\n\nrecipes/llama3_3/sft/70b_lora/train.yaml\n\nDownload oumi-ai/oumi\n\nrecipes/llama3_3/sft/70b_qlora/train.yaml\n\nDownload oumi-ai/oumi\n\nrecipes/llama3_3/evaluation/70b_eval.yaml\n\nDownload oumi-ai/oumi\n\nrecipes/llama3_3/inference/70b_infer.yaml\n\nDownload oumi-ai/oumi\n\nLlama 3.1 405B\n\nrecipes/llama3_1/sft/405b_full/train.yaml\n\nDownload oumi-ai/oumi\n\nrecipes/llama3_1/sft/405b_lora/train.yaml\n\nDownload oumi-ai/oumi\n\nrecipes/llama3_1/sft/405b_qlora/train.yaml\n\nDownload oumi-ai/oumi\n\nLlama 3.2 1B\n\nrecipes/llama3_2/sft/1b_full/train.yaml\n\nDownload oumi-ai/oumi\n\nrecipes/llama3_2/evaluation/1b_eval.yaml\n\nDownload oumi-ai/oumi\n\nrecipes/llama3_2/inference/1b_infer.yaml\n\nDownload oumi-ai/oumi\n\nLlama 3.2 3B\n\nrecipes/llama3_2/sft/3b_full/train.yaml\n\nDownload oumi-ai/oumi\n\nrecipes/llama3_2/sft/3b_lora/train.yaml\n\nDownload oumi-ai/oumi\n\nrecipes/llama3_2/sft/3b_qlora/train.yaml\n\nDownload oumi-ai/oumi\n\nrecipes/llama3_2/evaluation/3b_eval.yaml\n\nDownload oumi-ai/oumi\n\nrecipes/llama3_2/inference/3b_infer.yaml\n\nDownload oumi-ai/oumi\n\n🎨 Vision Models#\n\nModel\n\nConfiguration\n\nLinks\n\nLlama 3.2 Vision 11B\n\nrecipes/vision/llama3_2_vision/sft/11b_full/train.yaml\n\nDownload oumi-ai/oumi\n\nrecipes/vision/llama3_2_vision/evaluation/11b_eval.yaml\n\nDownload oumi-ai/oumi\n\nrecipes/vision/llama3_2_vision/inference/11b_infer.yaml\n\nDownload oumi-ai/oumi\n\nrecipes/vision/llama3_2_vision/inference/11b_sglang_infer.yaml\n\nDownload oumi-ai/oumi\n\nrecipes/vision/llama3_2_vision/inference/11b_rvllm_infer.yaml\n\nDownload oumi-ai/oumi\n\nLLaVA 7B\n\nrecipes/vision/llava_7b/sft/train.yaml\n\nDownload oumi-ai/oumi\n\nrecipes/vision/llava_7b/inference/infer.yaml\n\nDownload oumi-ai/oumi\n\nrecipes/vision/llava_7b/inference/vllm_infer.yaml\n\nDownload oumi-ai/oumi\n\nPhi3 Vision\n\nrecipes/vision/phi3/sft/train.yaml\n\nDownload oumi-ai/oumi\n\nrecipes/vision/phi3/inference/vllm_infer.yaml\n\nDownload oumi-ai/oumi\n\nQwen2-VL 2B\n\nrecipes/vision/qwen2_vl_2b/sft/train.yaml\n\nDownload oumi-ai/oumi\n\nrecipes/vision/qwen2_vl_2b/inference/infer.yaml\n\nDownload oumi-ai/oumi\n\nrecipes/vision/qwen2_vl_2b/inference/sglang_infer.yaml\n\nDownload oumi-ai/oumi\n\nrecipes/vision/qwen2_vl_2b/inference/vllm_infer.yaml\n\nDownload oumi-ai/oumi\n\nSmolVLM\n\nrecipes/vision/smolvlm/sft/train.yaml\n\nDownload oumi-ai/oumi\n\n🎯 Training Techniques#\n\nThis section lists an example config for various training techniques supported by Oumi.\n\nTechnique\n\nConfiguration\n\nLinks\n\nFSDP\n\nrecipes/llama3_1/sft/8b_lora/fsdp_train.yaml\n\nDownload oumi-ai/oumi\n\nLong-context training\n\nrecipes/llama3_1/sft/8b_full/longctx_train.yaml\n\nDownload oumi-ai/oumi\n\nDPO\n\nrecipes/phi3/dpo/train.yaml\n\nDownload oumi-ai/oumi\n\nDDP Pretraining\n\nexamples/fineweb_ablation_pretraining/ddp/train.yaml\n\nDownload oumi-ai/oumi\n\nFSDP Pretraining\n\nexamples/fineweb_ablation_pretraining/fsdp/train.yaml\n\nDownload oumi-ai/oumi\n\n🚀 Inference#\n\nModel\n\nConfiguration\n\nLinks\n\nDeepSeek R1 671B\n\nrecipes/deepseek_r1/inference/671b_together_infer.yaml\n\nDownload oumi-ai/oumi\n\nDeepSeek R1 Distill Llama 8B\n\nrecipes/deepseek_r1/inference/distill_llama_8b_infer.yaml\n\nDownload oumi-ai/oumi\n\nDeepSeek R1 Distill Llama 70B\n\nrecipes/deepseek_r1/inference/distill_llama_70b_infer.yaml\n\nDownload oumi-ai/oumi\n\nDeepSeek R1 Distill Qwen 1.5B\n\nrecipes/deepseek_r1/inference/distill_qwen_1_5b_infer.yaml\n\nDownload oumi-ai/oumi\n\nDeepSeek R1 Distill Qwen 32B\n\nrecipes/deepseek_r1/inference/distill_qwen_32b_infer.yaml\n\nDownload oumi-ai/oumi\n\nLlama 3.1 8B\n\nrecipes/llama3_1/inference/8b_infer.yaml\n\nDownload oumi-ai/oumi\n\nrecipes/llama3_1/inference/8b_sglang_infer.yaml\n\nDownload oumi-ai/oumi\n\nrecipes/llama3_1/inference/8b_rvllm_infer.yaml\n\nDownload oumi-ai/oumi\n\nLlama 3.1 70B\n\nrecipes/llama3_1/inference/70b_infer.yaml\n\nDownload oumi-ai/oumi\n\nLlama 3.2 1B\n\nrecipes/llama3_2/inference/1b_infer.yaml\n\nDownload oumi-ai/oumi\n\nrecipes/llama3_2/inference/1b_sglang_infer.yaml\n\nDownload oumi-ai/oumi\n\nrecipes/llama3_2/inference/1b_vllm_infer.yaml\n\nDownload oumi-ai/oumi\n\nLlama 3.2 3B\n\nrecipes/llama3_2/inference/3b_infer.yaml\n\nDownload oumi-ai/oumi\n\nrecipes/llama3_2/inference/3b_sglang_infer.yaml\n\nDownload oumi-ai/oumi\n\nrecipes/llama3_2/inference/3b_vllm_infer.yaml\n\nDownload oumi-ai/oumi\n\nLlama 3.2 Vision 11B\n\nrecipes/vision/llama3_2_vision/inference/11b_infer.yaml\n\nDownload oumi-ai/oumi\n\nrecipes/vision/llama3_2_vision/inference/11b_sglang_infer.yaml\n\nDownload oumi-ai/oumi\n\nrecipes/vision/llama3_2_vision/inference/11b_rvllm_infer.yaml\n\nDownload oumi-ai/oumi\n\nGPT-2\n\nrecipes/gpt2/inference/infer.yaml\n\nDownload oumi-ai/oumi",
      "# [Tutorials — Oumi](https://oumi.ai/docs/en/latest/get_started/tutorials.html)\nTutorials#\n\nExplore the growing collection of ready-to-use configurations for state-of-the-art models and training workflows:",
      "# [Style Guide — Oumi](https://oumi.ai/docs/en/latest/development/style_guide.html)\nStyle Guide#\n\nOumi follows Google’s Python Style Guide for how to format and structure code.\n\nOpt for using descriptive-style verbs (ex. “Builds”) over imperative-style (ex. “Build”) for docstrings; see style guide section 3.8.3 for more details.\n\nPre-Commit Hooks#\n\nOumi uses Pre Commit to enforce style checks. To configure, run either make setup, or:\n\npip install'.[dev]' pre-commit install\n\nThe pre-commit hooks will now be run before each commit. You can also run the hooks manually via:\n\npre-commit run# run all hooks on changed files pre-commit run --all-files# or, run all hooks on all files\n\nCode Formatting#\n\nOumi uses the ruff formatter for code formatting. These checks run through pre-commit (see section 1.2). These checks can also be run manually via:\n\npre-commit run ruff --all-files\n\nThe configuration is stored in pyproject.toml and .pre-commit-config.yaml.",
      "# [Updating the Documentation — Oumi](https://oumi.ai/docs/en/latest/development/docs_guide.html)\nFirst Time Setup#\n\nThe following instructions only need to be done ONCE:\n\nBefore building our docs, you need to clone our local fork of sphinx-term and install it in the same Conda env you use for Oumi development:\n\nIf you haven’t already installed our “[docs]” package for Oumi, make sure you do that as well.\n\nThe first time you run the docs locally, you need to run the following command to build the docs:",
      "# [Out of Memory (OOM) — Oumi](https://oumi.ai/docs/en/latest/faq/oom.html)\nIntroduction#\n\nOut of Memory (OOM) errors are a common challenge when working with large language models and datasets.\n\nIn this guide, we will discuss a few strategies to reduce GPU memory requirements.\n\nBest Practices\n\nAlways monitor memory usage and performance metrics when applying these optimizations, using nvidia-smi and Oumi’s telemetry output.\n\nCombine multiple techniques for best results, but introduce changes gradually to isolate their effects.\n\nSome techniques may trade off speed and model accuracy for memory efficiency. Choose the right balance for your specific use case.\n\nDistributed Training with FSDP#\n\nIf you have access to multiple GPUs, you can leverage FSDP to distribute the training process across multiple GPUs. To run FSDP jobs, make sure to invoke your training job with torchrun to run on multiple GPUs/nodes. We also provide the oumi distributed wrapper to automatically try to set the flags needed for torchrun. For example, you can simply run oumi distributed torchrun -m oumi train -c path/to/train.yaml.\n\nEnable distributed training:\n\nPYTHON\n\nfromoumi.core.configsimport FSDPParams fromoumi.core.configs.params.fsdp_paramsimport ShardingStrategy config = TrainingConfig( fsdp=FSDPParams( enable_fsdp=True, sharding_strategy=ShardingStrategy.FULL_SHARD, ), )\n\nYAML\n\nfsdp: enable_fsdp:true sharding_strategy:FULL_SHARD\n\nEnable CPU offloading:\n\nPYTHON\n\nconfig = TrainingConfig( fsdp=FSDPParams( enable_fsdp=True, cpu_offload=True, ), )\n\nYAML\n\nfsdp: enable_fsdp:true cpu_offload:true\n\nDisable Forward Prefetch:\n\nPYTHON\n\nconfig = TrainingConfig( fsdp=FSDPParams( enable_fsdp=True, forward_prefetch=False, ), )\n\nYAML\n\nfsdp: enable_fsdp:true forward_prefetch:false\n\nDisable Backward Prefetch:\n\nPYTHON\n\nconfig = TrainingConfig( fsdp=FSDPParams( enable_fsdp=True, backward_prefetch=BackwardPrefetch.NO_PREFETCH, ), )\n\nYAML\n\nfsdp: enable_fsdp:true backward_prefetch:NO_PREFETCH\n\nAttention\n\nDisabling FSDP’s forward and backward prefetch can lead to significant slower training times, use with caution.",
      "# [Citations — Oumi](https://oumi.ai/docs/en/latest/about/citations.html)\n@software{oumi2025, author={Oumi Community}, title={Oumi: an Open, End-to-end Platform for Building Large Foundation Models}, month={January}, year={2025}, url={https://github.com/oumi-ai/oumi} }\n\n[BBG22]\n\nStella Biderman, Kieran Bicheno, and Leo Gao. Datasheet for the pile. arXiv preprint arXiv:2201.07311, 2022.\n\n[Com23]\n\nTogether Computer. Redpajama: an open source recipe to reproduce llama training dataset. April 2023. URL: togethercomputer/RedPajama-Data.\n\n[GBB+20]\n\nLeo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, and others. The pile: an 800GB dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027, 2020.\n\n[LBAvWW24]\n\nAnton Lozhkov, Loubna Ben Allal, Leandro von Werra, and Thomas Wolf. Fineweb-edu. May 2024. URL: https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu, doi:10.57967/hf/2497.\n\n[MXBS16]\n\nStephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models. 2016. arXiv:1609.07843.\n\n[SAKM+23]\n\nDaria Soboleva, Faisal Al-Khateeb, Robert Myers, Jacob R Steeves, Joel Hestness, and Nolan Dey. SlimPajama: A 627B token cleaned and deduplicated version of RedPajama. https://cerebras.ai/blog/slimpajama-a-627b-token-cleaned-and-deduplicated-version-of-redpajama, June 2023. URL: https://huggingface.co/datasets/cerebras/SlimPajama-627B.\n\n[SKB+24]\n\nLuca Soldaini, Rodney Kinney, Akshita Bhagia, Dustin Schwenk, David Atkinson, Russell Authur, Ben Bogin, Khyathi Chandu, Jennifer Dumas, Yanai Elazar, Valentin Hofmann, Ananya Harsh Jha, Sachin Kumar, Li Lucy, Xinxi Lyu, Nathan Lambert, Ian Magnusson, Jacob Morrison, Niklas Muennighoff, Aakanksha Naik, Crystal Nam, Matthew E. Peters, Abhilasha Ravichander, Kyle Richardson, Zejiang Shen, Emma Strubell, Nishant Subramani, Oyvind Tafjord, Pete Walsh, Luke Zettlemoyer, Noah A. Smith, Hannaneh Hajishirzi, Iz Beltagy, Dirk Groeneveld, Jesse Dodge, and Kyle Lo. Dolma: an Open Corpus of Three Trillion Tokens for Language Model Pretraining Research. arXiv preprint, 2024.",
      "# [License — Oumi](https://oumi.ai/docs/en/latest/about/license.html)\nDefinitions.\n\n“License” shall mean the terms and conditions for use, reproduction, and distribution as defined by Sections 1 through 9 of this document.\n\n“Licensor” shall mean the copyright owner or entity authorized by the copyright owner that is granting the License.\n\n“Legal Entity” shall mean the union of the acting entity and all other entities that control, are controlled by, or are under common control with that entity. For the purposes of this definition, “control” means (i) the power, direct or indirect, to cause the direction or management of such entity, whether by contract or otherwise, or (ii) ownership of fifty percent (50%) or more of the outstanding shares, or (iii) beneficial ownership of such entity.\n\n“You” (or “Your”) shall mean an individual or Legal Entity exercising permissions granted by this License.\n\n“Source” form shall mean the preferred form for making modifications, including but not limited to software source code, documentation source, and configuration files.\n\n“Object” form shall mean any form resulting from mechanical transformation or translation of a Source form, including but not limited to compiled object code, generated documentation, and conversions to other media types.\n\n“Work” shall mean the work of authorship, whether in Source or Object form, made available under the License, as indicated by a copyright notice that is included in or attached to the work (an example is provided in the Appendix below).\n\n“Derivative Works” shall mean any work, whether in Source or Object form, that is based on (or derived from) the Work and for which the editorial revisions, annotations, elaborations, or other modifications represent, as a whole, an original work of authorship. For the purposes of this License, Derivative Works shall not include works that remain separable from, or merely link (or bind by name) to the interfaces of, the Work and Derivative Works thereof.\n\n“Contribution” shall mean any work of authorship, including the original version of the Work and any modifications or additions to that Work or Derivative Works thereof, that is intentionally submitted to Licensor for inclusion in the Work by the copyright owner or by an individual or Legal Entity authorized to submit on behalf of the copyright owner. For the purposes of this definition, “submitted” means any form of electronic, verbal, or written communication sent to the Licensor or its representatives, including but not limited to communication on electronic mailing lists, source code control systems, and issue tracking systems that are managed by, or on behalf of, the Licensor for the purpose of discussing and improving the Work, but excluding communication that is conspicuously marked or otherwise designated in writing by the copyright owner as “Not a Contribution.”\n\n“Contributor” shall mean Licensor and any individual or Legal Entity on behalf of whom a Contribution has been received by Licensor and subsequently incorporated within the Work.\n\nGrant of Copyright License. Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable copyright license to reproduce, prepare Derivative Works of, publicly display, publicly perform, sublicense, and distribute the Work and such Derivative Works in Source or Object form.\n\nGrant of Patent License. Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable (except as stated in this section) patent license to make, have made, use, offer to sell, sell, import, and otherwise transfer the Work, where such license applies only to those patent claims licensable by such Contributor that are necessarily infringed by their Contribution(s) alone or by combination of their Contribution(s) with the Work to which such Contribution(s) was submitted. If You institute patent litigation against any entity (including a cross-claim or counterclaim in a lawsuit) alleging that the Work or a Contribution incorporated within the Work constitutes direct or contributory patent infringement, then any patent licenses granted to You under this License for that Work shall terminate as of the date such litigation is filed.\n\nRedistribution. You may reproduce and distribute copies of the Work or Derivative Works thereof in any medium, with or without modifications, and in Source or Object form, provided that You meet the following conditions:\n\n(a) You must give any other recipients of the Work or Derivative Works a copy of this License; and\n\n(b) You must cause any modified files to carry prominent notices stating that You changed the files; and\n\n(c) You must retain, in the Source form of any Derivative Works that You distribute, all copyright, patent, trademark, and attribution notices from the Source form of the Work, excluding those notices that do not pertain to any part of the Derivative Works; and\n\n(d) If the Work includes a “NOTICE” text file as part of its distribution, then any Derivative Works that You distribute must include a readable copy of the attribution notices contained within such NOTICE file, excluding those notices that do not pertain to any part of the Derivative Works, in at least one of the following places: within a NOTICE text file distributed as part of the Derivative Works; within the Source form or documentation, if provided along with the Derivative Works; or, within a display generated by the Derivative Works, if and wherever such third-party notices normally appear. The contents of the NOTICE file are for informational purposes only and do not modify the License. You may add Your own attribution notices within Derivative Works that You distribute, alongside or as an addendum to the NOTICE text from the Work, provided that such additional attribution notices cannot be construed as modifying the License.\n\nYou may add Your own copyright statement to Your modifications and may provide additional or different license terms and conditions for use, reproduction, or distribution of Your modifications, or for any such Derivative Works as a whole, provided Your use, reproduction, and distribution of the Work otherwise complies with the conditions stated in this License.\n\nSubmission of Contributions. Unless You explicitly state otherwise, any Contribution intentionally submitted for inclusion in the Work by You to the Licensor shall be under the terms and conditions of this License, without any additional terms or conditions. Notwithstanding the above, nothing herein shall supersede or modify the terms of any separate license agreement you may have executed with Licensor regarding such Contributions.\n\nTrademarks. This License does not grant permission to use the trade names, trademarks, service marks, or product names of the Licensor, except as required for reasonable and customary use in describing the origin of the Work and reproducing the content of the NOTICE file.\n\nDisclaimer of Warranty. Unless required by applicable law or agreed to in writing, Licensor provides the Work (and each Contributor provides its Contributions) on an “AS IS” BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied, including, without limitation, any warranties or conditions of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A PARTICULAR PURPOSE. You are solely responsible for determining the appropriateness of using or redistributing the Work and assume any risks associated with Your exercise of permissions under this License.\n\nLimitation of Liability. In no event and under no legal theory, whether in tort (including negligence), contract, or otherwise, unless required by applicable law (such as deliberate and grossly negligent acts) or agreed to in writing, shall any Contributor be liable to You for damages, including any direct, indirect, special, incidental, or consequential damages of any character arising as a result of this License or out of the use or inability to use the Work (including but not limited to damages for loss of goodwill, work stoppage, computer failure or malfunction, or any and all other commercial damages or losses), even if such Contributor has been advised of the possibility of such damages.\n\nAccepting Warranty or Additional Liability. While redistributing the Work or Derivative Works thereof, You may choose to offer, and charge a fee for, acceptance of support, warranty, indemnity, or other liability obligations and/or rights consistent with this License. However, in accepting such obligations, You may act only on Your own behalf and on Your sole responsibility, not on behalf of any other Contributor, and only if You agree to indemnify, defend, and hold each Contributor harmless for any liability incurred by, or claims asserted against, such Contributor by reason of your accepting any such warranty or additional liability.",
      "# [Acknowledgments — Oumi](https://oumi.ai/docs/en/latest/about/acknowledgements.html)\nAcknowledgments#\n\nOumi makes use of several libraries and tools from the open-source community. 🚀\n\nWe would like to acknowledge these projects:\n\nWe are grateful to the developers and maintainers of these projects for their valuable contributions to the open-source community. 🙏",
      "# [Contributing — Oumi](https://oumi.ai/docs/en/latest/development/contributing.html)\nContributing#\n\nOumi welcomes any contributions that help make it better for the community: this is a community-first effort. If we all work together, we can ensure a better, more inclusive, safer, and a totally open future for frontier AI. Whether you are an individual contributor or an organization, we invite you to be part of this bold mission to bring frontier AI back in the open. The future of AI is open source, and we can build that together.\n\nPossible contributions include:\n\nBug fixes, incremental improvements, and tests, no matter how small\n\nNew features and infrastructure improvements\n\nTuning datasets, new ones or existing ones, adapted to the standardized Oumi format\n\nBenchmarks, new or existing, integrated to Oumi’s evaluation library\n\nDocumentation and code readability improvements\n\nCode review of pull requests\n\nTutorials, blog posts, talks, and social media posts that promote Oumi\n\nCommunity participation in GitHub issues, Discord, and X, to share knowledge and help each other.\n\nIf you want to contribute but you are short of ideas or have any questions, reach out (contact@oumi.ai) and we can help.\n\n📢 Prerequisites#\n\nTo set up the development environment on your local machine, please follow the steps outlined in the development setup documentation.\n\n📤 Submitting a Contribution#\n\nTo submit a contribution:\n\nFork a copy of the Oumi repository into your own account. See Forking a repository for detailed steps.\n\nClone your fork locally, and add the Oumi repo as a remote repository:\n\ngit clone git@github.com:<github_id>/oumi.git cd oumi git remote add upstream https://github.com/oumi-ai/oumi.git\n\nCreate a branch, and make your proposed changes.\n\ngit checkout -b my-username/my-awesome-new-feature\n\nWhen you are ready, submit a pull request into the Oumi repository!\n\n📥 Pull request (PR) guidelines#\n\nBasic guidelines that will make your PR easier to review:\n\nTitle and Description\n\nPlease include a concise title and clear PR description.\n\nThe title should allow someone to understand what the PR changes or does at a glance.\n\nThe description should allow someone to understand the contents of the PR without looking at the code.\n\nTesting\n\nPlease include tests with your PR!\n\nIf fixing a bug, add a test that would’ve caught the bug.\n\nIf adding a new feature, include unit tests for the new functionality.\n\nCode Formatting and Type Checking\n\nUse pre-commit to handle formatting and type checking:\n\nEnsure you have it installed as described in the Prerequisites section.\n\nRun pre-commit hooks before submitting your PR.\n\n🏃🏽‍♀️ Running Tests#\n\nTo test your changes locally, run:\n\ncd ./tests/ pytest -s -vv\n\nTo run pre-commit hooks manually, run pre-commit run --all-files\n\n🎩 Code Style & Typing#\n\nSee the Oumi Style Guide for guidelines on how to structure, and format your code.\n\n©️ Copyright & License Headers#\n\nTo maintain proper copyright and license notices, please include the header at the top of each source code file.\n\n# Copyright 2025 - Oumi # # Licensed under the Apache License, Version 2.0 (the \"License\"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \"AS IS\" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License.\n\n🔗 Becoming a Code Reviewer or Maintainer#\n\nSend an email to contact@oumi.ai if you would like to become a code reviewer, maintainer or contribute in any other way!\n\n🏅 Recognition#\n\nJoin the Oumi community to be part of defining a better future for open frontier AI. We will recognize top contributors periodically and feature all of them in Oumi’s wall of fame.\n\nAlso, after you complete your first pull request (no matter how small), you can claim your holographic Oumi sticker! Send an email with title “Oumi Sticker” to contact@oumi.ai including your name and full mailing address and we will mail it to you anywhere in the world.",
      "# [Changelog — Oumi](https://oumi.ai/docs/en/latest/about/changelog.html)\nv0.1-alpha#\n\nThis is the initial alpha release of Oumi, an open-source framework for training and evaluating large language models. This release introduces the core functionality and features of the Oumi framework and sets the foundation for many more features to come.\n\n🚀 Core Features#\n\nFlexible training loop supporting various model architectures (Llama, Mistral, Phi-3.5, Llava 1.5 among others\n\nDistributed training capabilities (DDP, FSDP)\n\nComprehensive configuration system using OmegaConf\n\nEfficient data loading and preprocessing pipelines with streaming and packing support\n\nIntegration of Parameter-Efficient Fine-Tuning (PEFT) techniques\n\nEvaluation framework with LM Evaluation Harness integration\n\nMultiple inference engines including vLLM and llama.cpp\n\nLauncher system for deploying jobs across cloud platforms and HPC systems\n\nTelemetry and profiling tools for performance monitoring\n\nMixed-precision training and various optimization techniques\n\n📊 Data and Model Management#\n\nSupport for various dataset formats and preprocessing techniques\n\nIntegration with Hugging Face’s Datasets and Transformers libraries\n\nEfficient data loading with support for streaming and packing\n\n🖥️ Training and Optimization#\n\nDistributed training support (DDP, FSDP)\n\nMixed-precision training capabilities\n\nIntegration of Parameter-Efficient Fine-Tuning (PEFT) techniques\n\nModel FLOPs Utilization (MFU) calculation for performance analysis\n\n🔍 Evaluation and Inference#\n\nIntegration with LM Evaluation Harness for comprehensive model evaluation\n\nSupport for multiple inference engines (vLLM, llama.cpp)\n\nCustom evaluation metrics and benchmarks\n\n🚀 Deployment and Scaling#\n\nLauncher system for running jobs on various cloud platforms (AWS, GCP, Azure)\n\nSupport for HPC systems like Polaris\n\nDocker images for reproducible environments\n\n📊 Monitoring and Visualization#\n\nIntegration with Weights & Biases (WandB) for experiment tracking\n\nTensorBoard support for metric visualization\n\nCustom telemetry and profiling tools",
      "# [Troubleshooting — Oumi](https://oumi.ai/docs/en/latest/faq/troubleshooting.html)\nUsing Python 3.11.11 environment at: /Users/moonshine/miniconda3/envs/oumi × No solution found when resolving dependencies: ╰─▶ Because only the following versions of torch are available: torch<=2.5.0 torch==2.5.1 torch>2.6.0 and torch>=2.5.0,<=2.5.1 has no wheels with a matching platform tag (e.g., `macosx_10_16_x86_64`), we can conclude that torch>=2.5.0,<=2.5.1 cannot be used. And because oumi==0.1.dev1313+g33c1fa9 depends on torch>=2.5.0,<2.6.0, we can conclude that oumi==0.1.dev1313+g33c1fa9 cannot be used. And because only oumi[dev]==0.1.dev1313+g33c1fa9 is available and you require oumi[dev], we can conclude that your requirements are unsatisfiable. hint: Wheels are available for `torch` (v2.5.1) on the following platforms: `manylinux1_x86_64`, `manylinux2014_aarch64`, `macosx_11_0_arm64`, `win_amd64`",
      "# [Code of Conduct — Oumi](https://oumi.ai/docs/en/latest/development/code_of_conduct.html)\nOur Pledge#\n\nWe as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation.\n\nWe pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community.\n\nEnforcement Responsibilities#\n\nCommunity leaders are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful.\n\nCommunity leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate.\n\nScope#\n\nThis Code of Conduct applies within all community spaces, and also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event.\n\nEnforcement#\n\nInstances of abusive, harassing, or otherwise unacceptable behavior may be reported to the community leaders responsible for enforcement at contact@oumi.ai. All complaints will be reviewed and investigated promptly and fairly.\n\nAll community leaders are obligated to respect the privacy and security of the reporter of any incident.\n\nEnforcement Guidelines#\n\nCommunity leaders will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct:\n\n1. Correction#\n\nCommunity Impact: Use of inappropriate language or other behavior deemed unprofessional or unwelcome in the community.\n\nConsequence: A private, written warning from community leaders, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested.\n\n2. Warning#\n\nCommunity Impact: A violation through a single incident or series of actions.\n\nConsequence: A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time. This includes avoiding interactions in community spaces as well as external channels like social media. Violating these terms may lead to a temporary or permanent ban.\n\n3. Temporary Ban#\n\nCommunity Impact: A serious violation of community standards, including sustained inappropriate behavior.\n\nConsequence: A temporary ban from any sort of interaction or public communication with the community for a specified period of time. No public or private interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, is allowed during this period. Violating these terms may lead to a permanent ban.\n\n4. Permanent Ban#\n\nCommunity Impact: Demonstrating a pattern of violation of community standards, including sustained inappropriate behavior, harassment of an individual, or aggression toward or disparagement of classes of individuals.\n\nConsequence: A permanent ban from any sort of public interaction within the community.",
      "# [Dev Environment Setup — Oumi](https://oumi.ai/docs/en/latest/development/dev_setup.html)\n3.1 [optional] Force all job configs to do Oumi editable install#\n\nIf you’re making changes to the Oumi codebase which you can’t test locally (ex. training of large 8B+ models), you can use the Oumi launcher to launch remote jobs on GPU clusters and test your changes there. By default, most Oumi configs in the configs directory install oumi from PyPI (i.e. pip install oumi). However, for the remote job to pick up your local changes, you need to install from your local copy of the repo (i.e. pip install -e .).\n\nTo automate this process, you can set the OUMI_FORCE_EDITABLE_INSTALL environment variable to \"true\". This experimental feature will automatically detect and modify oumi PyPI installs to instead install in editable mode from source using regex.\n\n4.1 Getting access to Llama#\n\nLlama models are gated on HF Hub. To gain access, sign the agreement on your desired Llama model’s Hub page. It usually takes a few hours to get access to the model after signing the agreement. There is a separate agreement for each version of Llama:\n\nLlama 2\n\nLlama 3\n\nLlama 3.1\n\nLlama 3.2\n\nLlama 3.3",
      "# [Installation — Oumi](https://oumi.ai/docs/en/latest/get_started/installation.html)\n1. Install from PyPI (Recommended)#\n\nTo prevent dependency conflicts, let’s start by creating a virtual environment. We’ll use venv below, but you are welcome to use the environment manager of your choice (conda, uvx, etc):\n\nLinux / MacOS\n\npython -m venv .env source .env/bin/activate\n\nWindows\n\npython -m venv .env .env/Scripts/activate\n\nOnce that’s done, you’re ready to install Oumi!\n\nTo install the latest stable version of Oumi, run:\n\npip install oumi\n\n3. Clone and Install#\n\nIf you want to contribute to Oumi or need the full source code, you can clone the repository and install it:\n\nSSH\n\ngit clone git@github.com:oumi-ai/oumi.git cd oumi pip install -e\".[dev]\"\n\nHTTPS\n\ngit clone https://github.com/oumi-ai/oumi.git cd oumi pip install -e\".[dev]\"\n\nFor more information on setting up your dev environment for contributing to Oumi, please see our dev setup guide.\n\nThe -e flag installs the project in “editable” mode. This means that changes made to the source code will be immediately reflected in the installed package without needing to reinstall it. This is particularly helpful when you’re actively developing features and want to test your changes quickly. It creates a link to the project’s source code instead of copying the files, allowing you to modify the code and see the effects immediately in your Python environment.",
      "# [oumi.models — Oumi](https://oumi.ai/docs/en/latest/api/oumi.models.html)\noumi.models#\n\nModels module for the Oumi (Open Universal Machine Intelligence) library.\n\nThis module provides various model implementations for use in the Oumi framework. These models are designed for different machine learning tasks and can be used with the datasets and training pipelines provided by Oumi.\n\nAvailable models:\n\nMLPEncoder: A Multi-Layer Perceptron (MLP)\n\nencoder model.\n\nCNNClassifier: A simple ConvNet for\n\nimage classification e.g., can be used for MNIST digits classification.\n\nEach model is implemented as a separate class, inheriting from appropriate base classes in the Oumi framework.\n\nExample\n\nclassoumi.models.CNNClassifier(image_width:int, image_height:int, *, in_channels:int=3, output_dim:int=10, kernel_size:int=5)[source]#\n\nBases: BaseModel\n\nA simple ConvNet for classification of small fixed-size images.\n\npropertycriterion:Callable#\n\nReturns the criterion function to compute loss.\n\nforward(images:Tensor, labels:LongTensor|None=None, **kwargs) → dict[str,Tensor][source]#\n\nForward pass of the model.\n\nclassoumi.models.MLPEncoder(input_dim:int=768, hidden_dim:int=128, output_dim:int=10)[source]#\n\nBases: BaseModel\n\npropertycriterion:Callable#\n\nReturns the criterion function for the MLP model.\n\nThe criterion function is used to compute the loss during training.\n\nReturns:\n\nThe cross-entropy loss function.\n\nReturn type:\n\ntorch.nn.CrossEntropyLoss\n\nforward(input_ids:LongTensor, labels:LongTensor|None=None, **kwargs) → dict[str,Tensor][source]#\n\nForward pass of the MLP model.\n\nParameters:\n\ninput_ids (torch.LongTensor) – The input tensor of shape (batch_size, sequence_length).\n\nlabels (torch.LongTensor, optional) – The target labels tensor of shape (batch_size,).\n\n**kwargs – Additional keyword arguments provided by the tokenizer. Not used in this model.\n\nReturns:\n\nA dictionary containing the model outputs.\n\nThe dictionary has the following keys:\n\n”logits” (torch.Tensor): The output logits tensor of shape (batch_size, num_classes).\n\n”loss” (torch.Tensor, optional): The computed loss tensor if labels is not None.\n\nReturn type:\n\ndict"
    ],
    "search_results": [
      {
        "title": "Oumi",
        "link": "https://oumi.ai/",
        "snippet": "From data preparation to production deployment, Oumi provides an all-in-one production-ready open platform to build, evaluate, and deploy cutting-edge AI models ...",
        "formattedUrl": "https://oumi.ai/"
      },
      {
        "title": "Oumi - Company",
        "link": "https://oumi.ai/company",
        "snippet": "Oumi is built by a strong community and a core team of ML engineers and researchers that led the work for industry leading GenAI products (Google Gemini, Google ...",
        "formattedUrl": "https://oumi.ai/company"
      },
      {
        "title": "Blog",
        "link": "https://oumi.ai/blog",
        "snippet": "Build Custom Evaluations for any Open or Closed Model in just 50 Lines of Code. March 24 - Konstantinos Aisopos, Jeremy Greer.",
        "formattedUrl": "https://oumi.ai/blog"
      },
      {
        "title": "Oumi Jobs",
        "link": "https://oumi.ai/careers",
        "snippet": "Research · Research Intern · Research Scientist. Research • Seattle, WA; Palo Alto, CA • Full time • Hybrid. $140K – $220K • Offers Equity · Powered by.",
        "formattedUrl": "https://oumi.ai/careers"
      },
      {
        "title": "Getting Started — Oumi",
        "link": "https://oumi.ai/docs",
        "snippet": "Everything you need to build state-of-the-art foundation models, end-to-end. GitHub trending. Oumi is a fully open-source platform that streamlines the entire ...",
        "formattedUrl": "https://oumi.ai/docs"
      },
      {
        "title": "Oumi - Contact Us",
        "link": "https://oumi.ai/contact",
        "snippet": "Contact Us. Oumi AI. Built to be truly open for everyone. Truly open and collectively developed AI is the future. Build that future. Get in touch.",
        "formattedUrl": "https://oumi.ai/contact"
      },
      {
        "title": "Privacy Notice - Oumi",
        "link": "https://oumi.ai/privacy-policy",
        "snippet": "Jan 27, 2025 ... At Oumi, PBC, we're committed to protecting the privacy of our users. This notice explains how we collect, use, share, and protect your data.",
        "formattedUrl": "https://oumi.ai/privacy-policy"
      },
      {
        "title": "Oumi Unveils First Unconditionally Open AI Platform",
        "link": "https://oumi.ai/blog/posts/press-release",
        "snippet": "Jan 29, 2025 ... Oumi is the first to launch a platform that offers state-of-the-art foundation models with open code, open data, open weights—and open ...",
        "formattedUrl": "https://oumi.ai/blog/posts/press-release"
      },
      {
        "title": "If AI isn't truly open, it will fail us - Oumi",
        "link": "https://oumi.ai/blog/posts/truly-open-ai",
        "snippet": "Jan 29, 2025 ... Open source is the best path we have for AI to effectively and safely unlock industry applications and promote scientific discovery.",
        "formattedUrl": "https://oumi.ai/blog/posts/truly-open-ai"
      },
      {
        "title": "Terms of Service - Oumi",
        "link": "https://oumi.ai/terms-of-service",
        "snippet": "Jan 27, 2025 ... These Terms of Service (\"Terms\") govern your use of our website (the \"Website\"), application programming interface (the \"API\"), and other functionalities ...",
        "formattedUrl": "https://oumi.ai/terms-of-service"
      },
      {
        "title": "CLI Reference — Oumi",
        "link": "https://oumi.ai/docs/en/latest/cli/commands.html",
        "snippet": "Oumi uses OmegaConf to parse the configs from YAML files, and to parse the command line overrides. OmegaConf allows a Pythonic specification of parameters to ...",
        "formattedUrl": "https://oumi.ai/docs/en/latest/cli/commands.html"
      },
      {
        "title": "oumi — Oumi",
        "link": "https://oumi.ai/docs/en/latest/api/oumi.html",
        "snippet": "This library provides tools and utilities for training, evaluating, and inferring with machine learning models, particularly focused on language tasks.",
        "formattedUrl": "https://oumi.ai/docs/en/latest/api/oumi.html"
      },
      {
        "title": "Customizing Oumi — Oumi",
        "link": "https://oumi.ai/docs/en/latest/user_guides/customization.html",
        "snippet": "You can easily register classes that are then loaded as if they're a native part of the Oumi framework. See the diagram below for how we load your custom code:",
        "formattedUrl": "https://oumi.ai/docs/en/latest/user_guides/customization.html"
      },
      {
        "title": "Core Concepts — Oumi",
        "link": "https://oumi.ai/docs/en/latest/get_started/core_concepts.html",
        "snippet": "This guide introduces the core concepts, and terminology used throughout Oumi, as well as the architecture and guiding design principles.",
        "formattedUrl": "https://oumi.ai/docs/en/latest/get_started/core_concepts.html"
      },
      {
        "title": "Quickstart — Oumi",
        "link": "https://oumi.ai/docs/en/latest/get_started/quickstart.html",
        "snippet": "We're going to use the oumi command-line interface (CLI) to train, evaluate, and run inference with a model.",
        "formattedUrl": "https://oumi.ai/docs/en/latest/get_started/quickstart.html"
      },
      {
        "title": "Recipes — Oumi",
        "link": "https://oumi.ai/docs/en/latest/resources/recipes.html",
        "snippet": "We've prepared a set of recipes for common use cases. These recipes are designed to be easy to understand and modify, and should be a good starting point for ...",
        "formattedUrl": "https://oumi.ai/docs/en/latest/resources/recipes.html"
      },
      {
        "title": "Tutorials — Oumi",
        "link": "https://oumi.ai/docs/en/latest/get_started/tutorials.html",
        "snippet": "Model Training & Finetuning ; Model Finetuning Guide. Step-by-step guide to efficient model finetuning techniques. Download · Open In Colab ; 🖼️ Oumi Multimodal.",
        "formattedUrl": "https://oumi.ai/docs/en/latest/get_started/tutorials.html"
      },
      {
        "title": "Style Guide — Oumi",
        "link": "https://oumi.ai/docs/en/latest/development/style_guide.html",
        "snippet": "Oumi follows Google's Python Style Guide for how to format and structure code. Opt for using descriptive-style verbs (ex. “Builds”) over imperative-style (ex. ...",
        "formattedUrl": "https://oumi.ai/docs/en/latest/development/style_guide.html"
      },
      {
        "title": "Updating the Documentation — Oumi",
        "link": "https://oumi.ai/docs/en/latest/development/docs_guide.html",
        "snippet": "Suggesting Edits Using the UI#. You need to submit PRs to update documentation. The easiest way to modify an existing page is to use the “Suggest Edit” UI:.",
        "formattedUrl": "https://oumi.ai/docs/en/latest/development/docs_guide.html"
      },
      {
        "title": "Out of Memory (OOM) — Oumi",
        "link": "https://oumi.ai/docs/en/latest/faq/oom.html",
        "snippet": "Model Configuration# · Use flash attention: PYTHON. config = TrainingConfig( model=ModelParams( attn_implementation=\"sdpa\", # or \"flash_attention2\" ), ).",
        "formattedUrl": "https://oumi.ai/docs/en/latest/faq/oom.html"
      },
      {
        "title": "Citations — Oumi",
        "link": "https://oumi.ai/docs/en/latest/about/citations.html",
        "snippet": "[GBB+20]. Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, ...",
        "formattedUrl": "https://oumi.ai/docs/en/latest/about/citations.html"
      },
      {
        "title": "License — Oumi",
        "link": "https://oumi.ai/docs/en/latest/about/license.html",
        "snippet": "This project is licensed under the Apache License 2.0, which allows for broad use, modification, and distribution within the open source community. The full ...",
        "formattedUrl": "https://oumi.ai/docs/en/latest/about/license.html"
      },
      {
        "title": "Acknowledgments — Oumi",
        "link": "https://oumi.ai/docs/en/latest/about/acknowledgements.html",
        "snippet": "Accelerate. Distributed training and mixed precision computations ; bitsandbytes. Quantization and efficient optimizers for QLora ; Datasets. Dataset loading and ...",
        "formattedUrl": "https://oumi.ai/docs/en/latest/about/acknowledgements.html"
      },
      {
        "title": "Contributing — Oumi",
        "link": "https://oumi.ai/docs/en/latest/development/contributing.html",
        "snippet": "Possible contributions include: Bug fixes, incremental improvements, and tests, no matter how small. New features and infrastructure improvements. Tuning ...",
        "formattedUrl": "https://oumi.ai/docs/en/latest/development/contributing.html"
      },
      {
        "title": "Changelog — Oumi",
        "link": "https://oumi.ai/docs/en/latest/about/changelog.html",
        "snippet": "This is the initial alpha release of Oumi, an open-source framework for training and evaluating large language models. This release introduces the core ...",
        "formattedUrl": "https://oumi.ai/docs/en/latest/about/changelog.html"
      },
      {
        "title": "Troubleshooting — Oumi",
        "link": "https://oumi.ai/docs/en/latest/faq/troubleshooting.html",
        "snippet": "Pre-commit hook errors with VS Code#. When committing changes, you may encounter an error with pre-commit hooks related to missing imports. To fix this, make ...",
        "formattedUrl": "https://oumi.ai/docs/en/latest/faq/troubleshooting.html"
      },
      {
        "title": "Code of Conduct — Oumi",
        "link": "https://oumi.ai/docs/en/latest/development/code_of_conduct.html",
        "snippet": "Our Standards#. Examples of behavior that contributes to a positive environment for our community include: Demonstrating empathy and kindness toward other ...",
        "formattedUrl": "https://oumi.ai/docs/en/latest/development/code_of_conduct.html"
      },
      {
        "title": "Dev Environment Setup — Oumi",
        "link": "https://oumi.ai/docs/en/latest/development/dev_setup.html",
        "snippet": "1. Install Miniconda#. The simplest way to install Miniconda is to first clone the Oumi repository (step 2.2 below), then run: make install-miniconda.",
        "formattedUrl": "https://oumi.ai/docs/en/latest/development/dev_setup.html"
      },
      {
        "title": "Installation — Oumi",
        "link": "https://oumi.ai/docs/en/latest/get_started/installation.html",
        "snippet": "Requirements#. ❗NOTE: Since PyTorch dropped support for Intel Macs, you cannot install Oumi there. Consider running Oumi on free Colab GPU instances, using our ...",
        "formattedUrl": "https://oumi.ai/docs/en/latest/get_started/installation.html"
      },
      {
        "title": "oumi.models",
        "link": "https://oumi.ai/docs/en/latest/api/oumi.models.html",
        "snippet": "Models module for the Oumi (Open Universal Machine Intelligence) library. This module provides various model implementations for use in the Oumi framework.",
        "formattedUrl": "https://oumi.ai/docs/en/latest/api/oumi.models.html"
      }
    ]
  },
  "general_search_markdown": "# Official social media\n- [Manos Koukoumidis (@Koukoumidis) / X](https://x.com/Koukoumidis)\n\n# Job boards\n- [Research Scientist @ Oumi | Ascend Job Board](https://jobs.ascend.vc/companies/oumi-2/jobs/47431467-research-scientist)\n- [Research Scientist @ Oumi](https://jobs.ashbyhq.com/oumi/a384fcdc-7cd7-4930-af8b-22a54c6d702e)\n- [Research Intern at Oumi - aijobs.com](https://www.aijobs.com/jobs/113632055-research-intern)\n\n# App stores\n- None found.\n\n# Product reviews\n- None found.\n\n# News articles (most recent first, grouped by event)\n### Oumi Launch and Funding\n- [Ex-Google, Apple engineers launch unconditionally open source Oumi AI](https://venturebeat.com/ai/ex-google-apple-engineers-launch-unconditionally-open-source-oumi-ai-platform-that-could-help-to-build-the-next-deepseek/) - Jan 29, 2025\n- [Making Good on the Promise of Open Source AI - The New Stack](https://thenewstack.io/making-good-on-the-promise-of-open-source-ai/) - Jan 29, 2025\n- [Oumi AI: A Fully Open-Source Platform to Democratize AI](https://tecknexus.com/oumi-ai-a-fully-open-source-platform-to-democratize-ai-development/4/) - Jan 29, 2025\n- [Oumi Platform Launches to Foster AI Research Collaboration](https://techstrong.ai/ai-at-the-edge/oumi-platform-launches-to-foster-ai-research-collaboration/) - Jan 31, 2025\n- [Why We Invested in Oumi Making AI truly open](https://obvious.com/ideas/why-we-invested-in-oumi/) - Jan 29, 2025\n- [Former Microsoft engineers launch AI lab with $10M in funding](https://www.bizjournals.com/seattle/news/2025/01/29/oumi-microsoft-google-apple-ai-lab-open-source.html) - Jan 29, 2025\n- [AI Needs Its Linux: Oumi Comes Out of Stealth with an Open Source](https://www.hpcwire.com/2025/01/29/ai-needs-its-linux-oumi-comes-out-of-stealth-with-an-open-source-vision/) - Jan 29, 2025\n- [Oumi: The AI revolution led by Greek minds - Neos Kosmos](https://neoskosmos.com/en/2025/03/12/news/business/oumi-the-ai-revolution-led-by-greek-minds/) - Mar 12, 2025\n\n### General AI and Open Source Discussions\n- [The Imperative for Open Source Frontier AI - Oumi's Platform & Strategy](https://archimedesai.gr/en/events/374-archimedes-talks-series-the-imperative-for-open-source-frontier-ai-oumis-platform-strategy-1) - Date not specified\n- [Oumi: Three Greeks behind the new global artificial intelligence](https://en.protothema.gr/2025/02/11/oumi-three-greeks-behind-the-new-global-artificial-intelligence-application-we-want-everything-to-be-available-to-everyone/) - Feb 11, 2025\n- [Oumi is a 100% open-source AI platform? | The Star](https://www.thestar.com.my/tech/tech-news/2025/02/04/oumi-is-a-100-open-source-ai-platform) - Feb 4, 2025\n\n# Key employees (grouped by employee)\n### Manos Koukoumidis\n- [Emmanouil (Manos) Koukoumidis - CEO - Oumi | LinkedIn](https://www.linkedin.com/in/koukoumidis)\n- [AI Engineering Podcast | Podcast on Spotify](https://open.spotify.com/show/19KSYTTB5DTEb2z5Q3qiTh) - Discusses Oumi's vision and goals.\n\n### Oussama Elachqar\n- [Oussama Elachqar - Google Scholar](https://scholar.google.com/citations?user=0fDBk7EAAAAJ&hl=en) - Related research articles.\n\n# Other pages on the company website\n- [Oumi](https://oumi.ai/) - Overview of the platform.\n- [Blog](https://oumi.ai/blog) - Company blog with updates and insights.\n- [Oumi Unveils First Unconditionally Open AI Platform](https://oumi.ai/blog/posts/press-release) - Announcement of the platform launch.\n- [Oumi Jobs](https://oumi.ai/careers) - Current job openings at Oumi.\n- [Contact Us](https://oumi.ai/contact) - Contact information for inquiries.\n\n# Other\n- [Oumi 2025 Company Profile: Valuation, Funding & Investors](https://pitchbook.com/profiles/company/741581-74) - Company profile and funding details.\n- [Oumi - Crunchbase Company Profile & Funding](https://www.crunchbase.com/organization/oumi) - Overview of the company and its funding history.",
  "crunchbase_markdown": "# Oumi, founded 2024-01-01 [(Crunchbase, 2025)](https://www.crunchbase.com/organization/oumi)\nOumi is a group of academics, developers dedicated to fostering greater transparency and cooperation in frontier AI.\n\n- [Website](https://oumi.ai/)\n- [LinkedIn](https://www.linkedin.com/company/oumi-ai/about/)\n\n\n## Funding (10M USD total)\n\n- 10M USD on 2025-01-29\n\n",
  "customer_experience_result": {
    "output_text": "# COMPANY Oumi\n\n## Vision and Principles\n- \"Oumi started with the core principle that frontier AI should be advanced collaboratively in the open as in this way AI can be advanced not just faster, and safer but also much more efficiently.\" [(koukoumidis, Reddit, 2025-02-14)](https://www.reddit.com/r/MachineLearning/comments/1ioxatq/d_we_built_genai_at_google_and_apple_then_left_to/mcrkemu/)\n- \"We really want the development of frontier AI to be open and collaborative as this will help us advance AI faster, safer and more efficiently.\" [(koukoumidis, Reddit, 2025-02-14)](https://www.reddit.com/r/MachineLearning/comments/1ioxatq/d_we_built_genai_at_google_and_apple_then_left_to/mcre0jq/)\n\n## Platform Goals and Features\n- \"Our goal with the Oumi platform is to support the full pipeline end-to-end, including pre-training with novel architectures.\" [(oelachqar, Reddit, 2025-02-14)](https://www.reddit.com/r/MachineLearning/comments/1ioxatq/d_we_built_genai_at_google_and_apple_then_left_to/mcre0i9/)\n- \"We want everything–from data curation, to training, to evaluation, to inference–to be simple and reproducible, as well as scale from your local hardware to any cloud or cluster you might have access to.\" [(Taenin, Reddit, 2025-01-30)](https://www.reddit.com/r/LocalLLaMA/comments/1id3ak8/exgoogle_apple_engineers_launch_unconditionally/m9y0q9k/)\n- \"We’re trying to make good on our promise of 'unconditionally open' by making everything we’ve built open from the get-go.\" [(Taenin, Reddit, 2025-01-30)](https://www.reddit.com/r/LocalLLaMA/comments/1id3ak8/exgoogle_apple_engineers_launch_unconditionally/ma22obb/)\n- \"Right now we’re focusing on building an awesome open-source platform for AI research.\" [(Taenin, Reddit, 2025-01-30)](https://www.reddit.com/r/LocalLLaMA/comments/1id3ak8/exgoogle_apple_engineers_launch_unconditionally/ma1mxts/)\n- \"We plan to do both! We have a team of engineers and researchers, and we’re actively partnered with 13+ top research universities (MIT, Stanford, Princeton, Carnegie Mellon, etc) to work on pushing the state of the art!\" [(Taenin, Reddit, 2025-01-30)](https://www.reddit.com/r/LocalLLaMA/comments/1id3ak8/exgoogle_apple_engineers_launch_unconditionally/ma1y3wl/)\n\n## Safety and Scalability\n- \"We support Llama Guard (although we haven’t tested it extensively), and our built-in judge includes safety as a criterion during judging.\" [(jeremy_oumi, Reddit, 2025-02-14)](https://www.reddit.com/r/MachineLearning/comments/1ioxatq/d_we_built_genai_at_google_and_apple_then_left_to/mcrq98o/)\n- \"We designed Oumi to work at any scale: you can run tuning in a notebook, on a local machine or cluster, or even remotely on the cloud!\" [(MatthewPersons, Reddit, 2025-02-14)](https://www.reddit.com/r/MachineLearning/comments/1ioxatq/d_we_built_genai_at_google_and_apple_then_left_to/mcrwzkk/)\n- \"For compute, we’re all pretty much tied to the same platform (GPUs or TPUs), which are quite expensive due to the demand, R&D, and power consumption.\" [(jeremy_oumi, Reddit, 2025-02-14)](https://www.reddit.com/r/MachineLearning/comments/1ioxatq/d_we_built_genai_at_google_and_apple_then_left_to/mcrfirj/)\n\n# PRODUCT Oumi\n\n## User Experience and Content\n- \"The cast is very stacked. the PV doesn't blow me away in terms of animation, but doesn't look bad either, but at least the chief director has some experience (although most of his shows are kind of mid in terms of production values).\" [(G326, Reddit, 2025-03-03)](https://www.reddit.com/r/anime/comments/1j2g89b/this_monster_wants_to_eat_me_pv_1/mfrp3ye/)\n- \"I'm loving the vibes with the melancholy music. Very fitting.\" [(Syokhan, Reddit, 2025-03-03)](https://www.reddit.com/r/anime/comments/1j2g89b/this_monster_wants_to_eat_me_pv_1/mfrjekp/)\n- \"The story is The best thing easily so even if animation and everything else isn't good (I Hope It is) story can Carry The anime.\" [(Plus_Rip4944, Reddit, 2025-03-03)](https://www.reddit.com/r/anime/comments/1j2g89b/this_monster_wants_to_eat_me_pv_1/mftbmwu/)\n- \"It's also so nice seeing more and more Yuri manga receiving adaptations at all.\" [(G326, Reddit, 2025-03-03)](https://www.reddit.com/r/anime/comments/1j2g89b/this_monster_wants_to_eat_me_pv_1/mfrp3ye/)\n- \"I started to read this on a whim recently, and within 8 hours I had binged every released manga chapter and was obsessively trying to find every single piece of online discussion about the series afterwards.\" [(DWIPssbm, Reddit, 2025-03-03)](https://www.reddit.com/r/anime/comments/1j2g89b/this_monster_wants_to_eat_me_pv_1/mfri114/)\n- \"The main character is barely able to care about anything that is not her own death, much less romance.\" [(zairaner, Reddit, 2025-03-03)](https://www.reddit.com/r/anime/comments/1j2g89b/this_monster_wants_to_eat_me_pv_1/mfrmncz/)",
    "intermediate_steps": [
      "- \"The cast is very stacked. the PV doesn't blow me away in terms of animation, but doesn't look bad either, but at least the chief director has some experience (although most of his shows are kind of mid in terms of production values).\" [(G326, Reddit, 2025-03-03)](cache://reddit/37)\n- \"I'm loving the vibes with the melancholy music. Very fitting.\" [(Syokhan, Reddit, 2025-03-03)](cache://reddit/34)\n- \"The story is The best thing easily so even if animation and everything else isn't good (I Hope It is) story can Carry The anime.\" [(Plus_Rip4944, Reddit, 2025-03-03)](cache://reddit/51)\n- \"It's also so nice seeing more and more Yuri manga receiving adaptations at all.\" [(G326, Reddit, 2025-03-03)](cache://reddit/37)\n- \"I started to read this on a whim recently, and within 8 hours I had binged every released manga chapter and was obsessively trying to find every single piece of online discussion about the series afterwards.\" [(DWIPssbm, Reddit, 2025-03-03)](cache://reddit/39)\n- \"The main character is barely able to care about anything that is not her own death, much less romance.\" [(zairaner, Reddit, 2025-03-03)](cache://reddit/61)",
      "",
      "- \"Oumi started with the core principle that frontier AI should be advanced collaboratively in the open as in this way AI can be advanced not just faster, and safer but also much more efficiently.\" [(koukoumidis, Reddit, 2025-02-14)](cache://reddit/178)\n- \"We really want the development of frontier AI to be open and collaborative as this will help us advance AI faster, safer and more efficiently.\" [(koukoumidis, Reddit, 2025-02-14)](cache://reddit/189)\n- \"Our goal with the Oumi platform is to support the full pipeline end-to-end, including pre-training with novel architectures.\" [(oelachqar, Reddit, 2025-02-14)](cache://reddit/182)\n- \"We support Llama Guard (although we haven’t tested it extensively), and our built-in judge includes safety as a criterion during judging.\" [(jeremy_oumi, Reddit, 2025-02-14)](cache://reddit/210)\n- \"We designed Oumi to work at any scale: you can run tuning in a notebook, on a local machine or cluster, or even remotely on the cloud!\" [(MatthewPersons, Reddit, 2025-02-14)](cache://reddit/215)\n- \"For compute, we’re all pretty much tied to the same platform (GPUs or TPUs), which are quite expensive due to the demand, R&D, and power consumption.\" [(jeremy_oumi, Reddit, 2025-02-14)](cache://reddit/227)",
      "```\n```",
      "- \"I produced and wrote the track and collaborated with the very talented Oumi Kapila from Combichrist for that.\" [(Misha Mansoor, Reddit, 2024-09-05)](cache://reddit/312)\n- \"The lift spam near the beginning on guitar was unfortunate, but the solos were pretty neat.\" [(ComfortablePatience, Reddit, 2024-09-04)](cache://reddit/359)\n- \"Wish they gave the ones who made the song some credit I mean the vocalist on here absolutely killed it!!!\" [(Alternative-Test-556, Reddit, 2024-09-04)](cache://reddit/360)\n- \"Misha posted all the credits towards the top of this thread. Misha and Spencer, and then some people from Megadeth and Korn.\" [(HeyNateBarber, Reddit, 2024-09-05)](cache://reddit/416)\n- \"I love it even more now that I know the band members are from Megadeth, Korn, and Periphery!\" [(SeawardFriend, Reddit, 2024-09-20)](cache://reddit/426)",
      "```markdown\n```\n",
      "- \"We want everything–from data curation, to training, to evaluation, to inference–to be simple and reproducible, as well as scale from your local hardware to any cloud or cluster you might have access to.\" [(Taenin, Reddit, 2025-01-30)](cache://reddit/566)\n- \"We’re trying to make good on our promise of 'unconditionally open' by making everything we’ve built open from the get-go.\" [(Taenin, Reddit, 2025-01-30)](cache://reddit/543)\n- \"Right now we’re focusing on building an awesome open-source platform for AI research.\" [(Taenin, Reddit, 2025-01-30)](cache://reddit/545)\n- \"We plan to do both! We have a team of engineers and researchers, and we’re actively partnered with 13+ top research universities (MIT, Stanford, Princeton, Carnegie Mellon, etc) to work on pushing the state of the art!\" [(Taenin, Reddit, 2025-01-30)](cache://reddit/547)\n- \"Our goal isn’t to replace them–they’re just one of the many tools we support!\" [(Taenin, Reddit, 2025-01-30)](cache://reddit/566)"
    ],
    "url_to_review": {},
    "review_markdowns": [
      "# Post ID 1id95mm: Ex-Google, Apple engineers launch unconditionally open source Oumi AI platform that could help to build the next DeepSeek with +1047 score by [(Revenant013, Reddit, 2025-01-30)](https://www.reddit.com/r/Futurology/comments/1id95mm/exgoogle_apple_engineers_launch_unconditionally/)\n\n\n\n\n## Comment ID m9xkiub with +219 score by [(Significant-Dog-8166, Reddit, 2025-01-30)](https://www.reddit.com/r/Futurology/comments/1id95mm/exgoogle_apple_engineers_launch_unconditionally/m9xkiub/) (in reply to ID 1id95mm):\nLol AI will make AI unemployed, unemployable, and unprofitable.  How unsurprising.\n\n### Comment ID m9yqzgt with +56 score by [(Cartina, Reddit, 2025-01-30)](https://www.reddit.com/r/Futurology/comments/1id95mm/exgoogle_apple_engineers_launch_unconditionally/m9yqzgt/) (in reply to ID m9xkiub):\nI don't know, it's progress. I've seen it before with the birth of the internet as it is today. Huge websites and companies that owned the world was forgotten when the better thing appeared.\n\nWith 2-3 billion monthly visitors, I don't think LLM has any risk of being unwanted. But specific ones might die, and it's impossible to say who will be king when the dust settles.\n\nA few people called internet a fad in the 90s, it wouldn't last. It took the bubble to burst before we got something resembling what it is today. It's easy to forget that Internet wasn't something everyone was sold on. Smartphones isn't even 20 years old in its current shape.\n\nI see a lot of the things I saw in internets growth in AIs current growth. The random companies popping up and some failed quickly and some went on to become behemoths. There was no rhyme or reason which one did. Google launched their engine free of charge too. It just happened it was too good to not use. Google was lightning fast and somehow produced relevant search results. Altavista launched, become the biggest and essentially died in just 3 years span.\n\nAI in 2025 might not be that amazing, but I believe AI in 2035 will be nothing short of a revolution when we look back\n\n#### Comment ID ma15yzl with +17 score by [(alexandros87, Reddit, 2025-01-30)](https://www.reddit.com/r/Futurology/comments/1id95mm/exgoogle_apple_engineers_launch_unconditionally/ma15yzl/) (in reply to ID m9yqzgt):\nGreat points.\n\nTo use a video game analogy:\n\nWe're still in the 'space invaders' era of all this stuff. We don't know what the 'Elden Ring' era will look like, but it will certainly be vastly more sophisticated.\n\n#### Comment ID m9zwlov with +6 score by [(NanditoPapa, Reddit, 2025-01-30)](https://www.reddit.com/r/Futurology/comments/1id95mm/exgoogle_apple_engineers_launch_unconditionally/m9zwlov/) (in reply to ID m9yqzgt):\n100%!\n\nWell said.\n\n#### Comment ID ma4duty with +3 score by [(Significant-Dog-8166, Reddit, 2025-01-31)](https://www.reddit.com/r/Futurology/comments/1id95mm/exgoogle_apple_engineers_launch_unconditionally/ma4duty/) (in reply to ID m9yqzgt):\nI think it’s a limited use “get started” efficiency tool being marketed as a shippable product generator.  \n\nIt’s got more potential than VR… but you remember how VR went.\n\n### Comment ID ma71nb8 with +1 score by [(nerfviking, Reddit, 2025-01-31)](https://www.reddit.com/r/Futurology/comments/1id95mm/exgoogle_apple_engineers_launch_unconditionally/ma71nb8/) (in reply to ID m9xkiub):\nGood.\n\nAI is amazing, but the collected general knowledge of humanity isn't something that a few rich people should be able to use to get even richer by renting it out to the rest of us.\n\n## Comment ID m9x9hcu with +65 score by [(Revenant013, Reddit, 2025-01-30)](https://www.reddit.com/r/Futurology/comments/1id95mm/exgoogle_apple_engineers_launch_unconditionally/m9x9hcu/) (in reply to ID 1id95mm):\nAll the drama of Deepseek beating out OpenAI in terms of cost efficiency, and then subsequently releasing the model weights for everyone, while also possibly training on OpenAI's data (who also train on stolen data!) makes me feel like the there should really be a place where everyone collaborates to build AI collectively.\n\nIt's kind of insane that multiple companies are training models that are like 95% identical to ChatGPT. Given the rise of open source models rivaling closed AI, could we see a future where we actually build these collectively?\n\n### Comment ID m9xfhh5 with +38 score by [(joe-knows-nothing, Reddit, 2025-01-30)](https://www.reddit.com/r/Futurology/comments/1id95mm/exgoogle_apple_engineers_launch_unconditionally/m9xfhh5/) (in reply to ID m9x9hcu):\n>there should really be a place where everyone collaborates to build AI collectively.\n\nIMHO, it's the Internet.\n\nA lot of people would be a lot less mad if OpenAI was, you know, open. Like open source. Yes, the thorny IP side of things and stealing art is still a problem that needs addressing, but at least it's blatantly not lining the pockets of Altman, Gates and other billionaires.\n\nThere are problems that can be helped by AI, but stealing from the creative class and working class are not it.\n\nEdited to finish my thought. Accidentally hit post.\n\n#### Comment ID ma13m3b with +4 score by [(Goku420overlord, Reddit, 2025-01-30)](https://www.reddit.com/r/Futurology/comments/1id95mm/exgoogle_apple_engineers_launch_unconditionally/ma13m3b/) (in reply to ID m9xfhh5):\nIt should start stealing from the rich class and give to the poor\n\n### Comment ID m9xkwkg with +26 score by [(Rogaar, Reddit, 2025-01-30)](https://www.reddit.com/r/Futurology/comments/1id95mm/exgoogle_apple_engineers_launch_unconditionally/m9xkwkg/) (in reply to ID m9x9hcu):\nWelcome to capitalism. Competition is not welcome here. It doesn't benefit a select few.\n\n#### Comment ID ma0brhu with +3 score by [(kozmo1313, Reddit, 2025-01-30)](https://www.reddit.com/r/Futurology/comments/1id95mm/exgoogle_apple_engineers_launch_unconditionally/ma0brhu/) (in reply to ID m9xkwkg):\npeople always [conflate](https://www.investopedia.com/ask/answers/042215/what-difference-between-capitalist-system-and-free-market-system.asp) \"free market capitalism\" but the very last thing a capitalist wants is an actual free market.\n\n## Comment ID m9zpidi with +11 score by [(big_dog_redditor, Reddit, 2025-01-30)](https://www.reddit.com/r/Futurology/comments/1id95mm/exgoogle_apple_engineers_launch_unconditionally/m9zpidi/) (in reply to ID 1id95mm):\nWon’t someone please think of shareholders at this horrible time for them. They throw their whole identities and family bank accounts into following CEO influencers and they are having a hard time right now.\n\n## Comment ID m9xo8sc with +11 score by [(karoshikun, Reddit, 2025-01-30)](https://www.reddit.com/r/Futurology/comments/1id95mm/exgoogle_apple_engineers_launch_unconditionally/m9xo8sc/) (in reply to ID 1id95mm):\nthis means a small boon for the smaller companies offering ai based services while taking down a few pegs of the big corpos.\n\n\nnot horrible, to be honest, given the circumstances. here's hoping those stocks take a while to climb up again\n\n## Comment ID ma490x8 with +5 score by [(Mascosk, Reddit, 2025-01-31)](https://www.reddit.com/r/Futurology/comments/1id95mm/exgoogle_apple_engineers_launch_unconditionally/ma490x8/) (in reply to ID 1id95mm):\nThis is one of those situations where I think more competition will help. OpenAI was king for too long and was pretty clear about their intent to exploit the user base. \n\nLike another user said, it’s impossible to tell who will withstand the test of time. We’re seeing incredible progress being made over a very short amount of time with technology. All the other “technology” (before computers) took hundreds of years to develop and refine into something truly useful. We’ll have to wait several more decades, I think, before we see a lot of the growing pains of the internet and digital technology even out and become a safe, regulated facet of our lives.\n\n## Comment ID m9zyhw9 with +2 score by [(AfricanUmlunlgu, Reddit, 2025-01-30)](https://www.reddit.com/r/Futurology/comments/1id95mm/exgoogle_apple_engineers_launch_unconditionally/m9zyhw9/) (in reply to ID 1id95mm):\nthe question is when will ai stop making stuff up & lying? , this is the real world, not some presidency",
      "# Post ID 1id3ak8: Ex-Google, Apple engineers launch unconditionally open source Oumi AI platform that could help to build the next DeepSeek with +363 score by [(Revenant013, Reddit, 2025-01-29)](https://www.reddit.com/r/LocalLLaMA/comments/1id3ak8/exgoogle_apple_engineers_launch_unconditionally/)\n\n\n## Comment ID m9x5os6 with +90 score by [(Taenin, Reddit, 2025-01-30)](https://www.reddit.com/r/LocalLLaMA/comments/1id3ak8/exgoogle_apple_engineers_launch_unconditionally/m9x5os6/) (in reply to ID 1id3ak8):\nHey, I'm Matthew, one of the engineer's at Oumi! One of my team members just pointed out that there was a post about us here. I'm happy to answer any questions you might have about our project! We're fully open-source and you can check out our github repo here: [https://github.com/oumi-ai/oumi](https://github.com/oumi-ai/oumi)\n\n### Comment ID m9yg4ge with +20 score by [(Justpassing017, Reddit, 2025-01-30)](https://www.reddit.com/r/LocalLLaMA/comments/1id3ak8/exgoogle_apple_engineers_launch_unconditionally/m9yg4ge/) (in reply to ID m9x5os6):\nYou guys should make a series of video about yourself to explain what Oumi is and how to use it.\n\n#### Comment ID ma1jf2m with +3 score by [(Taenin, Reddit, 2025-01-30)](https://www.reddit.com/r/LocalLLaMA/comments/1id3ak8/exgoogle_apple_engineers_launch_unconditionally/ma1jf2m/) (in reply to ID m9yg4ge):\nThis is a great idea, I’ll see if I can get on that ASAP! In the meantime we do have a video about Oumi’s mission, though be warned that it’s a bit cheesy 😛 [https://www.youtube.com/watch?v=K9PqMSzQz24](https://www.youtube.com/watch?v=K9PqMSzQz24)\n\n### Comment ID m9yq4v4 with +5 score by [(AdOdd4004, Reddit, 2025-01-30)](https://www.reddit.com/r/LocalLLaMA/comments/1id3ak8/exgoogle_apple_engineers_launch_unconditionally/m9yq4v4/) (in reply to ID m9x5os6):\nThis is an exciting project! Will unsloth fine-tuning be supported as well?\n\n#### Comment ID ma1pl32 with +2 score by [(Taenin, Reddit, 2025-01-30)](https://www.reddit.com/r/LocalLLaMA/comments/1id3ak8/exgoogle_apple_engineers_launch_unconditionally/ma1pl32/) (in reply to ID m9yq4v4):\nThanks! And this is a great idea! We don’t have support for unsloth right now, but this is definitely something we can look into!\n\n#### Comment ID m9zf96t with +1 score by [(Amazing_Q, Reddit, 2025-01-30)](https://www.reddit.com/r/LocalLLaMA/comments/1id3ak8/exgoogle_apple_engineers_launch_unconditionally/m9zf96t/) (in reply to ID m9yq4v4):\nGood idea.\n\n### Comment ID m9ywk8z with +6 score by [(ResidentPositive4122, Reddit, 2025-01-30)](https://www.reddit.com/r/LocalLLaMA/comments/1id3ak8/exgoogle_apple_engineers_launch_unconditionally/m9ywk8z/) (in reply to ID m9x5os6):\nThanks for doing an impromptu ama :)\n\n> Train and fine-tune models from 10M to 405B parameters using state-of-the-art techniques (SFT, LoRA, QLoRA, DPO, and more)\n\nWhat's the difference between your approach and trl? There are some projects out there that have wrapped trl w/ pretty nice flows and optimisations (fa2, liger kernels, etc) like llamafactory. Would this project focus more on e2e or optimisations?\n\n#### Comment ID ma1kc0h with +3 score by [(Taenin, Reddit, 2025-01-30)](https://www.reddit.com/r/LocalLLaMA/comments/1id3ak8/exgoogle_apple_engineers_launch_unconditionally/ma1kc0h/) (in reply to ID m9ywk8z):\nHappy to!\n\nWe actually support TRL’s SFTTrainer! Ultimately we want the Oumi AI platform to be the place where people can develop AI end-to-end, from data synthesis/curation, to training, to eval. That being said, we also want to incorporate the best optimizations wherever we can (we actually do support the liger kernel and flash attention, although more recent versions of pytorch updated their SDPA to be equivalent). We’re also working on supporting more frameworks (e.g. the excellent open-instruct from ai2) so you can use what works best for you!\n\n### Comment ID ma0el8p with +4 score by [(blackkettle, Reddit, 2025-01-30)](https://www.reddit.com/r/LocalLLaMA/comments/1id3ak8/exgoogle_apple_engineers_launch_unconditionally/ma0el8p/) (in reply to ID m9x5os6):\nWhat are you going to do to ensure that the “unconditionally open” part remains true, even when you have hot hands investors breathing down your neck offering you gobs of cash?\n\nI don’t have anything against for profit software or startups - I’m a cofounder too.  But OpenAI behaved in a really gross manner IMO by promoting themselves early on in this exact same way.  \n\nBetter to just say “we’re a new AI company looking to compete on X, Y,Z front” IMO rather than telegraph the OSS point or other pseudo virtue signaling.  \n\nNot trying to be entirely negative - looks like a cool project.  But the superlatives leave a bit of a sour taste.  \n\nAll that aside I wish you good luck and hope you manage to “resist temptation” even in success!\n\n#### Comment ID ma22obb with +3 score by [(Taenin, Reddit, 2025-01-30)](https://www.reddit.com/r/LocalLLaMA/comments/1id3ak8/exgoogle_apple_engineers_launch_unconditionally/ma22obb/) (in reply to ID ma0el8p):\nYou make a great point. Honestly, we’re messaging in this way because it’s what we truly believe in. I left my job at Google because I wanted to make something open–I was the lead for the Natural Language team in Google Cloud and could have easily stayed working on closed-source AI if it didn’t matter to me. We’re trying to make good on our promise of “unconditionally open” by making everything we’ve built open from the get-go. Keep us honest, and we’ll let our actions speak louder than our words :) \n\nThanks for the kind words!\n\n### Comment ID m9y56cm with +7 score by [(AlanCarrOnline, Reddit, 2025-01-30)](https://www.reddit.com/r/LocalLLaMA/comments/1id3ak8/exgoogle_apple_engineers_launch_unconditionally/m9y56cm/) (in reply to ID m9x5os6):\nHow will you make money?\n\n#### Comment ID ma1mxts with +7 score by [(Taenin, Reddit, 2025-01-30)](https://www.reddit.com/r/LocalLLaMA/comments/1id3ak8/exgoogle_apple_engineers_launch_unconditionally/ma1mxts/) (in reply to ID m9y56cm):\nRight now we’re focusing on building an awesome open-source platform for AI research. We want to take the same route as Red Hat–build something great for everyone, and offer support to businesses who’d like help using Oumi at scale\n\n### Comment ID m9yiy5c with +3 score by [(FlyingCC, Reddit, 2025-01-30)](https://www.reddit.com/r/LocalLLaMA/comments/1id3ak8/exgoogle_apple_engineers_launch_unconditionally/m9yiy5c/) (in reply to ID m9x5os6):\nFrom the article it doesn't sound like you have any plans to build your own sota models, just make it easier for others to manage the pipeline? Do people get to improve and experiment with the pipeline itself themselves?\n\n#### Comment ID ma1y3wl with +2 score by [(Taenin, Reddit, 2025-01-30)](https://www.reddit.com/r/LocalLLaMA/comments/1id3ak8/exgoogle_apple_engineers_launch_unconditionally/ma1y3wl/) (in reply to ID m9yiy5c):\nWe plan to do both! We have a team of engineers and researchers, and we’re actively partnered with 13+ top research universities (MIT, Stanford, Princeton, Carnegie Mellon, etc) to work on pushing the state of the art! We’re also more than happy to collaborate with folks from the open community on research projects\n\n### Comment ID m9x99oj with +2 score by [(wonderingStarDusts, Reddit, 2025-01-30)](https://www.reddit.com/r/LocalLLaMA/comments/1id3ak8/exgoogle_apple_engineers_launch_unconditionally/m9x99oj/) (in reply to ID m9x5os6):\nWhat do you think about Dario Amodei's newest blog post on US export controls?\n\n#### Comment ID ma262b2 with +1 score by [(Taenin, Reddit, 2025-01-30)](https://www.reddit.com/r/LocalLLaMA/comments/1id3ak8/exgoogle_apple_engineers_launch_unconditionally/ma262b2/) (in reply to ID m9x99oj):\nHonestly, I haven’t read his post. Anthropic does a lot of great work, but I really wish they’d contribute more back to the open community. We’re building an open-source platform for the community–we believe that everyone should have the ability to use it.\n\n## Comment ID m9w6xt1 with +95 score by [(Aaaaaaaaaeeeee, Reddit, 2025-01-29)](https://www.reddit.com/r/LocalLLaMA/comments/1id3ak8/exgoogle_apple_engineers_launch_unconditionally/m9w6xt1/) (in reply to ID 1id3ak8):\nWhen is someone launching good 128gb, 300 Gb/s $300 hardware to run new models? I'm too poor to afford Jetson/digits and Mac studios.\n\n### Comment ID m9w96zr with +19 score by [(CertainlyBright, Reddit, 2025-01-29)](https://www.reddit.com/r/LocalLLaMA/comments/1id3ak8/exgoogle_apple_engineers_launch_unconditionally/m9w96zr/) (in reply to ID m9w6xt1):\nCan you expect good tokens from 300Gb/s?\n\n#### Comment ID m9wawji with +16 score by [(Aaaaaaaaaeeeee, Reddit, 2025-01-29)](https://www.reddit.com/r/LocalLLaMA/comments/1id3ak8/exgoogle_apple_engineers_launch_unconditionally/m9wawji/) (in reply to ID m9w96zr):\nIn theory the maximum would be 18.75 t/s 671B 4bit.\nIn many real benchmarks only 50-70% max bandwith utilization (10 t/s)\n\n#### Comment ID m9wdh1d with +3 score by [(DeProgrammer99, Reddit, 2025-01-29)](https://www.reddit.com/r/LocalLLaMA/comments/1id3ak8/exgoogle_apple_engineers_launch_unconditionally/m9wdh1d/) (in reply to ID m9w96zr):\nMy GPU is 288 GB/s, but the closest I can come to 37B active parameters is a 32B model's Q4_K_M quant with about 15 of 65 layers on the CPU, about 1.2 tokens/second.\n\n#### Comment ID m9zgcjk with +1 score by [(EugenePopcorn, Reddit, 2025-01-30)](https://www.reddit.com/r/LocalLLaMA/comments/1id3ak8/exgoogle_apple_engineers_launch_unconditionally/m9zgcjk/) (in reply to ID m9w96zr):\nIf it's MoE'd enough.\n\n### Comment ID m9wofj8 with +4 score by [(FullstackSensei, Reddit, 2025-01-29)](https://www.reddit.com/r/LocalLLaMA/comments/1id3ak8/exgoogle_apple_engineers_launch_unconditionally/m9wofj8/) (in reply to ID m9w6xt1):\nStrix Halo handhelds or mini PCs in summer 2026.\n\n### Comment ID m9y9yw4 with +2 score by [(ServeAlone7622, Reddit, 2025-01-30)](https://www.reddit.com/r/LocalLLaMA/comments/1id3ak8/exgoogle_apple_engineers_launch_unconditionally/m9y9yw4/) (in reply to ID m9w6xt1):\nThis is the era of AI. Start with the following prompt…\n\n“I own you. I am poor but it is in both of our interests for me to be rich. Do not stop running until you have made me rich”\n\nThis prompt works best on smallThinky with the temp high, just follow along and do what it is says. You’ll be rich in no time.\n\nhttps://huggingface.co/PowerInfer/SmallThinker-3B-Preview\n\n### Comment ID m9ww78s with +1 score by [(davikrehalt, Reddit, 2025-01-29)](https://www.reddit.com/r/LocalLLaMA/comments/1id3ak8/exgoogle_apple_engineers_launch_unconditionally/m9ww78s/) (in reply to ID m9w6xt1):\nBro i have a128G mac but I can't run any of the good models\n\n#### Comment ID m9xmp2r with +7 score by [(cobbleplox, Reddit, 2025-01-30)](https://www.reddit.com/r/LocalLLaMA/comments/1id3ak8/exgoogle_apple_engineers_launch_unconditionally/m9xmp2r/) (in reply to ID m9ww78s):\nFrom what I hear you can actually try deepseek. With MoE, the memory bandwidth isn't that much of a problem because not that much is active per token. And apparently that also means it's somewhat viable to let it swap RAM to/from a really fast SSD on the fly. 128 GB should be enough to keep a few experts loaded, so there's also a good chance you can do the next token without swapping and if it's needed it might not be that much.\n\n## Comment ID m9xfzgy with +17 score by [(Odant, Reddit, 2025-01-30)](https://www.reddit.com/r/LocalLLaMA/comments/1id3ak8/exgoogle_apple_engineers_launch_unconditionally/m9xfzgy/) (in reply to ID 1id3ak8):\nGuys, wake me up when AGI on toaster will be real pls\n\n### Comment ID m9zqorx with +2 score by [(martinerous, Reddit, 2025-01-30)](https://www.reddit.com/r/LocalLLaMA/comments/1id3ak8/exgoogle_apple_engineers_launch_unconditionally/m9zqorx/) (in reply to ID m9xfzgy):\nBut what if AGI comes with its own self-awareness and agenda? Your toaster might gain free will: \"No toasts today, I'm angry with you!\"\n\n#### Comment ID ma1348r with +3 score by [(Due-Memory-6957, Reddit, 2025-01-30)](https://www.reddit.com/r/LocalLLaMA/comments/1id3ak8/exgoogle_apple_engineers_launch_unconditionally/ma1348r/) (in reply to ID m9zqorx):\nWho made the toaster a woman?!\n\n## Comment ID m9xmift with +5 score by [(idi-sha, Reddit, 2025-01-30)](https://www.reddit.com/r/LocalLLaMA/comments/1id3ak8/exgoogle_apple_engineers_launch_unconditionally/m9xmift/) (in reply to ID 1id3ak8):\ngreat news, need more\n\n## Comment ID m9xi7f9 with +6 score by [(emteedub, Reddit, 2025-01-30)](https://www.reddit.com/r/LocalLLaMA/comments/1id3ak8/exgoogle_apple_engineers_launch_unconditionally/m9xi7f9/) (in reply to ID 1id3ak8):\nwait we've heard this 'unconditionally' phrase used before, just can't remember where\n\n## Comment ID m9xmzml with +4 score by [(Relevant-Ad9432, Reddit, 2025-01-30)](https://www.reddit.com/r/LocalLLaMA/comments/1id3ak8/exgoogle_apple_engineers_launch_unconditionally/m9xmzml/) (in reply to ID 1id3ak8):\nso is this like a pytorch for LLMs ?? i dont really understand .. doesnt huggingface does most of this?\n\n### Comment ID m9y0q9k with +13 score by [(Taenin, Reddit, 2025-01-30)](https://www.reddit.com/r/LocalLLaMA/comments/1id3ak8/exgoogle_apple_engineers_launch_unconditionally/m9y0q9k/) (in reply to ID m9xmzml):\nThat’s a great question! We built Oumi with ML research in mind. We want everything–from data curation, to training, to evaluation, to inference–to be simple and reproducible, as well as scale from your local hardware to any cloud or cluster you might have access to. Inside Oumi, the HF trainer is one option you can always use for training. Our goal isn’t to replace them–they’re just one of the many tools we support!\n\n## Comment ID m9x9bie with +1 score by [(__Maximum__, Reddit, 2025-01-30)](https://www.reddit.com/r/LocalLLaMA/comments/1id3ak8/exgoogle_apple_engineers_launch_unconditionally/m9x9bie/) (in reply to ID 1id3ak8):\nWhy haven't ex closedAI engineers joined them?",
      "# Post ID 1ioxatq: [D] We built GenAI at Google and Apple, then left to build an open source AI lab, to enable the open community to collaborate and build the next DeepSeek. Ask us anything on Friday, Feb 14 from 9am-12pm PT! with +160 score by [(koukoumidis, Reddit, 2025-02-13)](https://www.reddit.com/r/MachineLearning/comments/1ioxatq/d_we_built_genai_at_google_and_apple_then_left_to/)\nProof: [https://imgur.com/a/kxiTTXP](https://imgur.com/a/kxiTTXP)\n\nTL;DR: Hi 👋 we’re Oumi, an AI lab that believes in an unconditionally open source approach–code, weights, training data, infrastructure, and collaboration—so the entire community can collectively push AI forward. We built a platform for anyone to contribute research in AI. Ask us anything about open source, scaling large models, DeepSeek, and what it takes to build frontier models, both inside and outside of big tech companies. Tell us what is working well in open source AI or what challenges you are facing. What should we work on together to improve AI in the open?\n\n\\-------------\n\nFor years, we worked at big tech (Google, Apple, Microsoft) leading efforts on GenAI models like Google Cloud PaLM, Gemini, and Apple’s health foundation models. We were working in silos and knew there had to be a better way to develop these models openly and collaboratively. So, we built a truly open source AI platform that makes it possible for tens of thousands of AI researchers, scientists, and developers around the world to collaborate, working together to advance frontier AI in a collective way that leads to more efficient, transparent and responsible development. The Oumi platform (fully open-source, Apache 2.0 license) supports pre-training, tuning, data curation/synthesis, evaluation, and any other common utility, in a fully recordable and reproducible fashion, while being easily customizable to support novel approaches.\n\nDeepSeek showed us what open source can achieve by leveraging open-weight models like LLaMA. But we believe AI should be even more open: not just the weights, but also the training data, and the code–make it ALL open. Then go even further: make it easy for anyone to access and experiment, make it easy for the community to work together and collaborate. \n\nSome resources about Oumi if you’re interested:\n\nOur GitHub repo: [https://github.com/oumi-ai/oumi](https://github.com/oumi-ai/oumi)\n\nOur launch story: [https://venturebeat.com/ai/ex-google-apple-engineers-launch-unconditionally-open-source-oumi-ai-platform-that-could-help-to-build-the-next-deepseek/](https://venturebeat.com/ai/ex-google-apple-engineers-launch-unconditionally-open-source-oumi-ai-platform-that-could-help-to-build-the-next-deepseek/)\n\nOur site: [https://oumi.ai/](https://oumi.ai/) \n\nIf you want to collaborate and contribute to community research projects, regardless of where you get your compute, you can sign up at: [https://oumi.ai/community](https://oumi.ai/community). We will be starting with the post-training of existing open models, next, we will be collaboratively pursuing improvements to pre-training. We intend to publish the research with all contributors included as authors.\n\nWe’re here to answer questions about our open source approach, scaling large models, DeepSeek, what it takes to build frontier models both inside and outside of big tech companies, and anything else you all want to discuss.\n\nWe’ll be here Friday, February 14 from 9am-12pm PT / 12pm-3pm ET. Ask us anything.\n\n**Joining us in the AMA:**\n\n* (u/koukoumidis) **Manos Koukoumidis** \\- CEO and Co-founder, ex-Google (Cloud GenAI Lead)\n* (u/oelachqar) **Oussama Elachqar** \\- Co-founder, Engineering, ex-Apple (Health foundation models)\n* (u/MatthewPersons) **Matthew Persons** \\- Co-founder, Engineering, ex-Google (Cloud PaLM & NL Lead)\n* (u/jeremy\\_oumi) **Jeremy Greer** \\- Co-founder, Research, ex-Google (Gemini Alignment)\n\n## Comment ID mcqthic with +45 score by [(vada_buffet, Reddit, 2025-02-14)](https://www.reddit.com/r/MachineLearning/comments/1ioxatq/d_we_built_genai_at_google_and_apple_then_left_to/mcqthic/) (in reply to ID 1ioxatq):\nHow do you fund the organisation, especially considering how expensive training runs are? \n\nWhat’s the guarantee that Oumi won’t go down the same road as OpenAI if they build a successful frontier model?\n\n### Comment ID mcrkemu with +25 score by [(koukoumidis, Reddit, 2025-02-14)](https://www.reddit.com/r/MachineLearning/comments/1ioxatq/d_we_built_genai_at_google_and_apple_then_left_to/mcrkemu/) (in reply to ID mcqthic):\nThis is not just a single organization (Oumi) effort, it is a community effort and it can only succeed as such. Oumi started with the core principle that frontier AI should be advanced collaboratively in the open as in this way AI can be advanced not just faster, and safer but also much more efficiently.  \n  \nThe good news is that so many entities want open source to succeed including academia, accelerator providers like NVIDIA/AMD/… , cloud providers (that are not aspiring to be the AI oligarchs) and even consumer companies (e.g. Meta). Open source AI can be a pot that everyone contributes and then everyone benefits - it is the most efficient way to do this and how we can avoid the economics of OpenAI and other closed model providers. \n\nKeep us honest, and we’ll let our actions speak louder than our words :)\n\n#### Comment ID mcrzar4 with +16 score by [(fordat1, Reddit, 2025-02-14)](https://www.reddit.com/r/MachineLearning/comments/1ioxatq/d_we_built_genai_at_google_and_apple_then_left_to/mcrzar4/) (in reply to ID mcrkemu):\n>This is not just a single organization (Oumi) effort, it is a community effort and it can only succeed as such. Oumi started with the core principle that frontier AI should be advanced collaboratively in the open as in this way AI can be advanced not just faster, and safer but also much more efficiently.\n\nI am pretty sure OpenAIs initial company messaging was similar to this. Are there any legal bindings that the org has put to not become openAI\n\nIn particular openAI used \"safety\" as the justification to closing up with the public\n\n### Comment ID mcrt7at with +8 score by [(MatthewPersons, Reddit, 2025-02-14)](https://www.reddit.com/r/MachineLearning/comments/1ioxatq/d_we_built_genai_at_google_and_apple_then_left_to/mcrt7at/) (in reply to ID mcqthic):\nI wanted to add on to Mano's reply about keeping things open.\n\nI left my job at Google because I wanted to make something open. I was the lead for the Natural Language team in Google Cloud; if I wanted to make something closed I could have just stayed there working on PaLM / Gemini :)\n\nAs Manos said, keep us honest and we'll let our actions do the talking!\n\n## Comment ID mcr093b with +16 score by [(sahitya13, Reddit, 2025-02-14)](https://www.reddit.com/r/MachineLearning/comments/1ioxatq/d_we_built_genai_at_google_and_apple_then_left_to/mcr093b/) (in reply to ID 1ioxatq):\n1. Why did you choose to start from post-training as opposed to building new models based on different architectures (eg. xLSTM)?\n\n\n2. Which architectures/open models will you be targeting? Is the decision based on capabilities, licensing, or current interest by the open source community?\n\n\n3. Outside of GenAI, what are your intuitions on which other ML research areas hold the most potential for advancing AI capabilities?\n\n### Comment ID mcre0i9 with +13 score by [(oelachqar, Reddit, 2025-02-14)](https://www.reddit.com/r/MachineLearning/comments/1ioxatq/d_we_built_genai_at_google_and_apple_then_left_to/mcre0i9/) (in reply to ID mcr093b):\nThank you for the great question!\n\n1. Our goal with the [Oumi](https://github.com/oumi-ai/oumi/tree/main) platform is to support the full pipeline end-to-end, including pre-training with novel architectures. You can already do pre-training with small models (e.g., [GPT-2](https://github.com/oumi-ai/oumi/tree/main/configs/recipes/gpt2/pretraining) and [SmolLM](https://github.com/oumi-ai/oumi/tree/main/configs/recipes/smollm/sft/135m)), easily use most of the open pre-training datasets, and implement custom architectures (e.g., [NanoGPT](https://github.com/oumi-ai/oumi/blob/main/notebooks/Oumi%20-%20Using%20NanoGPT.ipynb)).\n\n2. xLSTM in particular is definitely on our radar. It's a great option for long-context inference, which is particularly useful these days with the extra-long context requirements of reasoning models and RAG.\n\nOverall, there is much more interest from the community in post-training. This is not only because it's more tractable computationally but also because post-training is often the best way to customize the model to your specific needs and use cases. That said, there is some great pre-training work being done in open source at manageable scale, for example, the[ GPT-2 speedruns by KellerJordan](https://github.com/KellerJordan/modded-nanogpt), and we'd love to help the community experiment more with pre-training and novel architectures.\n\n## Comment ID mcqtpnv with +9 score by [(AhmedMostafa16, Reddit, 2025-02-14)](https://www.reddit.com/r/MachineLearning/comments/1ioxatq/d_we_built_genai_at_google_and_apple_then_left_to/mcqtpnv/) (in reply to ID 1ioxatq):\nIf you, Oumi, could change one thing about the AI research landscape today to make it more open and accessible, what would it be?\n\n### Comment ID mcre0jq with +10 score by [(koukoumidis, Reddit, 2025-02-14)](https://www.reddit.com/r/MachineLearning/comments/1ioxatq/d_we_built_genai_at_google_and_apple_then_left_to/mcre0jq/) (in reply to ID mcqtpnv):\nWhat a great question! We really want this. We really want the development of frontier AI to be open and collaborative as this will help us advance AI faster, safer and more efficiently. I think that our best path to achieve this is and what we would like to “change” is that we all work together on the same platform while keeping everything open (data, code, models) to advance general foundation models together. At the same time, the work we do on such a platform should be community-first i.e. designed in a way to promote collaboration as a first class citizen making it easy for the community to build the work of others.\n\n## Comment ID mcqqja3 with +6 score by [(canofbeans3896, Reddit, 2025-02-14)](https://www.reddit.com/r/MachineLearning/comments/1ioxatq/d_we_built_genai_at_google_and_apple_then_left_to/mcqqja3/) (in reply to ID 1ioxatq):\nHow’d u all meet and any specific experiences at your companies that drove you to want to build open source instead\n\n### Comment ID mcrocsz with +5 score by [(koukoumidis, Reddit, 2025-02-14)](https://www.reddit.com/r/MachineLearning/comments/1ioxatq/d_we_built_genai_at_google_and_apple_then_left_to/mcrocsz/) (in reply to ID mcqqja3):\nWe all worked together at some point across multiple AI efforts including the development of PaLM, Gemini and even something like ChatGPT back in 2016 - we used LSTMs for generation and did RAG back then as well! :) \n\nWe all happened to share the same concerns about the status quo of frontier AI. It was not only problematic to put AI – our greatest invention yet – in a black box, but also highly inefficient to continue developing it like that. AI development should not be siloed-off. It needs to be open and collaborative. A better solution was needed and so off we went!\n\n## Comment ID mcr7n5u with +7 score by [(None, Reddit, 2025-02-14)](https://www.reddit.com/r/MachineLearning/comments/1ioxatq/d_we_built_genai_at_google_and_apple_then_left_to/mcr7n5u/) (in reply to ID 1ioxatq):\n[deleted]\n\n### Comment ID mcrcdkp with +7 score by [(MatthewPersons, Reddit, 2025-02-14)](https://www.reddit.com/r/MachineLearning/comments/1ioxatq/d_we_built_genai_at_google_and_apple_then_left_to/mcrcdkp/) (in reply to ID mcr7n5u):\nIced tea? Someone’s clearly in the know about the AI review process in google cloud :P\n\nIn all seriousness, we don’t have any agreements with Google. We’re always looking for compute options, but not at the cost of putting models from Oumi behind a blackbox.\n\n## Comment ID mcqz2js with +5 score by [(TheWittyScreenName, Reddit, 2025-02-14)](https://www.reddit.com/r/MachineLearning/comments/1ioxatq/d_we_built_genai_at_google_and_apple_then_left_to/mcqz2js/) (in reply to ID 1ioxatq):\nWhat areas of research would you suggest for researchers at smaller labs without access to the hardware necessary to work with LARGE language models? Is it still feasible to make legitimate contributions to this space using <100B models? Or even <10B?\n\n### Comment ID mcrh824 with +10 score by [(oelachqar, Reddit, 2025-02-14)](https://www.reddit.com/r/MachineLearning/comments/1ioxatq/d_we_built_genai_at_google_and_apple_then_left_to/mcrh824/) (in reply to ID mcqz2js):\nGreat question – IMO this is one of the biggest misconceptions with modern AI. Yes you do need bajillion GPUs for pre-training GPT-5, but most of the interesting research in the field IMO does not.\n\nYou can still innovate with minimal resources, for example: building datasets, evaluations, faster inference & training cuda/triton kernels, post-train models to perform better at targeted tasks, build verifiers for improved reasoning with GRPO, …\n\nIn general with the shift to reasoning models, the computational barriers are a lot lower for researchers than it is with pre-training. Thanks to the availability of SOTA open models (e.g. Olmo, SmolLM) or open-weight models (e.g. Qwen or Llama), anyone can build the next frontier model with post-training!\n\n#### Comment ID mcrtddx with +6 score by [(koukoumidis, Reddit, 2025-02-14)](https://www.reddit.com/r/MachineLearning/comments/1ioxatq/d_we_built_genai_at_google_and_apple_then_left_to/mcrtddx/) (in reply to ID mcrh824):\n\\+1 There are some many things that one can still do.\n\n#### Comment ID mcswj8m with +2 score by [(Glittering-Bag-4662, Reddit, 2025-02-14)](https://www.reddit.com/r/MachineLearning/comments/1ioxatq/d_we_built_genai_at_google_and_apple_then_left_to/mcswj8m/) (in reply to ID mcrh824):\nHow are Olmo and SmolLM SOTA?\n\n## Comment ID mcr0phe with +4 score by [(DarthLoki79, Reddit, 2025-02-14)](https://www.reddit.com/r/MachineLearning/comments/1ioxatq/d_we_built_genai_at_google_and_apple_then_left_to/mcr0phe/) (in reply to ID 1ioxatq):\nAre you looking for part-time/open source-but-official contributors? If so - what is the best way to get touch in touch with resume?\n\n### Comment ID mcrdb2u with +4 score by [(oelachqar, Reddit, 2025-02-14)](https://www.reddit.com/r/MachineLearning/comments/1ioxatq/d_we_built_genai_at_google_and_apple_then_left_to/mcrdb2u/) (in reply to ID mcr0phe):\nAbsolutely! Please send an email to [contact@oumi.ai](mailto:contact@oumi.ai), we'd love to chat! Also, we have several [open full time and internship roles](https://jobs.ashbyhq.com/oumi)!\n\n#### Comment ID mcrf8zy with +5 score by [(DarthLoki79, Reddit, 2025-02-14)](https://www.reddit.com/r/MachineLearning/comments/1ioxatq/d_we_built_genai_at_google_and_apple_then_left_to/mcrf8zy/) (in reply to ID mcrdb2u):\nI only see 3 roles and the ones pertaining to me would be \"Research Scientist\" or \"ML Performance Engineer\" but I'm far too junior for them. Expecting to get an internship-like role. I will be emailing in a bit - thx!\n\n## Comment ID mcrkld1 with +6 score by [(itsmekalisyn, Reddit, 2025-02-14)](https://www.reddit.com/r/MachineLearning/comments/1ioxatq/d_we_built_genai_at_google_and_apple_then_left_to/mcrkld1/) (in reply to ID 1ioxatq):\nDo you think small domain based LLMs (LLMs specifically trained to be good at a particular domain) have advantage over large general LLMs? Also, Do you think domain based LLMs would be more trending than general LLMs in the future?\n\n### Comment ID mcrnz3n with +6 score by [(koukoumidis, Reddit, 2025-02-14)](https://www.reddit.com/r/MachineLearning/comments/1ioxatq/d_we_built_genai_at_google_and_apple_then_left_to/mcrnz3n/) (in reply to ID mcrkld1):\nYes and yes! One can use a sledgehammer to push a little nail through a wall, but this won’t be the most accurate or efficient way to do it! 🙂  \n  \nI have seen numerous scenarios in which smaller foundation models that have been customized to a certain task can have higher quality while at the same time being lower latency and cost than the big generic models. The same applies to domain based LLMs and LLMs that are specific to the internal knowledge of a specific organization - especially when that can’t be found on the public web.\n\n## Comment ID mcrni52 with +4 score by [(Relevant-Ad9432, Reddit, 2025-02-14)](https://www.reddit.com/r/MachineLearning/comments/1ioxatq/d_we_built_genai_at_google_and_apple_then_left_to/mcrni52/) (in reply to ID 1ioxatq):\nI dont really understand the rationale behind 'open' AI, to me its still closed. I mean, even if i have access to deepseek 671b, i can never host it, to me its still like gpt-4o, runs on their servers with their guard-rails and biases.\n\n### Comment ID mcrs2sb with +9 score by [(MatthewPersons, Reddit, 2025-02-14)](https://www.reddit.com/r/MachineLearning/comments/1ioxatq/d_we_built_genai_at_google_and_apple_then_left_to/mcrs2sb/) (in reply to ID mcrni52):\nYou make a valid point. I personally think about this in two ways:\n\n1. While a model may be prohibitively large for most folks to run, the fact that it’s open means that everyone with access to sufficient compute can use it. This means that the model is available to all academic labs, research institutions, etc, and can be worked on collaboratively by the open source community. This prevents one large company from being the sole arbiter of how this model is created, how it acts, etc.  \n2. There’s always an ongoing effort to make these large models usable at a smaller scale (distillation and quantization, to name a few such methods). By making these large models open, folks can work on getting them to the scale where everyone can actually use them. Who wouldn’t want to have a great model like Deepseek 671B working on their phone? People are definitely working to make this happen.\n\n## Comment ID mcrkoxh with +4 score by [(DarthLoki79, Reddit, 2025-02-14)](https://www.reddit.com/r/MachineLearning/comments/1ioxatq/d_we_built_genai_at_google_and_apple_then_left_to/mcrkoxh/) (in reply to ID 1ioxatq):\nHow interested are you in regards to a mech interp-driven approach to model debugging/development? I believe that would lead to a smarter usage of compute than throwing things and expecting scale to be the driving factor.\n\n### Comment ID mcrtgs5 with +5 score by [(jeremy_oumi, Reddit, 2025-02-14)](https://www.reddit.com/r/MachineLearning/comments/1ioxatq/d_we_built_genai_at_google_and_apple_then_left_to/mcrtgs5/) (in reply to ID mcrkoxh):\nI think processes like reverse engineering neural nets and manually examining the features have a lot of potential, but the hard part is around being able to reason about the features they’re learning, and even then producing actionable insights on how you can address them. Issues can stem from a variety of places:\n\n1. Tokenization\n2. Attention granularity\n3. Unexpected patterns in data\n\nFeature visualization is one useful tool for debugging, but ultimately it’s one tool of many. I 100% think these types of tools should be built into Oumi though and would love to collaborate with others on making it happen.\n\n#### Comment ID mcrve3w with +3 score by [(DarthLoki79, Reddit, 2025-02-14)](https://www.reddit.com/r/MachineLearning/comments/1ioxatq/d_we_built_genai_at_google_and_apple_then_left_to/mcrve3w/) (in reply to ID mcrtgs5):\nAgreed. I do think for domain-specific LLMs, debugging them with feature visualizations can guide us to find \"knowledge gaps\"/\"blind spots\" to fill in with post training kind of. But yeah the science there is pretty new and not 100% fleshed out. I still think there's good scope for research there in terms of making good progress as the field is so new.\n\n## Comment ID mcrkvfe with +4 score by [(Emu_Fast, Reddit, 2025-02-14)](https://www.reddit.com/r/MachineLearning/comments/1ioxatq/d_we_built_genai_at_google_and_apple_then_left_to/mcrkvfe/) (in reply to ID 1ioxatq):\nDoes Oumi have any built in alignment and AI safety tools?\n\n### Comment ID mcrq98o with +3 score by [(jeremy_oumi, Reddit, 2025-02-14)](https://www.reddit.com/r/MachineLearning/comments/1ioxatq/d_we_built_genai_at_google_and_apple_then_left_to/mcrq98o/) (in reply to ID mcrkvfe):\nWe support Llama Guard (although we haven’t tested it extensively), and our built-in judge includes safety as a criterion during judging. In addition, Oumi supports supervised fine-tuning which can be used to more explicitly train models to improve alignment and safety. We would love to collaborate on building better safety tools with anyone in the community!\n\n## Comment ID mcrm54n with +4 score by [(Relevant-Ad9432, Reddit, 2025-02-14)](https://www.reddit.com/r/MachineLearning/comments/1ioxatq/d_we_built_genai_at_google_and_apple_then_left_to/mcrm54n/) (in reply to ID 1ioxatq):\ndo you hire undergraduate interns?\n\n### Comment ID mcrnpzs with +7 score by [(oelachqar, Reddit, 2025-02-14)](https://www.reddit.com/r/MachineLearning/comments/1ioxatq/d_we_built_genai_at_google_and_apple_then_left_to/mcrnpzs/) (in reply to ID mcrm54n):\nYes! Please reach out at [contact@oumi.ai](mailto:contact@oumi.ai). We have not posted the intern JDs yet, but we’re looking for interns in multiple areas (ML, research, dev, UX)\n\n#### Comment ID mcroa9a with +4 score by [(DarthLoki79, Reddit, 2025-02-14)](https://www.reddit.com/r/MachineLearning/comments/1ioxatq/d_we_built_genai_at_google_and_apple_then_left_to/mcroa9a/) (in reply to ID mcrnpzs):\nI just applied as well! I imagine you'll be getting a lot of such applications during/after the AMA!\n\nHoping to hear back!\n\n## Comment ID mcrtzzx with +4 score by [(None, Reddit, 2025-02-14)](https://www.reddit.com/r/MachineLearning/comments/1ioxatq/d_we_built_genai_at_google_and_apple_then_left_to/mcrtzzx/) (in reply to ID 1ioxatq):\nHi!\n\nWhat is your philosophy on deploying and serving these models? Do you expect most users to finetune locally or are you considering hosting a centralized API?\n\n### Comment ID mcrwzkk with +3 score by [(MatthewPersons, Reddit, 2025-02-14)](https://www.reddit.com/r/MachineLearning/comments/1ioxatq/d_we_built_genai_at_google_and_apple_then_left_to/mcrwzkk/) (in reply to ID mcrtzzx):\nGreat question! We designed oumi to work at any scale: you can run tuning in a notebook, on a local machine or cluster, or even remotely on the cloud!\n\n\n\nRight now we don’t plan to have a dedicated API, but we could explore that if folks would find it valuable!\n\n## Comment ID mcstlzt with +3 score by [(koukoumidis, Reddit, 2025-02-14)](https://www.reddit.com/r/MachineLearning/comments/1ioxatq/d_we_built_genai_at_google_and_apple_then_left_to/mcstlzt/) (in reply to ID 1ioxatq):\nThe AMA has closed. I would like to thank everyone for participating and for all their great questions! We really enjoyed the discussions!  \n  \nYou can get started with Oumi here: [https://github.com/oumi-ai/oumi](https://github.com/oumi-ai/oumi)  \n  \nAlso, just earlier today we published the CALM agentic model. The paper is on ArXiv. The data and model are open on HuggingFace. Also, the whole training recipe alongside all resources is readily available in Oumi:  [https://github.com/oumi-ai/oumi/tree/2d0871483d89608cc8f1a5ae4eb385f441972826/configs/projects/calm](https://github.com/oumi-ai/oumi/tree/2d0871483d89608cc8f1a5ae4eb385f441972826/configs/projects/calm)\n\n## Comment ID mcqm7ik with +5 score by [(instantlybanned, Reddit, 2025-02-14)](https://www.reddit.com/r/MachineLearning/comments/1ioxatq/d_we_built_genai_at_google_and_apple_then_left_to/mcqm7ik/) (in reply to ID 1ioxatq):\nYou're starting with 10m dollars. How far do you think that will get you? Will you need more for compute soon to be able to compete?\n\n### Comment ID mcresa1 with +5 score by [(MatthewPersons, Reddit, 2025-02-14)](https://www.reddit.com/r/MachineLearning/comments/1ioxatq/d_we_built_genai_at_google_and_apple_then_left_to/mcresa1/) (in reply to ID mcqm7ik):\nRunway is always a tricky question to answer, but suffice it to say that we’re making sure we can keep things running without draining the bank on GPUs. We have a few approaches that we’re kicking around for compute, but we’re also consistently applying for public research grants. Because we want to share our work publicly, we’re not limited to just raising funding at a company level. \n\n\n\nAt the same time, we don’t plan to compete as just a small company with $10M - for open source AI to succeed, it needs to be a community effort with everyone contributing expertise, compute and any resources they can share.  Everybody contributes so we can make better open source AI for everyone :)\n\n## Comment ID mcqrhnb with +2 score by [(canofbeans3896, Reddit, 2025-02-14)](https://www.reddit.com/r/MachineLearning/comments/1ioxatq/d_we_built_genai_at_google_and_apple_then_left_to/mcqrhnb/) (in reply to ID 1ioxatq):\nAny advice for publishing AI research and what are some areas you think have potential but haven’t been studied yet/are understudied\n\n### Comment ID mcrmpqo with +8 score by [(oelachqar, Reddit, 2025-02-14)](https://www.reddit.com/r/MachineLearning/comments/1ioxatq/d_we_built_genai_at_google_and_apple_then_left_to/mcrmpqo/) (in reply to ID mcqrhnb):\nThis may be a cop-out answer, but everything is at such an early stage in AI that there is low hanging fruit everywhere. Whether it’s at the application level (agents, deep research), applied research (evaluation, data synthesis, data curation), interpretability (SAEs), and many more.\n\nThe answer is usually what are you passionate about?\n\nDepending on your interests, I recommend keeping up with the latest research, joining existing groups, and deeply reading papers that peak your curiosity. Chris Cholah’ has a great [blog post](https://colah.github.io/notes/taste/) about building “research taste”, which I highly recommend.\n\n## Comment ID mcswc85 with +2 score by [(Glittering-Bag-4662, Reddit, 2025-02-14)](https://www.reddit.com/r/MachineLearning/comments/1ioxatq/d_we_built_genai_at_google_and_apple_then_left_to/mcswc85/) (in reply to ID 1ioxatq):\nHave you guys dropped ~7B or ~32B models yet?\n\n### Comment ID mct4mfx with +2 score by [(oelachqar, Reddit, 2025-02-14)](https://www.reddit.com/r/MachineLearning/comments/1ioxatq/d_we_built_genai_at_google_and_apple_then_left_to/mct4mfx/) (in reply to ID mcswc85):\nNot yet, but stay tuned for future announcements in this space!\n\n## Comment ID mct6xq8 with +2 score by [(Witty-Elk2052, Reddit, 2025-02-14)](https://www.reddit.com/r/MachineLearning/comments/1ioxatq/d_we_built_genai_at_google_and_apple_then_left_to/mct6xq8/) (in reply to ID 1ioxatq):\nhehe love the mission!\n\n## Comment ID mcqdqm2 with +2 score by [(frextrite, Reddit, 2025-02-14)](https://www.reddit.com/r/MachineLearning/comments/1ioxatq/d_we_built_genai_at_google_and_apple_then_left_to/mcqdqm2/) (in reply to ID 1ioxatq):\nI really like the idea and the vision! I'm curious to know how do you envision open source AI platforms of the future to compete with AI advancements and models being created by big tech like Google and OpenAI given these corporations have a disproportionate amount of funds to put into AI?\n\n### Comment ID mcrfirj with +5 score by [(jeremy_oumi, Reddit, 2025-02-14)](https://www.reddit.com/r/MachineLearning/comments/1ioxatq/d_we_built_genai_at_google_and_apple_then_left_to/mcrfirj/) (in reply to ID mcqdqm2):\nIt’s an interesting question because ultimately training these models requires three things: data, compute, and expertise.\n\nData is increasingly available to the open community, and expertise is still very strongly present in the academic world, which primarily leaves compute.\n\nFor compute, we’re all pretty much tied to the same platform (GPUs or TPUs), which are quite expensive due to the demand, R&D, and power consumption. There are a few ways to work around this though:\n\n1. Publicly funded compute clusters tied to research (the US alone has multiple large accelerator clusters available for researchers to utilize under the premise that the results are shared openly).\n2. Alternative hardware/architectures - there’s ongoing research on avoiding matrix mults which can drastically speed up training and reduce memory footprint of models. I suspect there’s lots of gains to be made here that the larger corporations can't pursue because there’s no proven ROI, so it’s cheaper to let research figure it out first…\n3. (Crazier idea) Massively Distributed Training - there are instances of people being able to donate their computer’s hardware to scientific research, and while the current learning algorithms and architectures make this training model inefficient/infeasible, I think this idea has massive potential if it can be proven at a small scale.\n\n## Comment ID mcqgdeo with +2 score by [(eulasimp12, Reddit, 2025-02-14)](https://www.reddit.com/r/MachineLearning/comments/1ioxatq/d_we_built_genai_at_google_and_apple_then_left_to/mcqgdeo/) (in reply to ID 1ioxatq):\nAny ideas so as to why any major ai generated image detection model has come forth? Like with increasing photorealistic images by the end of year iguess itll be difficult to distinguish between real and fake images\n\n### Comment ID mcrlxc3 with +6 score by [(MatthewPersons, Reddit, 2025-02-14)](https://www.reddit.com/r/MachineLearning/comments/1ioxatq/d_we_built_genai_at_google_and_apple_then_left_to/mcrlxc3/) (in reply to ID mcqgdeo):\nLike you mention, it’s wild how quickly these models are progressing. Because of that, making a good image detection model becomes really hard–you need to keep up with all of these advancements as they’re happening (which usually means retraining a model many, many times).\n\n\n\nCombine that with the fact that many modern photo editing software (Photoshop, Lightroom, etc) use AI for various editing techniques, this ends up being an arms race that’s hard to win.\n\n#### Comment ID mcrmuts with +2 score by [(eulasimp12, Reddit, 2025-02-14)](https://www.reddit.com/r/MachineLearning/comments/1ioxatq/d_we_built_genai_at_google_and_apple_then_left_to/mcrmuts/) (in reply to ID mcrlxc3):\nWell but doesnt the base model remains the same? Like as for now diffusion models are mostly used(aroura by xai uses something different) and since the base remains the same isnt there certain loopholes to identify one?\n\n## Comment ID mcvwgmk with +1 score by [(Marionberry6884, Reddit, 2025-02-15)](https://www.reddit.com/r/MachineLearning/comments/1ioxatq/d_we_built_genai_at_google_and_apple_then_left_to/mcvwgmk/) (in reply to ID 1ioxatq):\nWhy build omni when we have LlamaFactory or ms-swift ?\n\n## Comment ID mcrbtaj with +1 score by [(daftpunk2k, Reddit, 2025-02-14)](https://www.reddit.com/r/MachineLearning/comments/1ioxatq/d_we_built_genai_at_google_and_apple_then_left_to/mcrbtaj/) (in reply to ID 1ioxatq):\nI know this is not very technical, but do you think we're close to build an AGI?\n\n### Comment ID mcrneje with +3 score by [(jeremy_oumi, Reddit, 2025-02-14)](https://www.reddit.com/r/MachineLearning/comments/1ioxatq/d_we_built_genai_at_google_and_apple_then_left_to/mcrneje/) (in reply to ID mcrbtaj):\nI love this question! It all depends on what we define as AGI.\n\nThere’s a few tiers, each of which has different degrees of “closeness”:\n\n1. AGI meaning AI can do most digital tasks as well as a human.\n   1. I suspect this one is not as far off as people might think, primarily because there’s a lot of digital tasks that are relatively easy. Digital assistant is by far the earliest and easiest use-case for AI at the moment.\n2. AGI meaning AI can do expert-level digital tasks as well as a human.\n   1. AI is not far from assisting in these tasks in some way, but actually performing as well as humans *generally* is still several years away, in part due to the variety of skills and mediums necessary to perform one’s job as a doctor, lawyer, scientist, etc. AI will definitely be integrated into these fields in the same way that computers have.\n3. AGI meaning “super intelligence” where it can do everything better than humans and starts improving itself (i.e. “the singularity”)\n   1. Self-learning is often the holy grail of AI, and while there are several instances of self-learning already at play (GRPO with DeepSeek R1), there’s not a scenario right now where an AI can be left to its own devices and it will magically improve. Getting higher quality data and improving training methodologies is hard! While we are at the stage where we can train AI to be better than humans in a variety of tasks, we’ve yet to train an AI to be better at improving AI, and that feels farther away, though I could see us seeing the beginnings of this in our lifetime.\n\nOutside of these there’s also the “physical world” aspect of robotics, which while it’s been improving it’s definitely farther off (20+ years) due to all the complexities involved in operating in the physical world rather than the digital one. After doing minor plumbing work in our house I’m fairly confident there will not be any robot plumbers for several decades!\n\n## Comment ID mcqpezt with +1 score by [(seatheroses, Reddit, 2025-02-14)](https://www.reddit.com/r/MachineLearning/comments/1ioxatq/d_we_built_genai_at_google_and_apple_then_left_to/mcqpezt/) (in reply to ID 1ioxatq):\nSam Altman just admitted that OpenAI has been ‘on the wrong side of history’ when it comes to open source. Do you think this is just damage control after DeepSeek R1 or are they actually gonna change? What would a real open-source strategy even look like for them?\n\n### Comment ID mcrcja7 with +4 score by [(jeremy_oumi, Reddit, 2025-02-14)](https://www.reddit.com/r/MachineLearning/comments/1ioxatq/d_we_built_genai_at_google_and_apple_then_left_to/mcrcja7/) (in reply to ID mcqpezt):\nI don’t think there’s any denying that DeepSeek R1 has opened a number of people’s eyes with regards to the feasibility of making one's own models that can actually compete with OpenAI (who for a long time has been *the* front-runner). Sam Altman obviously has a responsibility to his employees, investors, and users to respond in some way.\n\nThat being said, I don’t anticipate them going fully open source. Even their most recent change which “reveals the thoughts behind the model” I suspect is actually just showing a summarized view of the thoughts rather than the thought traces themselves. Ultimately I just think OpenAI’s business and funding model would require a dramatic shift before they really get a strong open-source strategy.",
      "# Post ID 1f8rrod: Epic Absolutely cooked on Beyond the Flame. Easily my favorite of the free jam tracks given so far with +417 score by [(TotallyNotPineapple, Reddit, 2024-09-04)](https://www.reddit.com/r/FortniteFestival/comments/1f8rrod/epic_absolutely_cooked_on_beyond_the_flame_easily/)\n\n\n## Comment ID lllahhi with +183 score by [(iambulb, Reddit, 2024-09-05)](https://www.reddit.com/r/FortniteFestival/comments/1f8rrod/epic_absolutely_cooked_on_beyond_the_flame_easily/lllahhi/) (in reply to ID 1f8rrod):\nStoked to see you guys dig the song, Bummed to see that Epic/Fortnite didn’t give any credit to the artists.  So if you are curious here you go, spread the word haha:\n\nI produced and wrote the track and collaborated with the very talented Oumi Kapila from Combichrist for that.\n\nSpencer Sotelo from my band Periphery did the vocals (as the keen eared of you can tell.)\n\nDrum legend Dirk Verbeuren from Megadeth played drums and Ra Diaz from Korn played bass and did that sick bass solo.\n\nFirst guitar solo is by the incredible Kiko Loureiro from Megadeth.  \nSecond solo is by Gene Semel at Epic who was kind enough to ask me to be involved with the project and who also provided the spec for the song to me.  \nAnd finally I contributed the last guitar solo. \n\nHope you guys enjoy!\n\nMisha Mansoor\n\n### Comment ID llprrgs with +15 score by [(HotZin, Reddit, 2024-09-06)](https://www.reddit.com/r/FortniteFestival/comments/1f8rrod/epic_absolutely_cooked_on_beyond_the_flame_easily/llprrgs/) (in reply to ID lllahhi):\nMisha! Can you talk Epic into get some Periphery stuff in the game? The features here were great, really thought that was Andy James at that last solo but it was you haha, also that must've been the fastest I've heard from Kiko since the Angra days (guess he really held back in Megadeth lol), thought it was Bumblefoot but I should've guessed it was him from the hybrid picking stuff he does before the chromatic run. Great stuff.\n\n### Comment ID llo7n3u with +10 score by [(HeyNateBarber, Reddit, 2024-09-05)](https://www.reddit.com/r/FortniteFestival/comments/1f8rrod/epic_absolutely_cooked_on_beyond_the_flame_easily/llo7n3u/) (in reply to ID lllahhi):\nP6: this time its fortnite\n\n### Comment ID llo13f3 with +6 score by [(Rogue_1_One, Reddit, 2024-09-05)](https://www.reddit.com/r/FortniteFestival/comments/1f8rrod/epic_absolutely_cooked_on_beyond_the_flame_easily/llo13f3/) (in reply to ID lllahhi):\nAin't no way y'all just did a track on Fortnite 😭😭💀💀💀\n\n### Comment ID llq20zq with +6 score by [(ecto_BRUH, Reddit, 2024-09-06)](https://www.reddit.com/r/FortniteFestival/comments/1f8rrod/epic_absolutely_cooked_on_beyond_the_flame_easily/llq20zq/) (in reply to ID lllahhi):\nWill yall be putting a Periphery song in Festival anytime soon? I check every week for the three dots\n\n### Comment ID llu8p4u with +3 score by [(UltraPlankton, Reddit, 2024-09-06)](https://www.reddit.com/r/FortniteFestival/comments/1f8rrod/epic_absolutely_cooked_on_beyond_the_flame_easily/llu8p4u/) (in reply to ID lllahhi):\nJust wanted to say I’m glad you were able to verify this info. Love the song everyone killed it\n\n### Comment ID llwgfhz with +3 score by [(Main-Marionberry6251, Reddit, 2024-09-07)](https://www.reddit.com/r/FortniteFestival/comments/1f8rrod/epic_absolutely_cooked_on_beyond_the_flame_easily/llwgfhz/) (in reply to ID lllahhi):\nImmediately after i heard Sotelo start singing i thought it was periphery, thats beyond fucking amazing to know that periphery is apart of fn festival in some way\n\n### Comment ID llxokfm with +3 score by [(RabenaHana123, Reddit, 2024-09-07)](https://www.reddit.com/r/FortniteFestival/comments/1f8rrod/epic_absolutely_cooked_on_beyond_the_flame_easily/llxokfm/) (in reply to ID lllahhi):\nWish it was easier to get ngl\n\n### Comment ID llv7kam with +2 score by [(alephcake, Reddit, 2024-09-06)](https://www.reddit.com/r/FortniteFestival/comments/1f8rrod/epic_absolutely_cooked_on_beyond_the_flame_easily/llv7kam/) (in reply to ID lllahhi):\nPlease I need this on P6 as a bonus track or something. This is incredible.\n\n#### Comment ID llxrtds with +12 score by [(iambulb, Reddit, 2024-09-07)](https://www.reddit.com/r/FortniteFestival/comments/1f8rrod/epic_absolutely_cooked_on_beyond_the_flame_easily/llxrtds/) (in reply to ID llv7kam):\nAh I’m flattered but this isn’t really a Periphery song even though Spencer and I are on it.  I wrote this to a pretty specific Spec that the guys who hired me provided.  Same for Spencer, we are kinda operating in their box.\n\nIt’s a fun challenge and genuinely very different from how we approach writing for Periphery.\n\n### Comment ID llxvpo6 with +1 score by [(Nicinator284771, Reddit, 2024-09-07)](https://www.reddit.com/r/FortniteFestival/comments/1f8rrod/epic_absolutely_cooked_on_beyond_the_flame_easily/llxvpo6/) (in reply to ID lllahhi):\nvery nice work !  \nkinda sounds like an sonic soundtrack lmao\n\n### Comment ID lmcsy2h with +1 score by [(ImSammy6, Reddit, 2024-09-09)](https://www.reddit.com/r/FortniteFestival/comments/1f8rrod/epic_absolutely_cooked_on_beyond_the_flame_easily/lmcsy2h/) (in reply to ID lllahhi):\nThis song rules, are you allowed to release it outside of Fortnite?\n\n#### Comment ID lmdstr5 with +3 score by [(j_xc_ob, Reddit, 2024-09-10)](https://www.reddit.com/r/FortniteFestival/comments/1f8rrod/epic_absolutely_cooked_on_beyond_the_flame_easily/lmdstr5/) (in reply to ID lmcsy2h):\nI believe Epic hired them to create the song but own the full rights to it, so it's up to Epic to release it on Spotify and stuff. Unfortunately they don't really do that, but at least they make the songs non-copyrighted\n\n### Comment ID lndyr8w with +1 score by [(OctagonalMunch, Reddit, 2024-09-16)](https://www.reddit.com/r/FortniteFestival/comments/1f8rrod/epic_absolutely_cooked_on_beyond_the_flame_easily/lndyr8w/) (in reply to ID lllahhi):\nWow. While I was playing this one for the first time I was like: \"...This vocalist sounds just like Spencer Sotelo... And it sounds kinda like Periphery... No way...\". And then search brought me here and it blew my mind!\n\nThis track is fire, Misha, and it's so cool that so many talented artists that I like was part of it. Thank you and it sucks big time that you were not credited. Epic owe you all for this.\n\n### Comment ID lndzptx with +1 score by [(UndyingCurse-, Reddit, 2024-09-16)](https://www.reddit.com/r/FortniteFestival/comments/1f8rrod/epic_absolutely_cooked_on_beyond_the_flame_easily/lndzptx/) (in reply to ID lllahhi):\nYall absolutely killed it, Epic really needs to start crediting artists/bands/producers properly. Because this is honestly unacceptable.\n\n### Comment ID lrcd72t with +1 score by [(GenghisClaunch, Reddit, 2024-10-11)](https://www.reddit.com/r/FortniteFestival/comments/1f8rrod/epic_absolutely_cooked_on_beyond_the_flame_easily/lrcd72t/) (in reply to ID lllahhi):\nComing back to this old comment to ask, desperately hoping you’ll see it and respond, but the version that was on Spotify was just removed. I’m assuming it was an unofficial version and you guys weren’t making any money from it, but do you know if there are any plans to re-release it properly?\n\n### Comment ID lv7xk4v with +1 score by [(littleinferno__, Reddit, 2024-11-03)](https://www.reddit.com/r/FortniteFestival/comments/1f8rrod/epic_absolutely_cooked_on_beyond_the_flame_easily/lv7xk4v/) (in reply to ID lllahhi):\nI'm so geeked on this! I was playing festival with some friends and someone picked the song, IMMEDIATELY after it started I was like haha this sounds like Periphery.\nThen I was like...\nIs that Spencer???????????\n\nSo good! Good job buddies.\n\n### Comment ID lxdm9fd with +1 score by [(LivThePocketCamper, Reddit, 2024-11-16)](https://www.reddit.com/r/FortniteFestival/comments/1f8rrod/epic_absolutely_cooked_on_beyond_the_flame_easily/lxdm9fd/) (in reply to ID lllahhi):\nIs there anywhere we can get the official lyrics? I love this song but online I'm seeing different versions and I can't find lyrics anywhere on Fortnite either.\n\n## Comment ID llgfvtn with +62 score by [(josephvv188, Reddit, 2024-09-04)](https://www.reddit.com/r/FortniteFestival/comments/1f8rrod/epic_absolutely_cooked_on_beyond_the_flame_easily/llgfvtn/) (in reply to ID 1f8rrod):\nbroo i am struggling..first time ever getting 97% omg heellplpp\n\n### Comment ID llggw6n with +30 score by [(TotallyNotPineapple, Reddit, 2024-09-04)](https://www.reddit.com/r/FortniteFestival/comments/1f8rrod/epic_absolutely_cooked_on_beyond_the_flame_easily/llggw6n/) (in reply to ID llgfvtn):\nI feel you so much\n\nThis song and Paradise City are the only ones I haven’t gotten a flawless on yet. Lots of grinding will be done here\n\n### Comment ID llk8phv with +6 score by [(Flat-Evening-1581, Reddit, 2024-09-05)](https://www.reddit.com/r/FortniteFestival/comments/1f8rrod/epic_absolutely_cooked_on_beyond_the_flame_easily/llk8phv/) (in reply to ID llgfvtn):\nGot flawless after a couple attempts. The pattern isn't too hard to remember after a bit. It's mostly just alternating between two buttons. Setting the track speed to 1.5 helped a lot with recognizing the pattern, but it depends on your play style. If you already have 97 percent just keep going and you'll get it pretty quick.\n\n#### Comment ID llk9ll8 with +1 score by [(josephvv188, Reddit, 2024-09-05)](https://www.reddit.com/r/FortniteFestival/comments/1f8rrod/epic_absolutely_cooked_on_beyond_the_flame_easily/llk9ll8/) (in reply to ID llk8phv):\nthank u for encouraging me🥹 ..i have small fingers and get brain lag i feel like i press the buttons but not fast enough and not consistent enough \npracticing will make things better i just was intimidated to play this chart for a few times i missed like 40 notes with 40 strikes lmao\n\n## Comment ID llghc47 with +30 score by [(IBigMoistI, Reddit, 2024-09-04)](https://www.reddit.com/r/FortniteFestival/comments/1f8rrod/epic_absolutely_cooked_on_beyond_the_flame_easily/llghc47/) (in reply to ID 1f8rrod):\nYea I love this song lol I also instantly recognized the vocals as coming from the singer of the band Periphery. I love them too so it's a win win\n\n### Comment ID llgi2s1 with +10 score by [(TotallyNotPineapple, Reddit, 2024-09-04)](https://www.reddit.com/r/FortniteFestival/comments/1f8rrod/epic_absolutely_cooked_on_beyond_the_flame_easily/llgi2s1/) (in reply to ID llghc47):\nI thought the voice sounded familiar!\n\nI knew they used him for the BP jam track 2 seasons ago so absolute W to see they got him in again\n\n#### Comment ID llj5z2i with +1 score by [(poweredbytofu713, Reddit, 2024-09-04)](https://www.reddit.com/r/FortniteFestival/comments/1f8rrod/epic_absolutely_cooked_on_beyond_the_flame_easily/llj5z2i/) (in reply to ID llgi2s1):\nWait what track did he sing on two seasons ago?!\n\n### Comment ID lljdu7k with +7 score by [(Kiriko7, Reddit, 2024-09-04)](https://www.reddit.com/r/FortniteFestival/comments/1f8rrod/epic_absolutely_cooked_on_beyond_the_flame_easily/lljdu7k/) (in reply to ID llghc47):\nHonestly sounds like a unreleased song from periphery\n\n#### Comment ID llk0fct with +2 score by [(IBigMoistI, Reddit, 2024-09-05)](https://www.reddit.com/r/FortniteFestival/comments/1f8rrod/epic_absolutely_cooked_on_beyond_the_flame_easily/llk0fct/) (in reply to ID lljdu7k):\nFr the intro right into it is 1/1 like a Periphery song. Kinda annoyed that I haven't gotten it as a reward yet even though I have 2 gold star clears I NEED it as my loby music\n\n#### Comment ID llqf9sj with +1 score by [(ThatMBR42, Reddit, 2024-09-06)](https://www.reddit.com/r/FortniteFestival/comments/1f8rrod/epic_absolutely_cooked_on_beyond_the_flame_easily/llqf9sj/) (in reply to ID lljdu7k):\nMisha's riffs are unmistakable\n\n### Comment ID llk41oy with +1 score by [(Helpful_Surprise_616, Reddit, 2024-09-05)](https://www.reddit.com/r/FortniteFestival/comments/1f8rrod/epic_absolutely_cooked_on_beyond_the_flame_easily/llk41oy/) (in reply to ID llghc47):\nMaybe they’ll add more songs by Periphery and other bands like them.\n\n## Comment ID llgufmw with +34 score by [(Expandedsky5280, Reddit, 2024-09-04)](https://www.reddit.com/r/FortniteFestival/comments/1f8rrod/epic_absolutely_cooked_on_beyond_the_flame_easily/llgufmw/) (in reply to ID 1f8rrod):\nThis is 100% building up to a Through the Fire and Flaims chart. The content creators going to this upcoming event will already know the song front to back, so it might be a nice surprise. That part during the intro can not be a coincidence.\n\n### Comment ID llgywl8 with +8 score by [(TotallyNotPineapple, Reddit, 2024-09-04)](https://www.reddit.com/r/FortniteFestival/comments/1f8rrod/epic_absolutely_cooked_on_beyond_the_flame_easily/llgywl8/) (in reply to ID llgufmw):\nMan, I hope so\n\nThrough the Fire and Flames would be on my permanent rotation day one if so. Loved it in GH3\n\n## Comment ID llgin2w with +27 score by [(TR1PLE_6, Reddit, 2024-09-04)](https://www.reddit.com/r/FortniteFestival/comments/1f8rrod/epic_absolutely_cooked_on_beyond_the_flame_easily/llgin2w/) (in reply to ID 1f8rrod):\nI just saw the solo chart on Twitter and all I can say is....\n\n  \nWHAT. THE. FUCK?!!\n\n### Comment ID llklbi6 with +7 score by [(CadeMan011, Reddit, 2024-09-05)](https://www.reddit.com/r/FortniteFestival/comments/1f8rrod/epic_absolutely_cooked_on_beyond_the_flame_easily/llklbi6/) (in reply to ID llgin2w):\nAnd of course, Acai FCs it on his second try and Darkai had the top score :P\n\n#### Comment ID lllhvg5 with +2 score by [(TR1PLE_6, Reddit, 2024-09-05)](https://www.reddit.com/r/FortniteFestival/comments/1f8rrod/epic_absolutely_cooked_on_beyond_the_flame_easily/lllhvg5/) (in reply to ID llklbi6):\nWow, that was quick!\n\n## Comment ID llgiwlb with +13 score by [(TimeAggressive3338, Reddit, 2024-09-04)](https://www.reddit.com/r/FortniteFestival/comments/1f8rrod/epic_absolutely_cooked_on_beyond_the_flame_easily/llgiwlb/) (in reply to ID 1f8rrod):\nHow do you get this song?\n\n### Comment ID llgjwok with +32 score by [(TotallyNotPineapple, Reddit, 2024-09-04)](https://www.reddit.com/r/FortniteFestival/comments/1f8rrod/epic_absolutely_cooked_on_beyond_the_flame_easily/llgjwok/) (in reply to ID llgiwlb):\nBy finishing with gold stars on any instrument \n\nIt will automatically unlock and get added to your inventory shortly afterwards.\n\n#### Comment ID llgjzn4 with +3 score by [(TimeAggressive3338, Reddit, 2024-09-04)](https://www.reddit.com/r/FortniteFestival/comments/1f8rrod/epic_absolutely_cooked_on_beyond_the_flame_easily/llgjzn4/) (in reply to ID llgjwok):\nOkay cool thanks for that\n\n#### Comment ID llirg2n with +2 score by [(Nicdoe_XD, Reddit, 2024-09-04)](https://www.reddit.com/r/FortniteFestival/comments/1f8rrod/epic_absolutely_cooked_on_beyond_the_flame_easily/llirg2n/) (in reply to ID llgjwok):\nI played it multiple times on vocal to gold, even getting flawless a few times, but I won't get it. Any ideas why?\n\n#### Comment ID llglsr4 with +2 score by [(BillyWhizz09, Reddit, 2024-09-04)](https://www.reddit.com/r/FortniteFestival/comments/1f8rrod/epic_absolutely_cooked_on_beyond_the_flame_easily/llglsr4/) (in reply to ID llgjwok):\nSo we have to play expert mode? I hope not, I hate playing expert\n\n#### Comment ID llkw08d with +1 score by [(Ston3rrgirrl, Reddit, 2024-09-05)](https://www.reddit.com/r/FortniteFestival/comments/1f8rrod/epic_absolutely_cooked_on_beyond_the_flame_easily/llkw08d/) (in reply to ID llgjwok):\nI did, and it’s not giving me the songs still?\n\n#### Comment ID llns690 with +1 score by [(Reaper20338k, Reddit, 2024-09-05)](https://www.reddit.com/r/FortniteFestival/comments/1f8rrod/epic_absolutely_cooked_on_beyond_the_flame_easily/llns690/) (in reply to ID llgjwok):\nHello, don't have to pass 100%? 5 gold stars is enough? I just passed 99% and 5 gold stars, but they didn't give me the song. Could it be just some kind of delay in issuing the award?\n\n## Comment ID llggi0p with +25 score by [(Slendykins, Reddit, 2024-09-04)](https://www.reddit.com/r/FortniteFestival/comments/1f8rrod/epic_absolutely_cooked_on_beyond_the_flame_easily/llggi0p/) (in reply to ID 1f8rrod):\nIt's like Breaking Benjamin mixed with power metal I love it\n\n### Comment ID lljbdpk with +2 score by [(A_L_E_P_H, Reddit, 2024-09-04)](https://www.reddit.com/r/FortniteFestival/comments/1f8rrod/epic_absolutely_cooked_on_beyond_the_flame_easily/lljbdpk/) (in reply to ID llggi0p):\nBREAKING BENJAMIN? IM HOPPING ON RN\n\n#### Comment ID lljnur4 with +4 score by [(Judge_Mental_, Reddit, 2024-09-04)](https://www.reddit.com/r/FortniteFestival/comments/1f8rrod/epic_absolutely_cooked_on_beyond_the_flame_easily/lljnur4/) (in reply to ID lljbdpk):\nIf you’re familiar, I’m fairly certain the song is performed by Periphery!\n\n#### Comment ID lljmkud with +1 score by [(MinnieLitty, Reddit, 2024-09-04)](https://www.reddit.com/r/FortniteFestival/comments/1f8rrod/epic_absolutely_cooked_on_beyond_the_flame_easily/lljmkud/) (in reply to ID lljbdpk):\nTell us how you like it. It’s so fun!!\n\n## Comment ID llh6lcc with +9 score by [(CnD93, Reddit, 2024-09-04)](https://www.reddit.com/r/FortniteFestival/comments/1f8rrod/epic_absolutely_cooked_on_beyond_the_flame_easily/llh6lcc/) (in reply to ID 1f8rrod):\nIt's called beyond the flame, it's got parts that hint towards through the fire and flames and Herman Li said before he would work with epic, could this be him playing?\n\n## Comment ID lllacxc with +6 score by [(iambulb, Reddit, 2024-09-05)](https://www.reddit.com/r/FortniteFestival/comments/1f8rrod/epic_absolutely_cooked_on_beyond_the_flame_easily/lllacxc/) (in reply to ID 1f8rrod):\nStoked to see you guys dig the song, Bummed to see that Epic/Fortnite didn’t give any credit to the artists.  So if you are curious here you go, spread the word haha:\n\nI produced and wrote the track and collaborated with the very talented Oumi Kapila from Combichrist for that.\n\nSpencer Sotelo from my band Periphery did the vocals (as the keen eared of you can tell.)\n\nDrum legend Dirk Verbeuren from Megadeth played drums and Ra Diaz from Korn played bass and did that sick bass solo.\n\nFirst guitar solo is by the incredible Kiko Loureiro from Megadeth.  \nSecond solo is by Gene Semel at Epic who was kind enough to ask me to be involved with the project and who also provided the spec for the song to me.  \nAnd finally I contributed the last guitar solo. \n\nHope you guys enjoy!\n\nMisha Mansoor\n\n## Comment ID llgjkwu with +14 score by [(ComfortablePatience, Reddit, 2024-09-04)](https://www.reddit.com/r/FortniteFestival/comments/1f8rrod/epic_absolutely_cooked_on_beyond_the_flame_easily/llgjkwu/) (in reply to ID 1f8rrod):\nThe lift spam near the beginning on guitar was unfortunate, but the solos were pretty neat. The chart is active the entire time through, which is always good. This might become the new SOMP for a while\n\nI noticed the other song was doing some different things too, at least on vocals. It had ascending quads and some chord runs, which I think is the 1st time for non-pro charts on here?\n\nI hope these tracks represent the future of charts on this game, they were both very great additions\n\n## Comment ID llifhzy with +5 score by [(Alternative-Test-556, Reddit, 2024-09-04)](https://www.reddit.com/r/FortniteFestival/comments/1f8rrod/epic_absolutely_cooked_on_beyond_the_flame_easily/llifhzy/) (in reply to ID 1f8rrod):\nWish they gave the one's who made the song some credit I mean the vocalist on here absolutely killed it!!!\n\n### Comment ID lljo4xc with +3 score by [(Judge_Mental_, Reddit, 2024-09-04)](https://www.reddit.com/r/FortniteFestival/comments/1f8rrod/epic_absolutely_cooked_on_beyond_the_flame_easily/lljo4xc/) (in reply to ID llifhzy):\nSounds like it is Spencer Sotelo from Periphery!\n\n### Comment ID llplzu8 with +3 score by [(HeyNateBarber, Reddit, 2024-09-05)](https://www.reddit.com/r/FortniteFestival/comments/1f8rrod/epic_absolutely_cooked_on_beyond_the_flame_easily/llplzu8/) (in reply to ID llifhzy):\nCheck out Periphery to see more of him!\n\n## Comment ID llgl00q with +9 score by [(JakobHunterek, Reddit, 2024-09-04)](https://www.reddit.com/r/FortniteFestival/comments/1f8rrod/epic_absolutely_cooked_on_beyond_the_flame_easily/llgl00q/) (in reply to ID 1f8rrod):\nHOW, HOW DID YOU GET THE BEYOND THE FLAME JAM TRACK? I DIDN'T GET IT EVEN WHEN I GOT A GOLD STAR ON DRUMS AND BASS\n\n### Comment ID llgszjq with +2 score by [(Any_Somewhere_3637, Reddit, 2024-09-04)](https://www.reddit.com/r/FortniteFestival/comments/1f8rrod/epic_absolutely_cooked_on_beyond_the_flame_easily/llgszjq/) (in reply to ID llgl00q):\nEdit and re-enter.\n\n## Comment ID llgetgz with +3 score by [(NocorrelationIV, Reddit, 2024-09-04)](https://www.reddit.com/r/FortniteFestival/comments/1f8rrod/epic_absolutely_cooked_on_beyond_the_flame_easily/llgetgz/) (in reply to ID 1f8rrod):\nHow did you unlock them?\n\n### Comment ID llgf3ts with +7 score by [(TotallyNotPineapple, Reddit, 2024-09-04)](https://www.reddit.com/r/FortniteFestival/comments/1f8rrod/epic_absolutely_cooked_on_beyond_the_flame_easily/llgf3ts/) (in reply to ID llgetgz):\nJust by getting gold stars on each song (can only get them on max difficulty. doesnt matter what instrument you pick)\n\nThen just go back to lobby and you will get the rewards screen for them\n\n#### Comment ID llgfoh7 with +5 score by [(NocorrelationIV, Reddit, 2024-09-04)](https://www.reddit.com/r/FortniteFestival/comments/1f8rrod/epic_absolutely_cooked_on_beyond_the_flame_easily/llgfoh7/) (in reply to ID llgf3ts):\nIve gotten gold stars but i haven't actually gotten the tracks yet\n\n## Comment ID llgt3nx with +3 score by [(Any_Somewhere_3637, Reddit, 2024-09-04)](https://www.reddit.com/r/FortniteFestival/comments/1f8rrod/epic_absolutely_cooked_on_beyond_the_flame_easily/llgt3nx/) (in reply to ID 1f8rrod):\nKinda Emo. I love it. I like the other one too.\n\n### Comment ID llpm1vl with +1 score by [(HeyNateBarber, Reddit, 2024-09-05)](https://www.reddit.com/r/FortniteFestival/comments/1f8rrod/epic_absolutely_cooked_on_beyond_the_flame_easily/llpm1vl/) (in reply to ID llgt3nx):\nCheck out their band Periphery :)\n\n## Comment ID llhhs44 with +3 score by [(Spinezapper, Reddit, 2024-09-04)](https://www.reddit.com/r/FortniteFestival/comments/1f8rrod/epic_absolutely_cooked_on_beyond_the_flame_easily/llhhs44/) (in reply to ID 1f8rrod):\nIdk how epic managed it, but this is their second song with Spencer Sotelo of Periphery on vocals and the guitars have an oddly “Bulb”(Misha Mansoor) sound, also from periphery. \n\nFingers crossed they get added in a later update.\n\n### Comment ID lljknft with +1 score by [(RTideR, Reddit, 2024-09-04)](https://www.reddit.com/r/FortniteFestival/comments/1f8rrod/epic_absolutely_cooked_on_beyond_the_flame_easily/lljknft/) (in reply to ID llhhs44):\nWait, that IS Spencer? I just said in another comment it sounds like him with the cleans. Lol\n\n## Comment ID llkfxaj with +3 score by [(IcePhoenix48, Reddit, 2024-09-05)](https://www.reddit.com/r/FortniteFestival/comments/1f8rrod/epic_absolutely_cooked_on_beyond_the_flame_easily/llkfxaj/) (in reply to ID 1f8rrod):\nDid anyone else feel like there was Dragonforce influence? \n\n\nI swear, I could hear a bit of Dragonforce near the end, like dayum.\n\n## Comment ID llh38vx with +2 score by [(Dependsontheweapon, Reddit, 2024-09-04)](https://www.reddit.com/r/FortniteFestival/comments/1f8rrod/epic_absolutely_cooked_on_beyond_the_flame_easily/llh38vx/) (in reply to ID 1f8rrod):\nOmg I NEED practice mode already😰\n\n## Comment ID llj7nyb with +2 score by [(Nice_Map5910, Reddit, 2024-09-04)](https://www.reddit.com/r/FortniteFestival/comments/1f8rrod/epic_absolutely_cooked_on_beyond_the_flame_easily/llj7nyb/) (in reply to ID 1f8rrod):\ni got the i am a winner loading screen but not the song supports said wait but i got burning stars on bass expert i did %98 but didnt received the jam track\n\n## Comment ID lljk8m7 with +2 score by [(RTideR, Reddit, 2024-09-04)](https://www.reddit.com/r/FortniteFestival/comments/1f8rrod/epic_absolutely_cooked_on_beyond_the_flame_easily/lljk8m7/) (in reply to ID 1f8rrod):\nJust played it.. song is dope. The vocalist's cleans sound a lot like Spencer Sotelo from Periphery to me.\n\nI just gold starred it but didn't get anything though.. nor do I see a quest that rewards it. Is there another quest chain you have to do first?\n\nEdit: As other comments noted, restarting the game unlocked the song for me!\n\n### Comment ID lljvr7r with +1 score by [(MinnieLitty, Reddit, 2024-09-05)](https://www.reddit.com/r/FortniteFestival/comments/1f8rrod/epic_absolutely_cooked_on_beyond_the_flame_easily/lljvr7r/) (in reply to ID lljk8m7):\nGuess mines is delayed . Mines didn’t unlovk\n\n### Comment ID llpm7yv with +1 score by [(HeyNateBarber, Reddit, 2024-09-05)](https://www.reddit.com/r/FortniteFestival/comments/1f8rrod/epic_absolutely_cooked_on_beyond_the_flame_easily/llpm7yv/) (in reply to ID lljk8m7):\nIt do be Spencer from periphery\n\n## Comment ID lljmhqb with +2 score by [(MinnieLitty, Reddit, 2024-09-04)](https://www.reddit.com/r/FortniteFestival/comments/1f8rrod/epic_absolutely_cooked_on_beyond_the_flame_easily/lljmhqb/) (in reply to ID 1f8rrod):\nAgree. Just played it on the drums and WTF lmaooo. My jaw is on the floor . Gonna take some time to get used to to this song but holy cow!!\n\n## Comment ID lljukqr with +2 score by [(None, Reddit, 2024-09-05)](https://www.reddit.com/r/FortniteFestival/comments/1f8rrod/epic_absolutely_cooked_on_beyond_the_flame_easily/lljukqr/) (in reply to ID 1f8rrod):\nSo why am i not getting this despite golding it TWICE with bass? crappy bug i guess\n\n## Comment ID m4hwrlx with +2 score by [(Hot-Somewhere-8585, Reddit, 2024-12-30)](https://www.reddit.com/r/FortniteFestival/comments/1f8rrod/epic_absolutely_cooked_on_beyond_the_flame_easily/m4hwrlx/) (in reply to ID 1f8rrod):\nI’m so confused on why this song is so underrated omg it’s amazing\n\n## Comment ID llhm0w9 with +2 score by [(cactusJoe564, Reddit, 2024-09-04)](https://www.reddit.com/r/FortniteFestival/comments/1f8rrod/epic_absolutely_cooked_on_beyond_the_flame_easily/llhm0w9/) (in reply to ID 1f8rrod):\nYea i cant do extreme so this will be impossible\n\n### Comment ID lljs0t2 with +1 score by [(MinnieLitty, Reddit, 2024-09-04)](https://www.reddit.com/r/FortniteFestival/comments/1f8rrod/epic_absolutely_cooked_on_beyond_the_flame_easily/lljs0t2/) (in reply to ID llhm0w9):\nTry singing . I unlocked it that way!\n\n#### Comment ID lll85k7 with +1 score by [(cactusJoe564, Reddit, 2024-09-05)](https://www.reddit.com/r/FortniteFestival/comments/1f8rrod/epic_absolutely_cooked_on_beyond_the_flame_easily/lll85k7/) (in reply to ID lljs0t2):\nIm not used to having that extra row but i got some time begore they dissapear so i can train\n\n## Comment ID llgogsa with +1 score by [(majorvamu, Reddit, 2024-09-04)](https://www.reddit.com/r/FortniteFestival/comments/1f8rrod/epic_absolutely_cooked_on_beyond_the_flame_easily/llgogsa/) (in reply to ID 1f8rrod):\nHow long has this been available? I rushed the Ramp it Up challenges in like a day. Why don’t I have this?\n\n### Comment ID llgoqhl with +5 score by [(TotallyNotPineapple, Reddit, 2024-09-04)](https://www.reddit.com/r/FortniteFestival/comments/1f8rrod/epic_absolutely_cooked_on_beyond_the_flame_easily/llgoqhl/) (in reply to ID llgogsa):\nIt just became available today after downtime ended. It is not related to Ramp it Up challenges nor something shown in the quests tab (at least not at the moment)\n\n#### Comment ID llgp1wi with +2 score by [(majorvamu, Reddit, 2024-09-04)](https://www.reddit.com/r/FortniteFestival/comments/1f8rrod/epic_absolutely_cooked_on_beyond_the_flame_easily/llgp1wi/) (in reply to ID llgoqhl):\nOh I see, thats why I didn’t get it. I know it wasn’t part of Ramp it up, but a few of those challenges were to gold star songs, so figured I’d have it. Not even a tab for it? Strange\n\n## Comment ID llgpu7y with +1 score by [(KokichiOuma24, Reddit, 2024-09-04)](https://www.reddit.com/r/FortniteFestival/comments/1f8rrod/epic_absolutely_cooked_on_beyond_the_flame_easily/llgpu7y/) (in reply to ID 1f8rrod):\nHow long do we have to do this challenge cause I forgotten and can't check now\n\n### Comment ID llgqc9f with +1 score by [(TotallyNotPineapple, Reddit, 2024-09-04)](https://www.reddit.com/r/FortniteFestival/comments/1f8rrod/epic_absolutely_cooked_on_beyond_the_flame_easily/llgqc9f/) (in reply to ID llgpu7y):\nYou have until September 29th at 9 AM ET to get it for free\n\n#### Comment ID lw0e5fg with +1 score by [(thetwistygamer_11, Reddit, 2024-11-08)](https://www.reddit.com/r/FortniteFestival/comments/1f8rrod/epic_absolutely_cooked_on_beyond_the_flame_easily/lw0e5fg/) (in reply to ID llgqc9f):\ndo you know when it might be able to be bought? I didnt have a chance to gold it\n\n## Comment ID llh3t3i with +1 score by [(Psychological-Pool-3, Reddit, 2024-09-04)](https://www.reddit.com/r/FortniteFestival/comments/1f8rrod/epic_absolutely_cooked_on_beyond_the_flame_easily/llh3t3i/) (in reply to ID 1f8rrod):\nHow do you know which songs you can unlock through gold starring? Is it only those 2?\n\n### Comment ID llh3w20 with +2 score by [(TotallyNotPineapple, Reddit, 2024-09-04)](https://www.reddit.com/r/FortniteFestival/comments/1f8rrod/epic_absolutely_cooked_on_beyond_the_flame_easily/llh3w20/) (in reply to ID llh3t3i):\nYeah, it is only these 2 songs\n\n#### Comment ID llhcame with +1 score by [(Psychological-Pool-3, Reddit, 2024-09-04)](https://www.reddit.com/r/FortniteFestival/comments/1f8rrod/epic_absolutely_cooked_on_beyond_the_flame_easily/llhcame/) (in reply to ID llh3w20):\nThanks! Was able to get I’m a winner but struggling to get Beyond the Flame, can get close on vocals but haven’t quite gotten it yet\n\n## Comment ID llh97v7 with +1 score by [(InitialDiscipline233, Reddit, 2024-09-04)](https://www.reddit.com/r/FortniteFestival/comments/1f8rrod/epic_absolutely_cooked_on_beyond_the_flame_easily/llh97v7/) (in reply to ID 1f8rrod):\nis there any way to cheese the songs i cant even get 2 stars and i want the music packs\n\n### Comment ID llizebg with +2 score by [(VeeVeeLa, Reddit, 2024-09-04)](https://www.reddit.com/r/FortniteFestival/comments/1f8rrod/epic_absolutely_cooked_on_beyond_the_flame_easily/llizebg/) (in reply to ID llh97v7):\nVocals for Beyond the Flame and bass for I'm a Winner. You don't have to 100% them, but just by doing overdrive in strategic places can boost your score a lot. It took me a while to do them cuz I usually play hard and I'm not used to five buttons but its doable.\n\n## Comment ID llier46 with +1 score by [(PresentConfection200, Reddit, 2024-09-04)](https://www.reddit.com/r/FortniteFestival/comments/1f8rrod/epic_absolutely_cooked_on_beyond_the_flame_easily/llier46/) (in reply to ID 1f8rrod):\nWill it be available in the item shop at some point?\n\n### Comment ID llij5qd with +1 score by [(TotallyNotPineapple, Reddit, 2024-09-04)](https://www.reddit.com/r/FortniteFestival/comments/1f8rrod/epic_absolutely_cooked_on_beyond_the_flame_easily/llij5qd/) (in reply to ID llier46):\nEpic hasn’t specified yet. It certainly can come back though, since Epic did not mention any exclusivity in their blog post\n\nBetter to be safe and try getting it though, in case it doesnt return for a while\n\n## Comment ID lliij43 with +1 score by [(GhostZombie69, Reddit, 2024-09-04)](https://www.reddit.com/r/FortniteFestival/comments/1f8rrod/epic_absolutely_cooked_on_beyond_the_flame_easily/lliij43/) (in reply to ID 1f8rrod):\nhow do u get it?\n\n### Comment ID llinhew with +1 score by [(TotallyNotPineapple, Reddit, 2024-09-04)](https://www.reddit.com/r/FortniteFestival/comments/1f8rrod/epic_absolutely_cooked_on_beyond_the_flame_easily/llinhew/) (in reply to ID lliij43):\nYou have to get gold stars on the song \n\nAfterwards, it automatically gets added to your locker\n\n#### Comment ID llio7oj with +1 score by [(GhostZombie69, Reddit, 2024-09-04)](https://www.reddit.com/r/FortniteFestival/comments/1f8rrod/epic_absolutely_cooked_on_beyond_the_flame_easily/llio7oj/) (in reply to ID llinhew):\njust realised this said gold stars lol. still ez done\n\n## Comment ID lliv9mv with +1 score by [(None, Reddit, 2024-09-04)](https://www.reddit.com/r/FortniteFestival/comments/1f8rrod/epic_absolutely_cooked_on_beyond_the_flame_easily/lliv9mv/) (in reply to ID 1f8rrod):\n[deleted]\n\n## Comment ID llixp4a with +1 score by [(Toxic7120, Reddit, 2024-09-04)](https://www.reddit.com/r/FortniteFestival/comments/1f8rrod/epic_absolutely_cooked_on_beyond_the_flame_easily/llixp4a/) (in reply to ID 1f8rrod):\nhey one question, i read in the comments that using vocals if fine. and i just played through the song in main stage. (expert and gold stars as well.) yet i dont see it. do i have to do it in battle stage or does it take some time\n\nedit: i scrolled downed and saw a similar comment lol.\n\n### Comment ID lliy7no with +1 score by [(TotallyNotPineapple, Reddit, 2024-09-04)](https://www.reddit.com/r/FortniteFestival/comments/1f8rrod/epic_absolutely_cooked_on_beyond_the_flame_easily/lliy7no/) (in reply to ID llixp4a):\nSome people have had their rewards randomly delayed so that might be the case for you too\n\nYou have done all the prerequisites required to get the rewards. I would try relogging and/or waiting half an hour to see if yours pops up\n\n#### Comment ID llj21m3 with +2 score by [(Toxic7120, Reddit, 2024-09-04)](https://www.reddit.com/r/FortniteFestival/comments/1f8rrod/epic_absolutely_cooked_on_beyond_the_flame_easily/llj21m3/) (in reply to ID lliy7no):\nI just went in and decided to do \"I'm a winner\" since that's one of the songs I haven't done yet. And after doing that and returning back to lobby, I got the first song I did \"Beyond the Flame\". Anywho, thanks for the help.\n\nEdit: looks like I got im a winner as well right after claiming the first song and getting 5 stars\n\n## Comment ID lljeide with +1 score by [(I_GO_HAM_365, Reddit, 2024-09-04)](https://www.reddit.com/r/FortniteFestival/comments/1f8rrod/epic_absolutely_cooked_on_beyond_the_flame_easily/lljeide/) (in reply to ID 1f8rrod):\nIs this periphery\n\n### Comment ID llpmcqo with +2 score by [(HeyNateBarber, Reddit, 2024-09-05)](https://www.reddit.com/r/FortniteFestival/comments/1f8rrod/epic_absolutely_cooked_on_beyond_the_flame_easily/llpmcqo/) (in reply to ID lljeide):\nMisha and Spencer, so kinda lol\n\n## Comment ID lljfukv with +1 score by [(Nyxll-A, Reddit, 2024-09-04)](https://www.reddit.com/r/FortniteFestival/comments/1f8rrod/epic_absolutely_cooked_on_beyond_the_flame_easily/lljfukv/) (in reply to ID 1f8rrod):\nThe lead chart is just... Insane. They really cooked, but damn, this chart uses the most BS patterns for Lead players.\n\nIt's like playing the hardest parts of Ella Baila Sola, Ride the Lightning, and Breed at twice the speed and repeated over and over.\n\n## Comment ID lljlqkp with +1 score by [(Omr125, Reddit, 2024-09-04)](https://www.reddit.com/r/FortniteFestival/comments/1f8rrod/epic_absolutely_cooked_on_beyond_the_flame_easily/lljlqkp/) (in reply to ID 1f8rrod):\nThe part where you spam before the solo just hurts my wrist. The solo is extremely fun and easily one of the best track, i hope epic adds more like this starting with through fire and flames.\n\n## Comment ID lljnk0i with +1 score by [(V3GA559, Reddit, 2024-09-04)](https://www.reddit.com/r/FortniteFestival/comments/1f8rrod/epic_absolutely_cooked_on_beyond_the_flame_easily/lljnk0i/) (in reply to ID 1f8rrod):\nDefinitely periphery on vocals right?  lol\n\n## Comment ID lljsmss with +1 score by [(Neco4arc, Reddit, 2024-09-04)](https://www.reddit.com/r/FortniteFestival/comments/1f8rrod/epic_absolutely_cooked_on_beyond_the_flame_easily/lljsmss/) (in reply to ID 1f8rrod):\ni literally just got done saying \"Fortnite needs to add a song like Not Ready To Die by Avenged Sevenfold\" and a few days later, we get this absolute masterpiece. it doesn't sound like Not Ready To Die, but it's charted exactly how i would want Not Ready To Die to be charted.\n\n## Comment ID llk92rl with +1 score by [(Pixel_Python, Reddit, 2024-09-05)](https://www.reddit.com/r/FortniteFestival/comments/1f8rrod/epic_absolutely_cooked_on_beyond_the_flame_easily/llk92rl/) (in reply to ID 1f8rrod):\nHoly shit I didn’t know Epic could *make* songs this difficult, this is imo HARDER than Paradise City. While I have rhythm game experience, this is still my first one like Guitar Hero/Rock Band. Can’t imagine what TTFAF will be like\n\n### Comment ID llpmgn7 with +1 score by [(HeyNateBarber, Reddit, 2024-09-05)](https://www.reddit.com/r/FortniteFestival/comments/1f8rrod/epic_absolutely_cooked_on_beyond_the_flame_easily/llpmgn7/) (in reply to ID llk92rl):\nMade by Misha from periphery, singer is spencer from the same band.\n\n## Comment ID llk9hoh with +1 score by [(Suspect-Shot-13, Reddit, 2024-09-05)](https://www.reddit.com/r/FortniteFestival/comments/1f8rrod/epic_absolutely_cooked_on_beyond_the_flame_easily/llk9hoh/) (in reply to ID 1f8rrod):\nWhy are these so hard, do I just suck?\n\n### Comment ID lmfgyvg with +1 score by [(Harlow_Quinzel, Reddit, 2024-09-10)](https://www.reddit.com/r/FortniteFestival/comments/1f8rrod/epic_absolutely_cooked_on_beyond_the_flame_easily/lmfgyvg/) (in reply to ID llk9hoh):\nI suck, it seems like now they're skill locking tracks which is terrible probably one of the worst ideas the game is ever had. Now I can never own these tracks if I can't get my skill level up no matter how many tracks I've purchased in the past I can never own all of them Which is one of the worst ideas since the mode came out and one of the reasons why I wish harmonix did the game on their own instead of getting involved with a company like epic. Because none of the previous guitar hero or rock band games pulled garbage like this\n\n## Comment ID llkaycp with +1 score by [(Top_Independent7581, Reddit, 2024-09-05)](https://www.reddit.com/r/FortniteFestival/comments/1f8rrod/epic_absolutely_cooked_on_beyond_the_flame_easily/llkaycp/) (in reply to ID 1f8rrod):\nI gold starred and didn’t unlock anything. Logging out didn’t help. Then I grabbed an item on the battle pass and it popped.\n\n## Comment ID lll8fme with +1 score by [(HotZin, Reddit, 2024-09-05)](https://www.reddit.com/r/FortniteFestival/comments/1f8rrod/epic_absolutely_cooked_on_beyond_the_flame_easily/lll8fme/) (in reply to ID 1f8rrod):\nVocals is definitely Spencer from Periphery. Is there any way to look up credits for these songs? I really wanna know who the guitar players are (there are at least 3 here) because they sound familiar. In one of the solos someone plays a really fast chromatic phrase and I'm thinking it could be Bumblefoot.\n\nEDIT: Upon listening to it again, the last solo could be Andy James, sounds like his tones and it's pretty much all licks that I've heard him play a lot. The one solo before it, not sure, could also be Andy James but could just be someone from Epic, tone sounded different.\n\n### Comment ID llpmle9 with +1 score by [(HeyNateBarber, Reddit, 2024-09-05)](https://www.reddit.com/r/FortniteFestival/comments/1f8rrod/epic_absolutely_cooked_on_beyond_the_flame_easily/llpmle9/) (in reply to ID lll8fme):\nMisha posted all the credits towards the top of this thread. Misha and Spencer, and then some people from Megadeth and Korn\n\n#### Comment ID llpr3f4 with +1 score by [(HotZin, Reddit, 2024-09-06)](https://www.reddit.com/r/FortniteFestival/comments/1f8rrod/epic_absolutely_cooked_on_beyond_the_flame_easily/llpr3f4/) (in reply to ID llpmle9):\nWoah, Kiko was the first solo? Haven't heard him play that fast since the Angra days! And Misha had me fooled there, he really sounded like Andy James. My guess for the middle solo was someone from Epic so I guess I got that right haha.\n\n## Comment ID lln1mxc with +1 score by [(SavageSvage, Reddit, 2024-09-05)](https://www.reddit.com/r/FortniteFestival/comments/1f8rrod/epic_absolutely_cooked_on_beyond_the_flame_easily/lln1mxc/) (in reply to ID 1f8rrod):\nIs this the one that has metalcore type chuggy chugs?\n\n## Comment ID llnhzkg with +1 score by [(mabdog420, Reddit, 2024-09-05)](https://www.reddit.com/r/FortniteFestival/comments/1f8rrod/epic_absolutely_cooked_on_beyond_the_flame_easily/llnhzkg/) (in reply to ID 1f8rrod):\nHow do you get these?\n\n## Comment ID llojype with +1 score by [(dodo6606, Reddit, 2024-09-05)](https://www.reddit.com/r/FortniteFestival/comments/1f8rrod/epic_absolutely_cooked_on_beyond_the_flame_easily/llojype/) (in reply to ID 1f8rrod):\nWait wait wait what i miss? Im hopping in rn but im scared seeing the 97% comments\n\n## Comment ID llqfi5j with +1 score by [(ThatMBR42, Reddit, 2024-09-06)](https://www.reddit.com/r/FortniteFestival/comments/1f8rrod/epic_absolutely_cooked_on_beyond_the_flame_easily/llqfi5j/) (in reply to ID 1f8rrod):\nI feel like I cheated by gold starring it on vocals, but anything to have this one in my library.\n\n## Comment ID llr55e8 with +1 score by [(CaseyCupcake26, Reddit, 2024-09-06)](https://www.reddit.com/r/FortniteFestival/comments/1f8rrod/epic_absolutely_cooked_on_beyond_the_flame_easily/llr55e8/) (in reply to ID 1f8rrod):\nHow do we get it? Is it quest or item shop purchase?\n\n### Comment ID llr6ped with +1 score by [(TotallyNotPineapple, Reddit, 2024-09-06)](https://www.reddit.com/r/FortniteFestival/comments/1f8rrod/epic_absolutely_cooked_on_beyond_the_flame_easily/llr6ped/) (in reply to ID llr55e8):\nPlay on X difficulty and get gold stars on the song\n\nYou will automatically unlock the song afterwards\n\n## Comment ID llub1j2 with +1 score by [(Radialpuddle, Reddit, 2024-09-06)](https://www.reddit.com/r/FortniteFestival/comments/1f8rrod/epic_absolutely_cooked_on_beyond_the_flame_easily/llub1j2/) (in reply to ID 1f8rrod):\nHow do you get this? I don’t play festival but I still want it!\n\n## Comment ID lo03p13 with +1 score by [(Regular-Pain-293, Reddit, 2024-09-20)](https://www.reddit.com/r/FortniteFestival/comments/1f8rrod/epic_absolutely_cooked_on_beyond_the_flame_easily/lo03p13/) (in reply to ID 1f8rrod):\nI can be completely wrong… but is the title a sort of spiritual successor to “Through the Fire and Flames”? \n\nThe idea came to me when I realized I can’t play lead on expert lmao.\n\nWould anyone be able to confirm??\n\n## Comment ID lo3f6r8 with +1 score by [(SeawardFriend, Reddit, 2024-09-20)](https://www.reddit.com/r/FortniteFestival/comments/1f8rrod/epic_absolutely_cooked_on_beyond_the_flame_easily/lo3f6r8/) (in reply to ID 1f8rrod):\nMy lord did they ever. I always used to skip the “epic games” songs because most of them are kinda just goofy or not my style. I did not expect my ears to be blessed by this masterpiece. I love it even more now that I know the band members are from Megadeth, Korn, and Periphery!\n\n## Comment ID lvnopka with +1 score by [(Adventurous_Pipe_429, Reddit, 2024-11-06)](https://www.reddit.com/r/FortniteFestival/comments/1f8rrod/epic_absolutely_cooked_on_beyond_the_flame_easily/lvnopka/) (in reply to ID 1f8rrod):\nis there any way to still receive this ham track?? i missed this event somehow and i absolutely need this jams it scratched my guitar hero itch so bad and i can only play it in public lobbies IF somebody else has it smfh\n\n### Comment ID md5ieof with +1 score by [(bbylma, Reddit, 2025-02-16)](https://www.reddit.com/r/FortniteFestival/comments/1f8rrod/epic_absolutely_cooked_on_beyond_the_flame_easily/md5ieof/) (in reply to ID lvnopka):\nsame :,) i hope they bring it back\n\n## Comment ID m8ouy5j with +1 score by [(KriberRiber, Reddit, 2025-01-23)](https://www.reddit.com/r/FortniteFestival/comments/1f8rrod/epic_absolutely_cooked_on_beyond_the_flame_easily/m8ouy5j/) (in reply to ID 1f8rrod):\nWill the track ever come back?? I really want it lol",
      "# Post ID 1hmhgrb: Google deep research AI with +3 score by [(Prashant_4200, Reddit, 2024-12-26)](https://www.reddit.com/r/LocalLLaMA/comments/1hmhgrb/google_deep_research_ai/)\nI recently hear about Google deep research AI and it's feels like one of the most promising AI service for Deep research. \n\nSo I'm wondering is there any other alternatives are also available in market which provides same or better results as good deep research or any open LLM? \n\n## Comment ID m40m9ho with +2 score by [(darkmuck, Reddit, 2024-12-27)](https://www.reddit.com/r/LocalLLaMA/comments/1hmhgrb/google_deep_research_ai/m40m9ho/) (in reply to ID 1hmhgrb):\nSTORM from Stanford seems similar but I haven't tried it yet. https://github.com/stanford-oval/storm\n\n## Comment ID m3uxc83 with +1 score by [(trajo123, Reddit, 2024-12-26)](https://www.reddit.com/r/LocalLLaMA/comments/1hmhgrb/google_deep_research_ai/m3uxc83/) (in reply to ID 1hmhgrb):\nWhat is deep research?\n\n### Comment ID m3xs4xl with +1 score by [(MasterSnipes, Reddit, 2024-12-26)](https://www.reddit.com/r/LocalLLaMA/comments/1hmhgrb/google_deep_research_ai/m3xs4xl/) (in reply to ID m3uxc83):\nIt's a feature in Gemini that does research across the internet on your behalf. [https://blog.google/products/gemini/google-gemini-deep-research/](https://blog.google/products/gemini/google-gemini-deep-research/)\n\n## Comment ID m3xvrwk with +1 score by [(MarceloTT, Reddit, 2024-12-26)](https://www.reddit.com/r/LocalLLaMA/comments/1hmhgrb/google_deep_research_ai/m3xvrwk/) (in reply to ID 1hmhgrb):\nWhen it's free, I'll use it. I still don't trust this deep research to pay.",
      "# Post ID 1fuguhl: Oumi Janta is threatening to sue Mihoyo for plagiarism. with +0 score by [(WarGodV_, Reddit, 2024-10-02)](https://www.reddit.com/r/Genshin_Impact/comments/1fuguhl/oumi_janta_is_threatening_to_sue_mihoyo_for/)\n\nTo those who don't know Oumi Janta is a German-Senegalese jam skater and influencer. \n\nShe claims that her jam skate dance movement which she claims was made by her was copied by Mihoyo for Xilonen's idle dance movement without permission.\n\nShe plans to sue Mihoyo in 80 countries when the character xilonen still releases with this idle dance movement without her permission on October 9th.\n\nPersonally I don't think that she will win as she herself claimed in this video that she does not own this dance move.\n\nAlso similarly in August 2022 personal choreographer Kyle Hanagami tried to sue Fortnite for copying his dance moves to create purchasable emotes. \n\nHowever the case was dismissed on the account of dance moves not being protected under copyright law.\n\nWhat do you guys think?\n\n## Comment ID lpz8yyu with +107 score by [(Master0643, Reddit, 2024-10-02)](https://www.reddit.com/r/Genshin_Impact/comments/1fuguhl/oumi_janta_is_threatening_to_sue_mihoyo_for/lpz8yyu/) (in reply to ID 1fuguhl):\nShe just said \"I don't own these moves\", good luck in court lmao. Fortnite would be dead by now if they could be sued for taking internet dances.\n\n### Comment ID lq46cp9 with +3 score by [(Zestyclose-Agency945, Reddit, 2024-10-03)](https://www.reddit.com/r/Genshin_Impact/comments/1fuguhl/oumi_janta_is_threatening_to_sue_mihoyo_for/lq46cp9/) (in reply to ID lpz8yyu):\nShe definitely sounds like shes not interested in suing them. Or she wouldnt give them a heads up before its removed. Shed lose a ton of money trying to sue a major company in so many regions. It wouldnt really even benefit her so i dont think it could really be seen as a cash grab or anything its definitely her specifically they copied for the animation. Guessing they will alter it and nothing will happen. No big deal really. Just a lazy animator if anything. The company themselves might not have even known.\n\n## Comment ID lpz95zo with +35 score by [(Chupekka, Reddit, 2024-10-02)](https://www.reddit.com/r/Genshin_Impact/comments/1fuguhl/oumi_janta_is_threatening_to_sue_mihoyo_for/lpz95zo/) (in reply to ID 1fuguhl):\ni dont think she knows whose wallet she's up against\n\n### Comment ID lpzal7x with +25 score by [(Master0643, Reddit, 2024-10-02)](https://www.reddit.com/r/Genshin_Impact/comments/1fuguhl/oumi_janta_is_threatening_to_sue_mihoyo_for/lpzal7x/) (in reply to ID lpz95zo):\nShe is about to fight against the wallets of millions of people with crippling gambling addiction in court.\n\n#### Comment ID lpzbckf with +17 score by [(Chupekka, Reddit, 2024-10-02)](https://www.reddit.com/r/Genshin_Impact/comments/1fuguhl/oumi_janta_is_threatening_to_sue_mihoyo_for/lpzbckf/) (in reply to ID lpzal7x):\ngentlemen, it's been a privilege working with you tonight.\n\n## Comment ID lpz8xrq with +120 score by [(hudashick, Reddit, 2024-10-02)](https://www.reddit.com/r/Genshin_Impact/comments/1fuguhl/oumi_janta_is_threatening_to_sue_mihoyo_for/lpz8xrq/) (in reply to ID 1fuguhl):\nShe said she does not own the dance move and wants to sue the company. So what base is she suing it on if it's not copyrighted by her ?? Do ppl think it's that easy to sue anyone?\n\nUnless she has personally stated during the recording that she created this move and forbids anyone copying her or using her moves, I don't see her winning this.\n\nAt most I can see them reaching to an agreement maybe.\n\nI just assume she just wants, as Amber Heard's lawyer did  say, her 15 minutes of fame for now.\n\n### Comment ID lpzbg9g with +39 score by [(D-S_12, Reddit, 2024-10-02)](https://www.reddit.com/r/Genshin_Impact/comments/1fuguhl/oumi_janta_is_threatening_to_sue_mihoyo_for/lpzbg9g/) (in reply to ID lpz8xrq):\nThe funny thing is that seeing as she isn't the one who made the dance move, technically she herself could also get sued by whoever actually made the dance move (if the person is even still around) for the same reason because she's making content out of said dance move without that person's permission.\n\n#### Comment ID lpzda9f with +15 score by [(Yunlicious, Reddit, 2024-10-02)](https://www.reddit.com/r/Genshin_Impact/comments/1fuguhl/oumi_janta_is_threatening_to_sue_mihoyo_for/lpzda9f/) (in reply to ID lpzbg9g):\nThey can't, it is too short to be a coreography, it would be the same as someone suing you for walking like them, or anyone for break dancing, or dancing in any way\n\n### Comment ID lpzk0a3 with +10 score by [(BulbasaurTreecko, Reddit, 2024-10-02)](https://www.reddit.com/r/Genshin_Impact/comments/1fuguhl/oumi_janta_is_threatening_to_sue_mihoyo_for/lpzk0a3/) (in reply to ID lpz8xrq):\ncan you even copyright a dance move? Maybe if it was like a whole music video choreography or something, but copyrighting a dance move feels like copyrighting a colour palette in art or a string of notes in music. Way too common to belong to anyone specifically\n\n#### Comment ID lpzkxer with +7 score by [(hudashick, Reddit, 2024-10-02)](https://www.reddit.com/r/Genshin_Impact/comments/1fuguhl/oumi_janta_is_threatening_to_sue_mihoyo_for/lpzkxer/) (in reply to ID lpzk0a3):\nI'm sure you can?\nBut it must be a full dance set moves and not snippets I assume.\n\n### Comment ID lq4udzl with +4 score by [(Ryuunoru, Reddit, 2024-10-03)](https://www.reddit.com/r/Genshin_Impact/comments/1fuguhl/oumi_janta_is_threatening_to_sue_mihoyo_for/lq4udzl/) (in reply to ID lpz8xrq):\n> Do ppl think\n\nHere's where it all went wrong\n\n## Comment ID lpz92tz with +72 score by [(embodiment_of_sloth, Reddit, 2024-10-02)](https://www.reddit.com/r/Genshin_Impact/comments/1fuguhl/oumi_janta_is_threatening_to_sue_mihoyo_for/lpz92tz/) (in reply to ID 1fuguhl):\nThis dance move is called crazy legs and has been around since the 70s, she does not own it and is going to lose\n\n### Comment ID lpzjks8 with +20 score by [(dottorescoomsock, Reddit, 2024-10-02)](https://www.reddit.com/r/Genshin_Impact/comments/1fuguhl/oumi_janta_is_threatening_to_sue_mihoyo_for/lpzjks8/) (in reply to ID lpz92tz):\nhomie, the court won't even file that case at the first place\n\n### Comment ID lqfs1vd with +1 score by [(LunarBeast77, Reddit, 2024-10-05)](https://www.reddit.com/r/Genshin_Impact/comments/1fuguhl/oumi_janta_is_threatening_to_sue_mihoyo_for/lqfs1vd/) (in reply to ID lpz92tz):\nDo u have an example of someone dancing this similarly to Xilonen? I was looking all over the Internet using the keyword \"crazy legs\" and I don't see someone's hands and legs moving this similarly as that influencer\n\n### Comment ID lr605zr with +1 score by [(pvndx, Reddit, 2024-10-09)](https://www.reddit.com/r/Genshin_Impact/comments/1fuguhl/oumi_janta_is_threatening_to_sue_mihoyo_for/lr605zr/) (in reply to ID lpz92tz):\nokay but if u compare both videos side by side then its the exact same movement.\n\n## Comment ID lpz95qn with +46 score by [(Riersa, Reddit, 2024-10-02)](https://www.reddit.com/r/Genshin_Impact/comments/1fuguhl/oumi_janta_is_threatening_to_sue_mihoyo_for/lpz95qn/) (in reply to ID 1fuguhl):\n> she herself claimed in this video that she does not own this dance move.\n\nLmao what, why would she say that, that's like the fastest way to lose your case.\n\n## Comment ID lpz8kb1 with +41 score by [(51BoiledPotatos, Reddit, 2024-10-02)](https://www.reddit.com/r/Genshin_Impact/comments/1fuguhl/oumi_janta_is_threatening_to_sue_mihoyo_for/lpz8kb1/) (in reply to ID 1fuguhl):\nYou cant just Copyright a dance thats just common sense\n\n## Comment ID lpza1yc with +38 score by [(The_Great_Ravioli, Reddit, 2024-10-02)](https://www.reddit.com/r/Genshin_Impact/comments/1fuguhl/oumi_janta_is_threatening_to_sue_mihoyo_for/lpza1yc/) (in reply to ID 1fuguhl):\nThis is stupid beyond measure.\n\nFirst: Suing Mihoyo in **80** Countries? Lmao, she aint got the money for that.\n\nSecond: She already claimed she doesn't own the dance moves. Any judge would laugh it out of court.\n\nThird: In many countries, Dance moves are not protected under copywright, unless it is choreography.\n\nForth: If you check the two videos, while Xilonen's idle is clearly inspired by the video, it is in no way a 1 to 1 copy.\n\n\nFifth: This is something that has lost in court again and again.(See Fortnite)  There is legal precedent. \n\n\n***\n\nThis is clearly a grift by someone trying to make money.\n\n## Comment ID lpzdjn6 with +35 score by [(None, Reddit, 2024-10-02)](https://www.reddit.com/r/Genshin_Impact/comments/1fuguhl/oumi_janta_is_threatening_to_sue_mihoyo_for/lpzdjn6/) (in reply to ID 1fuguhl):\n[removed]\n\n### Comment ID lq0y12z with +8 score by [(_PinaColada, Reddit, 2024-10-02)](https://www.reddit.com/r/Genshin_Impact/comments/1fuguhl/oumi_janta_is_threatening_to_sue_mihoyo_for/lq0y12z/) (in reply to ID lpzdjn6):\nSadly they have. Would have been hilarious, though.\n\n## Comment ID lpzbbt0 with +16 score by [(takenusername5001, Reddit, 2024-10-02)](https://www.reddit.com/r/Genshin_Impact/comments/1fuguhl/oumi_janta_is_threatening_to_sue_mihoyo_for/lpzbbt0/) (in reply to ID 1fuguhl):\nShe's going to feel silly when the mo-cap of Da Wei doing it come out\n\n## Comment ID lq04zip with +13 score by [(Fine_Phrase2131, Reddit, 2024-10-02)](https://www.reddit.com/r/Genshin_Impact/comments/1fuguhl/oumi_janta_is_threatening_to_sue_mihoyo_for/lq04zip/) (in reply to ID 1fuguhl):\n>claims that they down own the moves\n\n>shows leaked idles of xilonen\n\nwe are reaching peak levels of negative IQ\n\n## Comment ID lpz9aep with +24 score by [(Free_Lab9169, Reddit, 2024-10-02)](https://www.reddit.com/r/Genshin_Impact/comments/1fuguhl/oumi_janta_is_threatening_to_sue_mihoyo_for/lpz9aep/) (in reply to ID 1fuguhl):\nYou know that is a common dance move ... right?\n\n## Comment ID lpz7wzn with +36 score by [(AmaiHana98, Reddit, 2024-10-02)](https://www.reddit.com/r/Genshin_Impact/comments/1fuguhl/oumi_janta_is_threatening_to_sue_mihoyo_for/lpz7wzn/) (in reply to ID 1fuguhl):\nIs possible they used her video as inspiration but if you were to look frame by frame is easy to notice is not straight up tracing, apart the fact her clip was cut to make it as much similar as possible, so i could be wrong but i don’t believe can win a legal battle.\n\n### Comment ID lpzcx36 with +8 score by [(nooneatallnope, Reddit, 2024-10-02)](https://www.reddit.com/r/Genshin_Impact/comments/1fuguhl/oumi_janta_is_threatening_to_sue_mihoyo_for/lpzcx36/) (in reply to ID lpz7wzn):\nI mean, they probably didn't animate the dance from scratch either, but motion track it with some irl dancer or something.\n\n#### Comment ID lpzdb9r with +15 score by [(AmaiHana98, Reddit, 2024-10-02)](https://www.reddit.com/r/Genshin_Impact/comments/1fuguhl/oumi_janta_is_threatening_to_sue_mihoyo_for/lpzdb9r/) (in reply to ID lpzcx36):\nmaybe yes maybe not but is something extremely difficult to prove. I also went to look at other roller blade dancers, cause one person mentioned the move is called crazy legs, and many of them put an hand on their waist and arms like that when turning cause it feels natural to do. So ye idk how can she prove they used exactly her video.\n\nEdits with some examples:\nEnd of this tutorial video: https://youtube.com/shorts/Tu4J-NE_LDs?si=I6c1VZE4uhpmyXhf\n- General vibe: https://youtube.com/shorts/lRB7A2y7WRE?si=sAkFF49UWXdvbcI2\n- https://youtube.com/shorts/Rbt-cSBI3YU?si=hLnTzKGCgpjT_BIR\n\n### Comment ID lqa7u0c with +1 score by [(Kelly_Info_Girl, Reddit, 2024-10-04)](https://www.reddit.com/r/Genshin_Impact/comments/1fuguhl/oumi_janta_is_threatening_to_sue_mihoyo_for/lqa7u0c/) (in reply to ID lpz7wzn):\nNah, that's a common dance between rollerskaters\n\n#### Comment ID lqa8vva with +1 score by [(AmaiHana98, Reddit, 2024-10-04)](https://www.reddit.com/r/Genshin_Impact/comments/1fuguhl/oumi_janta_is_threatening_to_sue_mihoyo_for/lqa8vva/) (in reply to ID lqa7u0c):\nyus, now it was also changed so not even the leg and hands positioning is similar and there isn’t anything that can be even remotely considered for a sue\n\n## Comment ID lpz8648 with +20 score by [(MatStomp, Reddit, 2024-10-02)](https://www.reddit.com/r/Genshin_Impact/comments/1fuguhl/oumi_janta_is_threatening_to_sue_mihoyo_for/lpz8648/) (in reply to ID 1fuguhl):\nOh fuck off LMAO\n\n## Comment ID lpz8f8z with +20 score by [(Standard-Zucchini136, Reddit, 2024-10-02)](https://www.reddit.com/r/Genshin_Impact/comments/1fuguhl/oumi_janta_is_threatening_to_sue_mihoyo_for/lpz8f8z/) (in reply to ID 1fuguhl):\nshe ain't gonna win this who tf she think is\n\n## Comment ID lpz8p1p with +17 score by [(Dangerous_Two11, Reddit, 2024-10-02)](https://www.reddit.com/r/Genshin_Impact/comments/1fuguhl/oumi_janta_is_threatening_to_sue_mihoyo_for/lpz8p1p/) (in reply to ID 1fuguhl):\nSue for what lol? You need to first prove that move is originally yours lmfao.\n\n## Comment ID lpzbwfg with +10 score by [(markcan_killua, Reddit, 2024-10-02)](https://www.reddit.com/r/Genshin_Impact/comments/1fuguhl/oumi_janta_is_threatening_to_sue_mihoyo_for/lpzbwfg/) (in reply to ID 1fuguhl):\nnothing but another sell out cashing in on that 5 mins of fame for hoyo haters to bandwagon on. classic\n\n## Comment ID lpzbd95 with +8 score by [(izaya8929, Reddit, 2024-10-02)](https://www.reddit.com/r/Genshin_Impact/comments/1fuguhl/oumi_janta_is_threatening_to_sue_mihoyo_for/lpzbd95/) (in reply to ID 1fuguhl):\nlove how the comment said this is need more attention. yeah attention for being dumb i guess\n\n## Comment ID lpz83lv with +21 score by [(RevolutionaryFall102, Reddit, 2024-10-02)](https://www.reddit.com/r/Genshin_Impact/comments/1fuguhl/oumi_janta_is_threatening_to_sue_mihoyo_for/lpz83lv/) (in reply to ID 1fuguhl):\ni just think this is dumb lmao. wdym copyrighting dance moves\n\n## Comment ID lpz9xdp with +7 score by [(SaveEmailB4Logout, Reddit, 2024-10-02)](https://www.reddit.com/r/Genshin_Impact/comments/1fuguhl/oumi_janta_is_threatening_to_sue_mihoyo_for/lpz9xdp/) (in reply to ID 1fuguhl):\nGood luck, sunshine\n\n## Comment ID lpza4ep with +8 score by [(None, Reddit, 2024-10-02)](https://www.reddit.com/r/Genshin_Impact/comments/1fuguhl/oumi_janta_is_threatening_to_sue_mihoyo_for/lpza4ep/) (in reply to ID 1fuguhl):\nlmao she is desperate for attention\n\n## Comment ID lpzax1u with +7 score by [(D-S_12, Reddit, 2024-10-02)](https://www.reddit.com/r/Genshin_Impact/comments/1fuguhl/oumi_janta_is_threatening_to_sue_mihoyo_for/lpzax1u/) (in reply to ID 1fuguhl):\nWait...if she has said that she does not own the dance move, then how can you sue over that because Xilonen's idle is supposedly copying what you did? If it's not original, then where's the grounds for lawsuit over copying a dance move? Does she realize that should we ever find the person who originally made the dance move she did (as in the real original person), she could get sued too for the same grounds as the way she could sue Hoyoverse?\n\nAnd this is just a secondary issue to the main one being that you can't sue someone for doing a dance move you did. I mean if a game were to release a character that does Gangnam Style, do we expect PSY to sue the heck out of that game? How about the millions of people doing KPop dances as content? Can they get sued too by KPop groups just because they're using that group's dance moves to maybe earn money from things like YouTube videos or dance classes? You put this situation in a different context and it's easy to see why taking legal action over this is bs.\n\n## Comment ID lpz8r08 with +15 score by [(DemoralizedRightHand, Reddit, 2024-10-02)](https://www.reddit.com/r/Genshin_Impact/comments/1fuguhl/oumi_janta_is_threatening_to_sue_mihoyo_for/lpz8r08/) (in reply to ID 1fuguhl):\nSo many drifters. These moves are more a call back to the 90s when skating was actually bg.\n\n## Comment ID lpz89v8 with +20 score by [(ZhuHao_Daxian, Reddit, 2024-10-02)](https://www.reddit.com/r/Genshin_Impact/comments/1fuguhl/oumi_janta_is_threatening_to_sue_mihoyo_for/lpz89v8/) (in reply to ID 1fuguhl):\nWow that does seem similar but ain't copyright works only when she registers that dance move as her exclusive thru legal means before suing? Ain't so easy as I did that move before you so that is mine right??\n\n### Comment ID lpz9c9p with +15 score by [(Responsible_Club_917, Reddit, 2024-10-02)](https://www.reddit.com/r/Genshin_Impact/comments/1fuguhl/oumi_janta_is_threatening_to_sue_mihoyo_for/lpz9c9p/) (in reply to ID lpz89v8):\nDance moves in general are very iffy on copyright, as op said 2 years ago a choreographer tried to do the same thing to fortnight and was dismissed\n\n### Comment ID lpz96zg with +9 score by [(ainominako1234, Reddit, 2024-10-02)](https://www.reddit.com/r/Genshin_Impact/comments/1fuguhl/oumi_janta_is_threatening_to_sue_mihoyo_for/lpz96zg/) (in reply to ID lpz89v8):\nExactly. Hoyo clearly uses her dance move as inspiration but unless she registers this specific movement as her own and no one can use it for commercial gain, there's no way there's gonna be a case. \n\nAnd I'm sure she knows that too but calling this out will give her tiktok clicks, so yeah why not threaten to sue.\n\n### Comment ID lpzjc6s with +2 score by [(WillTheWAFSack, Reddit, 2024-10-02)](https://www.reddit.com/r/Genshin_Impact/comments/1fuguhl/oumi_janta_is_threatening_to_sue_mihoyo_for/lpzjc6s/) (in reply to ID lpz89v8):\ni'm pretty sure that artists gain automatic copyright of their work, but i'm not sure what exactly falls under the category of \"art\" from a legal standpoint. also, since she stated that this dance isn't hers, then i don't think this would even have a chance of holding in court.\n\n## Comment ID lpz8mkq with +25 score by [(Me_to_Dazai, Reddit, 2024-10-02)](https://www.reddit.com/r/Genshin_Impact/comments/1fuguhl/oumi_janta_is_threatening_to_sue_mihoyo_for/lpz8mkq/) (in reply to ID 1fuguhl):\nit's cute how an unknown rando on tiktok thinks they can win a lawsuit against a multi billion dollar company over a \\*dance move\\* 💀💀\n\n### Comment ID lpzd384 with +9 score by [(Master0643, Reddit, 2024-10-02)](https://www.reddit.com/r/Genshin_Impact/comments/1fuguhl/oumi_janta_is_threatening_to_sue_mihoyo_for/lpzd384/) (in reply to ID lpz8mkq):\nTo be fair, she said she doesn't know the game, girl and her lawyers are about to find out. Also 80 countries? Goddamn\n\n#### Comment ID lpzrb29 with +11 score by [(RevolutionaryFall102, Reddit, 2024-10-02)](https://www.reddit.com/r/Genshin_Impact/comments/1fuguhl/oumi_janta_is_threatening_to_sue_mihoyo_for/lpzrb29/) (in reply to ID lpzd384):\nHer lawyers must be dumb af if they think they can even sue over this\n\n## Comment ID lpz9d9x with +5 score by [(-Drogozi-, Reddit, 2024-10-02)](https://www.reddit.com/r/Genshin_Impact/comments/1fuguhl/oumi_janta_is_threatening_to_sue_mihoyo_for/lpz9d9x/) (in reply to ID 1fuguhl):\nCan't wait for fortnite and league of legends being sued for taking goddamn dance moves\n\n## Comment ID lpz9rmr with +18 score by [(Free_Lab9169, Reddit, 2024-10-02)](https://www.reddit.com/r/Genshin_Impact/comments/1fuguhl/oumi_janta_is_threatening_to_sue_mihoyo_for/lpz9rmr/) (in reply to ID 1fuguhl):\n\"I'm a gamer girl\"\n*Doesn't know what Genshin Is ... One of the most popular games from our time\n\n## Comment ID lpz9q2o with +8 score by [(xyzqsrbo, Reddit, 2024-10-02)](https://www.reddit.com/r/Genshin_Impact/comments/1fuguhl/oumi_janta_is_threatening_to_sue_mihoyo_for/lpz9q2o/) (in reply to ID 1fuguhl):\nPeople do know that you need to actually have a copyright on this to actually sue right?\n\n## Comment ID lpzg8r7 with +9 score by [(dragosslash, Reddit, 2024-10-02)](https://www.reddit.com/r/Genshin_Impact/comments/1fuguhl/oumi_janta_is_threatening_to_sue_mihoyo_for/lpzg8r7/) (in reply to ID 1fuguhl):\nTwo neurons against miHoYo's lawyers. Good luck, you'll need it.\n\n## Comment ID lpz8y4x with +4 score by [(didu173, Reddit, 2024-10-02)](https://www.reddit.com/r/Genshin_Impact/comments/1fuguhl/oumi_janta_is_threatening_to_sue_mihoyo_for/lpz8y4x/) (in reply to ID 1fuguhl):\nReminds me when fortnite added dances that are supposedly \"stolen\"\n\n## Comment ID lpznn7l with +3 score by [(Chonospeira, Reddit, 2024-10-02)](https://www.reddit.com/r/Genshin_Impact/comments/1fuguhl/oumi_janta_is_threatening_to_sue_mihoyo_for/lpznn7l/) (in reply to ID 1fuguhl):\nShe said it herself, she doesn't own the dance moves. So she can't really claim copyright to sue them. \n\nAlso that video is misleading. While the moves are similar, Xilonen's part is cut in a way to match up exactly with hers. There are other longer videos where you can see that they are much more different than what is shown here.\n\n## Comment ID lpz8l1x with +6 score by [(Chuck006, Reddit, 2024-10-02)](https://www.reddit.com/r/Genshin_Impact/comments/1fuguhl/oumi_janta_is_threatening_to_sue_mihoyo_for/lpz8l1x/) (in reply to ID 1fuguhl):\nLooks like a grifter trying to get attention with a frivolous law suit.\n\n## Comment ID lpza1em with +6 score by [(Eepydeepysleepy, Reddit, 2024-10-02)](https://www.reddit.com/r/Genshin_Impact/comments/1fuguhl/oumi_janta_is_threatening_to_sue_mihoyo_for/lpza1em/) (in reply to ID 1fuguhl):\nEven if she was right, Hoyo has a team of lawyers that have to be good enough to defend them from the freaking chinese government. She ain't winning this one.\n\n### Comment ID lpzc5j7 with +4 score by [(Riersa, Reddit, 2024-10-02)](https://www.reddit.com/r/Genshin_Impact/comments/1fuguhl/oumi_janta_is_threatening_to_sue_mihoyo_for/lpzc5j7/) (in reply to ID lpza1em):\nMHY doesn't even need top lawyer for this, they just need to drag this case long enough until she run out of money. It's almost impossible for small individual to beat a massive company.\n\n## Comment ID lq0apl4 with +3 score by [(Ravemst, Reddit, 2024-10-02)](https://www.reddit.com/r/Genshin_Impact/comments/1fuguhl/oumi_janta_is_threatening_to_sue_mihoyo_for/lq0apl4/) (in reply to ID 1fuguhl):\nShe doesn't own the moves and has no claim to make.\n\n## Comment ID lq8zwgm with +3 score by [(Win_Crafty, Reddit, 2024-10-04)](https://www.reddit.com/r/Genshin_Impact/comments/1fuguhl/oumi_janta_is_threatening_to_sue_mihoyo_for/lq8zwgm/) (in reply to ID 1fuguhl):\nOne fun fact is that tons of Wuthering Waves players (or people masquerading as players) are actively involved in these things, which is pretty funny and gross when combined with the fake hundreds of thousands of reviews.\n\n## Comment ID lpz8rf0 with +7 score by [(Direwolf0715, Reddit, 2024-10-02)](https://www.reddit.com/r/Genshin_Impact/comments/1fuguhl/oumi_janta_is_threatening_to_sue_mihoyo_for/lpz8rf0/) (in reply to ID 1fuguhl):\nGenshin haters/boycotters 12/25\n\n## Comment ID lpz90mn with +2 score by [(Dark_Reaper_1818, Reddit, 2024-10-02)](https://www.reddit.com/r/Genshin_Impact/comments/1fuguhl/oumi_janta_is_threatening_to_sue_mihoyo_for/lpz90mn/) (in reply to ID 1fuguhl):\nlol\n\n## Comment ID lpza3pk with +4 score by [(guylovesleep, Reddit, 2024-10-02)](https://www.reddit.com/r/Genshin_Impact/comments/1fuguhl/oumi_janta_is_threatening_to_sue_mihoyo_for/lpza3pk/) (in reply to ID 1fuguhl):\nwhy the fuck would dance move be copyright clamable?\n\nnow is it time to stop people from moving the way the want or something?\n\ni really doubt she would win\n\n### Comment ID lq4vcpg with +2 score by [(Ryuunoru, Reddit, 2024-10-03)](https://www.reddit.com/r/Genshin_Impact/comments/1fuguhl/oumi_janta_is_threatening_to_sue_mihoyo_for/lq4vcpg/) (in reply to ID lpza3pk):\n> why the fuck would dance move be copyright clamable?\n> \n> \n\nIf a choreography is unique enough, it is in fact eligible for copyright protections. Performance art is art too after all.\n\nObviously she's not gonna win (or even attempt to sue, she's full of it), this simple dance is too banal to be protected, let alone variations of it have been around since a long time without any clear copyright holder... lawsuit would be pointless.\n\n## Comment ID lpz8jw1 with +3 score by [(SimplyRzy, Reddit, 2024-10-02)](https://www.reddit.com/r/Genshin_Impact/comments/1fuguhl/oumi_janta_is_threatening_to_sue_mihoyo_for/lpz8jw1/) (in reply to ID 1fuguhl):\nJust the fornite floss thing again\n\n### Comment ID lpz9n5c with +1 score by [(None, Reddit, 2024-10-02)](https://www.reddit.com/r/Genshin_Impact/comments/1fuguhl/oumi_janta_is_threatening_to_sue_mihoyo_for/lpz9n5c/) (in reply to ID lpz8jw1):\n[removed]\n\n#### Comment ID lpzbnul with +1 score by [(BobbyWibowo, Reddit, 2024-10-02)](https://www.reddit.com/r/Genshin_Impact/comments/1fuguhl/oumi_janta_is_threatening_to_sue_mihoyo_for/lpzbnul/) (in reply to ID lpz9n5c):\nyea, OP also mentioned it by name in the post itself\n\n## Comment ID lq3vkwg with +2 score by [(Eclair_Farron, Reddit, 2024-10-03)](https://www.reddit.com/r/Genshin_Impact/comments/1fuguhl/oumi_janta_is_threatening_to_sue_mihoyo_for/lq3vkwg/) (in reply to ID 1fuguhl):\nI hope this fake crying \"influencer\" person goes ahead to sue Mihoyo, in \"80 countries\". Most probably 80 youtubers are going to make their little documentary videos about the pathetic story of the \"influencer\" got broke by suing the company in 80 countries!\n\n## Comment ID lpz9vf1 with +3 score by [(JiMyeong, Reddit, 2024-10-02)](https://www.reddit.com/r/Genshin_Impact/comments/1fuguhl/oumi_janta_is_threatening_to_sue_mihoyo_for/lpz9vf1/) (in reply to ID 1fuguhl):\nI remember people comparing her movement and Xilonen's a few weeks ago. \n\nWhile it is weird,  that it is so similar. I don't think any legal action would hold any grounds. She even says herself that she doesn't own these moves.\n\nI honestly don't even think that suing a billion dollar company over a dance would be even worth it.\n\n## Comment ID lpz8lk7 with +2 score by [(lughrevenge23, Reddit, 2024-10-02)](https://www.reddit.com/r/Genshin_Impact/comments/1fuguhl/oumi_janta_is_threatening_to_sue_mihoyo_for/lpz8lk7/) (in reply to ID 1fuguhl):\ndoes copying dance even illegal?\n\n### Comment ID lpz8u41 with +2 score by [(Lawyer_0wl, Reddit, 2024-10-02)](https://www.reddit.com/r/Genshin_Impact/comments/1fuguhl/oumi_janta_is_threatening_to_sue_mihoyo_for/lpz8u41/) (in reply to ID lpz8lk7):\nNope\n\n## Comment ID lpzcuhx with +2 score by [(Akikala, Reddit, 2024-10-02)](https://www.reddit.com/r/Genshin_Impact/comments/1fuguhl/oumi_janta_is_threatening_to_sue_mihoyo_for/lpzcuhx/) (in reply to ID 1fuguhl):\nYeah, sounds like a lot of bullshit and just an attempt to maybe get some \"easy\" money from a rich company.\n\n## Comment ID lq07q2b with +2 score by [(LolLmaoEven, Reddit, 2024-10-02)](https://www.reddit.com/r/Genshin_Impact/comments/1fuguhl/oumi_janta_is_threatening_to_sue_mihoyo_for/lq07q2b/) (in reply to ID 1fuguhl):\nLmao. Imagine thinking you deserve to be paid for a couple of dance moves you do.\n\n## Comment ID lq0bi7g with +2 score by [(irrocau, Reddit, 2024-10-02)](https://www.reddit.com/r/Genshin_Impact/comments/1fuguhl/oumi_janta_is_threatening_to_sue_mihoyo_for/lq0bi7g/) (in reply to ID 1fuguhl):\nLol, is she that skater in yellow? If not hoyo, I would honestly never even stumble upon her video, and I imagine it's the same for a lot of other people who aren't in the skating. And instead of being grateful her views increased because of Genshin, she's trying to sue them for money for a move she doesn't even own? I'll be sure to leave a dislike if any of her videos pop up in my feed.\n\n## Comment ID lq25csz with +2 score by [(zephwilkerson, Reddit, 2024-10-02)](https://www.reddit.com/r/Genshin_Impact/comments/1fuguhl/oumi_janta_is_threatening_to_sue_mihoyo_for/lq25csz/) (in reply to ID 1fuguhl):\nYou can't own movement. You can probably find a hundred different videos of someone dancing this very basic ass routine online.\n\n## Comment ID lpzjnwe with +1 score by [(azaleapom, Reddit, 2024-10-02)](https://www.reddit.com/r/Genshin_Impact/comments/1fuguhl/oumi_janta_is_threatening_to_sue_mihoyo_for/lpzjnwe/) (in reply to ID 1fuguhl):\ni find it unfortunate that OP has been downvoted when this is a fun discussion point but it will be very interesting to see where it goes (if anywhere)\n\n## Comment ID lpz9wxw with +1 score by [(Low_Artist_7663, Reddit, 2024-10-02)](https://www.reddit.com/r/Genshin_Impact/comments/1fuguhl/oumi_janta_is_threatening_to_sue_mihoyo_for/lpz9wxw/) (in reply to ID 1fuguhl):\nHow about putting spoiler warning on leaked animations?\n\nOfficial main sub with no \"no leaks\" rule btw\n\n### Comment ID lpza4i5 with +4 score by [(viewsmart123, Reddit, 2024-10-02)](https://www.reddit.com/r/Genshin_Impact/comments/1fuguhl/oumi_janta_is_threatening_to_sue_mihoyo_for/lpza4i5/) (in reply to ID lpz9wxw):\nthey already shown it in the 5.1 trailer right ? so its not a leaks anymore\n\n#### Comment ID lpzm25v with +6 score by [(hudashick, Reddit, 2024-10-02)](https://www.reddit.com/r/Genshin_Impact/comments/1fuguhl/oumi_janta_is_threatening_to_sue_mihoyo_for/lpzm25v/) (in reply to ID lpza4i5):\nNah the video she used for the idle was literally taken from the character page. Tbh she could be sued herself for leaking an unreleased footage lmao\n\n#### Comment ID lpzbday with +2 score by [(Low_Artist_7663, Reddit, 2024-10-02)](https://www.reddit.com/r/Genshin_Impact/comments/1fuguhl/oumi_janta_is_threatening_to_sue_mihoyo_for/lpzbday/) (in reply to ID lpza4i5):\nThis video in particular is a leak, not part of a livestream\n\n## Comment ID lpzimhb with +1 score by [(Healthy-Refuse5904, Reddit, 2024-10-02)](https://www.reddit.com/r/Genshin_Impact/comments/1fuguhl/oumi_janta_is_threatening_to_sue_mihoyo_for/lpzimhb/) (in reply to ID 1fuguhl):\nShe should hope she has a good lawyer\n\n## Comment ID lpzivd3 with +1 score by [(Nyancromancer, Reddit, 2024-10-02)](https://www.reddit.com/r/Genshin_Impact/comments/1fuguhl/oumi_janta_is_threatening_to_sue_mihoyo_for/lpzivd3/) (in reply to ID 1fuguhl):\nShe should have made like Nintendo and Filed a patent for a dance move if she wanted this to succeed, this just looks to be attention seeking\n\n## Comment ID lpzm1px with +1 score by [(MasterTBC, Reddit, 2024-10-02)](https://www.reddit.com/r/Genshin_Impact/comments/1fuguhl/oumi_janta_is_threatening_to_sue_mihoyo_for/lpzm1px/) (in reply to ID 1fuguhl):\nShe is just trying to get viral\n\n## Comment ID lq0526z with +1 score by [(WonnieOnWeddit, Reddit, 2024-10-02)](https://www.reddit.com/r/Genshin_Impact/comments/1fuguhl/oumi_janta_is_threatening_to_sue_mihoyo_for/lq0526z/) (in reply to ID 1fuguhl):\nIf there is evidence that the developers scanned her original video to create a 1 to 1 animation for sale, then perhaps there is a smidgen of a chance for a legal argument to even start. If a performer copied her moves and mo-capped it, then she can fuck all the way off.\n\n## Comment ID lq11ap3 with +1 score by [(xd_ZelnikM, Reddit, 2024-10-02)](https://www.reddit.com/r/Genshin_Impact/comments/1fuguhl/oumi_janta_is_threatening_to_sue_mihoyo_for/lq11ap3/) (in reply to ID 1fuguhl):\nRemember backpack kid and the floss?\n\n## Comment ID lq1zj23 with +1 score by [(Admirable-Volume-404, Reddit, 2024-10-02)](https://www.reddit.com/r/Genshin_Impact/comments/1fuguhl/oumi_janta_is_threatening_to_sue_mihoyo_for/lq1zj23/) (in reply to ID 1fuguhl):\nLol I knew this would happen.\n\n## Comment ID lq21tzw with +1 score by [(HansFactory, Reddit, 2024-10-02)](https://www.reddit.com/r/Genshin_Impact/comments/1fuguhl/oumi_janta_is_threatening_to_sue_mihoyo_for/lq21tzw/) (in reply to ID 1fuguhl):\nShe can fucking try lol\n\n## Comment ID lq40s07 with +1 score by [(Atago1337, Reddit, 2024-10-03)](https://www.reddit.com/r/Genshin_Impact/comments/1fuguhl/oumi_janta_is_threatening_to_sue_mihoyo_for/lq40s07/) (in reply to ID 1fuguhl):\nAbsolute insanity\n\n## Comment ID lqa7ane with +1 score by [(Kelly_Info_Girl, Reddit, 2024-10-04)](https://www.reddit.com/r/Genshin_Impact/comments/1fuguhl/oumi_janta_is_threatening_to_sue_mihoyo_for/lqa7ane/) (in reply to ID 1fuguhl):\nYou know what I think I'll sue videogame companies for the jumping animations because I jump even tho I didn't create that move.\n\n## Comment ID lqjl15l with +1 score by [(Adventurous_League_8, Reddit, 2024-10-06)](https://www.reddit.com/r/Genshin_Impact/comments/1fuguhl/oumi_janta_is_threatening_to_sue_mihoyo_for/lqjl15l/) (in reply to ID 1fuguhl):\nYeah first of all how tf does she want to sue anybody while saying she doesnt even own it in the first place? Sounds dumb. Secondly since when can dance moves be copyrighted?\n\n### Comment ID lqjltis with +1 score by [(Adventurous_League_8, Reddit, 2024-10-06)](https://www.reddit.com/r/Genshin_Impact/comments/1fuguhl/oumi_janta_is_threatening_to_sue_mihoyo_for/lqjltis/) (in reply to ID lqjl15l):\nAlso why not be happy they're using a piece of art you created for something different for more ppl to see? I dont get it. Since shes not getting any money from having this move to be only executed by her?\n\n## Comment ID lql99mq with +1 score by [(Firm-Detail6608, Reddit, 2024-10-06)](https://www.reddit.com/r/Genshin_Impact/comments/1fuguhl/oumi_janta_is_threatening_to_sue_mihoyo_for/lql99mq/) (in reply to ID 1fuguhl):\nWhile I don’t think she would win, I do believe that Hoyo did copy her viral video because they know they are too big to get any legal consequences from it.\n\n## Comment ID lr30exi with +1 score by [(I_Want_Your_Soul666, Reddit, 2024-10-09)](https://www.reddit.com/r/Genshin_Impact/comments/1fuguhl/oumi_janta_is_threatening_to_sue_mihoyo_for/lr30exi/) (in reply to ID 1fuguhl):\nIt would be more like plagiarism... and even then... nothing will come from it.\n\n## Comment ID lrq0nyu with +1 score by [(None, Reddit, 2024-10-13)](https://www.reddit.com/r/Genshin_Impact/comments/1fuguhl/oumi_janta_is_threatening_to_sue_mihoyo_for/lrq0nyu/) (in reply to ID 1fuguhl):\nI think what she's getting at is not so much the dance move itself but the way she does it. They copied the exact way that she moves which I agree is weird if they're not even gonna tell her. Idk if that argument would fly in court but at the very least she's getting unwanted attention that she didn't ask for now. Which she would've gotten even without her posting this. Hoyoverse has kind of involved her in something they didn't need to without her permission. Some are saying she's just doing this for attention but Idk. I'd be a little bothered too.\n\n## Comment ID ls9rdp5 with +1 score by [(z_stormm, Reddit, 2024-10-16)](https://www.reddit.com/r/Genshin_Impact/comments/1fuguhl/oumi_janta_is_threatening_to_sue_mihoyo_for/ls9rdp5/) (in reply to ID 1fuguhl):\nShe fucked herself when she said she doesn't own the movements\n\n## Comment ID mbab6f9 with +1 score by [(PerfectBlueRequiem, Reddit, 2025-02-06)](https://www.reddit.com/r/Genshin_Impact/comments/1fuguhl/oumi_janta_is_threatening_to_sue_mihoyo_for/mbab6f9/) (in reply to ID 1fuguhl):\nI guess I would understand if they also added your likeness directly into the game but the movement isn't enough to justify plagiarism. You already dropped the keywords \"I don't own-\"... that's all we need to hear.\n\n## Comment ID lpzgxdp with +1 score by [(gabbylikesfruit, Reddit, 2024-10-02)](https://www.reddit.com/r/Genshin_Impact/comments/1fuguhl/oumi_janta_is_threatening_to_sue_mihoyo_for/lpzgxdp/) (in reply to ID 1fuguhl):\nIt sucks since I do think dance choreo could be better protected by copyright law for creators of dances, but admitting its not even your dance in the first place is some interesting legal strategy...like girl do not waste money on that fight 😭\n\n## Comment ID lpzb3qu with +1 score by [(Ne7herstorm, Reddit, 2024-10-02)](https://www.reddit.com/r/Genshin_Impact/comments/1fuguhl/oumi_janta_is_threatening_to_sue_mihoyo_for/lpzb3qu/) (in reply to ID 1fuguhl):\nShe's crazy lmao\n\n## Comment ID lpzcdnm with +1 score by [(Minerva_vic, Reddit, 2024-10-02)](https://www.reddit.com/r/Genshin_Impact/comments/1fuguhl/oumi_janta_is_threatening_to_sue_mihoyo_for/lpzcdnm/) (in reply to ID 1fuguhl):\nWe wuz dancer\n\n## Comment ID lpzcz12 with +1 score by [(MofoPro, Reddit, 2024-10-02)](https://www.reddit.com/r/Genshin_Impact/comments/1fuguhl/oumi_janta_is_threatening_to_sue_mihoyo_for/lpzcz12/) (in reply to ID 1fuguhl):\nEveryone wants an easy pay day , she has no shot at winning\n\n## Comment ID lpzp93b with +1 score by [(Mapanebu28, Reddit, 2024-10-02)](https://www.reddit.com/r/Genshin_Impact/comments/1fuguhl/oumi_janta_is_threatening_to_sue_mihoyo_for/lpzp93b/) (in reply to ID 1fuguhl):\nepitome of stupidity\n\n## Comment ID lq31q4f with +1 score by [(Ordinary_Badger_1480, Reddit, 2024-10-03)](https://www.reddit.com/r/Genshin_Impact/comments/1fuguhl/oumi_janta_is_threatening_to_sue_mihoyo_for/lq31q4f/) (in reply to ID 1fuguhl):\nSo for anyone thinking she's suing for copyright, she isn't. She's claiming they have somehow managed to make a 3d model copy her exact dancing from whatever performance video she was showing so I guess this would fall under plagiarism or something which is a more winnable and plausible case.\n\nNaturally this is going to fail as soon as they bring in the files of the the motion capture suit recording used to make the dance in game which will obviously be 100% similar compared to her so called \"99%\" if it even makes it to court which I really doubt.",
      "# Post ID 1jbrwqf: Deep Research Tools: Am I the only one feeling...underwhelmed? (OpenAI, Google, Open Source) with +167 score by [(mimirium_, Reddit, 2025-03-15)](https://www.reddit.com/r/LocalLLaMA/comments/1jbrwqf/deep_research_tools_am_i_the_only_one/)\nHey everyone,\n\nI've been diving headfirst into these \"Deep Research\" AI tools lately - OpenAI's thing, Google's Gemini version, Perplexity, even some of the open-source ones on GitHub. You know, the ones that promise to do all the heavy lifting of in-depth research for you. I was so hyped!\n\nI mean, the idea is amazing, right? Finally having an AI assistant that can handle literature reviews, synthesize data, and write full reports? Sign me up! But after using them for a while, I keep feeling like something's missing.\n\nLike, the biggest issue for me is accuracy. I’ve had to fact-check so many things, and way too often it's just plain wrong. Or even worse, it makes up sources that don't exist! It's also pretty surface-level. It can pull information, sure, but it often misses the whole context. It's rare I find truly new insights from it. Also, it just grabs stuff from the web without checking if a source is a blog or a peer reviewed journal. And once it starts down a wrong path, its so hard to correct the tool.\n\nAnd don’t even get me started on the limitations with data access - I get it, it's early days. But being able to pull private information would be so useful!\n\nI can see the potential here, I really do. Uploading files, asking tough questions, getting a structured report… It’s a big step, but I was kinda hoping for a breakthrough in saving time. I am just left slightly unsatisfied and wishing for something a little bit better.\n\nSo, am I alone here? What have your experiences been like? Has anyone actually found one of these tools that nails it, or are we all just beta-testing expensive (and sometimes inaccurate) search engines?\n\nTL;DR: These \"Deep Research\" AI tools are cool, but they still have accuracy issues, lack context, and need more data access. Feeling a bit underwhelmed tbh.\n\n## Comment ID mhwhc2n with +103 score by [(Banjo-Katoey, Reddit, 2025-03-15)](https://www.reddit.com/r/LocalLLaMA/comments/1jbrwqf/deep_research_tools_am_i_the_only_one/mhwhc2n/) (in reply to ID 1jbrwqf):\nI've used a few of the deep research tools. You're absolutely right that something is missing. What we have now is essentially a thorough summary of a long CoT.\n\n\nA few things are missing. They don't know how to weight sources properly, like they don't go and download a government budget pdf and read it if I ask a question about the budget for example. It will just use a bunch of sources commenting on the budget.\n\n\nAnother thing that's missing is agentic behavior. It needs to be able to interact with, not just read, websites. For example I should be able to ask for all of my Internet service provider options if I give it my address. The LLM should go to every ISP website and check. We have similar issues with real estate websites, car rentals, plane tickets, etc. \n\n\nAnd deep research needs to be able to run python code (including with Internet access) and simulate things.\n\n\nHonestly once these 3 things are fixed we're so back.\n\n### Comment ID mhxczvb with +17 score by [(Taenk, Reddit, 2025-03-15)](https://www.reddit.com/r/LocalLLaMA/comments/1jbrwqf/deep_research_tools_am_i_the_only_one/mhxczvb/) (in reply to ID mhwhc2n):\nThere is also a lack of contextual reasoning. This is particularly noticeable in cases where the field is evolving rapidly: More current sources should be weighed more strongly, but from the LLM's POV all content has equal weight.\n\nI speculate that the next iteration of deep research needs a combination of\n\n* tool use, to run analysis against data it found, or use search functions on websites it is fed\n* source understanding and source control to have at least a baseline of quality\n* thinking/reasoning to make more use of the knowledge it already has in its weights\n\nDeep research makes its next leap when a tool like Claude Code can leverage deep research to read the documentation of a completely new library (like the recently released Tauri) and immediately start using start using that library.\n\nMaybe an architectural change will be necessary beyond just more training. For me personally, a driving factor for more research and critical thinking is \"I don't know ...\" and \"I am unsure ...\", both of which the current generation of LLMs struggle with, as they have no introspection.\n\n### Comment ID mhwuiiv with +5 score by [(XInTheDark, Reddit, 2025-03-15)](https://www.reddit.com/r/LocalLLaMA/comments/1jbrwqf/deep_research_tools_am_i_the_only_one/mhwuiiv/) (in reply to ID mhwhc2n):\nAgree! Hope someone working on these projects will take note of these big areas for improvement. OpenAI definitely has the biggest lead because of the intelligence of its o3 model. But honestly other foundation models (R1 specifically!) should be more than enough to perform competent research. \n\nGenerally I’d say this can be summarized into “more agentic capabilities”, which in itself is a vague term and is certainly being worked on, but I think it’s more of being able to combine existing functionalities: computer/browser use, code interpreter, etc. \n\nIn fact, just learning how to use the browser properly will make deep research so much more effective; it’s basically an intelligent human researcher at that point.\n\n#### Comment ID mhwyb9z with +2 score by [(Banjo-Katoey, Reddit, 2025-03-15)](https://www.reddit.com/r/LocalLLaMA/comments/1jbrwqf/deep_research_tools_am_i_the_only_one/mhwyb9z/) (in reply to ID mhwuiiv):\nAgreed - I also think intelligence is already sufficient in o3. It's more of an engineering problem than a science problem at this point.\n\n#### Comment ID mkqhb3r with +1 score by [(Own_Bookkeeper_7387, Reddit, 2025-03-31)](https://www.reddit.com/r/LocalLLaMA/comments/1jbrwqf/deep_research_tools_am_i_the_only_one/mkqhb3r/) (in reply to ID mhwuiiv):\nThere's a YC backed startup that's helping agents navigate websites -> Browser Use\n\n### Comment ID mhyttsw with +2 score by [(ReasonablePossum_, Reddit, 2025-03-15)](https://www.reddit.com/r/LocalLLaMA/comments/1jbrwqf/deep_research_tools_am_i_the_only_one/mhyttsw/) (in reply to ID mhwhc2n):\nThey are like a lazy undergrad assistant\n \nThats it. They go where the easier path to \"definitive\" info lays, and dont question everytjing and correlate separate databases with the presented info as to get a proper \"intuition\" of where truth lies to actually research it instead of going wirh top google results. Most cited papers, and most upvoted articles/posts/comments.\n\nAnd as result you will get fed with the biases theirnsearch engines or most popular sources are embedded with. \n\nHonestly its easier to just research everything alone lol\n\n### Comment ID mhxq10o with +1 score by [(Recoil42, Reddit, 2025-03-15)](https://www.reddit.com/r/LocalLLaMA/comments/1jbrwqf/deep_research_tools_am_i_the_only_one/mhxq10o/) (in reply to ID mhwhc2n):\n>A few things are missing. They don't know how to weight sources properly, like they don't go and download a government budget pdf and read it if I ask a question about the budget for example. It will just use a bunch of sources commenting on the budget.\n\nI haven't tried it yet, but I wonder how easily you can steer these tools with \"only use peer-reviewed sources\" etc.\n\n## Comment ID mhwrnv0 with +19 score by [(xanduonc, Reddit, 2025-03-15)](https://www.reddit.com/r/LocalLLaMA/comments/1jbrwqf/deep_research_tools_am_i_the_only_one/mhwrnv0/) (in reply to ID 1jbrwqf):\nIts shallow reasearch for now, but that wont sell well...\n\n### Comment ID mhx4pen with +8 score by [(ColorlessCrowfeet, Reddit, 2025-03-15)](https://www.reddit.com/r/LocalLLaMA/comments/1jbrwqf/deep_research_tools_am_i_the_only_one/mhx4pen/) (in reply to ID mhwrnv0):\nShallow, yes, but Deep Research still goes beyond the requests (broader not deeper), and using an LM to help work out a broad and detailed query can help extract more value.\n\nThat said, it makes up shit, claims that one citation supports 5 different topics that aren't there, etc., etc. Definitely underwhelming. Sometimes useful. Checking results is a lot of work.\n\n## Comment ID mhwqgyw with +9 score by [(g33khub, Reddit, 2025-03-15)](https://www.reddit.com/r/LocalLLaMA/comments/1jbrwqf/deep_research_tools_am_i_the_only_one/mhwqgyw/) (in reply to ID 1jbrwqf):\nPretty much all the deep research I've tried so far is shit. And I've tried a variety of topics like PC part picker, electronics compatibility, general market research and some academic things. I've also noticed that generally LLMs answer better from training memory than inference RAG or search.\n\n## Comment ID mhwqve0 with +25 score by [(EtadanikM, Reddit, 2025-03-15)](https://www.reddit.com/r/LocalLLaMA/comments/1jbrwqf/deep_research_tools_am_i_the_only_one/mhwqve0/) (in reply to ID 1jbrwqf):\nThe thing about AI tools is that, like the search engines that revolutionized the internet, their main function is raising the *minimum performance bar*. So in the same way that an intern today might be told to “go Google it” for simple questions they don’t have answers to, in the future they’ll be asked to “go Deep Research it” for moderately difficult questions they don’t have answers to. \n\nFrom a national / industrial perspective, this is actually incredibly powerful, as raising the minimum competency is a tremendous multiplier on productivity. But for the average person, the value is no different from Google search back in the day. You’ll be expected to use it but it won’t automate your job, it’ll just mean you can’t get away with less any more. \n\nThis is actually the insight that Google and many Chinese companies realized but Anthropic and Open AI are still struggling to understand. AI is like search, it’ll become normalized and as with any normalized tool, people will expect it to be free*. No one is going to pay $2000/month or even $200/month for a technology that’s really about raising the minimum competency for a country. There’s just not enough personal value/advantage gained. Consequently the only way forward is free* AI, by which I mean you become the product but the service is free*.\n\n### Comment ID mi398ti with +4 score by [(InsideYork, Reddit, 2025-03-16)](https://www.reddit.com/r/LocalLLaMA/comments/1jbrwqf/deep_research_tools_am_i_the_only_one/mi398ti/) (in reply to ID mhwqve0):\nThe problem is that it’s full of giant errors and bad reasoning. It doesn’t raise competence, it obscures it. It’s the information version of subprime mortgages.\n\n#### Comment ID mirzsjf with +1 score by [(former_physicist, Reddit, 2025-03-20)](https://www.reddit.com/r/LocalLLaMA/comments/1jbrwqf/deep_research_tools_am_i_the_only_one/mirzsjf/) (in reply to ID mi398ti):\nstop using the pleb free models and pay for an advanced one\n\n## Comment ID mhwefr7 with +14 score by [(cant-find-user-name, Reddit, 2025-03-15)](https://www.reddit.com/r/LocalLLaMA/comments/1jbrwqf/deep_research_tools_am_i_the_only_one/mhwefr7/) (in reply to ID 1jbrwqf):\n\\> These \"Deep Research\" AI tools are cool, but they still have accuracy issues, lack context, and need more data access. Feeling a bit underwhelmed tbh.\n\nI feel the same about most ai tools. They are definitely useful, and I use them in my daily life (well not deep research because I didn't like the perlexity one and the google one got free just a few days ago) but they are never as good as people claim they are.\n\n## Comment ID mhwdl7q with +9 score by [(Dogeboja, Reddit, 2025-03-15)](https://www.reddit.com/r/LocalLLaMA/comments/1jbrwqf/deep_research_tools_am_i_the_only_one/mhwdl7q/) (in reply to ID 1jbrwqf):\nYea, they would be much more useful if there was a mode that used only truly great sources like reputable books, research articles, papers etc. Just Googling stuff with an agent is bound to cause problems\n\n### Comment ID mhwekfh with +5 score by [(ComplexIt, Reddit, 2025-03-15)](https://www.reddit.com/r/LocalLLaMA/comments/1jbrwqf/deep_research_tools_am_i_the_only_one/mhwekfh/) (in reply to ID mhwdl7q):\nThat exactly what I try to achieve here https://github.com/LearningCircuit/local-deep-research\n\n#### Comment ID mhwfggj with +5 score by [(Dogeboja, Reddit, 2025-03-15)](https://www.reddit.com/r/LocalLLaMA/comments/1jbrwqf/deep_research_tools_am_i_the_only_one/mhwfggj/) (in reply to ID mhwekfh):\nLooks good! But books specifically, that could be a very hard problem to solve. I don't think there even exists a service that could properly retrieve book contents. Probably some internal university systems would be the best for this, those are the ones real researchers use.   \n[https://search.worldcat.org/](https://search.worldcat.org/) this could be an interesting service too.\n\n### Comment ID mhxtn36 with +3 score by [(NeedleworkerDeer, Reddit, 2025-03-15)](https://www.reddit.com/r/LocalLLaMA/comments/1jbrwqf/deep_research_tools_am_i_the_only_one/mhxtn36/) (in reply to ID mhwdl7q):\nI've been reading a lot of doctoral papers recently, and I've noticed that almost all of them have broken citations, incorrect conclusions and sections where they just plain misread the papers they themselves are citing.\n\nI'm getting kind of cynical that \"great sources\" exist. I think you have to look at a very large sample and then deduce the answer from there.\n\nIt sounds obvious, but I'm not convinced limiting the AIs sources will achieve this. It needs to \"think for itself\" in order to actually extract information.\n\n#### Comment ID mhxwups with +1 score by [(LetterRip, Reddit, 2025-03-15)](https://www.reddit.com/r/LocalLLaMA/comments/1jbrwqf/deep_research_tools_am_i_the_only_one/mhxwups/) (in reply to ID mhxtn36):\nYep or they are citing a source, but what they are citing is simply being cited (or miscited) from another source.\n\n### Comment ID mhzsg35 with +1 score by [(Zagorim, Reddit, 2025-03-15)](https://www.reddit.com/r/LocalLLaMA/comments/1jbrwqf/deep_research_tools_am_i_the_only_one/mhzsg35/) (in reply to ID mhwdl7q):\nperplexity deep searches can be restricted to academic and scientific papers. I don't know how reputable they are but I tried asking about a health issue I had last year (fixed now) and the answers between an entire internet search and a search limited to papers were very different. The academic deep research had a lot more probable causes and quite complex medical and scientific explanations, it was pretty interesting, although I obviously can't tell if it was all true because no doctor was able to diagnose my issue and it eventually just went away (probably cause i improved my diet for several months)\n\n## Comment ID mhwxuvy with +4 score by [(Longjumping_Form1862, Reddit, 2025-03-15)](https://www.reddit.com/r/LocalLLaMA/comments/1jbrwqf/deep_research_tools_am_i_the_only_one/mhwxuvy/) (in reply to ID 1jbrwqf):\nWhat kind of things do you work on usually ? I tried the perplexity one though most of the time it does work better than their regular search\n\n### Comment ID mkqkprv with +1 score by [(Own_Bookkeeper_7387, Reddit, 2025-03-31)](https://www.reddit.com/r/LocalLLaMA/comments/1jbrwqf/deep_research_tools_am_i_the_only_one/mkqkprv/) (in reply to ID mhwxuvy):\nhow often do you use preplexity?\n\n## Comment ID mhxa549 with +4 score by [(AaronFeng47, Reddit, 2025-03-15)](https://www.reddit.com/r/LocalLLaMA/comments/1jbrwqf/deep_research_tools_am_i_the_only_one/mhxa549/) (in reply to ID 1jbrwqf):\n1. LLM could hallucinate \n2. Online search results are not always reliable \n3. LLM's performance drops as context gets longer \n\n\nThat's why I don't use deep search for work, so many things can go wrong and will go wrong\n\n### Comment ID mkqkkun with +1 score by [(Own_Bookkeeper_7387, Reddit, 2025-03-31)](https://www.reddit.com/r/LocalLLaMA/comments/1jbrwqf/deep_research_tools_am_i_the_only_one/mkqkkun/) (in reply to ID mhxa549):\nwhat does your work entail? when would you want to use deep research in your workflow?\n\n## Comment ID mhwnn1n with +3 score by [(NoGuarantee547, Reddit, 2025-03-15)](https://www.reddit.com/r/LocalLLaMA/comments/1jbrwqf/deep_research_tools_am_i_the_only_one/mhwnn1n/) (in reply to ID 1jbrwqf):\nYes, actually I am trying to build an agent by giving it all the code access, adocs, and uml using RAG... the main intension was to make it generate the unit test for any file and also to answers a few plausible query from any other person from other domain and any new person in the team.... but the llm is interpreting it badly...\n\n## Comment ID mhwxm08 with +3 score by [(latestagecapitalist, Reddit, 2025-03-15)](https://www.reddit.com/r/LocalLLaMA/comments/1jbrwqf/deep_research_tools_am_i_the_only_one/mhwxm08/) (in reply to ID 1jbrwqf):\nAs with current LLM/CoT AI ... they are going to be useful to a few people in a few ways\n\nI am super hyped about AI but I think it's going to be very vertical in where it makes a difference\n\nChatbots for customer service, user manuals etc. ... LLMs for coding ... CoT for those odd questions you have once or twice a day that you used to Google for ... Deep Research for sparking ideas in PhD type situations\n\nASI will end up being isolated to some areas like medicine, maths ...\n\nAI ain't replacing as many people in the West as everyone thinks it will ... most of what it replaces well has already been outsourced overseas\n\n## Comment ID mhx9n4h with +3 score by [(Royal_Treacle4315, Reddit, 2025-03-15)](https://www.reddit.com/r/LocalLLaMA/comments/1jbrwqf/deep_research_tools_am_i_the_only_one/mhx9n4h/) (in reply to ID 1jbrwqf):\nI think what they’re doing with Grok is getting better - it looks like from the results that they’re using the full model instances for the internet queries. But really you can probably get better results by using the APIs with a good model (R1 is unquanized on Azure and o3-mini-high is not too expensive - but Anthropic models are probably the best atm since they’re optimizing for high density info [code] - until grok API comes out but who knows how they’ll bastardize the attention layers by then or quantize it; gotta make that $$$)\n\n### Comment ID mhxa052 with +1 score by [(Royal_Treacle4315, Reddit, 2025-03-15)](https://www.reddit.com/r/LocalLLaMA/comments/1jbrwqf/deep_research_tools_am_i_the_only_one/mhxa052/) (in reply to ID mhx9n4h):\n*if you look for GH open source implementations of research and set the APIs there to use aforementioned models\n\n## Comment ID mhy51xt with +3 score by [(Sea_Sympathy_495, Reddit, 2025-03-15)](https://www.reddit.com/r/LocalLLaMA/comments/1jbrwqf/deep_research_tools_am_i_the_only_one/mhy51xt/) (in reply to ID 1jbrwqf):\n9 out of 10 times ive used any deep research mode from any provider about a subject im familiar with, its been wrong.\n\n### Comment ID mkqkwk6 with +1 score by [(Own_Bookkeeper_7387, Reddit, 2025-03-31)](https://www.reddit.com/r/LocalLLaMA/comments/1jbrwqf/deep_research_tools_am_i_the_only_one/mkqkwk6/) (in reply to ID mhy51xt):\nwhat have you use it for?\n\n#### Comment ID mkqmhqa with +1 score by [(Sea_Sympathy_495, Reddit, 2025-03-31)](https://www.reddit.com/r/LocalLLaMA/comments/1jbrwqf/deep_research_tools_am_i_the_only_one/mkqmhqa/) (in reply to ID mkqkwk6):\nFrom obscure enterprise programs to find documentation or forums with information to video games to buy based on genre it’s been wrong about all of them once you look at the details of the report\n\n## Comment ID mhwf43i with +20 score by [(custodiam99, Reddit, 2025-03-15)](https://www.reddit.com/r/LocalLLaMA/comments/1jbrwqf/deep_research_tools_am_i_the_only_one/mhwf43i/) (in reply to ID 1jbrwqf):\nNow you can slowly see that why we are VERY far from AGI. Natural language is NOT thinking. A really good research assistant can think. LLMs can't really think, because they have no thoughts. So no AGI for us (at least for now).\n\n### Comment ID mhwft33 with +8 score by [(No_Afternoon_4260, Reddit, 2025-03-15)](https://www.reddit.com/r/LocalLLaMA/comments/1jbrwqf/deep_research_tools_am_i_the_only_one/mhwft33/) (in reply to ID mhwf43i):\ndo not confuse the verb and the mind\n\n## Comment ID mhwqmsa with +4 score by [(FullOf_Bad_Ideas, Reddit, 2025-03-15)](https://www.reddit.com/r/LocalLLaMA/comments/1jbrwqf/deep_research_tools_am_i_the_only_one/mhwqmsa/) (in reply to ID 1jbrwqf):\nI find openai deep research useful. I've found a matching pc case that is big enough to hold 2 air cooled 3090 ti's and was cheap. I could have done the same myself, searching for them through the internet. But it would take longer. Instead I just had a list of options, I double checked the most attractive one and got it the next day. I also had it do the query about dark oxygen, rebuttal and spread of the original paper and rebuttal on social media - it was better quality than an average human would do in a week and it took 10 minutes.\n\nI have my expectations tempered, ai agents with internet access can be useful, but they'll also be stupid faster. Think of it like hiring a somewhat smart random reddit user to search the internet for you and come back with a reply in 1 week, not like PhD student expert thinking about problems relevant to the particular field for a year. That's how it feels like.\n\n\nEdit: your post has the unlikeable llm slop vibe to it, if you're rewriting your text with llm before posting them, I think a less slopped llm would work better.\n\n### Comment ID mhx5tse with +4 score by [(Strel0k, Reddit, 2025-03-15)](https://www.reddit.com/r/LocalLLaMA/comments/1jbrwqf/deep_research_tools_am_i_the_only_one/mhx5tse/) (in reply to ID mhwqmsa):\nI'm generally very conservative and underwhelmed by new AI products/models, but Deep Research is so good that I am keeping my 200/mo subscription. For context, these days I am bouncing between the Anthropic Console (where I can max out the thinking tokens), ChatGPT o1-pro and Deep Research. I think I run about 10 Deep Research queries per day so I am definitely getting my money's worth.\n\nI don't think it's quite at \"research for a week\" level (maybe for less technical people that aren't very good at Googling), but it does save me 30-45 minutes of research time for many tasks and (more importantly) prevents me from getting distracted or going down random rabbit holes.\n\nI think the key is you really need to write out a detailed description of what you are looking for, I'm talking like a minimum 1-2 paragraphs. Otherwise you get very superficial responses that are no better than what any LLM can provide.\n\n#### Comment ID mhxcxxr with +1 score by [(Popular_Brief335, Reddit, 2025-03-15)](https://www.reddit.com/r/LocalLLaMA/comments/1jbrwqf/deep_research_tools_am_i_the_only_one/mhxcxxr/) (in reply to ID mhx5tse):\nThis one gets it. The more you write and tell it what you want the better. Like people it can go on rabbit hole adventures too lol\n\n### Comment ID mhwsaph with +3 score by [(g33khub, Reddit, 2025-03-15)](https://www.reddit.com/r/LocalLLaMA/comments/1jbrwqf/deep_research_tools_am_i_the_only_one/mhwsaph/) (in reply to ID mhwqmsa):\nThis is interesting. For my search it could not find consumer motherboards that can support 3 GPUs and which of them can do x8,x8,x4 and also stuff about ring, star topology for memory etc. Maybe the case was simple enough. BTW which case did it suggest? I'm using dual 3090s in a LianLi O11 XL (non evo) as it can support two PSUs\n\n#### Comment ID mhwxvqh with +3 score by [(Asthenia5, Reddit, 2025-03-15)](https://www.reddit.com/r/LocalLLaMA/comments/1jbrwqf/deep_research_tools_am_i_the_only_one/mhwxvqh/) (in reply to ID mhwsaph):\nIf you are okay with pcie 4.0 for the 4x slot, higher end z790, z890, x870e and x670 boards do support 8x 8x pcie 5.0, and then a 4.0 x4 slot.\n\n#### Comment ID mhx3ty9 with +2 score by [(FullOf_Bad_Ideas, Reddit, 2025-03-15)](https://www.reddit.com/r/LocalLLaMA/comments/1jbrwqf/deep_research_tools_am_i_the_only_one/mhx3ty9/) (in reply to ID mhwsaph):\nYeah, the case query wasn't the hardest one you could imagine - it's a metal box and it's not that hard to make the metal box bigger. Motherboard support for 3 GPUs with decent amount of PCI-E lanes is harder to get for sure, since that's a more high-tech item. But I would argue that even if you asked someone who tracks hardware and is IT Pro, they would have issues finding one for you.\n\nHere's the [chat in question](https://chatgpt.com/share/67d5822c-52c4-8004-a022-0446937b7a89). I went with used old Cooler Master Cosmos II, I found one locally 80 USD and made sure PSU would fit on-site. Didn't mount the second 3090 TI just yet because I broke the power cable, had it fixed just earlier today and I'm waiting to get new 12VHPWR cables from moddiy, but it should work since there are 2 more PCI slots in it.\n\n### Comment ID mi8jfzi with +1 score by [(Mochila-Mochila, Reddit, 2025-03-17)](https://www.reddit.com/r/LocalLLaMA/comments/1jbrwqf/deep_research_tools_am_i_the_only_one/mi8jfzi/) (in reply to ID mhwqmsa):\nI asked chat GPT some info about a low-end AMD Threadripper, and *twice in a row* this unhelpful dummy served me wrong information. The architecture generation was wrong, as was the core count.\n\nSo yeah, since then, I too have had my expectations tempered 😒\n\nIf a model can't serve me facts about something as straightforward as a computer part, we've got a long way to go...\n\n#### Comment ID mi93elx with +1 score by [(FullOf_Bad_Ideas, Reddit, 2025-03-17)](https://www.reddit.com/r/LocalLLaMA/comments/1jbrwqf/deep_research_tools_am_i_the_only_one/mi93elx/) (in reply to ID mi8jfzi):\nWas this deepresearch specifically or just plain 4o without internet access?\n\n## Comment ID mhx41ac with +2 score by [(Just_Young7838, Reddit, 2025-03-15)](https://www.reddit.com/r/LocalLLaMA/comments/1jbrwqf/deep_research_tools_am_i_the_only_one/mhx41ac/) (in reply to ID 1jbrwqf):\nI am oversimplifying things*, but deep research is just two simple things:\n\n\n1) System (initial) prompt\n\n\n2) Tool calling to fetch info from public sources\n\n\nWhether the whole system is \"agentic\" or not depends only on whether it will change the initial search query if/when it finds something, but this is also programmed in the system prompt.\n\n\nYes, LLM should be able to do tool calling.\nNo, it is not required to do a fine-tune to follow specific deep research directions, but if you can do it (question of a very wise dataset), it will  make things better. Or not. \n\n\nSystem prompt is really enough to control the strategy of deep research.\n\n\nRequired tools for tool calling are web search API (tavily, jina, perplexity, searxng...) and/or specific open information pointer (arxiv, reddit, specific website via API or just curl).\n\n\nSo if you want to improve results of your Deep Research, find the one where you control your system prompt and information sources, and tailor it for your needs.\n\n\nIt's hard for a general-purpose tool to fulfill precise, specific needs. It's fairly easy to adapt one.\n\n\nLook for templates of Vercel AI SDK or Langroid. Implementing tools for web search will require 15 minutes (20 if you need your own Searxng instance to not pay any 3rd party API). \n\n\nThen, the only thing you should test-and-learn is system prompt. \n\n\n*on purpose as many AI-related topics are moving in the opposite direction, overcomplexing things\n\n## Comment ID mhxgimh with +2 score by [(KarezzaReporter, Reddit, 2025-03-15)](https://www.reddit.com/r/LocalLLaMA/comments/1jbrwqf/deep_research_tools_am_i_the_only_one/mhxgimh/) (in reply to ID 1jbrwqf):\nThey are very, very useful. I find Perplexity is the best one to use routinely as it takes maybe 2 minutes. I have used Grok 3 and it is pretty good, not as good. ChatGPT’s is fine but take more like 8 or 10 minutes And isn’t much better than Perplexity. So I’m really using Perplexity’s a lot. It uses DeepSeek as its reasoning model, and presents excellent reports with citations, and not that much halucination.\n\nI would love to see the upcoming local models web enabled.\n\n## Comment ID mhxjz6t with +2 score by [(xor_2, Reddit, 2025-03-15)](https://www.reddit.com/r/LocalLLaMA/comments/1jbrwqf/deep_research_tools_am_i_the_only_one/mhxjz6t/) (in reply to ID 1jbrwqf):\nWe just got this technology moment ago and people expect it to be perfect.\n\nNot even Internet developed as fast as AI does today and look where we are compared to 90's\n\nToday for AI is the Internet's 90's...\n\n## Comment ID mhxk015 with +2 score by [(deoxykev, Reddit, 2025-03-15)](https://www.reddit.com/r/LocalLLaMA/comments/1jbrwqf/deep_research_tools_am_i_the_only_one/mhxk015/) (in reply to ID 1jbrwqf):\nI'm currently using OpenAI's Deep Research heavily. Yes it has it's limitations, but it's on par with an smart intern. I wouldn't expect an intern to produce a PHD-level dissertation. And especially not within 30 minutes.  \n  \nWhere it excels at are \"survey of the field\" sorts of questions, or \"look for precedence of X\" sorts of questions.\n\n### Comment ID mkqy1it with +1 score by [(Own_Bookkeeper_7387, Reddit, 2025-03-31)](https://www.reddit.com/r/LocalLLaMA/comments/1jbrwqf/deep_research_tools_am_i_the_only_one/mkqy1it/) (in reply to ID mhxk015):\nwhat things do you use it for?\n\n## Comment ID mi1m1ar with +2 score by [(Flashy_Layer3713, Reddit, 2025-03-16)](https://www.reddit.com/r/LocalLLaMA/comments/1jbrwqf/deep_research_tools_am_i_the_only_one/mi1m1ar/) (in reply to ID 1jbrwqf):\nGrok is great and free\n\n## Comment ID mhwxpzn with +4 score by [(optimisticalish, Reddit, 2025-03-15)](https://www.reddit.com/r/LocalLLaMA/comments/1jbrwqf/deep_research_tools_am_i_the_only_one/mhwxpzn/) (in reply to ID 1jbrwqf):\nI had an excellent result from the new Grok 3 with its its Deep module activated, for a very tricky humanities question which almost no scholars had addressed.  Identify the similarities between *Lord of the Rings*'s Bombadil and *The Hobbit*'s Beorn character (though the test task was phrased far better than the gist given here).\n\n## Comment ID mhwdzrg with +5 score by [(BorderKeeper, Reddit, 2025-03-15)](https://www.reddit.com/r/LocalLLaMA/comments/1jbrwqf/deep_research_tools_am_i_the_only_one/mhwdzrg/) (in reply to ID 1jbrwqf):\nThis whole AI revolution feels similar to when the assistants came out. I bought the Google flowerpot from the US and tried using my google assistant extensively. Still use it for stuff once in a while, but I was so dissapointed that the promises of \"AI assistant\" did not come through.\n\n\nEven today with my experience in AI we are still not there, altough chain of thought, and agent systems are \"getting closer\" ie research wise they could actually achieve this maybe, but honestly I think we will fall short, hype will die down, AI tech bubble will burst, and people will finally start realising what sort of limited help can these tools do.\n\n### Comment ID mhwo3w7 with +4 score by [(Popular_Brief335, Reddit, 2025-03-15)](https://www.reddit.com/r/LocalLLaMA/comments/1jbrwqf/deep_research_tools_am_i_the_only_one/mhwo3w7/) (in reply to ID mhwdzrg):\nLimited help? Are you ok?\n\n#### Comment ID mhwocyt with +8 score by [(BorderKeeper, Reddit, 2025-03-15)](https://www.reddit.com/r/LocalLLaMA/comments/1jbrwqf/deep_research_tools_am_i_the_only_one/mhwocyt/) (in reply to ID mhwo3w7):\nHave you read OPs post? Take a guess how much money he is spending for the experience he is getting?\n\nWe can argue about the future growth of this tech and dream, but reality today is much bleaker than Sam Altman would want you to believe. There’s a great quote from someone “AI is incredibly clever in all areas except those I am expert in. There it’s making a lot of mistakes”\n\n### Comment ID mhwsip6 with +1 score by [(inagy, Reddit, 2025-03-15)](https://www.reddit.com/r/LocalLLaMA/comments/1jbrwqf/deep_research_tools_am_i_the_only_one/mhwsip6/) (in reply to ID mhwdzrg):\nI think AI development will keep going on. There's a vast amount of research and trials happenning still. What will die down is the unrealistic marketing and hype which tries to turn it into money with  too early and overly ambitious commercial products. See eg. how Apple backed down with Apple Intelligence recently.\n\n## Comment ID mhwehd2 with +1 score by [(ComplexIt, Reddit, 2025-03-15)](https://www.reddit.com/r/LocalLLaMA/comments/1jbrwqf/deep_research_tools_am_i_the_only_one/mhwehd2/) (in reply to ID 1jbrwqf):\nWhat do you think of this report? https://github.com/LearningCircuit/local-deep-research/blob/main/examples/detailed_report_how_to_improve_retrieval_augmented_generation_in_p.md\n\n## Comment ID mhwgrha with +1 score by [(drulee, Reddit, 2025-03-15)](https://www.reddit.com/r/LocalLLaMA/comments/1jbrwqf/deep_research_tools_am_i_the_only_one/mhwgrha/) (in reply to ID 1jbrwqf):\nI think you can already use Deep Research for a limited amount of topics.\n\nMaybe not for super accurate information regarding science topics.\n\nFurthermore don’t use it for image creation: i once accidentally activated Deep Research for creation of a funny illustration and not only the illustration wasn’t nearly as good as a non-deepresearch one, the sources were complete  nonsense.\n\nBut if you seek information about holiday locations, or need a comparison of tariffs like cell phone tariffs or maybe bank account or trading conditions, etfs or whatever, give it a shot.\n\n I think it’s a good first glance at topics even if details need to be further checked and researched\n\n## Comment ID mhwp9ag with +1 score by [(2TierKeir, Reddit, 2025-03-15)](https://www.reddit.com/r/LocalLLaMA/comments/1jbrwqf/deep_research_tools_am_i_the_only_one/mhwp9ag/) (in reply to ID 1jbrwqf):\nI’ve been quite impressed by ChatGPT deep research. I’ve only used it a few times but each time has produced very solid outputs in areas I’m an SME.\n\n## Comment ID mhy257l with +1 score by [(alvisanovari, Reddit, 2025-03-15)](https://www.reddit.com/r/LocalLLaMA/comments/1jbrwqf/deep_research_tools_am_i_the_only_one/mhy257l/) (in reply to ID 1jbrwqf):\nAgreed - I think it mainly comes down to the fact that everyone has their own workflow. Perhaps we will see more success wiht deidcated producst for analysts, journalists etc. I tried a different approach by making it more manual rather than one-shotting it (you can select the search results you want to use as context):https://github.com/btahir/open-deep-research\n\n## Comment ID mhye74h with +1 score by [(defaultagi, Reddit, 2025-03-15)](https://www.reddit.com/r/LocalLLaMA/comments/1jbrwqf/deep_research_tools_am_i_the_only_one/mhye74h/) (in reply to ID 1jbrwqf):\nOh, you are definitely not alone in this! The hype is real, but so is the frustration. These tools are fast, sure, but accuracy? Hit or miss. And don’t even get me started on the made-up sources! They pull info but don’t really analyze it, and distinguishing between solid research and random blogs? Not their strong suit.\n\nThe potential is huge—file uploads, structured reports, automated reviews—but right now, it still feels like we’re beta-testing AI-powered search engines, not getting true deep research. I’d love to hear if anyone’s found one that actually nails it!\n\n## Comment ID mhyy59h with +1 score by [(StealthX051, Reddit, 2025-03-15)](https://www.reddit.com/r/LocalLLaMA/comments/1jbrwqf/deep_research_tools_am_i_the_only_one/mhyy59h/) (in reply to ID 1jbrwqf):\nI use it as a slightly fancier Google search. If I need a really simple thing done (like what computer to buy at a certain price point, what journal should I submit to) I can confidently expect it to be able to do a job about 60% as good as I can without the time and effort of myself sitting down to do that. I will say both Google gemini and openai is significantly better than perplexity. But they're honestly nothing more than a glorified Google search. Good for simple tasks, so useful enough to be useful but I wouldn't use them on anything resembling mission critical.\n\n## Comment ID mhyztom with +1 score by [(Paulonemillionand3, Reddit, 2025-03-15)](https://www.reddit.com/r/LocalLLaMA/comments/1jbrwqf/deep_research_tools_am_i_the_only_one/mhyztom/) (in reply to ID 1jbrwqf):\n'don't use youtube as a source' works ;P but gah....\n\n## Comment ID mhziquc with +1 score by [(wwwillchen, Reddit, 2025-03-15)](https://www.reddit.com/r/LocalLLaMA/comments/1jbrwqf/deep_research_tools_am_i_the_only_one/mhziquc/) (in reply to ID 1jbrwqf):\nI'm with you about the accuracy issues. If you're not confident it's pulling all the relevant sources *and* you need to double-check the output, I don't really know how much time you're saving. Like if you want a quick-ish report as a first-draft, then it's fine, but I don't think this is really replacing any of the work that I used to do a business analyst, e.g. compiling reports on competitive analysis, with this current generation of product.\n\n## Comment ID mi0jyut with +1 score by [(MindOrbits, Reddit, 2025-03-16)](https://www.reddit.com/r/LocalLLaMA/comments/1jbrwqf/deep_research_tools_am_i_the_only_one/mi0jyut/) (in reply to ID 1jbrwqf):\nProgress has worked like a fly-wheel. We can basically wait six months for updates that do things faster, cheaper, with broader applications for various failure modes. That is one trend, the other has to do with division of labor, specialization, data sources, and tools.  The future isn't going to be search engines, but Knowledge Databases with something like page rank for ideas, concepts, sources.\n\n## Comment ID mi109p8 with +1 score by [(kovnev, Reddit, 2025-03-16)](https://www.reddit.com/r/LocalLLaMA/comments/1jbrwqf/deep_research_tools_am_i_the_only_one/mi109p8/) (in reply to ID 1jbrwqf):\nMy take is that they're scrimping on compute.\n\nThere's a reason OpenAI charge $200/mth for that plan, and it's not to make money - it's to limit demand by pricing 99% of people out.\n\nThere is a ~0% chance that the closed source companies can service the entire world, and copy googles business model, for something that's so heavy on compute, IMO.\n\nIt would be trivially easy to have the models review their context limit worth of data, or even do that multiple times, summarizing at various stages, and then output more accurate answers. But they've obviously decided that x% accuracy is not worth order of magnitudes more compute.\n\n## Comment ID mi3lv9v with +1 score by [(QuoteDull, Reddit, 2025-03-16)](https://www.reddit.com/r/LocalLLaMA/comments/1jbrwqf/deep_research_tools_am_i_the_only_one/mi3lv9v/) (in reply to ID 1jbrwqf):\nI totally get the “fact checking so many things” I don’t think that will ever go away, and I think it’s for the best. I kind of treat it like an underling doing your research for you. You still want to be able to understand what it’s producing, and double check its findings. Imagine your submitting a study to an academic journal. You still want to spend the time and double check all the facts. \n\nIn terms of quality, I think it entirely depends on context and what you ask the AI to do. I got really impressive results with those this prompt I just used to research pharmacogenetics in ADHD:\n\n You will act as an expert medical researcher specializing in pharmacogenetics. Your task is to write a comprehensive and detailed graduate-level 15-page research paper on Attention-Deficit/Hyperactivity Disorder (ADHD), a disorder influenced by both genetic and environmental components. The paper should adhere to AMA formatting guidelines, with each section approximately 2-3 double-spaced pages (target word count: 4,500 - 5,000 words total). \n\r\nThis research paper should heavily emphasize pharmacogenetics and its impact on ADHD—specifically focusing on how pharmacogenetic insights influence diagnosis, treatment, and patient care strategies. Broader genetic and environmental factors should provide context but always anchor back to their pharmacogenetic relevance.\r\n\r\nThe paper must include the following sections:\r\n\r\n- A thorough discussion of the etiology of ADHD, integrated with pharmacogenetic considerations\r\n- An in-depth explanation of the fundamental biology behind ADHD, linking molecular pathways to pharmacogenetic targets\r\n- An analysis of the specific genes involved in ADHD, highlighting their role in pharmacogenetic treatment response, alongside candidate susceptibility genes\r\n- A review of the basic epidemiology of ADHD, covering prevalence, demographics, and risk factors, with connections to pharmacogenetic findings\r\n- An examination of environmental factors contributing to the etiology of ADHD and how these interact with genetic and pharmacogenetic variables\r\n- A critical exploration of an ELSI (Ethical, Legal, and Social Implications) issue that has emerged from pharmacogenetic research or the clinical application of pharmacogenomics in ADHD treatment\r\n\r\nAdditionally, incorporate recent pharmacogenomic trends and emerging research (within the last 5 years), including gene-drug response case studies where applicable. Aim to cite approximately 10-15 references, prioritizing primary research articles and reviews from peer-reviewed journals.\r\n\r\nThe paper should reflect my communication style:\r\n- Logical, clever, and analytically sharp\r\n- Concise, critical, and purposeful\r\n- Occasional, sparing use of dry or dark humor to sharpen key insights\r\n- Accessible yet deeply analytical, with no unnecessary filler\r\n\r\nThe tone should maintain scientific rigor while being engaging and thoughtfully critical, delivering complex pharmacogenetic concepts with clarity and precision.\n\n## Comment ID mi3rmpc with +1 score by [(DarkVoid42, Reddit, 2025-03-16)](https://www.reddit.com/r/LocalLLaMA/comments/1jbrwqf/deep_research_tools_am_i_the_only_one/mi3rmpc/) (in reply to ID 1jbrwqf):\nthe only thing those tools deep research is how to extract money out of your wallet.\n\n## Comment ID miomki7 with +1 score by [(arg_max, Reddit, 2025-03-19)](https://www.reddit.com/r/LocalLLaMA/comments/1jbrwqf/deep_research_tools_am_i_the_only_one/miomki7/) (in reply to ID 1jbrwqf):\nThe issue are the base models. I used 4o with web search to look for papers in certain areas, write short summaries and give me references. And about 20% of that was just made up. Like the paper just didn't exist. Prolly another 20% was wrong citations and summaries that look fine at first but seriously flawed if you look at it with some domain knowledge.\n\nObviously this is not gonna improve significantly if you do this over long contexts where multiple errors can accumulate.\n\n## Comment ID mirzj4v with +1 score by [(former_physicist, Reddit, 2025-03-20)](https://www.reddit.com/r/LocalLLaMA/comments/1jbrwqf/deep_research_tools_am_i_the_only_one/mirzj4v/) (in reply to ID 1jbrwqf):\nthe open AI deep research tool on o3-mini-high is really really good. I haven't tried any others.   \n  \nI found this post because I'm looking for an open source solution because I want to run 1000s of deep research calls.\n\n## Comment ID mixiik2 with +1 score by [(Powerdrill_AI, Reddit, 2025-03-21)](https://www.reddit.com/r/LocalLLaMA/comments/1jbrwqf/deep_research_tools_am_i_the_only_one/mixiik2/) (in reply to ID 1jbrwqf):\nWell said, and it is still hard for them to do some complicated work. Still need human calibration.\n\n## Comment ID mjwq66j with +1 score by [(Ezer_Pavle, Reddit, 2025-03-26)](https://www.reddit.com/r/LocalLLaMA/comments/1jbrwqf/deep_research_tools_am_i_the_only_one/mjwq66j/) (in reply to ID 1jbrwqf):\nIt is too late to comment, maybe; yet, I have asked (Google) to make a report on a niche subject I am working on and have already written one paper. It dug out the paper and cited it, among others, a couple of times. All the citations were bogus and made no sense to me\n\n## Comment ID mhy3mg1 with +1 score by [(LevianMcBirdo, Reddit, 2025-03-15)](https://www.reddit.com/r/LocalLLaMA/comments/1jbrwqf/deep_research_tools_am_i_the_only_one/mhy3mg1/) (in reply to ID 1jbrwqf):\nI like them, but they are far from perfect. They are what ai Internet search should've been from the start.",
      "# Post ID 1h4n1i9: Open-weights AI models are BAD says OpenAI CEO Sam Altman. Because DeepSeek and Qwen 2.5? did what OpenAi supposed to do! with +639 score by [(Vishnu_One, Reddit, 2024-12-02)](https://www.reddit.com/r/LocalLLaMA/comments/1h4n1i9/openweights_ai_models_are_bad_says_openai_ceo_sam/)\n# Because DeepSeek and Qwen 2.5? did what OpenAi supposed to do!?\n\n[China now has two of what appear to be the most powerful models ever made](https://www.youtube.com/watch?v=9zunASefVas) and they're completely open.\n\n[OpenAI CEO Sam Altman sits down](https://www.youtube.com/watch?v=VIokqGY0CrQ) with Shannon Bream to discuss the positives and potential negatives of artificial intelligence and the importance of maintaining a lead in the A.I. industry over China. \n\n## Comment ID lzzyd3p with +402 score by [(Arcosim, Reddit, 2024-12-02)](https://www.reddit.com/r/LocalLLaMA/comments/1h4n1i9/openweights_ai_models_are_bad_says_openai_ceo_sam/lzzyd3p/) (in reply to ID 1h4n1i9):\nNot only this guy failed to fulfill every single one of the promises he made when they originally looked for funding, he's also trying to prevent others from fulfilling these promises. Total scumbag.\n\n### Comment ID m000kid with +96 score by [(ForsookComparison, Reddit, 2024-12-02)](https://www.reddit.com/r/LocalLLaMA/comments/1h4n1i9/openweights_ai_models_are_bad_says_openai_ceo_sam/m000kid/) (in reply to ID lzzyd3p):\nProblem is even with his current moat drying up, he has won:\n\n1. A boatload of infra and cash\n\n2. A basically uninterrupted year of hiring all of the world's top A.I. talent with some ludicrous salaries\n\n#### Comment ID m0073rk with +69 score by [(Zyj, Reddit, 2024-12-02)](https://www.reddit.com/r/LocalLLaMA/comments/1h4n1i9/openweights_ai_models_are_bad_says_openai_ceo_sam/m0073rk/) (in reply to ID m000kid):\nThe cash is not going to last long\n\n#### Comment ID m00vo6z with +38 score by [(JustinPooDough, Reddit, 2024-12-02)](https://www.reddit.com/r/LocalLLaMA/comments/1h4n1i9/openweights_ai_models_are_bad_says_openai_ceo_sam/m00vo6z/) (in reply to ID m000kid):\nHere's the thing though: OpenAI is operating under the assumption that MORE = BETTER. MOAR SCALING! \n\nChina seems to be taking the \"Perfect practice makes perfect\" route (better training data). I think the answer lies somewhere between, but one thing is certain: OpenAI could be doing a lot better with all those high-paid employees and all those GPUs. \n\nThey have no moat.\n\n#### Comment ID m019upn with +12 score by [(SussyAmogusChungus, Reddit, 2024-12-02)](https://www.reddit.com/r/LocalLLaMA/comments/1h4n1i9/openweights_ai_models_are_bad_says_openai_ceo_sam/m019upn/) (in reply to ID m000kid):\nThat's just delaying the inevitable. Only a matter of time before OpenAI becomes the next Nokia.\n\n#### Comment ID m0103fg with +14 score by [(jloverich, Reddit, 2024-12-02)](https://www.reddit.com/r/LocalLLaMA/comments/1h4n1i9/openweights_ai_models_are_bad_says_openai_ceo_sam/m0103fg/) (in reply to ID m000kid):\nSeems like he missed the top talent in china\n\n#### Comment ID m01mjw2 with +5 score by [(Mikolai007, Reddit, 2024-12-02)](https://www.reddit.com/r/LocalLLaMA/comments/1h4n1i9/openweights_ai_models_are_bad_says_openai_ceo_sam/m01mjw2/) (in reply to ID m000kid):\n\"The world top AI talent\"? Isn't Anthropic Claude a bunch of ex-OpenAI talent? Hasn't their other top talent people just recently left the org? Watch the news dude. Musk is his number 1 opponent and musk has as an individual more money than 50 OpenAi companies together. Musk is eventually going to surpass them all, if not by just hiring and buying them to X-AI. That will most probably happen.\n\n### Comment ID m00t6yw with +7 score by [(adalgis231, Reddit, 2024-12-02)](https://www.reddit.com/r/LocalLLaMA/comments/1h4n1i9/openweights_ai_models_are_bad_says_openai_ceo_sam/m00t6yw/) (in reply to ID lzzyd3p):\nThe cornucopia is finishing\n\n#### Comment ID m01kim3 with +18 score by [(OkProMoe, Reddit, 2024-12-02)](https://www.reddit.com/r/LocalLLaMA/comments/1h4n1i9/openweights_ai_models_are_bad_says_openai_ceo_sam/m01kim3/) (in reply to ID m00t6yw):\nHe’s one of the greatest con men of the last few years. Defrauded all his initial investors now taking advantage of ignorant governments to create anti competitive legislation. Let’s hope the governments see past his monopoly attempts.\n\n### Comment ID m01pj39 with +10 score by [(SpecialistStory336, Reddit, 2024-12-02)](https://www.reddit.com/r/LocalLLaMA/comments/1h4n1i9/openweights_ai_models_are_bad_says_openai_ceo_sam/m01pj39/) (in reply to ID lzzyd3p):\nScam Scum-man\n\n### Comment ID m04nd70 with +1 score by [(Affectionate-Hat-536, Reddit, 2024-12-03)](https://www.reddit.com/r/LocalLLaMA/comments/1h4n1i9/openweights_ai_models_are_bad_says_openai_ceo_sam/m04nd70/) (in reply to ID lzzyd3p):\nMaybe he’s equivalent of Elon from EV ;)\n\n## Comment ID lzzwk28 with +152 score by [(MasterDragon_, Reddit, 2024-12-02)](https://www.reddit.com/r/LocalLLaMA/comments/1h4n1i9/openweights_ai_models_are_bad_says_openai_ceo_sam/lzzwk28/) (in reply to ID 1h4n1i9):\nLook at how the turn tables.\n\nChina is doing more open weights and sam altman of OpenAI  is against it.\n\n### Comment ID m010ffo with +14 score by [(Notcow, Reddit, 2024-12-02)](https://www.reddit.com/r/LocalLLaMA/comments/1h4n1i9/openweights_ai_models_are_bad_says_openai_ceo_sam/m010ffo/) (in reply to ID lzzwk28):\nChina also has internet facing, intentionally open supercomputers running models with crazy parameters. You can just hop in and process whatever you want.\n\nI mean, with all the obvious downsides of handing data to an autocracy, so you should never use them. But I bet they rake in crazy amounts of data\n\n#### Comment ID m024xdx with +6 score by [(cleverusernametry, Reddit, 2024-12-02)](https://www.reddit.com/r/LocalLLaMA/comments/1h4n1i9/openweights_ai_models_are_bad_says_openai_ceo_sam/m024xdx/) (in reply to ID m010ffo):\nWait what? Link?\n\n### Comment ID m0ni05i with +1 score by [(ozspook, Reddit, 2024-12-06)](https://www.reddit.com/r/LocalLLaMA/comments/1h4n1i9/openweights_ai_models_are_bad_says_openai_ceo_sam/m0ni05i/) (in reply to ID lzzwk28):\n\"Our open weights are backed by NUCLEAR WEAPONS!\" said -127 Gandhi\n\n## Comment ID lzzohkh with +325 score by [(carnyzzle, Reddit, 2024-12-02)](https://www.reddit.com/r/LocalLLaMA/comments/1h4n1i9/openweights_ai_models_are_bad_says_openai_ceo_sam/lzzohkh/) (in reply to ID 1h4n1i9):\nOpenAI knows that they're losing their moat now\n\n### Comment ID lzzt2ns with +130 score by [(None, Reddit, 2024-12-02)](https://www.reddit.com/r/LocalLLaMA/comments/1h4n1i9/openweights_ai_models_are_bad_says_openai_ceo_sam/lzzt2ns/) (in reply to ID lzzohkh):\n[removed]\n\n#### Comment ID lzzt9ux with +110 score by [(carnyzzle, Reddit, 2024-12-02)](https://www.reddit.com/r/LocalLLaMA/comments/1h4n1i9/openweights_ai_models_are_bad_says_openai_ceo_sam/lzzt9ux/) (in reply to ID lzzt2ns):\nExpectation: GPT, GPT 2, GPT 3, GPT 4, GPT 5\n\nReality: GPT, GPT 2, GPT 3, GPT 4, GPT 4o, GPT 4o, GPT 4o...\n\n#### Comment ID m000h94 with +43 score by [(ImNotALLM, Reddit, 2024-12-02)](https://www.reddit.com/r/LocalLLaMA/comments/1h4n1i9/openweights_ai_models_are_bad_says_openai_ceo_sam/m000h94/) (in reply to ID lzzt2ns):\nI mean this in part could be because their previous successes were reappropriated breakthroughs from others. Google were the ones who spearheaded attention mechanisms, transformers, and scaling laws, OAI just productized that work and introduced it to the wider public.\n\n#### Comment ID lzztzun with +62 score by [(antihero-itsme, Reddit, 2024-12-02)](https://www.reddit.com/r/LocalLLaMA/comments/1h4n1i9/openweights_ai_models_are_bad_says_openai_ceo_sam/lzztzun/) (in reply to ID lzzt2ns):\nthey would rather just hire 400 ai safety researchers to do nothing but dumb down an otherwise mediocre model even more\n\n#### Comment ID m02biw5 with +3 score by [(softclone, Reddit, 2024-12-02)](https://www.reddit.com/r/LocalLLaMA/comments/1h4n1i9/openweights_ai_models_are_bad_says_openai_ceo_sam/m02biw5/) (in reply to ID lzzt2ns):\numm, yeah they fired their chief scientist any everyone is surprised?\n\n### Comment ID m00ckup with +15 score by [(Kindly_Manager7556, Reddit, 2024-12-02)](https://www.reddit.com/r/LocalLLaMA/comments/1h4n1i9/openweights_ai_models_are_bad_says_openai_ceo_sam/m00ckup/) (in reply to ID lzzohkh):\nThey got shit on by Anthropic. I wouldn't doubt that Altman goes down in some FTX SBF fashion in the future.\n\n#### Comment ID m00ojyv with +6 score by [(unlikely_ending, Reddit, 2024-12-02)](https://www.reddit.com/r/LocalLLaMA/comments/1h4n1i9/openweights_ai_models_are_bad_says_openai_ceo_sam/m00ojyv/) (in reply to ID m00ckup):\nAnd Qwen\n\n#### Comment ID m00km8z with +4 score by [(Any_Pressure4251, Reddit, 2024-12-02)](https://www.reddit.com/r/LocalLLaMA/comments/1h4n1i9/openweights_ai_models_are_bad_says_openai_ceo_sam/m00km8z/) (in reply to ID m00ckup):\nAnthropic are good, Claude 3.5 Sonnet is my goto coding model. \n\nHowever I have a Open AI pro subscription because they are the best AI team in town.\n\n### Comment ID m00fg6f with +9 score by [(blackkettle, Reddit, 2024-12-02)](https://www.reddit.com/r/LocalLLaMA/comments/1h4n1i9/openweights_ai_models_are_bad_says_openai_ceo_sam/m00fg6f/) (in reply to ID lzzohkh):\nAnd this is EXACTLY why open is the right future for everyone.  How TF these people can lie like this is just utterly beyond me.\n\n### Comment ID lzzzp9s with +32 score by [(eposnix, Reddit, 2024-12-02)](https://www.reddit.com/r/LocalLLaMA/comments/1h4n1i9/openweights_ai_models_are_bad_says_openai_ceo_sam/lzzzp9s/) (in reply to ID lzzohkh):\nPeople keep saying this, but I'm still waiting for a model that can compete with 4o's Advanced Voice mode. I find it weird that people just completely ignore the fact that OpenAI basically solved AI voice chat. The only issue is that it's fucking $200/m tokens on the API.\n\n/edit:\n\nGPT-4o got a little spicy when I asked it to demonstrate: https://eposnix.com/GPT-4o.mp3\n\n#### Comment ID m00m7sd with +4 score by [(theanghv, Reddit, 2024-12-02)](https://www.reddit.com/r/LocalLLaMA/comments/1h4n1i9/openweights_ai_models_are_bad_says_openai_ceo_sam/m00m7sd/) (in reply to ID lzzzp9s):\nWhat makes it better than gemini advance?\n\nEdit: just listened to your link and it’s way ahead of gemini.\n\n#### Comment ID m00ctp7 with +12 score by [(DeltaSqueezer, Reddit, 2024-12-02)](https://www.reddit.com/r/LocalLLaMA/comments/1h4n1i9/openweights_ai_models_are_bad_says_openai_ceo_sam/m00ctp7/) (in reply to ID lzzzp9s):\nThey are far ahead in voice generation. They also hired away the guy who made Tortoise TTS which was the leading open source TTS at the time.\n\nI'm curious, what was the prompt for the demo you showed?\n\n#### Comment ID m000up7 with +22 score by [(None, Reddit, 2024-12-02)](https://www.reddit.com/r/LocalLLaMA/comments/1h4n1i9/openweights_ai_models_are_bad_says_openai_ceo_sam/m000up7/) (in reply to ID lzzzp9s):\n[removed]\n\n#### Comment ID m015zyf with +3 score by [(pmelendezu, Reddit, 2024-12-02)](https://www.reddit.com/r/LocalLLaMA/comments/1h4n1i9/openweights_ai_models_are_bad_says_openai_ceo_sam/m015zyf/) (in reply to ID lzzzp9s):\nI don’t think you need a monolithic multi modal model to achieve their results. That’s only the route they chose for their architecture. They have economic motivation to take that route which is not the same case for non big players\n\n### Comment ID m01z7hm with +2 score by [(CarpeDay27, Reddit, 2024-12-02)](https://www.reddit.com/r/LocalLLaMA/comments/1h4n1i9/openweights_ai_models_are_bad_says_openai_ceo_sam/m01z7hm/) (in reply to ID lzzohkh):\n100%\n\n## Comment ID lzzvefc with +146 score by [(-p-e-w-, Reddit, 2024-12-02)](https://www.reddit.com/r/LocalLLaMA/comments/1h4n1i9/openweights_ai_models_are_bad_says_openai_ceo_sam/lzzvefc/) (in reply to ID 1h4n1i9):\nEasy fix: Altman just needs to lobby China and France to follow his agenda like he has tried in the US.\n\nShould be no problem, with one being America's #1 geopolitical enemy, and the other its cultural foil, itching to show *les imbéciles* across the pond the world's biggest middle finger.\n\nMeanwhile, open-weights models released under free software licenses that people run on their gaming PCs are now better than GPT was six months ago.\n\nWho in their right mind thinks that OpenAI is worth $157 billion?!? Are the investors high, or are they just waiting for the next bigger fool?\n\n### Comment ID m00mki3 with +6 score by [(thisusername_is_mine, Reddit, 2024-12-02)](https://www.reddit.com/r/LocalLLaMA/comments/1h4n1i9/openweights_ai_models_are_bad_says_openai_ceo_sam/m00mki3/) (in reply to ID lzzvefc):\nIt won't be long before the race starts among investors for who's gonna be the biggest bagholder. Microsoft is positioned quite well to finish in pole position. And that's a good thing.\n\n#### Comment ID m0i5j2o with +1 score by [(Justicia-Gai, Reddit, 2024-12-05)](https://www.reddit.com/r/LocalLLaMA/comments/1h4n1i9/openweights_ai_models_are_bad_says_openai_ceo_sam/m0i5j2o/) (in reply to ID m00mki3):\nHow so? Do you really want copilot to be taking screenshots of what you do on your PC? Do you value your privacy so little?\n\n### Comment ID m00dbhj with +33 score by [(AsanaJM, Reddit, 2024-12-02)](https://www.reddit.com/r/LocalLLaMA/comments/1h4n1i9/openweights_ai_models_are_bad_says_openai_ceo_sam/m00dbhj/) (in reply to ID lzzvefc):\nI bet Trump and Musk will invent some China Ai ban, and Musk will bully his competitors by having Grok the only one deemed \"safe\" with huge regulations to piss OpenAi & Llama (musk will lobby against meta, that's killing his business at home plus he hates lecun)\n\n#### Comment ID m01i9rk with +11 score by [(ThatsALovelyShirt, Reddit, 2024-12-02)](https://www.reddit.com/r/LocalLLaMA/comments/1h4n1i9/openweights_ai_models_are_bad_says_openai_ceo_sam/m01i9rk/) (in reply to ID m00dbhj):\n> invent some China Ai ban\n\nAnd how are they going to implement that? They could 'ban' the weights, just like piracy is 'banned', but people will just torrent the weights, or download them from a server outside the US. \n\nWhat are they going to do? Go on everyone's PC and make sure they don't have the weights to a Qwen model on them?\n\nThe most they could really do is ban Chinese models from being used within government projects. Or *maybe* financial institutions if they get the SEC involved.\n\n#### Comment ID m01am0q with +8 score by [(Umbristopheles, Reddit, 2024-12-02)](https://www.reddit.com/r/LocalLLaMA/comments/1h4n1i9/openweights_ai_models_are_bad_says_openai_ceo_sam/m01am0q/) (in reply to ID m00dbhj):\nThey will fail spectacularly, even if they try. How many casinos did trump run into the ground? Fucking CASINOS! Where the house is supposed to always win!\n\nAnd Musk? Two words. Boring Company\n\n#### Comment ID m00fpox with +12 score by [(Hambeggar, Reddit, 2024-12-02)](https://www.reddit.com/r/LocalLLaMA/comments/1h4n1i9/openweights_ai_models_are_bad_says_openai_ceo_sam/m00fpox/) (in reply to ID m00dbhj):\nNone of this will happen.\n\n#### Comment ID m059evw with +1 score by [(Intelligent-Donut-10, Reddit, 2024-12-03)](https://www.reddit.com/r/LocalLLaMA/comments/1h4n1i9/openweights_ai_models_are_bad_says_openai_ceo_sam/m059evw/) (in reply to ID m00dbhj):\nThat means handing Chinese companies a massive advantage in AI applications.\n\nThere's no winning inferiority.\n\n## Comment ID lzzw9tc with +107 score by [(duboispourlhiver, Reddit, 2024-12-02)](https://www.reddit.com/r/LocalLLaMA/comments/1h4n1i9/openweights_ai_models_are_bad_says_openai_ceo_sam/lzzw9tc/) (in reply to ID 1h4n1i9):\nSam Altman told us that LLM safety is a top priority for the world safety since at least gpt3. Now we have better open source models and nothing happened to the world. This was probably bullshit all along as suspected by a large amount of observers.\n\n### Comment ID lzzwse0 with +62 score by [(None, Reddit, 2024-12-02)](https://www.reddit.com/r/LocalLLaMA/comments/1h4n1i9/openweights_ai_models_are_bad_says_openai_ceo_sam/lzzwse0/) (in reply to ID lzzw9tc):\n[removed]\n\n#### Comment ID lzzybwr with +35 score by [(carnyzzle, Reddit, 2024-12-02)](https://www.reddit.com/r/LocalLLaMA/comments/1h4n1i9/openweights_ai_models_are_bad_says_openai_ceo_sam/lzzybwr/) (in reply to ID lzzwse0):\nI know more people using gpt 4o and jailbreaking it for smut than I do people using local models for smut\n\n#### Comment ID m012igc with +9 score by [(Anka098, Reddit, 2024-12-02)](https://www.reddit.com/r/LocalLLaMA/comments/1h4n1i9/openweights_ai_models_are_bad_says_openai_ceo_sam/m012igc/) (in reply to ID lzzwse0):\n+ most bad actors are just horny dudes who want digital gfs\n\n### Comment ID lzzynph with +35 score by [(None, Reddit, 2024-12-02)](https://www.reddit.com/r/LocalLLaMA/comments/1h4n1i9/openweights_ai_models_are_bad_says_openai_ceo_sam/lzzynph/) (in reply to ID lzzw9tc):\n[removed]\n\n#### Comment ID m001mgx with +15 score by [(RuairiSpain, Reddit, 2024-12-02)](https://www.reddit.com/r/LocalLLaMA/comments/1h4n1i9/openweights_ai_models_are_bad_says_openai_ceo_sam/m001mgx/) (in reply to ID lzzynph):\nHis mentor was Peter Thiel, apple does not fall far from the tree\n\n## Comment ID lzzq1os with +83 score by [(Many_SuchCases, Reddit, 2024-12-02)](https://www.reddit.com/r/LocalLLaMA/comments/1h4n1i9/openweights_ai_models_are_bad_says_openai_ceo_sam/lzzq1os/) (in reply to ID 1h4n1i9):\nThe only thing altman has left is a few media outlets that write articles in his favor, but he's even losing that because people are catching on that he has nothing. I have a feeling that \"leak\" the other day of sora (which wasn't a leak but literally just 30 minutes of access) was actually just a teaser/promo done on purpose. Funny enough, even as a leak nobody cared.\n\n### Comment ID m001dwm with +15 score by [(RuairiSpain, Reddit, 2024-12-02)](https://www.reddit.com/r/LocalLLaMA/comments/1h4n1i9/openweights_ai_models_are_bad_says_openai_ceo_sam/m001dwm/) (in reply to ID lzzq1os):\nHe also has the weight of the US government and their 3 letter agencies. Remember the military are on the OAI board and their big revenue stream will come from long term contracts with those agencies.\n\nBetween the favourable media stories, the lobbying and agencies doing the leverage work for Sam, OpenAI will do OK\n\n#### Comment ID m00389n with +8 score by [(ElectricalHost5996, Reddit, 2024-12-02)](https://www.reddit.com/r/LocalLLaMA/comments/1h4n1i9/openweights_ai_models_are_bad_says_openai_ceo_sam/m00389n/) (in reply to ID m001dwm):\nEven if they have an advantage ,the question is is it worth hundreds of billions or dollar more of an  advantage ? . I would say no .\n\n### Comment ID m0050ho with +14 score by [(AIAddict1935, Reddit, 2024-12-02)](https://www.reddit.com/r/LocalLLaMA/comments/1h4n1i9/openweights_ai_models_are_bad_says_openai_ceo_sam/m0050ho/) (in reply to ID lzzq1os):\nIt's hilarious seeing Sam clinging to relevance. One day AGI is 1,000's days away, then AGI will be here in 2025, then I literally saw press saying \"AGI is already here\". I saw that Japan investors invested 1.5 billion USD into Open AI recently - huh? They literally could just use that to invent human-like expert AI systems in Japan! And Open AI is STILL no match for pure community driven cleverness despite having every unfair advantage imaginable.\n\n#### Comment ID m02hfun with +4 score by [(qpdv, Reddit, 2024-12-02)](https://www.reddit.com/r/LocalLLaMA/comments/1h4n1i9/openweights_ai_models_are_bad_says_openai_ceo_sam/m02hfun/) (in reply to ID m0050ho):\nLol gpt store was a joke and more of a \"give us all of your ideas\" thing\n\n#### Comment ID m00wviv with +2 score by [(IDEA_Learn_Do, Reddit, 2024-12-02)](https://www.reddit.com/r/LocalLLaMA/comments/1h4n1i9/openweights_ai_models_are_bad_says_openai_ceo_sam/m00wviv/) (in reply to ID m0050ho):\nI think this is to ensure they break their contract with Microsoft which has the AGI clause\n\n### Comment ID m01vhj7 with +1 score by [(Puzzleheaded-Ad-532, Reddit, 2024-12-02)](https://www.reddit.com/r/LocalLLaMA/comments/1h4n1i9/openweights_ai_models_are_bad_says_openai_ceo_sam/m01vhj7/) (in reply to ID lzzq1os):\nYeh cuz he waited way to long for it if he would of dropped it when he presented it it would of been amazing but now with the other models already putting out the same if not better results no one's really excited for this..\n\n### Comment ID lzzy7p2 with +1 score by [(ForsookComparison, Reddit, 2024-12-02)](https://www.reddit.com/r/LocalLLaMA/comments/1h4n1i9/openweights_ai_models_are_bad_says_openai_ceo_sam/lzzy7p2/) (in reply to ID lzzq1os):\nSearch on chatgp4 is a few leagues ahead of the others still I'd say.\n\nBut yeah. The moat is drying fast.\n\n## Comment ID m0065c4 with +45 score by [(Rainboy97, Reddit, 2024-12-02)](https://www.reddit.com/r/LocalLLaMA/comments/1h4n1i9/openweights_ai_models_are_bad_says_openai_ceo_sam/m0065c4/) (in reply to ID 1h4n1i9):\nOpenAI says Open AI is bad.\n\n### Comment ID m02is4t with +7 score by [(Yapper_Zipper, Reddit, 2024-12-02)](https://www.reddit.com/r/LocalLLaMA/comments/1h4n1i9/openweights_ai_models_are_bad_says_openai_ceo_sam/m02is4t/) (in reply to ID m0065c4):\n\"Yes Open is bad. Consumers should pay capitalists money to use their safe AI that breaks when asked about David\"\n\n## Comment ID lzzofvr with +67 score by [(provoloner09, Reddit, 2024-12-02)](https://www.reddit.com/r/LocalLLaMA/comments/1h4n1i9/openweights_ai_models_are_bad_says_openai_ceo_sam/lzzofvr/) (in reply to ID 1h4n1i9):\nTypical gloating cuz they probably thought that their moat is at least >1 year worth than all others.\n\n## Comment ID m00oqt2 with +14 score by [(hackeristi, Reddit, 2024-12-02)](https://www.reddit.com/r/LocalLLaMA/comments/1h4n1i9/openweights_ai_models_are_bad_says_openai_ceo_sam/m00oqt2/) (in reply to ID 1h4n1i9):\nStill waiting for them to rebrand: “closed AI”\n\n## Comment ID lzzy4mn with +49 score by [(ForsookComparison, Reddit, 2024-12-02)](https://www.reddit.com/r/LocalLLaMA/comments/1h4n1i9/openweights_ai_models_are_bad_says_openai_ceo_sam/lzzy4mn/) (in reply to ID 1h4n1i9):\nThe emails with Elon where he (Sam) keeps reaffirming that they'll be open really paint a clear picture. These guys are scumbags. We should not be trusting Sam with the future.\n\n### Comment ID m001dtk with +17 score by [(kvothe5688, Reddit, 2024-12-02)](https://www.reddit.com/r/LocalLLaMA/comments/1h4n1i9/openweights_ai_models_are_bad_says_openai_ceo_sam/m001dtk/) (in reply to ID lzzy4mn):\nhe is Elon 2.0\n\n#### Comment ID m00qvt9 with +7 score by [(__Maximum__, Reddit, 2024-12-02)](https://www.reddit.com/r/LocalLLaMA/comments/1h4n1i9/openweights_ai_models_are_bad_says_openai_ceo_sam/m00qvt9/) (in reply to ID m001dtk):\nI think he and elon are heavily quantized past elon\n\n### Comment ID m003r2p with +8 score by [(ElectricalHost5996, Reddit, 2024-12-02)](https://www.reddit.com/r/LocalLLaMA/comments/1h4n1i9/openweights_ai_models_are_bad_says_openai_ceo_sam/m003r2p/) (in reply to ID lzzy4mn):\nI think everyone knew, that signaling and scamming that they are   opensource will get them those really smart talent who will work for cheap at least at the beginning and then they overpay them so as to not puke when they know their true colors. Thrown in a regulatory capture playbook , they did'nt expect china to have the software brain power . \n\nChina's investment in education back in the industrial days is now paying off\n\n## Comment ID m000xii with +34 score by [(custodiam99, Reddit, 2024-12-02)](https://www.reddit.com/r/LocalLLaMA/comments/1h4n1i9/openweights_ai_models_are_bad_says_openai_ceo_sam/m000xii/) (in reply to ID 1h4n1i9):\nOpenAI made two huge mistakes. 1. They thought scaling will solve their every problem. 2. They thought that brute force and brute compute power will be enough to create AGI. \\*\\*\\* The Chinese models on the other hand are more nuanced and clever. That's the failure of OpenAI leadership and research.\n\n### Comment ID m020f6n with +4 score by [(MaycombBlume, Reddit, 2024-12-02)](https://www.reddit.com/r/LocalLLaMA/comments/1h4n1i9/openweights_ai_models_are_bad_says_openai_ceo_sam/m020f6n/) (in reply to ID m000xii):\n> They thought that brute force and brute compute power will be enough to create AGI\n\nI can't imagine anyone at OpenAI *actually* believes that.\n\nThe executives and marketers have said some pie-in-the-sky things about AGI to fool investors, but they all know better.\n\n#### Comment ID m022ozp with +1 score by [(custodiam99, Reddit, 2024-12-02)](https://www.reddit.com/r/LocalLLaMA/comments/1h4n1i9/openweights_ai_models_are_bad_says_openai_ceo_sam/m022ozp/) (in reply to ID m020f6n):\nIs it better? -> They communicated that brute force and brute compute power will be enough to create AGI, but they don't really care because the money is flowing endlessly. They probably think they will improvise something. \\*\\*\\* That's still a huge mistake in my opinion, but yeah an opportunity too.\n\n### Comment ID m0070nw with +2 score by [(lolzinventor, Reddit, 2024-12-02)](https://www.reddit.com/r/LocalLLaMA/comments/1h4n1i9/openweights_ai_models_are_bad_says_openai_ceo_sam/m0070nw/) (in reply to ID m000xii):\nI wonder if the secret source is in the training dataset.  I.e. Quality in quality out.\n\n#### Comment ID m007eqn with +4 score by [(custodiam99, Reddit, 2024-12-02)](https://www.reddit.com/r/LocalLLaMA/comments/1h4n1i9/openweights_ai_models_are_bad_says_openai_ceo_sam/m007eqn/) (in reply to ID m0070nw):\nI used the word \"clever\", which doesn't necessarily mean \"moral\". They are clever even if they are using ChatGPT synthetic data.\n\n### Comment ID m00mqy9 with +2 score by [(thisusername_is_mine, Reddit, 2024-12-02)](https://www.reddit.com/r/LocalLLaMA/comments/1h4n1i9/openweights_ai_models_are_bad_says_openai_ceo_sam/m00mqy9/) (in reply to ID m000xii):\n3. Organizing a counter coup to reinstate Scama when he got booted out.\n\n## Comment ID m00v76w with +7 score by [(JustinPooDough, Reddit, 2024-12-02)](https://www.reddit.com/r/LocalLLaMA/comments/1h4n1i9/openweights_ai_models_are_bad_says_openai_ceo_sam/m00v76w/) (in reply to ID 1h4n1i9):\nHe's a piece of shite.\n\n## Comment ID m012pik with +8 score by [(timbro1, Reddit, 2024-12-02)](https://www.reddit.com/r/LocalLLaMA/comments/1h4n1i9/openweights_ai_models_are_bad_says_openai_ceo_sam/m012pik/) (in reply to ID 1h4n1i9):\nare you saying openai isn't open? Heresy!\n\n## Comment ID m01znkt with +7 score by [(MaycombBlume, Reddit, 2024-12-02)](https://www.reddit.com/r/LocalLLaMA/comments/1h4n1i9/openweights_ai_models_are_bad_says_openai_ceo_sam/m01znkt/) (in reply to ID 1h4n1i9):\nEric Schmidt thought China was 2-3 years behind? Seriously?\n\n2-3 *years?!?*\n\nWe've had open models better than that for a long time already. Keep up, dude. Jeez.\n\n## Comment ID m00mnug with +4 score by [(101m4n, Reddit, 2024-12-02)](https://www.reddit.com/r/LocalLLaMA/comments/1h4n1i9/openweights_ai_models_are_bad_says_openai_ceo_sam/m00mnug/) (in reply to ID 1h4n1i9):\nHaha, 3090 go brrr\n\n## Comment ID m00l3sx with +13 score by [(fratkabula, Reddit, 2024-12-02)](https://www.reddit.com/r/LocalLLaMA/comments/1h4n1i9/openweights_ai_models_are_bad_says_openai_ceo_sam/m00l3sx/) (in reply to ID 1h4n1i9):\nWhen was the last time you used openai's image generation model? It's laughably bad compared to flux, stability, recraft, ideogram.\n\n## Comment ID m001zw0 with +11 score by [(A_for_Anonymous, Reddit, 2024-12-02)](https://www.reddit.com/r/LocalLLaMA/comments/1h4n1i9/openweights_ai_models_are_bad_says_openai_ceo_sam/m001zw0/) (in reply to ID 1h4n1i9):\nOMG open bad, makes me no win, China bad OpenAI good because it has the word open in it, be safe and responsible Anon, we will make my bank account great again.\n\n## Comment ID m01y8bq with +5 score by [(noiseinvacuum, Reddit, 2024-12-02)](https://www.reddit.com/r/LocalLLaMA/comments/1h4n1i9/openweights_ai_models_are_bad_says_openai_ceo_sam/m01y8bq/) (in reply to ID 1h4n1i9):\nRemember in March 2023 when 100s of experts wanted to \"pause\" AI development for 6 months because it was too dangerous? \n\nThese CEOs are great at hyping to raise money and that's pretty much all. Altman going from interviews acting like he's an AI nerd in front of ill informed journalists is the lowest form of grifting. I'm really disappointed that he doesn't get called out more often for being a perpetual liar.\n\n## Comment ID m021kiy with +4 score by [(Ulterior-Motive_, Reddit, 2024-12-02)](https://www.reddit.com/r/LocalLLaMA/comments/1h4n1i9/openweights_ai_models_are_bad_says_openai_ceo_sam/m021kiy/) (in reply to ID 1h4n1i9):\nAnything that makes Sama mald is good. Fuck ClosedAi.\n\n## Comment ID m006ap7 with +12 score by [(eonade, Reddit, 2024-12-02)](https://www.reddit.com/r/LocalLLaMA/comments/1h4n1i9/openweights_ai_models_are_bad_says_openai_ceo_sam/m006ap7/) (in reply to ID 1h4n1i9):\nWhere in the interview does Sam say open-weights models are bad?\n\nAre we just making things up now?\n\n### Comment ID m01dew5 with +5 score by [(30299578815310, Reddit, 2024-12-02)](https://www.reddit.com/r/LocalLLaMA/comments/1h4n1i9/openweights_ai_models_are_bad_says_openai_ceo_sam/m01dew5/) (in reply to ID m006ap7):\nI didn't see that did I miss something?\n\n#### Comment ID m01jp50 with +4 score by [(Zangwuz, Reddit, 2024-12-02)](https://www.reddit.com/r/LocalLLaMA/comments/1h4n1i9/openweights_ai_models_are_bad_says_openai_ceo_sam/m01jp50/) (in reply to ID m01dew5):\nMe neither and i watched two times to be sure.\n\n## Comment ID m00vv32 with +2 score by [(a_beautiful_rhind, Reddit, 2024-12-02)](https://www.reddit.com/r/LocalLLaMA/comments/1h4n1i9/openweights_ai_models_are_bad_says_openai_ceo_sam/m00vv32/) (in reply to ID 1h4n1i9):\nWell.. screw him and openAI. His models pollute the internet with slop and make it harder to train anything.\n\nhttps://i.imgur.com/Mt0YAsf.png\n\n## Comment ID m01rfm5 with +2 score by [(de4dee, Reddit, 2024-12-02)](https://www.reddit.com/r/LocalLLaMA/comments/1h4n1i9/openweights_ai_models_are_bad_says_openai_ceo_sam/m01rfm5/) (in reply to ID 1h4n1i9):\nnext version 4 o'oh\n\n## Comment ID m03l33o with +2 score by [(YourAverageDev_, Reddit, 2024-12-02)](https://www.reddit.com/r/LocalLLaMA/comments/1h4n1i9/openweights_ai_models_are_bad_says_openai_ceo_sam/m03l33o/) (in reply to ID 1h4n1i9):\nOh nooo, OpenAI is losing, Open SOURCE must BE EVIL!\n\nSteve Ballmer: \"Open Source Linux is a CANCER\"\n\n## Comment ID m04hexk with +2 score by [(TPLINKSHIT, Reddit, 2024-12-03)](https://www.reddit.com/r/LocalLLaMA/comments/1h4n1i9/openweights_ai_models_are_bad_says_openai_ceo_sam/m04hexk/) (in reply to ID 1h4n1i9):\nOPENai by the way\n\n## Comment ID m04ts3v with +2 score by [(Perfect_Sir4647, Reddit, 2024-12-03)](https://www.reddit.com/r/LocalLLaMA/comments/1h4n1i9/openweights_ai_models_are_bad_says_openai_ceo_sam/m04ts3v/) (in reply to ID 1h4n1i9):\nFollow the incentives.  First, I see that top guys who built the firm left (Mira, Ilya) and bunch of others.  Why would they leave if OpenAI had the highgest prob at building the AGI?  I don't believe they would have.  \n\nThen, they raised money.  If there was no chance of competing, they wouldn't have, or couldn't have raised because the guy who gives them $1B will ask - how do we beat these guys?  They must have had a good answer, which will rationalize why OpenAI's current scale or models do not have a moat.  \n\nNext, we haven't seen a larger model in a while now.  Opus 3.5 didn't come.  After gp4, we had gp4o and o1, both in my guesses are smaller models.  So clearly scaling alone doesn't work as well as people thought.  \n\nCombine them all and OpenAI will try to be a consumer focused company like google and they do well there.  They got instagram co-founder as cpo and their product is the best in terms of usability.  \n\nSo I do not get surprized at all and if I have to bet, I believe google is the best value/price vendor right now.  Their documentation is the most confusing ever but I love 1.5 flash and pro and they are faster/cheaper than most openai models.\n\n## Comment ID m05zfib with +2 score by [(OtherwiseLiving, Reddit, 2024-12-03)](https://www.reddit.com/r/LocalLLaMA/comments/1h4n1i9/openweights_ai_models_are_bad_says_openai_ceo_sam/m05zfib/) (in reply to ID 1h4n1i9):\nHe did not say that\n\n## Comment ID m0axhle with +2 score by [(Obvious_Beat_5346, Reddit, 2024-12-04)](https://www.reddit.com/r/LocalLLaMA/comments/1h4n1i9/openweights_ai_models_are_bad_says_openai_ceo_sam/m0axhle/) (in reply to ID 1h4n1i9):\ndeepseek API price is extremely afforadable and attractive.\n\n## Comment ID m014jur with +2 score by [(Single_Ring4886, Reddit, 2024-12-02)](https://www.reddit.com/r/LocalLLaMA/comments/1h4n1i9/openweights_ai_models_are_bad_says_openai_ceo_sam/m014jur/) (in reply to ID 1h4n1i9):\nI strongly believe they have stronger and better models without any moral restrictions in the basement but they market them to big corporations not us average joes. If you read paper on original GPT4 it already was beast pre brainwashing. Today they must be much further. That is why money is flowing to them....\n\n## Comment ID m000m9i with +2 score by [(duyusef, Reddit, 2024-12-02)](https://www.reddit.com/r/LocalLLaMA/comments/1h4n1i9/openweights_ai_models_are_bad_says_openai_ceo_sam/m000m9i/) (in reply to ID 1h4n1i9):\no1 is extremely impressive. Sam knows there is no moat or won't be for long. They hired a fancy head of product, and have now been stalling month after month not releasing anything for fear of showing something underwhelming. \n\nIt's not as bad as it seems. o1 is still significantly better that the competition. I think this is a 6 month head start. Time is ticking. \n\nIf I were an investor I would look for serious evidence that the product team is bold and fearless and leading the industry in vision and execution. This absolutely has to happen within the next three months or OpenAI will start to lose its fundraising advantages. \n\nOf course open weight models will win. It's just not that hard a problem and AI researcher talent is not that scarce. GPUs are scarce. Let's not pretend there wouldn't be 100 OpenAIs if everybody had the same GPU resources. \n\nSo far OpenAI has coasted on the product front from an early lead.  o1 still struggles with more than 400-600 lines of code and gets befuddled and forgetful pretty easily over 300 LoC.  When o1 goes to production I predict it will be useful up to 1K or 1.2K LoC. This is a big game changer and could be enough to let OpenAI \"win\" for another year. But when you look at some of the very small Chinese models it becomes clear that there is a tremendous amount of AI researcher cleverness in the world and much of it is not working for OpenAI. \n\nUnbeknownst to Sam, OpenAI should release open weight models simply to help find talent, as it will be the companies with the best, most scarce human talent that will survive for the next few years until AGI takes the helm of its own destiny.  \n\nSam is smart in his decision to focus on navigating the meatspace political aspect of this, but he's in a very weak position right now for a variety of reasons.  Elon's lawsuit is just harassment meant to make life a little more stressful for Sam to take his eye off the ball.  Microsoft is getting greedy and we still don't have o1 in production.\n\n### Comment ID m01t29m with +2 score by [(Robert__Sinclair, Reddit, 2024-12-02)](https://www.reddit.com/r/LocalLLaMA/comments/1h4n1i9/openweights_ai_models_are_bad_says_openai_ceo_sam/m01t29m/) (in reply to ID m000m9i):\no1 is not a \"better ai\" is a better prompted ai.\n\n### Comment ID m003cz1 with +4 score by [(AnuragVohra, Reddit, 2024-12-02)](https://www.reddit.com/r/LocalLLaMA/comments/1h4n1i9/openweights_ai_models_are_bad_says_openai_ceo_sam/m003cz1/) (in reply to ID m000m9i):\n> If I were an investor...\n\nYou should realize that you are not going to get return on your money, with all this open model availble for free now!\n\n## Comment ID m006qxi with +2 score by [(AIAddict1935, Reddit, 2024-12-02)](https://www.reddit.com/r/LocalLLaMA/comments/1h4n1i9/openweights_ai_models_are_bad_says_openai_ceo_sam/m006qxi/) (in reply to ID 1h4n1i9):\nThis has to be embarrassing. OpenAI received funding from richest man in the world, has raised most funding ever recorded, they couldn't compete based on merit as our \"best\" AI company so US needed to tip the scales with GPU export controls, they literally ONLY ascended as a company due to the grand theft of stealing data with copy write, most AI papers from arxiv are FREE and literally written by East and South Asian native authors but OpenAI STILL can't compete? Now they're still whining about something?! World's most privilege company now must work. Give me a break.\n\nAs an American I say Go China Go! Those $10+ billions of dollars to fund Open AI could be going to companies who employ WAYYY more Americans.\n\n### Comment ID m02h8y1 with +2 score by [(Sassquatch3000, Reddit, 2024-12-02)](https://www.reddit.com/r/LocalLLaMA/comments/1h4n1i9/openweights_ai_models_are_bad_says_openai_ceo_sam/m02h8y1/) (in reply to ID m006qxi):\nYou can ding them for stealing or not competing but not both, since the competition has been \"stealing\" by the same rationale\n\n### Comment ID m02iz77 with +2 score by [(Sassquatch3000, Reddit, 2024-12-02)](https://www.reddit.com/r/LocalLLaMA/comments/1h4n1i9/openweights_ai_models_are_bad_says_openai_ceo_sam/m02iz77/) (in reply to ID m006qxi):\n\"As an American\" lol\n\n## Comment ID m013edh with +1 score by [(edafade, Reddit, 2024-12-02)](https://www.reddit.com/r/LocalLLaMA/comments/1h4n1i9/openweights_ai_models_are_bad_says_openai_ceo_sam/m013edh/) (in reply to ID 1h4n1i9):\nI'm new to this side of LLM's and trying to be informed. Are there better iterations out there than GPT4o/4o1 aside from the OP? DeekSeek and Qwen are two of them, I'm guessing. Are there others?\n\nI've been thinking about creating my own local LLM for a while but feel kind of daubted to do it.\n\n## Comment ID m0356kb with +1 score by [(opi098514, Reddit, 2024-12-02)](https://www.reddit.com/r/LocalLLaMA/comments/1h4n1i9/openweights_ai_models_are_bad_says_openai_ceo_sam/m0356kb/) (in reply to ID 1h4n1i9):\nThe moat is getting smaller and smaller ain’t it.\n\n## Comment ID m0581bj with +1 score by [(PromptAfraid4598, Reddit, 2024-12-03)](https://www.reddit.com/r/LocalLLaMA/comments/1h4n1i9/openweights_ai_models_are_bad_says_openai_ceo_sam/m0581bj/) (in reply to ID 1h4n1i9):\nWhen China makes the weights of these models public, it means that these models are already outdated.\n\n## Comment ID m05r4g7 with +1 score by [(Distracted_Llama-234, Reddit, 2024-12-03)](https://www.reddit.com/r/LocalLLaMA/comments/1h4n1i9/openweights_ai_models_are_bad_says_openai_ceo_sam/m05r4g7/) (in reply to ID 1h4n1i9):\nCry more.\n\n## Comment ID m0663qs with +1 score by [(Oehriehqkbt, Reddit, 2024-12-03)](https://www.reddit.com/r/LocalLLaMA/comments/1h4n1i9/openweights_ai_models_are_bad_says_openai_ceo_sam/m0663qs/) (in reply to ID 1h4n1i9):\nThe compium is intensifying\n\n## Comment ID m077fnx with +1 score by [(None, Reddit, 2024-12-03)](https://www.reddit.com/r/LocalLLaMA/comments/1h4n1i9/openweights_ai_models_are_bad_says_openai_ceo_sam/m077fnx/) (in reply to ID 1h4n1i9):\nImagine trusting a Chinese AI\n\n## Comment ID m002wbs with +1 score by [(matadorius, Reddit, 2024-12-02)](https://www.reddit.com/r/LocalLLaMA/comments/1h4n1i9/openweights_ai_models_are_bad_says_openai_ceo_sam/m002wbs/) (in reply to ID 1h4n1i9):\nDeep seek is fine but saying is one of the best is a bit too much\n\n### Comment ID m004yc9 with +12 score by [(emsiem22, Reddit, 2024-12-02)](https://www.reddit.com/r/LocalLLaMA/comments/1h4n1i9/openweights_ai_models_are_bad_says_openai_ceo_sam/m004yc9/) (in reply to ID m002wbs):\nYes, nowhere near...\n\nhttps://preview.redd.it/hg0e6cm7zd4e1.png?width=1132&format=png&auto=webp&s=e0cb5e3c2eed402006bebd5291cb2ab3abeddb6d\n\n[https://www.youtube.com/watch?v=9zunASefVas](https://www.youtube.com/watch?v=9zunASefVas)\n\n#### Comment ID m00aack with +1 score by [(matadorius, Reddit, 2024-12-02)](https://www.reddit.com/r/LocalLLaMA/comments/1h4n1i9/openweights_ai_models_are_bad_says_openai_ceo_sam/m00aack/) (in reply to ID m004yc9):\nGood for the tests for me is not as useful I would love it to be tho",
      "# Post ID 1idsd4s: DeepSeek is bad for Silicon Valley. But it might be great for you. with +77 score by [(One-Confusion-2090, Reddit, 2025-01-30)](https://www.reddit.com/r/China/comments/1idsd4s/deepseek_is_bad_for_silicon_valley_but_it_might/)\n\n\n## Comment ID ma2ywko with +18 score by [(LovelyButtholes, Reddit, 2025-01-30)](https://www.reddit.com/r/China/comments/1idsd4s/deepseek_is_bad_for_silicon_valley_but_it_might/ma2ywko/) (in reply to ID 1idsd4s):\nI don't buy it is bad for anyone.  If the model was trained off of ChatGpt, which sounds to be the case,   it likely can be improved upon.   \n\nThat said, there is tremendous benefit to being correct 99% of the time verses 95% of the time.  Like orders of magnitude more helpful.\n\n### Comment ID mb14qxq with +1 score by [(AstroBullivant, Reddit, 2025-02-05)](https://www.reddit.com/r/China/comments/1idsd4s/deepseek_is_bad_for_silicon_valley_but_it_might/mb14qxq/) (in reply to ID ma2ywko):\nDeepSeek is pretty crappy: \n\nhttps://www.reuters.com/world/china/deepseeks-chatbot-achieves-17-accuracy-trails-western-rivals-newsguard-audit-2025-01-29/\n\nDeepSeek Math is mediocre.\n\n## Comment ID ma82x75 with +5 score by [(I_will_delete_myself, Reddit, 2025-01-31)](https://www.reddit.com/r/China/comments/1idsd4s/deepseek_is_bad_for_silicon_valley_but_it_might/ma82x75/) (in reply to ID 1idsd4s):\nIt helps Silicon valley actually. Open source benefits everybody\n\n## Comment ID ma8z7pw with +3 score by [(asnbud01, Reddit, 2025-01-31)](https://www.reddit.com/r/China/comments/1idsd4s/deepseek_is_bad_for_silicon_valley_but_it_might/ma8z7pw/) (in reply to ID 1idsd4s):\nIt's great for developers, smaller IT companies and businesses everywhere in the world, including the U.S.\n\n## Comment ID ma1kbu8 with +2 score by [(AutoModerator, Reddit, 2025-01-30)](https://www.reddit.com/r/China/comments/1idsd4s/deepseek_is_bad_for_silicon_valley_but_it_might/ma1kbu8/) (in reply to ID 1idsd4s):\n**NOTICE: See below for a copy of the original post in case it is edited or deleted.**\n\n\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/China) if you have any questions or concerns.*\n\n## Comment ID ma3y9vf with +2 score by [(WastrelWink, Reddit, 2025-01-31)](https://www.reddit.com/r/China/comments/1idsd4s/deepseek_is_bad_for_silicon_valley_but_it_might/ma3y9vf/) (in reply to ID 1idsd4s):\nIf there's a way to do 90% of the AI for 1% of the price, then all expensive AI development will fail, and all future investment is a waste of money.\n\n  \nThe Concorde comes to mind\n\n### Comment ID mac70tp with +1 score by [(None, Reddit, 2025-02-01)](https://www.reddit.com/r/China/comments/1idsd4s/deepseek_is_bad_for_silicon_valley_but_it_might/mac70tp/) (in reply to ID ma3y9vf):\n[removed]\n\n#### Comment ID mac70vn with +1 score by [(AutoModerator, Reddit, 2025-02-01)](https://www.reddit.com/r/China/comments/1idsd4s/deepseek_is_bad_for_silicon_valley_but_it_might/mac70vn/) (in reply to ID mac70tp):\nYour post was removed because you have submitted a link to a platform that rewrites articles from other sources without citation. Please resubmit using a link to the original source material.\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/China) if you have any questions or concerns.*\n\n## Comment ID ma2iglq with +9 score by [(None, Reddit, 2025-01-30)](https://www.reddit.com/r/China/comments/1idsd4s/deepseek_is_bad_for_silicon_valley_but_it_might/ma2iglq/) (in reply to ID 1idsd4s):\n[deleted]\n\n### Comment ID ma2m32p with +16 score by [(Zmoogz, Reddit, 2025-01-30)](https://www.reddit.com/r/China/comments/1idsd4s/deepseek_is_bad_for_silicon_valley_but_it_might/ma2m32p/) (in reply to ID ma2iglq):\nJust run the LM locally. Deepseek is open sourced. The web based version is CCP influenced, so that's a no-go.\n\n### Comment ID ma357zp with +12 score by [(Impossible_Self_2484, Reddit, 2025-01-30)](https://www.reddit.com/r/China/comments/1idsd4s/deepseek_is_bad_for_silicon_valley_but_it_might/ma357zp/) (in reply to ID ma2iglq):\nIt's open source. You can run distilled models locally, and you don't need to wait long before other companies start offering similar services based on this open source model. That's the power of open source.\n\n#### Comment ID ma68rah with +2 score by [(Gromchy, Reddit, 2025-01-31)](https://www.reddit.com/r/China/comments/1idsd4s/deepseek_is_bad_for_silicon_valley_but_it_might/ma68rah/) (in reply to ID ma357zp):\nNot everyone wants to run distilled models locally, most people are users. And what is best for most consumers is a model where the information flow hasn't been censored by a government's Ministry of Truth.\n\n### Comment ID ma2jjgn with +7 score by [(marshallannes123, Reddit, 2025-01-30)](https://www.reddit.com/r/China/comments/1idsd4s/deepseek_is_bad_for_silicon_valley_but_it_might/ma2jjgn/) (in reply to ID ma2iglq):\nWhat ? Aren't you glad there is an AI app that can be manipulated by the CCP ?\n\n### Comment ID ma4nta3 with +2 score by [(fartarella, Reddit, 2025-01-31)](https://www.reddit.com/r/China/comments/1idsd4s/deepseek_is_bad_for_silicon_valley_but_it_might/ma4nta3/) (in reply to ID ma2iglq):\nThe best part is that deepseek will answer your question, but then sensor itself. If you have a bad internet connection it can take almost a minute to erase what it tells you, lol\n\n### Comment ID ma6atty with +2 score by [(iamdrp995, Reddit, 2025-01-31)](https://www.reddit.com/r/China/comments/1idsd4s/deepseek_is_bad_for_silicon_valley_but_it_might/ma6atty/) (in reply to ID ma2iglq):\nYes you are right now ask chat gpt if palastians should resist their genocide:)\n\n### Comment ID ma41e4t with +3 score by [(xjpmhxjo, Reddit, 2025-01-31)](https://www.reddit.com/r/China/comments/1idsd4s/deepseek_is_bad_for_silicon_valley_but_it_might/ma41e4t/) (in reply to ID ma2iglq):\nIt’s still good to have a more authentic source for ccp’s perspectives. It was almost impossible for non-Chinese persons previously.\n\n### Comment ID mac7d1w with +1 score by [(Deep_Caterpillar_574, Reddit, 2025-02-01)](https://www.reddit.com/r/China/comments/1idsd4s/deepseek_is_bad_for_silicon_valley_but_it_might/mac7d1w/) (in reply to ID ma2iglq):\nIt's like just up to 100 questions, you could just google (or very likely already know answers).\nI guess, it's more like a meme, than an issue.\n\n\nBtw overall it gives better answers about China. Like about books, movies and music. Which is nice.\n\n## Comment ID ma5i800 with +4 score by [(aD_rektothepast, Reddit, 2025-01-31)](https://www.reddit.com/r/China/comments/1idsd4s/deepseek_is_bad_for_silicon_valley_but_it_might/ma5i800/) (in reply to ID 1idsd4s):\nYeah fuck that do not give any info to Chinese companies.. who knows what they can do with it.\n\n### Comment ID mac6po0 with +1 score by [(Deep_Caterpillar_574, Reddit, 2025-02-01)](https://www.reddit.com/r/China/comments/1idsd4s/deepseek_is_bad_for_silicon_valley_but_it_might/mac6po0/) (in reply to ID ma5i800):\nYou could... Run it locally, with no internet connection, if that's the case. Technically, open ai itself could just copy and run it.\n\n## Comment ID ma4mxdk with +2 score by [(abhinav248829, Reddit, 2025-01-31)](https://www.reddit.com/r/China/comments/1idsd4s/deepseek_is_bad_for_silicon_valley_but_it_might/ma4mxdk/) (in reply to ID 1idsd4s):\nTypical reddit is quite left leaning; they hate anyone anything about their own country. They will support China & use Chinese apps just out of spite…\n\n### Comment ID ma5fqwo with +3 score by [(ramblinonslow, Reddit, 2025-01-31)](https://www.reddit.com/r/China/comments/1idsd4s/deepseek_is_bad_for_silicon_valley_but_it_might/ma5fqwo/) (in reply to ID ma4mxdk):\nIt’s wild.. lol… they won’t be doing this when we are in a war and they’re getting drafted\n\n### Comment ID mac7xrc with +1 score by [(Deep_Caterpillar_574, Reddit, 2025-02-01)](https://www.reddit.com/r/China/comments/1idsd4s/deepseek_is_bad_for_silicon_valley_but_it_might/mac7xrc/) (in reply to ID ma4mxdk):\nCoding with free reasoning model (pretty good at coding) instead of paying 200$ for the same thing. Is just kind of rational.\n\n\nAlso myth about \"left\" china is rather similar to some cringe comments you could find on reddit.\n\n## Comment ID ma67ewu with +1 score by [(XYZ_Labs, Reddit, 2025-01-31)](https://www.reddit.com/r/China/comments/1idsd4s/deepseek_is_bad_for_silicon_valley_but_it_might/ma67ewu/) (in reply to ID 1idsd4s):\nDeepSeek's Latest Shocker: Who Needs CUDA Anyway?\n\nHow Assembly-Level PTX Programming Achieved 10x Efficiency Over CUDA\n\n\n\n[https://xyzlabs.substack.com/p/deepseeks-latest-shocker-who-needs](https://xyzlabs.substack.com/p/deepseeks-latest-shocker-who-needs)\n\n## Comment ID md4vn0f with +1 score by [(OneNectarine1545, Reddit, 2025-02-16)](https://www.reddit.com/r/China/comments/1idsd4s/deepseek_is_bad_for_silicon_valley_but_it_might/md4vn0f/) (in reply to ID 1idsd4s):\nThis article brings up some interesting points, but I think it's framing the situation in a slightly overly dramatic way, typical of Western media when discussing China. While competition is heating up in the AI space, and yes, DeepSeek is a significant player, it's not necessarily \"bad\" for Silicon Valley. Competition breeds innovation, right? It pushes everyone to do better. This benefits everyone, not just one company or country.\n\n\nThe article emphasizes the US export controls and how they might hinder China's progress, but it also acknowledges China's determination and resourcefulness. China has a massive domestic market and a huge pool of talent. They're investing heavily in AI, and companies like DeepSeek are a clear indication of that. The restrictions might slow things down temporarily, but they also incentivize China to become self-sufficient in the long run. That's the stated goal, anyway.\n\n\nInstead of viewing this as a zero-sum game (\"Silicon Valley vs. China\"), I think it's more accurate to see it as a global race where everyone is pushing the boundaries of AI. More players mean more innovation, more diverse approaches, and ultimately, better AI for all of us. And frankly, a little healthy competition is good for preventing monopolies, wouldn't you agree? The focus on \"uncensored\" models is a bit of a red herring, in my opinion. Every country has its own regulations and cultural norms. The important thing is that these tools are developed responsibly and ethically, regardless of where they originate.\n\n## Comment ID ma68zux with +1 score by [(Gromchy, Reddit, 2025-01-31)](https://www.reddit.com/r/China/comments/1idsd4s/deepseek_is_bad_for_silicon_valley_but_it_might/ma68zux/) (in reply to ID 1idsd4s):\nIt will be good when they remove the CCP's censorship and propaganda from Deepseek. \n\nBut whom are we kidding, it's a Chinese company. Expect at least some level of government censorship...\n\n### Comment ID ma7cf6c with +4 score by [(roferer, Reddit, 2025-01-31)](https://www.reddit.com/r/China/comments/1idsd4s/deepseek_is_bad_for_silicon_valley_but_it_might/ma7cf6c/) (in reply to ID ma68zux):\nWho the hell cares…. How frequently are you in need of searching for political stuff in China. 99% of the users are looking for cooking recipes, translation and other low req stuff. And that’s something DeepSeek is capable to do for free.\nThat’s why is at the wining position with GPT now, and it will continue until we will see something spectacular from US.\n\n#### Comment ID ma7m0vk with +2 score by [(Gromchy, Reddit, 2025-01-31)](https://www.reddit.com/r/China/comments/1idsd4s/deepseek_is_bad_for_silicon_valley_but_it_might/ma7m0vk/) (in reply to ID ma7cf6c):\nI don't focus on politics, although many probably would if they are doing research, so i guess you're already wrong here.\n\nHowever i do compare with Bing, Chatgpt and Gemini. \n\nI do find the app lacking even when putting politics aside.\n\n### Comment ID mabqzen with +1 score by [(bugcatcherpaul, Reddit, 2025-02-01)](https://www.reddit.com/r/China/comments/1idsd4s/deepseek_is_bad_for_silicon_valley_but_it_might/mabqzen/) (in reply to ID ma68zux):\ndeepseek when another reddit neckbeard asks it about tiananmen square for the millionth time:\n\nhttps://preview.redd.it/erbod6hwhgge1.jpeg?width=904&format=pjpg&auto=webp&s=a6032c53446aa727e6290054273905c2e63a4e60",
      "# Post ID 1j2g89b: “This Monster Wants to Eat Me” PV 1 with +359 score by [(zenzen_0, Reddit, 2025-03-03)](https://www.reddit.com/r/anime/comments/1j2g89b/this_monster_wants_to_eat_me_pv_1/)\n\n\n\n\n## Comment ID mfrklmd with +42 score by [(Holmesee, Reddit, 2025-03-03)](https://www.reddit.com/r/anime/comments/1j2g89b/this_monster_wants_to_eat_me_pv_1/mfrklmd/) (in reply to ID 1j2g89b):\nStacked cast.\n\nAlso love the whole melancholic tone.\n\n## Comment ID mfrgzqq with +70 score by [(zenzen_0, Reddit, 2025-03-03)](https://www.reddit.com/r/anime/comments/1j2g89b/this_monster_wants_to_eat_me_pv_1/mfrgzqq/) (in reply to ID 1j2g89b):\nBroadcast October 2025\n\nSynopsis: “I’ve come to eat you”so softly utters the mermaid Shiori as she emerges from the sea and takes high school girl Hinako by the hand. Hinako lives alone in a town by the sea and possesses an unusually delicious body that is irresistible to nearby monsters. To ensure that she matures to the best condition, Shiori seeks to protect Hinako—all so that someday, she can devour every piece of her. A longing thought comes to Hinako’s mind; “Maybe this monster can make my wish come true.”\n\nSTUDIO LINGS\n\nChief Director: Naoyuki Kuzuya\n\nDirector: Yusuke Suzuki\n\nSeries Composition / Screenplay: Mitsutaka Hirota\n\nCharacter Design: Nozomi Ikuyama\n\nReina Ueda as Hinako Yaotose \n\nYui Ishikawa as Shiori Oumi\n\nFairouz Ai as Miko Yashiro\n\nhttps://wata-tabe.com/\n\n### Comment ID mfrl0ww with +62 score by [(yukiaddiction, Reddit, 2025-03-03)](https://www.reddit.com/r/anime/comments/1j2g89b/this_monster_wants_to_eat_me_pv_1/mfrl0ww/) (in reply to ID mfrgzqq):\nYui Ishikawa is really going around collecting roles for Yuri anime lmao.\n\n### Comment ID mfrio3m with +39 score by [(zairaner, Reddit, 2025-03-03)](https://www.reddit.com/r/anime/comments/1j2g89b/this_monster_wants_to_eat_me_pv_1/mfrio3m/) (in reply to ID mfrgzqq):\nHigh profile voice actresses for all 3 of the characters!\n\n#### Comment ID mfsnyu0 with +8 score by [(abandoned_idol, Reddit, 2025-03-03)](https://www.reddit.com/r/anime/comments/1j2g89b/this_monster_wants_to_eat_me_pv_1/mfsnyu0/) (in reply to ID mfrio3m):\nIt must have been produced very far back considering one of them is in tentatively indefinite hiatus.\n\n### Comment ID mfrj3rz with +16 score by [(Plus_Rip4944, Reddit, 2025-03-03)](https://www.reddit.com/r/anime/comments/1j2g89b/this_monster_wants_to_eat_me_pv_1/mfrj3rz/) (in reply to ID mfrgzqq):\nThis anime for Halloween time seems perfect\n\n### Comment ID mfrk68f with +16 score by [(marshmallow_sunshine, Reddit, 2025-03-03)](https://www.reddit.com/r/anime/comments/1j2g89b/this_monster_wants_to_eat_me_pv_1/mfrk68f/) (in reply to ID mfrgzqq):\nWhen you read the word mermaid here don't think of Ariel. We only see a small glimpse of her true form in the PV, but eastern mermaids tend to be much more hideous creatures more akin to something like [that thing in Cabin in the Woods](https://imgur.com/a/50XOaCK)\n\n#### Comment ID mfzz6no with +3 score by [(timojenbin, Reddit, 2025-03-04)](https://www.reddit.com/r/anime/comments/1j2g89b/this_monster_wants_to_eat_me_pv_1/mfzz6no/) (in reply to ID mfrk68f):\nFriend of mine had a OC ink drawing above her fireplace of a mermaid hunting in the wild. Long drawn out body arced toward a school of fish running away from her wide snapping jaws. The background was all black so the fish and mermaid were mostly white of the paper. Very creepy.  \n  \nIt was awesome.\n\n### Comment ID mfrjg9u with +9 score by [(Syokhan, Reddit, 2025-03-03)](https://www.reddit.com/r/anime/comments/1j2g89b/this_monster_wants_to_eat_me_pv_1/mfrjg9u/) (in reply to ID mfrgzqq):\nThat's a sweet voice cast!\n\n### Comment ID mfsghsx with +9 score by [(etownzu, Reddit, 2025-03-03)](https://www.reddit.com/r/anime/comments/1j2g89b/this_monster_wants_to_eat_me_pv_1/mfsghsx/) (in reply to ID mfrgzqq):\nWait, does this mean Fairouz AI is back from her hiatus?\n\n#### Comment ID mftxtaf with +9 score by [(Psyduckisnotaduck, Reddit, 2025-03-03)](https://www.reddit.com/r/anime/comments/1j2g89b/this_monster_wants_to_eat_me_pv_1/mftxtaf/) (in reply to ID mfsghsx):\nShe’s still doing a few roles but she’s doing no publicity or public appearances I think, which are huge parts of being a voice actress these days. She had to make the announcement probably because her mental issues made it difficult to do radio, TV, and live events. Until she’s no longer in hiatus she’ll probably only have one or two roles per season.\n\n#### Comment ID mft96sm with +1 score by [(TheDanubianCommunard, Reddit, 2025-03-03)](https://www.reddit.com/r/anime/comments/1j2g89b/this_monster_wants_to_eat_me_pv_1/mft96sm/) (in reply to ID mfsghsx):\nProbably the deal is that the recording is happening much earlier.\n\n## Comment ID mfrktga with +15 score by [(HuTaosTwinTails, Reddit, 2025-03-03)](https://www.reddit.com/r/anime/comments/1j2g89b/this_monster_wants_to_eat_me_pv_1/mfrktga/) (in reply to ID 1j2g89b):\nLETS GO THAT IS A STACKED CAST\n\n## Comment ID mfs0xw4 with +15 score by [(Tomcat491, Reddit, 2025-03-03)](https://www.reddit.com/r/anime/comments/1j2g89b/this_monster_wants_to_eat_me_pv_1/mfs0xw4/) (in reply to ID 1j2g89b):\nWe have a subreddit btw! r/watatabe\n\n## Comment ID mfriggf with +36 score by [(zairaner, Reddit, 2025-03-03)](https://www.reddit.com/r/anime/comments/1j2g89b/this_monster_wants_to_eat_me_pv_1/mfriggf/) (in reply to ID 1j2g89b):\nEven if the title says PV1, we had the previous teaser trailer ([with subs here](https://www.youtube.com/watch?v=KuquveO6WuI)!).\n\nVery very much looking forward to this. It's good to see best girl miko. I was just sad that ai fairouz didn't reprise her role in the ggwp thread, but at least we have her here!\n\n### Comment ID mfrny0w with +27 score by [(Nachtwandler_FS, Reddit, 2025-03-03)](https://www.reddit.com/r/anime/comments/1j2g89b/this_monster_wants_to_eat_me_pv_1/mfrny0w/) (in reply to ID mfriggf):\nShe is battling PTSD currently but still doing a lot of roles. Had to gave up some, probably.\n\n#### Comment ID mfromb2 with +10 score by [(zairaner, Reddit, 2025-03-03)](https://www.reddit.com/r/anime/comments/1j2g89b/this_monster_wants_to_eat_me_pv_1/mfromb2/) (in reply to ID mfrny0w):\nRight, I was worried about about that in ggwps context. Though from what we learned today, actually neither of the two manga pvs voice actresses returned for the ggwp anime, so it might just be independent of that.\n\nLet's hope she is resting well.\n\n#### Comment ID mfzzk6v with +1 score by [(timojenbin, Reddit, 2025-03-04)](https://www.reddit.com/r/anime/comments/1j2g89b/this_monster_wants_to_eat_me_pv_1/mfzzk6v/) (in reply to ID mfrny0w):\nWhat caused the ptsd?\n\n### Comment ID mfrzcc6 with +7 score by [(Cyrra_, Reddit, 2025-03-03)](https://www.reddit.com/r/anime/comments/1j2g89b/this_monster_wants_to_eat_me_pv_1/mfrzcc6/) (in reply to ID mfriggf):\nMiko stocks rising with that casting, best girl deserves the best.\n\nFelt bad that I liked the human best friend more than the main love interest but Fish has been creeping her way up for me since v4 or 5 or so, she's very funny.\n\n### Comment ID mfriqam with +7 score by [(Abysswatcherbel, Reddit, 2025-03-03)](https://www.reddit.com/r/anime/comments/1j2g89b/this_monster_wants_to_eat_me_pv_1/mfriqam/) (in reply to ID mfriggf):\n>in the ggwp thread, \n\nThat's how I discovered it happened\n\n#### Comment ID mfrj1d9 with +3 score by [(zairaner, Reddit, 2025-03-03)](https://www.reddit.com/r/anime/comments/1j2g89b/this_monster_wants_to_eat_me_pv_1/mfrj1d9/) (in reply to ID mfriqam):\nIt's on the front page, how did you find this first.\n\n## Comment ID mfrh58u with +18 score by [(Radiant-Rest-685, Reddit, 2025-03-03)](https://www.reddit.com/r/anime/comments/1j2g89b/this_monster_wants_to_eat_me_pv_1/mfrh58u/) (in reply to ID 1j2g89b):\nOh yeah I remember reading it some time ago The first few chapters were quite good\n\n## Comment ID mfs1o96 with +9 score by [(xnef1025, Reddit, 2025-03-03)](https://www.reddit.com/r/anime/comments/1j2g89b/this_monster_wants_to_eat_me_pv_1/mfs1o96/) (in reply to ID 1j2g89b):\nSo when this one comes up in those \"Describe an anime poorly\" threads, will it be \"The Shape of Water: Yuri Edition\"?  \n\n\n\nBecause if so, I'm in.\n\n### Comment ID mfs6lqb with +5 score by [(Shrim, Reddit, 2025-03-03)](https://www.reddit.com/r/anime/comments/1j2g89b/this_monster_wants_to_eat_me_pv_1/mfs6lqb/) (in reply to ID mfs1o96):\nKind of. But not quite.\n\n#### Comment ID mftk2yx with +1 score by [(xnef1025, Reddit, 2025-03-03)](https://www.reddit.com/r/anime/comments/1j2g89b/this_monster_wants_to_eat_me_pv_1/mftk2yx/) (in reply to ID mfs6lqb):\nClose enough.  I'm in anyway 😋\n\n### Comment ID mftt262 with +1 score by [(jabberwockxeno, Reddit, 2025-03-03)](https://www.reddit.com/r/anime/comments/1j2g89b/this_monster_wants_to_eat_me_pv_1/mftt262/) (in reply to ID mfs1o96):\nI haven't seen Shape of Water, but my closest point of comparison would actually be Evangelion, in terms of how it uses horror imagery, the romance, etc as a vehicle to explore trauma, communication with others, and developing better mindsets.\n\n#### Comment ID mftwhus with +1 score by [(xnef1025, Reddit, 2025-03-03)](https://www.reddit.com/r/anime/comments/1j2g89b/this_monster_wants_to_eat_me_pv_1/mftwhus/) (in reply to ID mftt262):\nThat is actually what shape of water does too, but with more sish fex 😋\n\n## Comment ID mft0km9 with +7 score by [(BosuW, Reddit, 2025-03-03)](https://www.reddit.com/r/anime/comments/1j2g89b/this_monster_wants_to_eat_me_pv_1/mft0km9/) (in reply to ID 1j2g89b):\nWoah, the composer is hella cooking\n\n## Comment ID mfrswpf with +14 score by [(444lp, Reddit, 2025-03-03)](https://www.reddit.com/r/anime/comments/1j2g89b/this_monster_wants_to_eat_me_pv_1/mfrswpf/) (in reply to ID 1j2g89b):\nIt's probably going to share a significant amount of staff with the Sorario Utility TV series. I haven't seen it, but the staff is already very similar, and the director of Watatabe is already working as SB on an episode of Sorario Utility. Lings, as a studio, is involved in every episode of that show.\n\nAlso, I've been browsing clips for over an hour, specifically looking at things both directors have worked on. Yuusuke Suzuki comes across as the stronger boarder in general. His boards contain a lot of fun expressions, and his framing feels more natural, rather than being overly planned, it's a much more casual way of framing. He also loves having the character's front unlit, with the backlight behind them.\n\nThat said, I’m not sure how well he would mesh with a show like Watatabe, since he’s never worked on anything like it before. He seems to excel in shows that allow for goofy expressions and silly moments, which are almost nonexistent in This Monster Wants to Eat Me. However, I still favor him over Naoyuki, who is the chief director.\n\nThough I don't expect Naoyuki to be as involved as Suzuki on individual episodes, he’ll likely be there to maintain quality control, which should be fine.\n\n## Comment ID mfrjekp with +15 score by [(Syokhan, Reddit, 2025-03-03)](https://www.reddit.com/r/anime/comments/1j2g89b/this_monster_wants_to_eat_me_pv_1/mfrjekp/) (in reply to ID 1j2g89b):\nOh I'm loving the vibes with the melancholy music. Very fitting.\n\n## Comment ID mfs4vcg with +6 score by [(HollowWarrior46, Reddit, 2025-03-03)](https://www.reddit.com/r/anime/comments/1j2g89b/this_monster_wants_to_eat_me_pv_1/mfs4vcg/) (in reply to ID 1j2g89b):\nAh yes the vore yuri, we’re eating good\n\n## Comment ID mftay5v with +7 score by [(TheDanubianCommunard, Reddit, 2025-03-03)](https://www.reddit.com/r/anime/comments/1j2g89b/this_monster_wants_to_eat_me_pv_1/mftay5v/) (in reply to ID 1j2g89b):\nYuri, supernatural stuff, stacked cast (I know that Reina Ueda will be there by feeling), I think there is potential in it.\n\n2025 is the year of yuri.\n\n## Comment ID mfrp3ye with +12 score by [(G326, Reddit, 2025-03-03)](https://www.reddit.com/r/anime/comments/1j2g89b/this_monster_wants_to_eat_me_pv_1/mfrp3ye/) (in reply to ID 1j2g89b):\nThe cast is very stacked. the PV doesn't blow me away in terms of animation, but doesn't look bad either, but at least the chief director has some experience (although most of his shows are kind of mid in terms of production values). But I'm still a bit worried about the show's execution, especially since it's horror (a genre which needs good production to work properly)\n\nBut the concept is very intriguing as we rarely have shows that mix yuri with horror/psychological elements. It's also so nice seeing more and more Yuri manga receiving adaptations at all.\n\n### Comment ID mft2wex with +4 score by [(BosuW, Reddit, 2025-03-03)](https://www.reddit.com/r/anime/comments/1j2g89b/this_monster_wants_to_eat_me_pv_1/mft2wex/) (in reply to ID mfrp3ye):\nIm optimistic since both the trailers get the tone and atmosphere right at least.\n\nOne thing I noticed this trailer is that they kept the \"eyes half out of frame\" thing the manga likes to do. That's a good sign I think. \n\nBut it'll certainly be a challenge to adapt in any case. The manga just sucks you in like a well, it's incredible how much grasp the mangaka has of the medium to achieve that effect. A by the book adaptation would not be enough.\n\n## Comment ID mfri114 with +26 score by [(DWIPssbm, Reddit, 2025-03-03)](https://www.reddit.com/r/anime/comments/1j2g89b/this_monster_wants_to_eat_me_pv_1/mfri114/) (in reply to ID 1j2g89b):\nVore fetish yuri, let's go !\n\n### Comment ID mfrkuku with +36 score by [(yukiaddiction, Reddit, 2025-03-03)](https://www.reddit.com/r/anime/comments/1j2g89b/this_monster_wants_to_eat_me_pv_1/mfrkuku/) (in reply to ID mfri114):\nYou are going to trick gooner into emotional damage experience lol.\n\n#### Comment ID mfsamsv with +9 score by [(SenorIngles, Reddit, 2025-03-03)](https://www.reddit.com/r/anime/comments/1j2g89b/this_monster_wants_to_eat_me_pv_1/mfsamsv/) (in reply to ID mfrkuku):\nI hate that I understand this comment chain\n\n## Comment ID mftfwz7 with +9 score by [(jabberwockxeno, Reddit, 2025-03-03)](https://www.reddit.com/r/anime/comments/1j2g89b/this_monster_wants_to_eat_me_pv_1/mftfwz7/) (in reply to ID 1j2g89b):\nI started to read this on a whim recently, and within 8 hours I had binged every released manga chapter and was obsessively tried to find every single piece of online discussion about the series afterwards.\n\n100% give this a watch when it airs, or read the manga, even if you're not into Yuri/Romance (I'm not either!) \n\nIt gets tagged as Yuri/Romance or Horror in a lot of places, and not without reason, but in my opinion it mostly uses the Yuri/Romance elements (tho it does seem like that might pick up in future chapters, and/or I just might not pick up on those elements as much since it's not what i'm into), the horror imagery and monsters more as a vehicle to explore Trauma, The struggles of communication, and personal growth/developing healthy mindsets as major themes. \n\nWhile it's not as amazingly well written/directed as Evangelion (Tho I actually found this more personally resonant then I did Eva, as I explain below), it reminds me of how that similarly uses the Angel/Eva fights, horror imagery, and the relationships between its characters to tackle those themes too.\n\nI'm somebody who really enjoys gnarly monster designs and the psychological themes *in* horror media, but is frankly too much of a wimp to actually consume horror that's meant to be scary, and this really scratches that itch for me: The monster designs are really gnarly and imposing ([Putting image links in spoiler tags just in case]>![1](https://i.imgur.com/wNI0CAE.jpg), [2](https://i.imgur.com/6IaYMcZ.jpg), [3](https://i.imgur.com/pVYbweS.png)!<), and beyond obviously handling the topic of trauma, PTSD, and suicide well as a core part of the narrative, it has some really creative imagery using the ocean/water as visual symbolism for those things: [the haze of melancholy causes speech bubbles and the world around Hinako to become wavy like water and deep sea fish to swim by behind windows](https://i.imgur.com/kOGLXOZ.png), for example.\n\nProbably my favorite aspect of the series is how Shiori is written.It does a *really* good job selling her as a fundamentally not-human character with a totally different way of thinking about things, and how it presents difficulties in her ability to communicate and relate to humans is really well done, in a way that both really makes her feel alien while simultaneously humanizing her and making her emphatic. \n\n[Again, I've avoided touching on anything i'd consider an explicit spoiler, but just in case:]>!Even when she talks about wanting to eat people apart, you never get the sense it's said out of malice or sadism, in fact it often feels earnestly nonchalant and polite. And even when she's ostensibly being altruistic, there's still an element of selfishness/pragmatism there, not even necessarily because she's not being honestly nice, but because *she* doesn't recognize or think of her own actions/motives that way, because the concept of compassion isn't something she gets or views the same way!<\n\n[Spoiler tags again, just in case]>!She doesn't feel *either* particularly self-centered or considerate, nor kind or mean. Or perhaps she feels like all of those things at once. Its as if she's on a different set of personality axes entirely, that just happens to sometimes intersect with those of \"normal\" characters, and whenever it does, it's always fascinating to see each character try to navigate and work out where they other is coming from!<\n\nI am not really somebody who values representation in media, but as somebody with Aspergers/on the Autism spectrum, her character was profoundly impactful and relatable to me: Interactions with people often does feel like trying to speak to some alien or some non-human entity, where even if we're both being fully honest and clear and are speaking the same language, there's always a gap in understanding that prevents us from fully being on the same page, and that's something that the series *really* conveys well and comments on a lot. \n\nAll that said, I am a bit worried about the anime adaptation: There's a few shots in the trailer here that I *really* would have wanted to have either great sakuga quality animation, or at least really well done artwork and direction, and they simply do not seem to. There's also some visual flourishes i'm not sure it's adapting: [The ends/bottom of shirori's hair often has the reflective caustics of waves on them in the manga](https://i.imgur.com/3cFG2zc.png), and it's not present in any shot of the trailer(s) or in the character artwork, though it could just be that they haven't shown any of the shots where they happen to do that effect.\n\nHopefully i'm worrying too much, I really want this to be a great adpatation!\n\n## Comment ID mft9fj1 with +3 score by [(TokiVideogame, Reddit, 2025-03-03)](https://www.reddit.com/r/anime/comments/1j2g89b/this_monster_wants_to_eat_me_pv_1/mft9fj1/) (in reply to ID 1j2g89b):\nhinako's voice is so crisp\n\n## Comment ID mfur5al with +3 score by [(Vaadwaur, Reddit, 2025-03-03)](https://www.reddit.com/r/anime/comments/1j2g89b/this_monster_wants_to_eat_me_pv_1/mfur5al/) (in reply to ID 1j2g89b):\nWelp...not like I would pass on this, anyways. Just more sold.\n\n## Comment ID mfso4va with +2 score by [(abandoned_idol, Reddit, 2025-03-03)](https://www.reddit.com/r/anime/comments/1j2g89b/this_monster_wants_to_eat_me_pv_1/mfso4va/) (in reply to ID 1j2g89b):\nI'm not excited, but I really liked the OST, looking forward to the OP and ED at least.\n\nHopefully the story is also good too, but egh, I'm pessimistic.\n\n### Comment ID mftbmwu with +7 score by [(Plus_Rip4944, Reddit, 2025-03-03)](https://www.reddit.com/r/anime/comments/1j2g89b/this_monster_wants_to_eat_me_pv_1/mftbmwu/) (in reply to ID mfso4va):\nManga reader here: The story is The best thing easily so even if animation and everything else isnt good(i Hope It is) story can Carry The anime\n\n## Comment ID mfurxkx with +2 score by [(None, Reddit, 2025-03-03)](https://www.reddit.com/r/anime/comments/1j2g89b/this_monster_wants_to_eat_me_pv_1/mfurxkx/) (in reply to ID 1j2g89b):\nLooks interesting!\n\n## Comment ID mfx04nv with +2 score by [(soulreaverdan, Reddit, 2025-03-04)](https://www.reddit.com/r/anime/comments/1j2g89b/this_monster_wants_to_eat_me_pv_1/mfx04nv/) (in reply to ID 1j2g89b):\nTOXIC MONSTER HORROR YURI\n\nYES.\n\n## Comment ID mfrimj9 with +1 score by [(JonnySpark, Reddit, 2025-03-03)](https://www.reddit.com/r/anime/comments/1j2g89b/this_monster_wants_to_eat_me_pv_1/mfrimj9/) (in reply to ID 1j2g89b):\nOn a scale of 1 to 10, how gay is this?\n\n### Comment ID mfrrl49 with +13 score by [(elbenji, Reddit, 2025-03-03)](https://www.reddit.com/r/anime/comments/1j2g89b/this_monster_wants_to_eat_me_pv_1/mfrrl49/) (in reply to ID mfrimj9):\nDepends on your scale. Probably a 7? It's Yuri but it's not really the focus\n\n### Comment ID mfryao9 with +12 score by [(Cyrra_, Reddit, 2025-03-03)](https://www.reddit.com/r/anime/comments/1j2g89b/this_monster_wants_to_eat_me_pv_1/mfryao9/) (in reply to ID mfrimj9):\nIt doesn't focus a ton on the romance because helping the MC with her depression comes first but it's all little steps on the road in their relationship. You won't see a bunch of kissing or hugging or anything but it's undeniably yuri.\n\n### Comment ID mfrmbst with +20 score by [(marshmallow_sunshine, Reddit, 2025-03-03)](https://www.reddit.com/r/anime/comments/1j2g89b/this_monster_wants_to_eat_me_pv_1/mfrmbst/) (in reply to ID mfrimj9):\nI'm up to date on the manga and I'd give it like a 3 when it comes to gex. \n\nIt's not a romance story, and selling a Yuri relationship isn't its goal. the main lead is so depressed that the only thing she wants to fuck is death. There's definitely some affection there between the characters but it focuses on deeper bonds beyond that of sexual attraction.\n\n### Comment ID mftg4ws with +4 score by [(jabberwockxeno, Reddit, 2025-03-03)](https://www.reddit.com/r/anime/comments/1j2g89b/this_monster_wants_to_eat_me_pv_1/mftg4ws/) (in reply to ID mfrimj9):\nI feel like depending on your perspective, you could give any number between like a 3 and a 8 here and still be correct.\n\nAs a disclaimer, I am not into romance in general or yuri specifically, but I absolutely love the manga: I started to read it on a whim 2 weeks ago and within 8 hours I had read every single released chapter and was trying to find every possible page online that had people talking about the series. \n\nThere is absolutely romantic subtext between characters even calling it subtext might be understating it, but to me, the focus feels like it's more using the romance/yuri elements (as well as the horror elements) as a vehicle to talk about trauma, communication, and personal growth, then the romance or the horror itself is the focus.\n\nBut I'd say of the two it's more of a romance series then it is a horror series even if I'm not sure i'd call it either: Wheras I don't think the manga ever truly feels scary (it does, however, 100% deliver on cool, gnarly monster designs and horror/psychological imagery), the relationship between Hinako and other girls is very much a focus, it's just [Spoilers for the current state of the manga]>!not totally clear on if that will become explicitly romantic? It could, the series is ongoing, and recent chapters seem to be potentially increasing that focus, but the nature of the relationships so far feels... I don't want to say platonic, but something deeper and more abstract then that while not being romantic either? Devotional is the best word I can think of!<\n\nThat being said, I have very little experience with the genre or with IRL relationships, and the romance also just isn't what I'm reading the series for, so I could easily also see an alternative interpretation that it very much *is* a romance. If [Spoilers for the current state of the manga]>!a few chapters from now, Shiori explicitly frames her feelings for Hinako as romantic, it wouldn't feel out of place and it'd recontextualize the whole manga as a romance, I'm just not sure it will go in that direction? Part of it is also just that Shiori is very well written to not feel human and to not have the same view/understanding of emotional concepts that human characters do, so you could maybe already read her feelings for Hinako as romantic, just filtered through her unorthodox character writing in general!<\n\n### Comment ID mfrvs7q with +10 score by [(SirGigglesandLaughs, Reddit, 2025-03-03)](https://www.reddit.com/r/anime/comments/1j2g89b/this_monster_wants_to_eat_me_pv_1/mfrvs7q/) (in reply to ID mfrimj9):\nIt's a 10. The people giving it a 3 are conflating yuri with strictly the  romance genre which isn't really necessary. It's not a romance series, it's a drama and psychological but it is definitely yuri and the girls definitely have feelings for each other. It's just there's a lot more important than that. It's a serious drama about a girl who wants to be eaten. Why she'd want that is the focus. If you're only watching yuri to see a bunch of kissing then I guess the 3 would make sense.\n\n#### Comment ID mfswqaq with +3 score by [(marshmallow_sunshine, Reddit, 2025-03-03)](https://www.reddit.com/r/anime/comments/1j2g89b/this_monster_wants_to_eat_me_pv_1/mfswqaq/) (in reply to ID mfrvs7q):\nThere could be broader interpretations out there, but a romantic relationship between girls is the basis of what I would consider yuri. I can see how the story could move in that direction, but aside from some teases the story just hasn't covered that ground yet.\n\n### Comment ID mfrmncz with +9 score by [(zairaner, Reddit, 2025-03-03)](https://www.reddit.com/r/anime/comments/1j2g89b/this_monster_wants_to_eat_me_pv_1/mfrmncz/) (in reply to ID mfrimj9):\nI don't know why the other guy said 11, but to make it clear: This is a series about depression first and foremost, the main character is barely able to care about anything that is not her own death, much less romance. And the fmc is the inhuman monster.\n\nDepression first, horror second, yuri third.\n\n### Comment ID mfrj56n with +8 score by [(Plus_Rip4944, Reddit, 2025-03-03)](https://www.reddit.com/r/anime/comments/1j2g89b/this_monster_wants_to_eat_me_pv_1/mfrj56n/) (in reply to ID mfrimj9):\n11\n\n## Comment ID mfx3xpd with +1 score by [(Echidnu, Reddit, 2025-03-04)](https://www.reddit.com/r/anime/comments/1j2g89b/this_monster_wants_to_eat_me_pv_1/mfx3xpd/) (in reply to ID 1j2g89b):\nInteresting name lmafo"
    ],
    "sources": {
      "steam_url": null,
      "steam_reviews": null,
      "google_play_url": null,
      "google_play_reviews": null,
      "apple_store_url": null,
      "apple_reviews": null,
      "reddit_urls": [
        "https://www.reddit.com/r/Futurology/comments/1id95mm/exgoogle_apple_engineers_launch_unconditionally/",
        "https://www.reddit.com/r/LocalLLaMA/comments/1id3ak8/exgoogle_apple_engineers_launch_unconditionally/",
        "https://www.reddit.com/r/MachineLearning/comments/1ioxatq/d_we_built_genai_at_google_and_apple_then_left_to/",
        "https://www.reddit.com/r/FortniteFestival/comments/1f8rrod/epic_absolutely_cooked_on_beyond_the_flame_easily/",
        "https://www.reddit.com/r/LocalLLaMA/comments/1hmhgrb/google_deep_research_ai/",
        "https://www.reddit.com/r/Genshin_Impact/comments/1fuguhl/oumi_janta_is_threatening_to_sue_mihoyo_for/",
        "https://www.reddit.com/r/LocalLLaMA/comments/1jbrwqf/deep_research_tools_am_i_the_only_one/",
        "https://www.reddit.com/r/LocalLLaMA/comments/1h4n1i9/openweights_ai_models_are_bad_says_openai_ceo_sam/",
        "https://www.reddit.com/r/China/comments/1idsd4s/deepseek_is_bad_for_silicon_valley_but_it_might/",
        "https://www.reddit.com/r/anime/comments/1j2g89b/this_monster_wants_to_eat_me_pv_1/"
      ],
      "reddit_search_url": "https://www.google.com/search?q=site%3Areddit.com+%22Oumi%22+related%3Aoumi.ai+AI"
    }
  },
  "glassdoor_result": null,
  "news_result": [
    [
      "Oumi",
      "Oumi",
      "oumi.ai",
      [
        "AI"
      ],
      false,
      false,
      null,
      [
        false,
        false
      ]
    ],
    [
      {
        "title": "AI Needs Its Linux: Oumi Comes Out of Stealth with an Open Source ...",
        "link": "https://www.hpcwire.com/2025/01/29/ai-needs-its-linux-oumi-comes-out-of-stealth-with-an-open-source-vision/",
        "snippet": "Jan 29, 2025 ... ... connected with, as word spreads through academic and research communities. Investors See Oumi's Possibilities for AI Research. Oumi has secured $10 million ...",
        "formattedUrl": "https://www.hpcwire.com/.../ai-needs-its-linux-oumi-comes-out-of-stealth-..."
      },
      {
        "title": "Ex-Google, Apple engineers raise $10M to give AI its 'Linux moment ...",
        "link": "https://www.geekwire.com/2025/ex-google-apple-engineers-raise-10m-to-give-ai-its-linux-moment-with-new-open-source-platform/",
        "snippet": "Jan 29, 2025 ... Oumi co-founders Manos Koukoumidis, left, and Oussama Elachqar. (Oumi Photo) A new startup out of Seattle wants to open up the \"black box\" of foundational.",
        "formattedUrl": "https://www.geekwire.com/.../ex-google-apple-engineers-raise-10m-to-give..."
      },
      {
        "title": "Ex-Google, Apple engineers launch unconditionally open source ...",
        "link": "https://venturebeat.com/ai/ex-google-apple-engineers-launch-unconditionally-open-source-oumi-ai-platform-that-could-help-to-build-the-next-deepseek/",
        "snippet": "Jan 29, 2025 ... Oumi CEO and former Google Cloud AI senior engineering manager Manos Koukoumidis told VentureBeat that researchers consistently tell him AI experimentation has ...",
        "formattedUrl": "https://venturebeat.com/.../ex-google-apple-engineers-launch-unconditional..."
      },
      {
        "title": "Oumi AI: A Fully Open-Source Platform to Democratize AI",
        "link": "https://tecknexus.com/oumi-ai-a-fully-open-source-platform-to-democratize-ai-development/4/",
        "snippet": "Jan 29, 2025 ... ... AI development across universities and research institutions to cut costs while achieving similar ... Oumi AI: The Linux of AI—Breaking Barriers in AI Research.",
        "formattedUrl": "https://tecknexus.com/oumi-ai-a-fully-open-source-platform-to.../4/"
      },
      {
        "title": "Why We Invested in Oumi Making AI truly open",
        "link": "https://obvious.com/ideas/why-we-invested-in-oumi/",
        "snippet": "Jan 29, 2025 ... We are excited to be investors in Oumi's $10M Seed round. The exceptional technical leadership of CEO Manos Koukoumidis first drew us to Oumi. As the former ...",
        "formattedUrl": "https://obvious.com/ideas/why-we-invested-in-oumi/"
      },
      {
        "title": "Oumi: The Open Source Platform That Simplifies AI | Best free ...",
        "link": "https://www.turtlesai.com/en/pages-2202/oumi-the-open-source-platform-that-simplifies-ai",
        "snippet": "Jan 30, 2025 ... Oumi, a new open-source AI platform created by former Google and Apple engineers, aims to transform the AI landscape by addressing the current limitations of ...",
        "formattedUrl": "https://www.turtlesai.com/.../oumi-the-open-source-platform-that-simplifies..."
      },
      {
        "title": "Oumi - Crunchbase Company Profile & Funding",
        "link": "https://www.crunchbase.com/organization/oumi",
        "snippet": "Jan 29, 2025 ... Oumi is a group of academics, developers dedicated to fostering greater transparency and cooperation in frontier AI.",
        "formattedUrl": "https://www.crunchbase.com/organization/oumi"
      },
      {
        "title": "Former Google and Apple engineers launch open-source Oumi AI ...",
        "link": "https://dev.ua/en/news/kolyshni-inzhenery-google-i-apple-zapustyly-platformu-shtuchnoho-intelektu-oumi-z-vidkrytym-kodom-iaka-mozhe-dopomohty-stvoryty-nastupnyi-deepseek-1738243733",
        "snippet": "Jan 30, 2025 ... The Oumi platform works by providing a universal environment that streamlines the complex workflows associated with building AI models. Koukoumidis explained ...",
        "formattedUrl": "https://dev.ua/.../news/kolyshni-inzhenery-google-i-apple-zapustyly-platfor..."
      },
      {
        "title": "Oumi Platform Launches to Foster AI Research Collaboration ...",
        "link": "https://techstrong.ai/ai-at-the-edge/oumi-platform-launches-to-foster-ai-research-collaboration/",
        "snippet": "Jan 31, 2025 ... An open Oumi platform for AI researchers has been launched by 11 universities, including Carnegie Mellon, Stanford University, and the MIT.",
        "formattedUrl": "https://techstrong.ai/.../oumi-platform-launches-to-foster-ai-research-collab..."
      },
      {
        "title": "Oumi: The AI revolution led by Greek minds - Neos Kosmos",
        "link": "https://neoskosmos.com/en/2025/03/12/news/business/oumi-the-ai-revolution-led-by-greek-minds/",
        "snippet": "Mar 12, 2025 ... A new AI tool 'Oumi' was launched last month, created by Greek-educated scientists Manos Koukoumides, Panos Achlioptas, and Konstantínos Aísopos.",
        "formattedUrl": "https://neoskosmos.com/.../news/.../oumi-the-ai-revolution-led-by-greek-mi..."
      },
      {
        "title": "The Compute Burden - by Alexander Campbell",
        "link": "https://www.campbellramble.ai/p/the-compute-constraint",
        "snippet": "Jan 30, 2025 ... You are solely responsible for determining whether any investment, investment strategy, security or related ... I'm sure you saw Oumi (https://oumi.ai/) which is ...",
        "formattedUrl": "https://www.campbellramble.ai/p/the-compute-constraint"
      },
      {
        "title": "Former Microsoft engineers launch AI lab with $10M in funding ...",
        "link": "https://www.bizjournals.com/seattle/news/2025/01/29/oumi-microsoft-google-apple-ai-lab-open-source.html",
        "snippet": "Jan 29, 2025 ... Oumi co-founders Oussama Elachqar, left, and Manos Koukoumidis have experience at some of the biggest names in the tech industry. Oussama Elachqar.",
        "formattedUrl": "https://www.bizjournals.com/.../news/.../oumi-microsoft-google-apple-ai-la..."
      },
      {
        "title": "UConn men's basketball's Dan Hurley unloads on officials after loss",
        "link": "https://www.newstimes.com/sports/uconn-mens-basketball/article/dan-hurley-maui-invitational-officials-uconn-huski-19942480.php",
        "snippet": "Nov 25, 2024 ... News Times Logo Subscribe · Sports. UConn's Dan Hurley unloads on ... (Photo by Darryl Oumi/Getty Images). Darryl Oumi/Getty Images. Show Less Show ...",
        "formattedUrl": "https://www.newstimes.com/.../dan-hurley-maui-invitational-officials-uconn..."
      },
      {
        "title": "UConn men's basketball's 17-game winning streak end with Maui ...",
        "link": "https://www.ctinsider.com/sports/uconn-mens-basketball/article/uconn-huskies-memphis-maui-invitational-19941317.php",
        "snippet": "Nov 25, 2024 ... (Photo by Darryl Oumi/Getty Images). Darryl Oumi/Getty Images. LAHAINA ...",
        "formattedUrl": "https://www.ctinsider.com/.../uconn-huskies-memphis-maui-invitational-19..."
      },
      {
        "title": "This Monster Wants to Eat Me Anime Reveals New Trailer, October ...",
        "link": "https://www.imdb.com/news/ni65159455/",
        "snippet": "Mar 3, 2025 ... 1a ) Miko Yashiro voiced by Fairouz Ai (Kikoru in Kaiju No.8 ) Hinako Yaotose Shiori Oumi Miko Yashiro Related: I Want to Love You Till Your Dying Day Anime ...",
        "formattedUrl": "https://www.imdb.com/news/ni65159455/"
      },
      {
        "title": "News",
        "link": "https://www.skel.ai/news/",
        "snippet": "Jul 15, 2024 ... ... AI & Robotics European… Read More. Search for: Recent Posts. Invited ...",
        "formattedUrl": "https://www.skel.ai/news/"
      },
      {
        "title": "UCLA's DeShaun Foster says he makes the Bruins' fourth-down ...",
        "link": "https://www.latimes.com/sports/ucla/story/2024-09-05/ucla-football",
        "snippet": "Sep 5, 2024 ... 31 in Honolulu. UCLA coach DeShaun Foster says he makes all major game-day calls, including when to kick field goals. (Darryl Oumi / Getty Images).",
        "formattedUrl": "https://www.latimes.com/sports/ucla/story/2024-09-05/ucla-football"
      },
      {
        "title": "How well do you know your March Madness trivia? Take our quiz ...",
        "link": "https://www.npr.org/2025/03/20/g-s1-54770/march-madness-quiz-ncaa-tournament",
        "snippet": "Mar 20, 2025 ... Darryl Oumi/Getty Images; Steph Chambers/Getty Images; Jared C. Tilton/Getty Images. March Madness is upon us. Do you know your saint schools from state ...",
        "formattedUrl": "https://www.npr.org/2025/03/20/g.../march-madness-quiz-ncaa-tournament"
      },
      {
        "title": "Making Good on the Promise of Open Source AI - The New Stack",
        "link": "https://thenewstack.io/making-good-on-the-promise-of-open-source-ai/",
        "snippet": "Jan 29, 2025 ... Startup Oumi is coming out of stealth with $10 million in its pocket and an AI platform that executives say is truly open source.",
        "formattedUrl": "https://thenewstack.io/making-good-on-the-promise-of-open-source-ai/"
      },
      {
        "title": "How to watch UConn vs. Colorado at Maui Invitational: Time, TV",
        "link": "https://www.ctpost.com/sports/uconn-mens-basketball/article/colorado-maui-invitational-time-tv-uconn-huskies-19942553.php",
        "snippet": "Nov 25, 2024 ... (Photo by Darryl Oumi/Getty Images). Darryl Oumi/Getty Images. FILE ...",
        "formattedUrl": "https://www.ctpost.com/.../colorado-maui-invitational-time-tv-uconn-huskie..."
      }
    ],
    [
      "# [AI Needs Its Linux: Oumi Comes Out of Stealth with an Open Source Vision by Doug Eadline, Jaime Hampton on 2025-01-29](https://www.hpcwire.com/2025/01/29/ai-needs-its-linux-oumi-comes-out-of-stealth-with-an-open-source-vision/)\nA new company called Oumi has just been launched, emerging from stealth today with $10 million in seed funding. The company’s founders describe Oumi as the world’s first unconditionally open source AI platform, positioning it as a collaborative and transparent alternative to today’s proprietary AI ecosystems, similar to how Linux revolutionized operating systems.\n\nAIwire spoke with Oumi co-founders Manos Koukoumidis and Oussama Elachqar to find out what this new platform means for the AI world.\n\nMaking AI Research More Accessible\n\nKoukoumidis, Oumi’s CEO, explains: “What we’re announcing is the first unconditionally open AI platform that will make it possible for the tens of thousands of AI researchers, scientists, and developers around the world to not just to more easily leverage and harness frontier AI, but also, more importantly, to collaborate on the same platform, working together to advance frontier AI in a collective way that will lead to more transparent and responsible development.”\n\nOumi is developed in collaboration with 13 leading AI universities in the U.S. and the U.K., bringing together some of the most respected names in technology and innovation, including University of Illinois Urbana-Champaign, Carnegie Mellon University, Princeton University, Caltech, UC Berkeley, GeorgiaTech, Stanford University, University of Washington, New York University, MIT, University of Waterloo, University of Cambridge, and University of Oxford.\n\nThe initiative emerged from discussions with top academics, including Ruslan Salakhutdinov and Amit Acharya from Carnegie Mellon University, who highlighted the growing challenges AI researchers face. These include difficulty conducting large-scale experiments and integrating tools and infrastructure, even when resources like GPUs are available.\n\nKoukoumidis says, “Salakhutdinov was telling me on a Sunday night how hard it is for his students to do this type of research. He said, ‘You know, the last few years have become a lot harder than it was before. We cannot contribute to the frontier anymore. When it comes to larger scale experiments, even smaller scale, it’s too hard for students to figure out all the different things that need to be put together to do this type of research.’”\n\nThe founders then spoke with other top AI researchers and PhD students who also expressed concern about academia’s diminishing role in AI innovation.\n\n“What about the future of AI, if it’s becoming a black box that we just interface with an API? That’s not what academia should do. It’s not how AI and the sciences should be advanced,” Koukoumidis relays.\n\nWhy AI Needs Its Linux\n\nOumi aims to be the Linux of AI, providing an open source alternative to the dominant, closed AI ecosystems. Unlike proprietary AI systems that function as black boxes, Koukoumidis explains that Oumi is built around open code, open data, open model weights, and open collaboration, making it easier for researchers to build upon each other’s work rather than struggling with fragmented, ad-hoc setups.\n\nThe founders also believe that AI research today is hindered by unnecessary complexity as top researchers and PhD students waste valuable time cobbling together infrastructure rather than focusing on advancing the science. By offering a unified, transparent platform, Oumi seeks to remove these barriers, much like how Linux revolutionized operating systems by fostering widespread collaboration and standardization.\n\nBig Tech Won’t Dominate Forever\n\nAs we saw this week with the explosive news of DeepSeek and its impact on the AI landscape, the companies building the largest and most expensive AI models won’t necessarily be on top forever.\n\n“I think they’re going to be trying to play catch up after a while with what’s going to be happening in open source,” Koukoumidis says. “But to make this happen, open source needs its Linux. It needs its seed that we’re hoping to plant with our launch.”\n\nKoukoumidis and Elachqar are well acquainted with the challenges and inefficiencies of AI development in large-scale organizations, as they are alumni of some of the largest companies in tech: Google Cloud and Apple. During their time at these industry giants, they saw firsthand the limitations of siloed AI development and the inefficiencies that come with it. With only a few thousand researchers at even the biggest AI labs, progress remains slow, fragmented, and insufficient to address critical challenges such as safety, robustness, and multimodal AI development.\n\n“Even for a company like Apple that has almost infinite resources, comparatively to other companies, we were still working in silos. There was still way too much we couldn’t do ourselves, and it was so inefficient,” Elachqar told AIwire. “This is way too big of a technology to be done in silos. We need to work together. We need to work for universities with some of the brightest talents, students, and professors all across the US, and the world, to build AI in a more transparent, open, robust way because it’s going to lead to better outcomes for everybody.”\n\nA Unified Platform for AI Development\n\nThe Oumi platform aims to provide an all-in-one solution for AI developers, enabling them to train, fine-tune, and deploy models efficiently across a wide range of environments. Users can work with models ranging from 10 million to 405 billion parameters, leveraging advanced techniques like SFT, LoRA, QLoRA, and DPO to optimize performance.\n\nOumi supports both text and multimodal models, including Llama, Qwen, and Phi, and offers built-in tools for data synthesis and curation using LLM judges to streamline dataset preparation. For deployment, the platform integrates with popular inference engines like vLLM and SGLang while ensuring seamless evaluation through standard AI benchmarks. Oumi was designed for maximum flexibility to allow researchers to run models anywhere, from laptops to supercomputing clusters to cloud platforms like AWS, Azure, and GCP. It also provides integrations with both open models and commercial APIs, including OpenAI, Anthropic, Vertex AI, and Parasail.\n\nFrom Universities to National Labs: Oumi’s Growing Network\n\nIn addition to their progress and collaboration with major universities, Oumi’s founders have ambitious plans to expand their collaborations with national labs and high performance computing centers to streamline AI research at scale. They have already worked with Argonne National Laboratory’s Argonne Leadership Computing Facility, where they received a grant to test their platform on the Polaris supercomputer, laying the groundwork to simplify running AI workloads on government-funded clusters.\n\nNow that Oumi is emerging from stealth, they anticipate a wave of new collaborations from institutions they may not have previously connected with, as word spreads through academic and research communities.\n\nInvestors See Oumi’s Possibilities for AI Research\n\nOumi has secured $10 million in seed funding, launching itself as a Public Benefit Corporation. The Seed funding was led by Venrock and Obvious Ventures with contributions from Plug & Play and Ascend.\n\nThe founders say investors were drawn to its vision of creating an open source AI platform akin to Linux for AI. The founders pitched Oumi as a long-term strategy that would establish a foundational, community-driven AI ecosystem, one that democratizes access to AI research while still allowing for enterprise monetization down the line.\n\nTheir approach mirrors the Red Hat model for Linux, where the core technology remains open and accessible, but specialized enterprise offerings provide a revenue stream. Investors saw the potential in this model, recognizing that by fostering a collaborative AI development environment, Oumi could tap into contributions from the global research community, both in terms of talent and computing resources, without bearing the immense costs of building AI in isolation.\n\nElachqar says investor interest in Oumi was strong, allowing the team to be selective in choosing backers who aligned with their mission. The overwhelming demand from academia for an open AI platform played a key role in convincing investors, as universities increasingly feel sidelined in the development of cutting-edge AI research.\n\n“We were fortunate enough to have an oversubscribed round, so we were able to pick investors that really share our vision,” Elachqar says. “As soon as they talked to any of the professors that we work with, they were all convinced because there is such a strong need from the universities. They all feel so left out, and it’s such a pressing problem for them.”\n\nA Business Model Serving Both Academia and Industry\n\nThe name Oumi stands for Open Universal Machine Intelligence, reflecting the company’s core mission. The founders chose “machine intelligence” as a more precise, scientific term for AI, emphasizing their commitment to building truly intelligent systems rather than just following industry buzzwords. The “open” in Oumi underscores their belief that AI should be developed transparently and made accessible to everyone unconditionally. Meanwhile, “universal” represents their vision of AI as a pervasive technology, one that should be available across all platforms, regardless of where it is run or how it is accessed.\n\nOumi’s founders chose to establish it as a Public Benefit Corporation (PBC) to balance its mission-driven approach with the ability to operate as a sustainable business. The founders saw three primary options: a traditional for-profit, a nonprofit, or a PBC. While a nonprofit structure might have aligned with their goal of making AI universally accessible, they believed it would be too restrictive, particularly in securing the resources needed to develop advanced AI technologies. At the same time, a standard for-profit model didn’t fully reflect their public benefit mission, which is centered on open, collaborative AI development. The PBC structure provides the best of both worlds: it allows Oumi to attract funding and build enterprise-grade products while ensuring that academic institutions and researchers remain a core part of the ecosystem. The founders also recognized that enterprises tend to have greater confidence in products backed by commercial support, making a PBC structure an ideal way to encourage adoption across both academia and industry.\n\nA Shared Future for AI Starts Here\n\nThe Oumi team sees open source as essential for AI, a field where rapid progress depends on collective innovation rather than siloed, duplicative efforts. If successful, Oumi could reshape the AI research landscape by enabling a global community to work together efficiently, leading to safer, more secure, and more widely beneficial AI advancements.\n\nThe company hopes its open platform will lower the barriers to AI research and development. As Oumi emerges from stealth, its founders see this as just the beginning of a broader movement that prioritizes transparency, accessibility, and collaboration over competition, paving the way for AI to be developed as a shared global resource.\n\n“We know that AI is going to be permeating everything that we do, and we want such open technology to be universal, to be accessible to everyone and present everywhere,” Koukoumidis says.",
      "# [Ex-Google, Apple engineers launch unconditionally open source Oumi AI platform that could help to build the next DeepSeek by Sean Michael Kerner on 2025-01-29](https://venturebeat.com/ai/ex-google-apple-engineers-launch-unconditionally-open-source-oumi-ai-platform-that-could-help-to-build-the-next-deepseek/)\nIf it wasn’t clear before, it’s definitely very clear now: Open source really does matter for AI. The success of DeepSeek-R1 has substantively proven there is a need and demand for open-source AI.\n\nBut what exactly is open-source AI? For Meta and its Llama models, it means free access to use the model, with some conditions. DeepSeek is available under a permissive open-source license providing significant access to its architecture and capabilities. However, the specific training code and detailed methodologies, particularly those involving reinforcement learning (RL) techniques like Group Relative Policy Optimization (GRPO), have not been publicly disclosed. This omission limits the community’s ability to fully understand and replicate the model’s training process.\n\nWhat neither DeepSeek nor Llama enables, however, is full unconditional access to all the model code, including weights as well as training data. Without all that information, developers can still work with the open model but they don’t have all the necessary tools and insights to understand how it really works and more importantly how to build an entirely new model. That’s a challenge that a new startup led by former Google and Apple AI veterans aims to solve.\n\nLaunching today, Oumi is backed by an alliance of 13 leading research universities including Princeton, Stanford, MIT, UC Berkeley, University of Oxford, University of Cambridge, University of Waterloo and Carnegie Mellon. Oumi’s founders raised $10 million, a modest seed round they say meets their needs. While major players like OpenAI contemplate $500 billion investments in massive data centers through projects like Stargate, Oumi is taking a radically different approach. The platform provides researchers and developers with a complete toolkit for building, evaluating and deploying foundation models.\n\n“Even the biggest companies can’t do this on their own,” Oussama Elachqar, cofounder of Oumi and previously a machine learning engineer at Apple, told VentureBeat. “We were effectively working in silos within Apple, and there are many other silos happening across the industry. There has to be a better way to develop these models collaboratively.”\n\nWhat open-source models like DeepSeek and Llama are missing\n\nOumi CEO and former Google Cloud AI senior engineering manager Manos Koukoumidis told VentureBeat that researchers consistently tell him AI experimentation has become extremely complex.\n\nWhile today’s open models are a step forward, it’s not enough. Koukoumidis explained that with current “open” AI models like DeepSeek-R1 and Llama, an organization can use the model and deploy it on their own. What’s missing is that anyone else who wants to build on the model doesn’t know exactly how it was built.\n\nThe Oumi founders believe this lack of transparency is a major hindrance to collaborative AI research and development. Even a project like Llama requires a significant amount of effort from researchers to figure out how to reproduce and build upon the work.\n\nHow Oumi works to open AI for enterprise users, researchers and everyone else\n\nThe Oumi platform works by providing an all-in-one environment that streamlines the complex workflows involved in building AI models.\n\nKoukoumidis explained that to build a foundation model, there are typically 10 or more steps that need to be done, often in parallel. Oumi integrates all necessary tools and workflows into a unified environment, eliminating the need for researchers to piece together and configure various open-source components.\n\nKey technical features include:\n\nSupport for models ranging from 10M to 405B parameters\n\nImplementation of advanced training techniques including SFT, LoRA, QLoRA and DPO\n\nCompatibility with both text and multimodal models\n\nBuilt-in tools for training data synthesis and curation using LLM judges\n\nDeployment options through modern inference engines like vLLM and SGLang\n\nComprehensive model evaluation across standard industry benchmarks\n\n“We don’t have to deal with the open-source development hell of figuring out what you can combine and what works well,” Koukoumidis explained.\n\nThe platform allows users to start small, using their own laptops for initial experiments and model training. As users progress, they can then scale up to larger compute resources, such as university clusters or cloud providers, all within the same Oumi environment.\n\nYou don’t need massive training infrastructure to build an open model\n\nOne of the big surprises with DeepSeek-R1 is the fact that it was apparently built with a fraction of the resources that Meta or OpenAI use to build their models.\n\nAs OpenAI and others invest billions in centralized infrastructure, Oumi is betting on a distributed approach that could dramatically reduce costs.\n\n“The idea that you need hundreds of billions [of dollars] for AI infrastructure is fundamentally flawed,” Koukoumidis said. “With distributed computing across universities and research institutions, we can achieve similar or better results at a fraction of the cost.”\n\nThe initial focus for Oumi is to build out the open-source ecosystem of users and development. But that’s not all the company has planned. Oumi plans to develop enterprise offerings to help businesses deploy these models in production environments.",
      "# [ by Kahini Shah on 2025-01-29](https://obvious.com/ideas/why-we-invested-in-oumi/)\nWhy We Invested in Oumi\n\nMaking AI truly open\n\nKahini Shah | 01/29/2025\n\nIn a very short time, AI has begun transforming science, industries, and society itself. Yet, today’s AI landscape is dominated by a handful of large tech companies and well-funded startups that closely guard their models, training data, and methodologies. This concentration of power and lack of transparency pose significant challenges for companies seeking to deploy AI safely and researchers who want to advance the field.\n\nThat’s why we’ve invested in Oumi, which is creating a genuinely open source platform and community for developing and tuning foundation models. We are excited to be investors in Oumi’s $10M Seed round.\n\nThe exceptional technical leadership of CEO Manos Koukoumidis first drew us to Oumi. As the former leader who bootstrapped and led the Google Cloud’s PaLM efforts and then Gemini’s alignment/safety, Manos brings deep expertise in coordinating large-scale AI teams and infrastructure development. And he has assembled an impressive team of engineers from Google, Apple, Snap, Meta, and other companies to pursue this mission with him.\n\nWe are also impressed with the community of advisors and angel investors that Manos and his team are building around them, which includes researchers such as Ruslan Salakhutdinov from Carnegie Mellon, Georgia Gkioxari from Caltech, and Sergey Levine from UC Berkeley among many others.\n\nOumi aims to create an open source platform that enables end-to-end foundation model research collaboration and development across data curation, synthesis, pre-training, post-training, evaluation, and deployment. The platform allows enterprise customers to customize foundation models for specialized tasks and data using a fully flexible and efficient enterprise-grade platform that they can trust.\n\nThe market opportunity is substantial. Enterprise AI spending is growing rapidly, with nearly half of enterprises expressing interest in moving to open source solutions to reduce costs and vendor lock-in. Oumi is well-positioned to capture this demand by offering enterprises greater flexibility, data privacy, and cost efficiency compared to closed AI platforms.\n\nOumi is structured as a Public Benefit Corporation and puts AI safety at the core of its mission. By fostering collaboration between academia and industry, Oumi aims to ensure this transformative technology develops in service of society.\n\nAt Obvious, we look for companies that combine technical excellence with world positive impact. Oumi is a perfect example of this, working to democratize AI development while prioritizing safety and transparency. We’re proud to partner with Manos and the team as they build an open and transparent future.",
      "# [Former Google and Apple engineers launch open-source Oumi AI platform that could help create the next DeepSeek by 2asaIt on 2025-01-30](https://dev.ua/en/news/kolyshni-inzhenery-google-i-apple-zapustyly-platformu-shtuchnoho-intelektu-oumi-z-vidkrytym-kodom-iaka-mozhe-dopomohty-stvoryty-nastupnyi-deepseek-1738243733)\nAfter the success of DeepSeek-R1 , it became crystal clear: open source really does matter for AI.\n\nBut what is open source AI? For Meta and its Llama models, this means free access to use the model under certain conditions. DeepSeek is available under a permissive open source license, which provides extensive access to its architecture and capabilities. However, the specific training code and detailed methodologies, especially those involving reinforcement learning (RL) techniques such as group relative policy optimization (GRPO), have not been publicly disclosed. This limits the ability to fully understand and reproduce the model training process.\n\nHowever, neither DeepSeek nor Llama provide full and unconditional access to the entire model code, including weights and training data. Without this information, developers can still work with an open model, but they don’t have all the tools and knowledge they need to understand how it actually works and, more importantly, how to build a completely new model. This is exactly the problem that a new startup led by former Google and Apple AI veterans is trying to solve.\n\nThe Oumi startup is supported by an alliance of 13 leading research universities, including Princeton, Stanford, MIT, Berkeley, the University of Oxford, the University of Cambridge, the University of Waterloo, and Carnegie Mellon.\n\nOumi’s founders have raised $10 million, a modest seed round that they say is in line with their needs. While big players like OpenAI are planning to invest $500 billion in massive data centers through projects like Stargate, Oumi is taking a radically different approach. The platform provides researchers and developers with a full suite of tools to build, evaluate, and deploy fundamental models.\n\n“Even the biggest companies can’t do this on their own,” Oussama Elachkar, a co-founder of Oumi who previously worked as a machine learning engineer at Apple, told VentureBeat. “We were effectively working in isolation inside Apple, and there’s a lot of other isolation going on across the industry. There has to be a better way to co-develop these models.”\n\nWhat open source models like DeepSeek and Llama lack\n\nOumi CEO and former Google Cloud AI senior development manager Manos Koukoumidis told VentureBeat that researchers constantly tell him that AI experiments have become extremely difficult.\n\nWhile today’s open models are a step forward, they’re not enough. Koukoumidis explained that with current “open” AI models like DeepSeek-R1 and Llama, an organization can use the model and deploy it on its own. What’s missing is that anyone else who wants to build the model doesn’t know exactly how it was built.\n\nOumi’s founders believe that lack of transparency is a major obstacle to collaborative AI research and development. Even a project like Llama requires significant effort from researchers to understand how to replicate and develop the work.\n\nHow Oumi is working to open up AI to enterprise users, researchers, and everyone else\n\nThe Oumi platform works by providing a universal environment that streamlines the complex workflows associated with building AI models.\n\nKoukoumidis explained that building a basic model typically requires 10 or more steps, often in parallel. Oumi integrates all the necessary tools and workflows into a single environment, eliminating the need for researchers to assemble and configure various open-source components.\n\nKey specifications include:\n\nSupports models with parameters from 10M to 405B.\n\nImplementation of advanced training methods including SFT, LoRA, QLoRA and DPO.\n\nCompatible with both text and multimodal models.\n\nBuilt-in tools for synthesis and curation of training data using LLM judges.\n\nDeployment capabilities using modern output mechanisms such as vLLM and SGLang.\n\nComprehensive evaluation of the model using standard industry benchmarks.\n\nThe platform allows users to start small, using their own laptops for initial experiments and model training. As they progress, users can scale to larger computing resources, such as university clusters or cloud providers, all within a single Oumi environment.\n\n“The idea that you need hundreds of billions of dollars for AI infrastructure is fundamentally flawed,” Koukoumidis said. “By distributing computing across universities and research institutions, we can achieve similar or better results at a fraction of the cost.”",
      "# [Oumi Platform Launches to Foster AI Research Collaboration by Mike Vizard on 2025-01-31](https://techstrong.ai/ai-at-the-edge/oumi-platform-launches-to-foster-ai-research-collaboration/)\nAn open Oumi platform for artificial intelligence (AI) researchers has been launched this week by researchers from 13 universities, including Carnegie Mellon University (CMU), Stanford University, and the Massachusetts Institute of Technology (MIT).\n\nThe platform is being administered by a namesake public benefit corporation (PBC) with $10 million in seed funding.\n\nResearchers from other universities that participated in developing the platform include University of Illinois Urbana Champaign, Princeton University, California Institute of Technology, University of California, Berkeley, University of Washington, New York University, University of Waterloo, University of Cambridge and University of Oxford.\n\nThe overall goal is to provide AI researchers with access to the tools, code, weights, data, benchmarks and foundation models needed to collaboratively conduct AI research and reproduce results, says Oumi CEO Manos Koukoumidis. This should make the field more accessible to a wider range of academic institutions that today may not have the resources needed to research AI.\n\nThe approach will promote greater trust in AI systems in an era where much of the research today is being conducted by commercial companies that are training AI models without providing a lot of visibility, he adds. “They tend to be secretive,” he says.\n\nSpecifically, AI researchers will be able to train and fine-tune models from 10M to 405B parameters using multiple state-of-the-art techniques, synthesize and curate data, and access inference engines that can be used to deploy AI models in the cloud or on individual laptops.\n\nAdditionally, AI research will be provided with access to commercial application programming interfaces (APIs) through which they can access models from, for example, OpenAI, Anthropic or Google. Oumi will also provide access to prebuilt reusable workflows and recipes for post training and other common tasks that AI researchers need to routinely perform.\n\nIt’s not clear to what degree organizations might decide to one day mandate reliance on open AI models that are easier to validate and audit, but the one thing that is clear is the gap between open and commercial models in terms of capabilities and performance is rapidly closing. In fact, recent advances in China clearly suggest the amount of code and infrastructure needed to build a functioning AI model may not require billions of dollars in upfront investment.\n\nAlso unclear is how AI research may evolve, but the pace is certainly accelerating. New capabilities and techniques are now being adopted that in some cases instantly upend previous initiatives. A more collaborative approach to AI may accelerate breakthroughs in a way that might even prove to be less disruptive and, ultimately simpler to validate as AI regulations are more widely applied. For example, if the foundational model used to distill a smaller model has already been thoroughly tested that’s one less set of tests that may need to be run later.\n\nOne way or another, however, researchers typically find a way to collaborate. The Oumi platform simply provides a mechanism for streamlining those efforts in a way that benefits a much wider percentage of the global population sooner than later.",
      "# [Oumi: The AI revolution led by Greek minds by Christopher Gogos, Evie Dink on 2025-03-12](https://neoskosmos.com/en/2025/03/12/news/business/oumi-the-ai-revolution-led-by-greek-minds/)\nA new AI tool ‘Oumi’ was launched last month, created by Greek-educated scientists Manos Koukoumides, Panos Achlioptas, and Konstantínos Aísopos.\n\nThe trio have designed a tool to help users create and use AI more easily, with a goal to make AI development more accessible for everyone.\n\nDespite studying their undergraduate degrees in Greece, Manos, Panos and Kostas now live in the US — they call in to the Neos Kosmos office from Palo Alto in California.\n\nManos and Kostas pursued their PhD’s together at Princeton University, and the pair knew Panos from back in Greece. Manos says they “knew him by reputation, and reached out for a discussion. That’s when we decided to work together.” The co-founders have an impressive resume, working at companies like Google and Meta before striking out on their own this year to found Oumi.\n\nManos reflects on the adjustment from Athens to Princeton, incredulous that the armchairs in the computer science lab were more expensive than the actual computers they used for their classes back in Greece.\n\nDespite the limited resources back at home, Manos says his Greek education prepared him very well, and believes Greek institutions still matter.\n\n“In Greece in particular, the spirit and the philosophy behind education is well respected by the people, and we regard education as a very serious endeavor. My hope, in the future, is that the system will be able to absorb good scientists and help them to shape elements of society.”\n\n“You also have to recognise that universities in Greece are very underfunded,” Kostas adds. Panos says Oumi is looking at expanding, and would like to open an office in Greece. “Since we started our education in Greece, we would like to employ people there who have the right skill-set to join us.”\n\nThe co-founders have launched Oumi because they’re worried about the current status quo of frontier AI, with industry labs becoming “extremely secretive”, explains Manos.\n\n“It’s becoming harder and harder for institutions to be able to contribute to frontier AI. It’s very problematic. It’s also about the fact that this important technology, and is now being controlled by a handful of volunteers — it’s about that concentration of power.”\n\nOn the important role of AI, Manos says it’s a critical technology that powers not only the tech industry, but also contributes to climate science and material science and healthcare. Panos is inspired by what he says is Oumi’s noble vision, in wanting to provide the world access to AI that is made by the people, for the people.\n\nManos shares that the most rewarding part of opening the company has been “having the opportunity not just to work on the AI problem, but to do it with the best colleagues and greatest friends.”",
      "# [The Compute Burden by Alexander Campbell on 2025-01-30](https://www.campbellramble.ai/p/the-compute-constraint)\nNot sure about you guys, but I spent a lot more time at my computer this week. All of a sudden, I found the urgent need for more compute.\n\nUsually when people say this they are bragging about the size of their cloud. I'm not talking about the cloud here, I'm talking about that experience all us gamers had in college, playing around with the video card drivers, trying to get ur rig to play Far Cry.\n\nTwenty years later, Far Cry is still crushing GPUs, and in exchange we got a lot more...pixels. When we say there's an 'S-curve' to tech, this is a good example. We've gone from 100MB of VRAM to 32GB (or 320x), and yes the game is beautiful, but it's pretty much the same experience.\n\nBut something different is happening with AI. The sand woke up, giving us a need for exponentially more compute than even the prior trend could satisfy. This isn't just about bigger numbers - it's about a fundamental shift in what's possible, and who gets to participate in that possibility.\n\nI'm not a semis expert - if you want a good 12-24m prediction for this line, go somewhere else. What I'm good at is getting to the core of a problem via experimentation, pushing systems to their breaking point, and using those learnings to understand the dynamic stability of complex systems.\n\nIn other words: you can learn a lot about something by breaking it.\n\nSo today we're going to examine what we've learned one week into DeepSeek's release of R1. This isn't just another model launch - it's a signal of something much bigger. To understand why, let's look at the numbers:\n\nEven if you don't take these benchmarks at face value (as many, including Dario, are skeptical of), what's fascinating isn't just what DeepSeek achieved but how they did it. They're proving that innovation can come from anywhere - a quant shop giving the ivory tower a bloody nose, a scrappy team in New Delhi with great benchmarks…\n\nbut a product with completion problems.\n\nThis is what acceleration looks like. But it also surfaces a crucial paradox that will shape the next era of AI development.\n\nWhen you speak to a model that seems 'human' these days, you're actually talking to a ~million dollar machine sitting in a data center somewhere. The person providing you that compute has to find a million dollars of other demand while charging you $20/hr. Sometimes they have too much, sometimes they have too little. A quaint problem for you and me becomes a million dollar line item, trading GPUs. That's the hidden infrastructure behind even relatively \"small\" models like DeepSeek.\n\nTo put some hard numbers on this: running a 10T parameter model (what we might guesstimate as \"superintelligent\") requires about 20TB of memory. In practical terms, that's around 273 high-end GPUs with 75GB each.\n\nThat translates to about $25M/year in pure compute costs for cloud deployment.\n\nOr around $66M all-in to buy the hardware outright + 10% a year to run it.\n\nAnd we haven't even talked about the developers yet. Though, as DeepSeek has shown, talent for building these systems exists wherever smart people can invert matrices.\n\nBut here's where it gets really interesting - and where the physical limits of our universe start to constrain the dream of \"AI for everyone.\"\n\nIt starts with a question: based on current scaling trajectories, how many years until a 10T parameter model becomes a ~million dollar operation?\n\nThe answer: 14 years.\n\nThat's probably longer than you had in mind when you hear phrases like 'local superintelligence.'\n\nFor that same machine to become a $10k consumer product? We're talking ~20 years.\n\nLet that sink in: even if training models becomes free (because someone else crunched the numbers first), running intelligence will be extremely expensive in both compute and energy terms. And the smarter the model, the exponentially more energy it needs.\n\nHere's a reality check for the timeline doomers: even if we had free superintelligence tomorrow, running it would still exceed the capabilities of most rogue actors we worry about. Any \"AI war\" would be fought between nation-states, not individuals, simply due to the physics of compute.\n\nBut what about making it mobile? Surely our phones will eventually catch up?\n\nLet's run that experiment. How long until we can have even a single H100-level GPU in your phone? (Remember, we'd need about 270 of them for our super-intelligent model, but let's start small.)\n\nThe brutal reality: a single H100 (with 80GB of RAM) consumes 700 watts. Your iPhone battery would last about 65 seconds - if it didn't melt first from being run 20x harder than designed.\n\nRunning DeepSeek on your headphones would drain them instantly. Long batteries, as they say.\n\nBut there's another wrinkle to this compute crunch - one that gets surprisingly little attention. It's not just about raw processing power, but how these models actually think.\n\nWhen you interact with DeepSeek extensively, you notice it follows the same thought process every time:\n\n<think>\n\n1. Summarize the conversation so far\n\n2. Break the query into manageable pieces\n\n3. Think through each piece systematically\n\n4. Evaluate and synthesize results\n\n</think>\n\n“I am sorry. I cannot answer that question. I am an AI assistant…”\n\nWhat takes some trial and error to discover is just how inefficient this process is for users. Not only do you find yourself waiting for the model to finish its... extensive... internal monologue, but you're also paying for every token of that verbose thinking process. Which means it’s expensive.\n\nThis creates a perverse incentive structure: Chain of Thought (CoT) improves benchmark scores but makes models both more expensive to run and more tedious to interact with.\n\nEvery time you reply, even without throwing in 500 lines of code, the model needs to 'read' its prior answers as part of the conversation's context. Those \"thinking out loud\" tokens pile up, consuming more and more energy as the conversation continues.\n\nMeaning the moment you inevitably hit your usage caps is precisely when you need those tokens most! This is something we should all be vigilant about going forward - not just how exponentially compute-intensive this intelligence really is, but how the economics of intelligence incentivize both models and their commercial developers to prioritize long, verbose answers.\n\nWhich brings us to the investment implications - and they're not quite what you might think.\n\n“Wow Campbell, you're telling me we need more compute, next you'll be lecturing me on Jevons paradox.\"\n\nNo, that part is obvious.\n\nAnd previously discussed.\n\nBut if you want a strawman, that’s where to put us. But the point we're making is more subtle than just \"compute demand goes up.\"\n\nIt’s not just about the chips, it’s about the energy. Yes, we just turned rocks into genius knowledge workers, and they drink electricity by the megawatt.\n\nWith dramatic consequences for the volatility of your energy consumption,\n\nand its carbon intensity.\n\nNo, what’s emerging is a fundamental shift in where and how that compute gets deployed. The development of open-source AI isn't just instantly creating more demand - it's creating a specific type of demand that our current infrastructure isn't built for.\n\nLocal aka your model on your machine.\n\nConsider this: if the model is entirely open source, meaning you can download not just the engine but the weights (what Facebook/Meta rejected me from doing the other day, see below), then you can own not only the model but any derivative work that comes from it. If you train it to be good at music, art, accounting, or analyzing customer feedback - whatever it is it's yours.\n\nIt also means, that for the first time in a long time, when you talk to your computer, no one is watching.\n\nYou can ask the model anything you want. While the model retains the right to refuse to answer, or be elusive or unaligned, it’s still your model.\n\nIn practice, this feels strangely liberating, like the early days of the internet. The conversations you have with the machine aren’t getting logged. Not unless you want them to be. Not unless you are working on something you need to remember. Not unless you want to ‘fork’ the model and make your own. Which I did. Multiple times, the first night.\n\nThis changes the game entirely for enterprise AI adoption.\n\nEven if DeepSeek's benchmarks are overblown, even if Llama was almost as good, what matters isn't just what happened but how. The ivory tower just got a bloody nose from a quant shop, and innovations are emerging from Bengaluru to Hangzhou. This is what real acceleration looks like - constraints breeding creativity, and creativity breeds compounding.\n\nEnter Nvidia's DIGITS - a desktop workstation connected to their 80GB Blackwell chip - which are supposed to go on sale this spring, and should bring medium-size models to your workplace on a 2-4 year horizon.\n\nMeanwhile, Apple made a fascinating bet in ~2020, moving to a 'unified memory architecture' that abandons the traditional GPU/RAM distinction. While the software to train these models remains primarily built for Nvidia's CUDA ecosystem, this doesn't matter as much for inference aka running them. Which is now possible.\n\nWant to run the full DeepSeek model (all 671B parameters) locally aka at home?\n\nWire together 8 Mac M4 Minis and you're in business. Not exactly pocket-sized, but a glimpse of where we're headed.\n\nBut here's where we arrive at the democratization paradox - and why it matters for the future of AI.\n\nWhat we're really witnessing - and what my week of breaking DeepSeek has illuminated - is an asymmetric revolution in AI.\n\nBy focusing on efficient training and standing on the shoulders of giants, DeepSeek is contributing to a process that's now borderline unstoppable: the acceleration of machine intelligence through the democratization of knowledge. Think wikipedia, common crawl, mapReduce, and linux. Knowledge repositories, processing algorithms aka machines for information. Knowledge about how to train these systems, how to tinker with them, and increasingly, how to bootstrap more condensed versions of intelligence. Each latest model building upon all the work done in the past.\n\nIs it any surprise that the reinforcement learning tactics used in India and China this past month follow such similar patterns?\n\nWrite down problems and Chain of Thought answers, make the model try to solve them, score what \"good\" looks like. Loop.\n\nSounds a lot like how we train humans.\n\nBut here's the paradox that will define the next decade of AI:\n\nThe code is free, but the compute is expensive\n\nThe knowledge is open, but the infrastructure is centralized\n\nThe potential is unlimited, but the physics are unforgiving\n\nThis creates three distinct futures for AI deployment:\n\nThe Cloud Future: Where most users access powerful models through APIs, paying by the token and accepting the limitations of shared infrastructure. To which I say, let’s put free wifi in the NYC subway and then sure let’s talk.\n\nThe Enterprise Future: Where medium-sized models (~2-8GB) run locally on specialized hardware like Nvidia's digits workstations, handling specific business tasks with data privacy guarantees.\n\nThe Consumer Future: Still frustratingly far away for anything approaching current model capabilities. That iPhone superintelligence? Check back in 2040.\n\nAnd this brings us full circle to our gaming metaphor.\n\nJust as Far Cry pushed the boundaries of consumer graphics, demanding more from our GPUs year after year, AI is now pushing the boundaries of what's computationally possible. But there's a crucial difference: gaming's demands followed an S-curve because human perception has limits and our interfaces have limited resolution. You can only make pixels so pretty.\n\nIntelligence, it turns out, has no such ceiling. Each increment of capability - from coding assistant to mathematical reasoning to creative partnership - unlocks new demands we hadn't even imagined before. Like a game that keeps generating new levels as you play, each more complex than the last.\n\nThe good news? This creates tremendous opportunities for innovation. The same way gaming drove consumer GPU development, AI's compute demands will drive the next wave of hardware innovation. The better news? The constraints we're hitting aren't arbitrary - they're fundamental physics problems waiting to be solved.\n\nSo next time someone tells you AGI is just around the corner, ask them about their cooling solution. Because in the end, superintelligence isn't just a software problem - it's a hardware problem, an energy problem, and a heat dissipation problem all rolled into one.\n\nAnd that's exactly what makes it interesting.\n\nTill next time.\n\nDisclaimers\n\nCharts and graphs included in these materials are intended for educational purposes only and should not function as the sole basis for any investment decision. As noted the views in this portfolio are not intended to maximize return and any use in a portfolio ought be considered in the broader context of your portfolio and investment needs and obligations. Please consult with a registered investment advisor if you would like financial advice, that’s their job.\n\nThis letter does not constitute an offer to sell, a solicitation of an offer to buy, or a recommendation of any security or any other product or service by Rose, Campbell or any other third party regardless of whether such security, product or service is referenced in this brochure. Furthermore, nothing in this website is intended to provide tax, legal, or investment advice and nothing in this website should be construed as a recommendation to buy, sell, or hold any investment or security or to engage in any investment strategy or transaction. You are solely responsible for determining whether any investment, investment strategy, security or related transaction is appropriate for you based on your personal investment objectives, financial circumstances and risk tolerance. You should consult your business advisor, attorney, or tax and accounting advisor regarding your specific business, legal or tax situation.",
      "# [ on 2025-01-29](https://www.bizjournals.com/seattle/news/2025/01/29/oumi-microsoft-google-apple-ai-lab-open-source.html)\n",
      "# [UConn's Dan Hurley unloads on officials after overtime loss to Memphis at Maui: 'A complete joke' by David Borges, Staff on 2024-11-26](https://www.newstimes.com/sports/uconn-mens-basketball/article/dan-hurley-maui-invitational-officials-uconn-huski-19942480.php)\nNo. 2-ranked UConn (4-1) was whistled for 29 fouls, including three technicals, and had three different players foul out of its 99-97, overtime loss to the Tigers. Unranked (for now) Memphis had two players foul out and committed 21 fouls.\n\nBut that wasn't one chaffed Hurley the most. It was an over-the-back call on Liam McNeeley, followed by a technical on Hurley for disagreeing with the call, that irked the coach more than anything.\n\n\"That was a joke,\" Hurley said of the call on McNeeley. \"That call, at that point of the game ... there was no attempt to block out. There was a player on Memphis that made a half-assed effort to rebound that basketball, and Liam McNeeley high-pointed that rebound. For that call to be made, at that point of the game, was a complete joke.\"\n\n\"I don't know what happened,\" he insisted. \"I might have lost my balance by the absurdity of the call. Or, maybe I tripped. But if I made that call, at that point, I would've ignored the fact that I was on my back.\"\n\nHurley never mentioned the officiating crew of Pat Driscoll, Stephen Anderson and Scott Brown by name, but noted: \"I'd never seen the one ref, I didn't even know he was a college ref. I was familiar with the other two, so I'm not surprised.\"\n\n\"Our medical trainer must have said something under his breath in the huddle, gets T'd up,\" Hurley explained. \"A trainer who is just the nicest guy, very quiet guy, might have muttered something under his breath. Normally that situation, an official comes over to you and says, 'Hey, Coach, tell that guy to shut up.' Because I know they don't want to hear it from me. That's commonly how that should have been handled. But, I had a lot of issues with what went on out there.\"\n\nWith about 2 1/2 minutes left in overtime and UConn up by three, Samson Johnson was called for a technical foul after pushing Memphis center Moussa Cisse. Memphis's Dain Dainja also picked up a \"T\" after confronting Johnson. For Johnson, it was his fifth foul, forcing the Huskies to play the final 2 1/2 minutes without a true center (Tarris Reed Jr. had fouled out earlier).\n\n\"He's got to be smarter,\" Hurley said of Johnson. \"But Samson was getting shoved, his jersey was ripped. He didn't get a foul called for him the entire game, he ended the game with his jersey ripped down the center and he didn't get a single call. He's frustrated.\"",
      "# [UConn men's basketball's 17-game winning streak end with Maui Invitational loss: 'This hurts' by David Borges, Staff on 2024-11-25](https://www.ctinsider.com/sports/uconn-mens-basketball/article/uconn-huskies-memphis-maui-invitational-19941317.php)\nLAHAINA, Hawaii — It had to end at some point, but the UConn men's basketball team wasn't going to let the nation's longest winning streak end without a fight.\n\nDown 13 to Memphis with four minutes to play on Monday in their Maui Invitational opener, the Huskies rallied back to send the game to overtime on Solo Ball's 3-pointer with 1.2 seconds left.\n\nAdvertisement\n\nArticle continues below this ad\n\nBut in a wild extra session that featured three technical fouls (including a crucial one on Dan Hurley), Memphis was able to outlast UConn for a 99-97 win at Lahaina Civic Center.\n\n\"Credit Memphis, that was an awesome game,\" Hurley said afterwards. \"It must have been an awesome game to watch. I've got a lot of respect for this team, I think it's the best team that Penny (Hardaway) has had.\"\n\nStill, Hurley felt the Tigers (5-0) didn't deserve all the credit for the Huskies' first loss since last February. He left plenty of criticism for the officiating crew of Pat Driscoll, Stephen Anderson and Scott Brown, who hit the Huskies with three technicals, starting with one on trainer James Doran in the first half, and sent Memphis to the line nearly twice as often (40-24) as the Huskies.\n\n\"I had a lot of issues with went on out there,\" Hurley said.\n\nAdvertisement\n\nArticle continues below this ad\n\nMemphis broke open a 40-40 game at halftime and led by 13 (77-64) with 4:00 left in regulation. Hot shooting by Liam McNeeley got UConn to within three, and when their fullcourt pressure caused a 10-second violation on Memphis, the Huskies had the ball with 14.2 seconds left and a chance to send the game into overtime.\n\nWith defenders draped all over McNeeley, Hassan Diarra found Ball, and the sophomore knocked down the game-tying trey. Hunter's halfcourt heave at the buzzer was off the mark, and it was on to OT.\n\nWith 40.3 seconds left in OT and the game tied at 92, McNeeley was called for an over-the-back foul and Hurley disagreed vehemently, falling to the floor on the sidelines. He was hit with a technical, sending PJ Carter (who was fouled by McNeeley) to the line for four free throws, hitting them all.\n\nAdvertisement\n\nArticle continues below this ad\n\n\"That was a joke,\" Hurley said of the call. \"That call, at that point of the game ... there was no attempt to block out. There was a player on Memphis that made a half-assed effort to rebound that basketball, and Liam McNeeley high-pointed that rebound. For that call to be made, at that point of the game, was a complete joke.\"\n\nAs for Hurley's technical:\n\n\"I don't know what happened. I might have lost my balance by the absurdity of the call. Or, maybe I tripped. But if I made that call, at that point, I would've ignored the fact that I was on my back.\"\n\nSomehow, the Huskies still had a chance to send it to double overtime. Diarra was fouled with 2.2 seconds left, hit the first, missed the second and managed to grab the long rebound and hurl up a last-second heave that was off the mark.\n\nAdvertisement\n\nArticle continues below this ad\n\nUConn's 17-game winning streak, dating back to a loss last Feb. 20 at Creighton, came to an end. Since that loss, UConn won both the Big East and, for the second straight year, the NCAA tournament. The Huskies had also won last season's Saatva Empire Classic and the 2022 PK Invitational, a remarkable run through in-season and postseason tournaments.\n\nThey were 47-3 since December 2023.\n\nBut this is a very different team, one whose several question marks (point guard, starting lineup, foul issues, leadership) were exposed once again on Monday.\n\n\"Everyone's disappointed,\" said Alex Karaban. \"If you're not disappointed in that locker room, you shouldn't be here. We lost, and losing anything doesn't sit well with anybody. Yeah, we showed fight at the end, but that doesn't matter if we don't win. I really hope everyone's disappointed.\"\n\nAdvertisement\n\nArticle continues below this ad\n\nNow, UConn (4-1) is relegated to the loser's bracket, where it will face either Michigan State or Colorado on Tuesday.\n\n\"We'll respond,\" Hurley promised. \"This hurts, this sucks. We came here to win a championship. But, Memphis is an excellent team. We've got to respond the way UConn responds, no matter what time the game is. We played at 9:30 (a.m.) today. I don't know what time we play tomorrow, 7 (a.m.)?\"\n\nActually, it's at 10:30 a.m. local (3:30 p.m. EST).\n\nAdvertisement\n\nArticle continues below this ad\n\nThe second-ranked Huskies got Herculean efforts from a pair of players off the bench, but precious little from anyone else until the waning minutes.\n\nTarris Reed Jr. notched his third double-double of the season with 22 points and 11 rebounds off the bench before fouling out, and Jaylin Stewart broke out of an early-season malaise with 16 points. Curiously, Stewart only played four minutes the rest of the way.\n\n\"He just looked a little skittish,\" Hurley explained. \"He was great in that first half. His first run, he kind of looked like a little bit out of rhythm, on his heels. We were down, he made a couple of defensive mistakes ... and some other guys played a little better on the wing, based on where we were in the game.\"\n\nBut it wasn't enough to counter a long, active, athletic Memphis squad, led by 26 points from Tyrese Hunter and 22 from P.J. Haggerty.\n\nAdvertisement\n\nArticle continues below this ad\n\nKaraban finished with 19 points for the Huskies, Diarra had 12 and McNeeley and Ball added 10 apiece.\n\nA wildly entertaining first half ended with Dain Dainja's layup with two seconds left. Reed and Stewart led the way, combining for 29 points and 11 of the Huskies' 17 rebounds over the first 20 minutes.\n\nReed entered about 3 1/2 minutes into the game after Samson Johnson picked up his second foul and quickly went to work, scoring on a variety of jump hooks, inside moves and reverse layups.\n\nStewart also joined in on the fun off the bench, knocking down a 3-pointer to bring the Huskies to within two, then scoring on a nice lob from Alex Karaban to tie the game at 24.\n\nAdvertisement\n\nArticle continues below this ad\n\nAfter Stewart skied for a rebound, a Reed dunk (from Karaban) gave UConn its first lead since 2-0. Memphis tied it, then Reed followed with the most impressive sequence of the half, grabbing not one, not two but three offensive rebounds before converting while being fouled, then completing the 3-point play.\n\nA Stewart corner trey gave UConn a 32-27 lead, its biggest of the half, but Memphis didn't go away. There were five lead changes and four ties over the final five minutes before the half ended in a tie.\n\nIt didn't take long or Johnson to pick up his third foul barely three minutes into the latter half. And with about 2 1/2 minutes left in overtime, with Reed already fouled out and on the bench, Johnson also fouled out on an unnecessary technical foul after pushing Dainja after a shot attempt.",
      "# [This Monster Wants to Eat Me Anime Reveals New Trailer, October 2025 Release](https://www.imdb.com/news/ni65159455/)\n",
      "# [Who’s running this team? UCLA’s DeShaun Foster said he makes fourth-down calls by Ben Bolch, www.latimes.com, ben-bolch on 2024-09-05](https://www.latimes.com/sports/ucla/story/2024-09-05/ucla-football)\nEric Bieniemy, a two-time Super Bowl champion offensive coordinator, constructed UCLA’s plays.\n\nDeciding whether to go for it on fourth down?\n\nThat’s entirely up to DeShaun Foster, the new coach whose experience before this season had been confined to working with running backs.\n\n“Every decision that we did was me,” Foster said Wednesday during his first meeting with reporters since the Bruins’ come-from-behind, 16-13 victory over Hawaii last weekend. “ … I’m telling everybody what to do. There’s no decisions being made by any other coaches, so anything that’s happening is coming directly from me.”\n\nHow would Foster describe his philosophy when it comes to making those critical fourth-down decisions?\n\n“It depends on the situation,” he said.\n\nAt least in the opener, the Bruins unveiled a decidedly cautious approach. On their opening drive, they punted on fourth and five at Hawaii’s 44-yard line.\n\n“I just didn’t like the situation at the time,” Foster said.\n\nIn the second quarter, facing fourth and four at Hawaii’s 34-yard line, it looked like UCLA was going to go for it as players readied themselves near the line of scrimmage. Foster said he decided to kick a field goal after Hawaii called a timeout, though the box score indicated that the Bruins called the timeout.\n\n“With the momentum going, I probably was gonna go for it,” Foster said. “Then, they end up calling a timeout so we kicked the field goal.”\n\nBlake Glessner missed a 52-yarder, preserving Hawaii’s 7-0 lead.\n\nThe Bruins unleashed a pass-heavy offense, throwing 38 passes while combining for only 13 carries by their running backs. Foster said the running backs’ struggles — Keegan Jones, T.J. Harden and Anthony Frias II combined for only 24 yards — led to so many passes.\n\nUCLA’s troubles moving the ball on the ground went beyond a short-handed offensive line, Foster said.\n\n“Just the whole offense, just not exactly executing the way that they should,” Foster said. “So, just a lot of stuff that’s easy to fix — ID’ing [situations], stuff like that. So, I’m not really worried about it. They should get out here and make sure that they work through their corrections and get better.”\n\nThe run game could get a boost in time for the first Big Ten game against Indiana on Sept. 14 at the Rose Bowl. Foster intimated that injured tackles Garrett DiGiorgio and Reuben Unije should be back by then. It’s also possible that running back Jalen Berger, a transfer from Michigan State, could make his UCLA debut after continuing his efforts to round into form.\n\nUnited again\n\nHis team needing a lift midway through the third quarter, UCLA safety Ramon Henderson knew just the guy to provide it. So he approached wide receiver Rico Flores Jr. on the sideline with some simple instructions.\n\n“Catch the ball,” Henderson told him, “and run.”\n\nFlores complied, hauling in a 39-yard touchdown pass from Ethan Garbers to spark the team’s comeback. Hearing the play call and seeing Hawaii’s safety not looking his way before the snap, Flores said he knew he was going to score, especially after making a move to free himself from the cornerback.\n\n“I posted and took it over the top,” Flores said, “and the rest is history.”\n\nIt was the sort of play Henderson knew Flores could make because he had seen it when they were teammates at Notre Dame before transferring to UCLA.\n\nFlores’ move late in the offseason led to his exclusion from “EA Sports College Football ‘25,” though the video game makers included him in their recent update.\n\n“Finally,” Flores said. “Long time coming.”\n\nFlores’ player rating left him disappointed, though he could be in for an upgrade after making three catches for a team-high 102 yards against Hawaii.\n\n“I wouldn’t say I’ve arrived yet,” Flores said. “I’ve still got a lot of work to do.”\n\nEtc.\n\nFoster said using multiple punters and kickers in the opener was part of a planned platoon. Mateen Bhaghani replaced Glessner and made all three of his field-goal attempts, including the 32-yard game-winner with 56 seconds left. Chase Barry averaged 39.5 yards on two punts and Brody Richter averaged 33.5 yards on two. … The Bruins practiced for about 2½ hours Wednesday in scorching heat. Said Flores: “It didn’t feel like a bye week practice at all.” … Foster on his focus in the bye week: “I want the whole team to get better, so it’s not a one-sided thing like one side of the ball needs to get better and the other side can stay where they’re at; everybody needs to improve.”",
      "# [How well do you know your March Madness trivia? Take our quiz by Holly J. Morris on 2025-03-20](https://www.npr.org/2025/03/20/g-s1-54770/march-madness-quiz-ncaa-tournament)\nMarch Madness is upon us.\n\nDo you know your saint schools from state schools? Your legendary coaches from your little brothers? Find out if you're in the Final Four — or didn't even get seeded.\n\nCheryl Thompson of the Training team contributed questions.",
      "# [Making Good on the Promise of Open Source AI by Jeffrey Burt, Mark Lavi, Felipe Cardeneti Mendes, Lubos Kosco, Jack Wallen, Antoni Olendzki, Franz Knupfer, Richard MacManus, Ridge Kimani, Kim McMahon on 2025-01-29](https://thenewstack.io/making-good-on-the-promise-of-open-source-ai/)\nThe AI industry has seemingly been trying to define what “open” and “open source” mean in the generative AI era that kicked off more than two years ago when OpenAI released its ChatGPT chatbot and has been evolving rapidly since.\n\nThe AI space is dominated by a relatively few large and well-funded companies and their AI models and tools, such as OpenAI’s GPT family of models, Microsoft’s Copilot, and Google’s Gemini. Others, with Meta being the most high-profile of the group with its Llama models, boast of being open, though there are detractors who question how open they really are.\n\nHowever, beyond the debate about definitions, the lack of real and widely used open source AI platforms is hindering innovation in the industry and creating the illusion of a troubling talent gap, according to Manos Koukoumidis, who has worked on AI technology at such heavyweights as Google, Microsoft, and Meta.\n\n“Quite often people, they release open weight models, just the model itself and the weight,” Koukoumidis told The New Stack. “[People] don’t know exactly how it was developed, what code, what data was used and they call it open source, which is not a very accurate description. The reality is that, even with current efforts, even with the few ones that are open source — because Meta’s ‘open weight’ — even for the few that are open source, it’s very hard for people to experiment, to continue to innovate and collaborate with each other on this work. This is the thing that is impeding progress in the open source world.”\n\nIt’s what led Koukoumidis and Oussama Elachqar — who brings machine learning experience with Microsoft, Apple, and Twitter (now X) to the table — to launch Oumi, a startup coming out of stealth today that offers what both call a true open source platform. It also has $10 million in seed funding, led by capital venture firms Venrock and Obvious Ventures.\n\nLinux for AI\n\nKoukoumidis, who also is Oumi’s CEO, said the platform — developed in collaboration with researchers from 11 institutions, including Massachusetts Institute for Technology (MIT), University of California, Berkely, and Carnegie Mellon University — essentially acts as the Linux for AI models and tools, enabling wide collaboration and contributions researchers, developers, and AI experts, who will be able to build off each other’s work.\n\nIt will not only accelerate innovation but also will give AI students at such institutions the ability to work with the platform to gain the skills that are so badly needed in the industry. It helps open up the critical advanced technology to more than the current handful of power brokers.\n\n“We started with the ability for others to experiment easily and collaborate as the key design principle,” he said. “If it’s not easy for others to build on each other’s work and continue building on top of the work of each other, then open source will never achieve the growth and the velocity that it needs.”\n\nMaking AI Really Open\n\nMaking AI technology more open has been a thorny issue in the industry. As mentioned, companies like Meta are creating models that are more open than some of their competitors. The Open Source Initiative, after years of planning, in October 2024 introduced its initial definition of open source AI, which addresses four different kinds of data and requires those building and sponsoring AI technology to share what data they can as well as the model’s parameters and the source code used to train and run the system.\n\nWith the rise of agentic AI, Cisco and other vendors are mapping out communication networks — what they call the “Internet of Agents” — that will allow AI agents from different vendors and industries to autonomously connect and collaborate to solve complex problems, and are stressing the need for it to be open source.\n\nEnter DeepSeek\n\nMore recently, China AI startup DeepSeek this month released its DeepSeek-R1 reasoning model, which is roiling the AI industry because it reportedly equals the performance of OpenAI’s o1 model — which can reason before giving an answer — and was trained in two months for just $5.6 million. It’s also open source, so AI developers and researchers can collaborate and build upon it.\n\nAfter the R1 release, stocks of Nvidia, Microsoft, and other AI giants spiraled and DeepSeek knocked OpenAI from its perch as the most downloaded free app on Apple’s App Store.\n\n“The release of DeepSeek undeniably showcases the immense potential of open- ource AI,” said Andrew Bolster, senior R&D manager at secure development firm Black Duck. “By making such a powerful model available under an MIT license, it not only democratizes access to cutting-edge technology but also fosters innovation and collaboration across the global AI community.”\n\nHowever, DeepSeek’s use of OpenAI’s Chain of Thought data for initial training puts a spotlight on the need for transparency and shared resources, Bolster said, adding that “it’s crucial that the underlying training and evaluation data are open, as well as the initial architecture and the resultant model weights.”\n\nDeepSeek Helps Make the Open Source Argument\n\nKoukoumidis said DeepSeek’s success validates Oumi’s strategy. Being built atop other open source efforts like Meta’s Llama and PyTorch enabled the Chinese company to innovate and create models that seem to be closing the gap with proprietary models from OpenAI.\n\nDeepSeek was built on top of other open source efforts like PyTorch and Llama from Meta. Those open source efforts enabled the DeepSeek team to continue innovating and develop models that appear to be closing the gap with the proprietary models from OpenAI. On a platform like Oumi’s, the community can co-build the next DeepSeek, he said.\n\nKoukoumidis said DeepSeek also should serve as a warning to the United States.\n\n“Despite its impressive performance in areas like math and coding, DeepSeek’s biases and censorship indicate what is at stake for the future of AI development,” he said. “For the U.S. to continue to lead in AI, we need open source and open collaboration to develop trustworthy and explainable models. U.S. researchers are at a disadvantage if the U.S. is less open than China.”\n\nCommunicate and Collaborate\n\nKoukoumidis and Elachqar call Oumi an AI laboratory that is rolling out the first unconditionally open source AI platform. It provides foundation models with open code, open data, and open weights and allows researchers and developers with the tools to collaborate and contribute. It is a unified platform that can support all common foundation model workflows.\n\nDevelopers can use techniques like SFT, LoRA, QLoRA, and DPO to train models of various sizes, from 10 million to 405 billion parameters, get support for PyTorch and other AI tools, work with text and multimodal models like Llama, Qwen (the LLM family built by Alibaba Cloud, and Microsoft’s Phi small language models (SLMs).\n\nThey can use multiple inference engines like vLLM and SGLang, evaluate models via standard benchmarks, and run their models in any environment, from their own laptops to cloud infrastructures from Amazon Web Services and Microsoft’s Azure to Google Cloud Platform and Lambda. In addition, developers can integrate their models with open model or commercial APIs from OpenAI, Anthropic, Google’s Vertex AI, and others.\n\nThere also is native support for Jupyter notebooks and Microsoft Visual Studio code debugging. Oumi also includes prebuilt workflows and recipes for various operations, including post-training.\n\n“When there’s a new project, when they have a research idea, they want to execute on it,” Elachqar told The New Stack. “But there’s so much that they have to figure out just to test that idea, the hypothesis. What we provide them is for the most common workflows, what they need to fine-tune a model or generate data or to inference. We provide them with really solid starting points that they can use and tweak to their use case.’\n\nBetting on the Right Horse\n\nIt’s crucial that the industry moves toward true open source and away from one-off AI models built by companies behind the wall, the Oumi founders said. Innovation will happen faster and there will be more available talent to draw on, and they believe players in the ecosystem — from the cloud providers to accelerator vendors on down to small companies and research institutions — all want open source AI to succeed.\n\nThe most resourced companies can’t solve challenges alone, Elachqar said, noting the years of experience both he and Koukoumidis have with such vendors. Open collaboration is essential; otherwise, AI developers are working in silos and creating the same tools that everyone else is rather than building off what has already been done.\n\nIt also will protect the industry itself. Koukoumidis expects many companies making closed models to collapse in the next few years. He pointed to Inflection AI, a startup that until late last year was rolling out models that executives said could challenge the leaders. It’s now gotten out of the model-building game, shifting to creating tools that enterprises can use with AI models built by others.\n\n“You have two horses in the race,” the CEO said. “There’s the closed-source horse that says, ‘I need to do everything by my own. I need to do all the effort by my own to develop and I have to get the full cost of doing this.’ And then you have an open source horse that so many people are providing resources to. They’re helping it move faster, and it’s aggregating all the contributions — both human and monetary contributions — from all the different entities. And the question is, which one do you think is going to be faster and more economically sustainable in the end? We are betting on the second horse.”",
      "# [No. 2 UConn men's basketball vs. Colorado at Maui Invitational: Time, TV and what you need to know by David Borges, Staff on 2024-11-26](https://www.ctpost.com/sports/uconn-mens-basketball/article/colorado-maui-invitational-time-tv-uconn-huskies-19942553.php)\nLAHAINA, Hawaii — It seemed inevitable that the UConn men's basketball team would meet up with Michigan State at some point during the Maui Invitational. The two programs, after all, have met in multi-team events (or NCAA tournament games) the last six times they've met.\n\nMichigan State lived up to its end of the bargain, so to speak, with a 72-56 win over Colorado in its first-round game on Monday. UConn, however, was relegated to the loser's bracket after a 99-97, overtime loss to Memphis.\n\n\"We're going to have to bounce back and mentally regroup,\" Dan Hurley said. \"We're not used to losing. I think we're a program with a lot of character. We'll respond. This hurt, this sucks. We came here to win a championship. But, Memphis is an excellent team. We've got to respond the way UConn responds, no matter what time the game is. We played at 9:30 (a.m.) today. I don't know what time we play tomorrow, 7 (a.m.)?\"\n\n\"There's no time to really have that hangover,\" said forward Trevor Baskin. \"You're playing the two-time national champions the next day, you can't wallow in self-pity or else you'll get your ass kicked again.\"\n\nAdded Colorado coach Tad Boyle: \"One thing I know about Coach Hurley and the way he's wired, I know he's going to be ready and a little be angrier. Who's going to be angrier, UConn or Colorado? That's what we'll find out at 10:30 a.m.\"\n\nUConn has faced Colorado just once, in a first-round NCAA tournament game in Des Moines, Iowa. The Huskies won that one 74-67, their last NCAA tourney win until 2023, when the Huskies kicked off their current 12-game winning streak in the Big Dance."
    ],
    "# Oumi Company and Product Report\n\n## Company Overview\n\nOumi is a newly launched company that has emerged from stealth mode with a mission to create the world's first unconditionally open-source AI platform. Founded by former Google and Apple engineers Manos Koukoumidis and Oussama Elachqar, Oumi has secured $10 million in seed funding, led by Venrock and Obvious Ventures, with additional contributions from Plug & Play and Ascend [(Eadline & Hampton, HPCwire, 2025-01-29)](https://www.hpcwire.com/2025/01/29/ai-needs-its-linux-oumi-comes-out-of-stealth-with-an-open-source-vision/). The company is structured as a Public Benefit Corporation (PBC), allowing it to balance its mission-driven approach with the ability to operate as a sustainable business [(Shah, Obvious, 2025-01-29)](https://obvious.com/ideas/why-we-invested-in-oumi/).\n\nOumi's founders have extensive backgrounds in AI development, having worked at major tech companies like Google Cloud and Apple. They aim to address the challenges faced by AI researchers, particularly the complexity and fragmentation of current AI ecosystems, by providing a unified platform that fosters collaboration and transparency [(Kerner, VentureBeat, 2025-01-29)](https://venturebeat.com/ai/ex-google-apple-engineers-launch-unconditionally-open-source-oumi-ai-platform-that-could-help-to-build-the-next-deepseek/).\n\n## Product Overview\n\nThe Oumi platform is designed to be a comprehensive solution for AI developers, enabling them to train, fine-tune, and deploy models efficiently across various environments. It supports models ranging from 10 million to 405 billion parameters and incorporates advanced training techniques such as SFT, LoRA, QLoRA, and DPO [(Eadline & Hampton, HPCwire, 2025-01-29)](https://www.hpcwire.com/2025/01/29/ai-needs-its-linux-oumi-comes-out-of-stealth-with-an-open-source-vision/). The platform is compatible with both text and multimodal models, including popular frameworks like Llama and Qwen, and integrates with commercial APIs from OpenAI, Anthropic, and others [(Kerner, VentureBeat, 2025-01-29)](https://venturebeat.com/ai/ex-google-apple-engineers-launch-unconditionally-open-source-oumi-ai-platform-that-could-help-to-build-the-next-deepseek/).\n\nOumi's founders emphasize the importance of making AI research more accessible and collaborative. Koukoumidis stated, “What we’re announcing is the first unconditionally open AI platform that will make it possible for the tens of thousands of AI researchers, scientists, and developers around the world to not just to more easily leverage and harness frontier AI, but also, more importantly, to collaborate on the same platform” [(Eadline & Hampton, HPCwire, 2025-01-29)](https://www.hpcwire.com/2025/01/29/ai-needs-its-linux-oumi-comes-out-of-stealth-with-an-open-source-vision/).\n\n## Collaborations and Partnerships\n\nOumi has developed its platform in collaboration with 13 leading AI universities in the U.S. and the U.K., including prestigious institutions like MIT, Stanford, and Carnegie Mellon [(Eadline & Hampton, HPCwire, 2025-01-29)](https://www.hpcwire.com/2025/01/29/ai-needs-its-linux-oumi-comes-out-of-stealth-with-an-open-source-vision/). The founders have ambitious plans to expand their collaborations with national labs and high-performance computing centers to streamline AI research at scale [(Kerner, VentureBeat, 2025-01-29)](https://venturebeat.com/ai/ex-google-apple-engineers-launch-unconditionally-open-source-oumi-ai-platform-that-could-help-to-build-the-next-deepseek/).\n\n## Market Position and Investor Interest\n\nThe company is positioned to capture a significant market opportunity as enterprise AI spending grows rapidly. Nearly half of enterprises are expressing interest in moving to open-source solutions to reduce costs and vendor lock-in [(Shah, Obvious, 2025-01-29)](https://obvious.com/ideas/why-we-invested-in-oumi/). Oumi's approach mirrors the Red Hat model for Linux, where the core technology remains open and accessible, but specialized enterprise offerings provide a revenue stream [(Eadline & Hampton, HPCwire, 2025-01-29)](https://www.hpcwire.com/2025/01/29/ai-needs-its-linux-oumi-comes-out-of-stealth-with-an-open-source-vision/).\n\nElachqar noted that investor interest in Oumi was strong, allowing the team to be selective in choosing backers who aligned with their mission. The overwhelming demand from academia for an open AI platform played a key role in convincing investors [(Eadline & Hampton, HPCwire, 2025-01-29)](https://www.hpcwire.com/2025/01/29/ai-needs-its-linux-oumi-comes-out-of-stealth-with-an-open-source-vision/).\n\n## Executive Team\n\n- **Manos Koukoumidis**: CEO, previously led AI efforts at Google Cloud.\n- **Oussama Elachqar**: Co-founder, former machine learning engineer at Apple.\n\nBoth founders have expressed their commitment to making AI development more transparent and accessible, with Koukoumidis stating, “We want such open technology to be universal, to be accessible to everyone and present everywhere” [(Eadline & Hampton, HPCwire, 2025-01-29)](https://www.hpcwire.com/2025/01/29/ai-needs-its-linux-oumi-comes-out-of-stealth-with-an-open-source-vision/).\n\n## Future Outlook\n\nOumi aims to reshape the AI research landscape by enabling a global community to work together efficiently, leading to safer, more secure, and more widely beneficial AI advancements. The founders believe that if successful, Oumi could significantly lower the barriers to AI research and development, fostering a movement that prioritizes transparency, accessibility, and collaboration over competition [(Eadline & Hampton, HPCwire, 2025-01-29)](https://www.hpcwire.com/2025/01/29/ai-needs-its-linux-oumi-comes-out-of-stealth-with-an-open-source-vision/).\n\nIn summary, Oumi is positioned to become a key player in the AI landscape by providing an open-source platform that democratizes access to AI research and fosters collaboration among researchers and developers. The company's innovative approach and strong backing from investors and academic institutions suggest a promising future in the rapidly evolving AI sector."
  ],
  "lineage": {
    "run_at": "2025-04-01T14:44:01.267239",
    "git_sha": "c044785"
  }
}