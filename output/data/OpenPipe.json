{
  "summary_markdown": "# About OpenPipe\n\nOpenPipe is a technology company founded on January 1, 2023, that specializes in providing tools for fine-tuning and deploying custom machine learning models, particularly large language models (LLMs). The company aims to offer a cost-effective alternative to existing LLM providers, enabling businesses to create tailored models that meet their specific needs [(Crunchbase, 2025)](https://www.crunchbase.com/organization/openpipe).\n\nOpenPipe's primary product is its platform for training and deploying fine-tuned models. The platform is designed to be user-friendly, allowing users to get started quickly and save significantly on costs compared to traditional LLMs. Users have reported up to 25x savings relative to OpenAI, with higher quality outputs [(OpenPipe, 2024-06-24)](https://openpipe.ai/blog/mixture-of-agents).\n\nThe company operates in a B2B model, serving a diverse range of customers, including enterprises that utilize OpenPipe to train custom models for applications such as voice bots and inventory management. Notable customers include InVintory, which processes millions of wine labels monthly and found OpenPipe's services to be significantly more cost-effective than using GPT-4 [(AI Rabbit, Medium, 2025-02-04)](https://medium.com/codex/benchmarking-llms-against-your-own-data-ee8f495160ff).\n\nOpenPipe generates revenue through its platform services, which include a unified SDK for data collection, data capture, request logs, data upload/export, fine-tuning, pruning rules, model hosting, caching, and evaluations. The company's products are distributed through its platform, which is accessible to developers and businesses looking to leverage AI technology without incurring high costs [(OpenPipe, 2024-06-24)](https://openpipe.ai/blog/mixture-of-agents).\n\nThe company has raised a total of $6.7 million in seed funding, with a notable funding round on March 26, 2024 [(FinSMEs, 2024-04-17)](https://www.finsmes.com/2024/03/openpipe-raises-6-7m-in-seed-funding.html).\n\n# Key Personnel\n\nThe leadership team at OpenPipe includes David, the Chief Technology Officer (CTO), who is actively engaged with the community and shares insights on fine-tuning and model performance. The team comprises experienced professionals in the AI and technology sectors, positioning the company well for future growth and innovation [(Asif Razzaq, MarkTechPost, 2024-06-24)](https://www.marktechpost.com/2024/06/24/openpipe-introduces-a-new-family-of-mixture-of-agents-moa-models-optimized-for-generating-synthetic-training-data-outperform-gpt-4-at-1-25th-the-cost/).\n\n# News\n\n## Funding and Expansion\n\nOpenPipe has been in the news for its successful seed funding round, raising $6.7 million to expand its team and enhance its offerings. This funding round was reported by multiple sources, including FinSMEs, GeekWire, and SiliconANGLE, highlighting the company's focus on helping developers fine-tune lightweight but powerful LLMs [(FinSMEs, 2024-04-17)](https://www.finsmes.com/2024/03/openpipe-raises-6-7m-in-seed-funding.html), [(GeekWire, 2024-04-17)](https://www.geekwire.com/2024/seattle-startup-openpipe-raises-6-7m-to-help-companies-reduce-costs-for-llm-development/), [(SiliconANGLE, 2024-04-17)](https://siliconangle.com/2024/03/26/openpipe-raises-6-7m-help-developers-fine-tune-lightweight-powerful-llms/).\n\n## Product Developments\n\nOpenPipe's flagship product, the Mixture of Agents (MoA) model, has achieved state-of-the-art results in various benchmarking tests, outperforming established models like GPT-4 in several tasks. The MoA model is optimized for generating synthetic training data, which can be used to fine-tune smaller models, significantly reducing costs and improving performance [(OpenPipe, 2024-06-24)](https://openpipe.ai/blog/mixture-of-agents).\n\nThe MoA model employs a three-prompt chain to generate completions, ensuring high-quality and diverse responses. This structured approach allows the model to critique and refine its outputs, enhancing overall performance [(OpenPipe, 2024-06-24)](https://openpipe.ai/blog/mixture-of-agents).\n\n## Market Position\n\nOpenPipe operates in a rapidly evolving AI landscape, where competition is intensifying. The company has positioned itself as a key player in the AI infrastructure space, focusing on fine-tuning and model deployment. As the demand for AI solutions continues to grow, OpenPipe's innovative approach to model customization and cost reduction will likely attract interest from investors and prospective candidates looking to join a forward-thinking organization [(Kelvin Mu, AI Timeline, 2024-12-03)](https://kelvinmu.substack.com/p/2024-backward-pass-the-definitive).\n\n## Partnerships\n\nOpenPipe has established partnerships with various organizations, including integration with AI agent frameworks like AutoGen from Microsoft Research. This collaboration enhances the company's offerings and expands its reach in the AI market [(Agency AI, FinSMEs, 2024-08-28)](https://www.finsmes.com/2024/08/agency-ai-raises-2-6m-in-pre-seed-funding.html).",
  "target": [
    "OpenPipe",
    "OpenPipe",
    "openpipe.ai",
    null,
    false,
    false,
    null,
    [
      false,
      false
    ]
  ],
  "webpage_result": {
    "summary_markdown": "# OpenPipe Company Overview\n\n## Company History\nOpenPipe is a technology company focused on providing tools for fine-tuning and deploying custom machine learning models, particularly in the realm of large language models (LLMs). The company has positioned itself as a cost-effective alternative to existing LLM providers, enabling businesses to create tailored models that meet their specific needs.\n\n## Services\nOpenPipe offers a range of services designed to simplify the process of model training and deployment:\n\n- **Unified SDK**: Collect and utilize interaction data to fine-tune custom models.\n  \n- **Data Capture**: Automatically captures every request and response for future use.\n  \n- **Request Logs**: Logs past requests and tags them for easy filtering.\n  \n- **Upload/Export Data**: Allows importing fine-tuning data and exporting request logs.\n  \n- **Fine-Tuning**: Simplifies the process of selecting data for model training.\n  \n- **Pruning Rules**: Reduces the size of incoming requests to save on inference costs.\n  \n- **Model Hosting**: Automatically hosts trained models.\n  \n- **Caching**: Improves performance by caching previously generated responses.\n  \n- **Evaluations**: Compares models against one another and against OpenAI base models.\n\n## Products\nOpenPipe's primary product is its platform for training and deploying fine-tuned models. The platform is designed to be user-friendly, allowing users to get started quickly and save significantly on costs compared to traditional LLMs. Users have reported up to 25x savings relative to OpenAI, with higher quality outputs.\n\n## Customers\nOpenPipe serves a diverse range of customers, including:\n\n- **InVintory**: A company that processes millions of wine labels monthly, which found OpenPipe's services to be significantly more cost-effective than using GPT-4.\n  \n- **Various Enterprises**: Companies that have utilized OpenPipe to train custom models for applications such as voice bots and inventory management, benefiting from lower latency and improved user experiences.\n\n## Leadership Team\nThe leadership team includes David, the CTO, who actively engages with the community and shares insights on fine-tuning and model performance.\n\n## Culture\nOpenPipe emphasizes a culture of innovation and support, providing unparalleled assistance to its users. The company is committed to helping clients reduce time to production and focus on their intellectual property by simplifying the model training and deployment process.\n\n## Conclusion\nOpenPipe is a forward-thinking company that offers a robust platform for fine-tuning and deploying custom machine learning models. With a focus on cost savings, ease of use, and customer support, OpenPipe is well-positioned to serve businesses looking to leverage LLMs effectively. \n\nFor more information, visit [OpenPipe](https://openpipe.ai/).",
    "page_markdowns": [
      "# [Tuning for Developers](https://openpipe.ai/)\nOpenPipe increased our inference speed by 3x compared to GPT-4o while reducing cost by >10x. It’s a no-brainer for any company that uses LLMs in prod.\n\nWe used OpenPipe to process a huge dataset we needed classified. GPT-4 would have cost us $60K, but with OpenPipe it was only a few hundred dollars. The process was super simple and results were great. The OpenPipe team is the real deal and really knows their stuff.\n\nInVintory processes millions of wine labels every month, and GPT-4 was prohibitively expensive to continue using. OpenPipe allowed us to train a model that is just as accurate at 1/8th the cost, I’d highly recommend them for fine-tuning task specific models!\n\nOpenPipe has been huge for us! They’ve made it easy and cheap to deploy fine tunes and rapidly iterate on them. We’ve deployed ~10 fine tunes on OpenPipe in the last few months and have been able to ship some big improvements to our quest + inventory features because of them. Their support has also been amazing!\n\nFor us, the biggest benefit was lowering time to production. OpenPipe lets us focus on our IP and use the platform to train, review and deploy models in a few clicks with confidence.\n\nWe’re using OpenPipe to train our custom voice bots. Our fine-tuned models are much lower latency than OpenAI’s, so we’re able to provide a much better user experience for our customers\n\nOpenPipe is the easiest way to train and deploy your own fine-tuned models. It only takes a few minutes to get started and can save you 25x relative to OpenAI with higher quality.\n\nOpenPipe is the easiest way to train and deploy your own fine-tuned models. It only takes a few minutes to get started and can save you 25x relative to OpenAI with higher quality.",
      "# [Tuning for Enterprise AI](https://openpipe.ai/enterprise)\n___\n\nDitch reliance on third-party models. Use OpenPipe to easily create custom models for enterprise applications for increased reliability, observability, and feedback loops. Create unlimited models for each step of your pipeline and deploy on-prem or via OpenPipe's servers.\n\n___\n\nOpenPipe's enterprise solution architects will guide you every step of the way. Get unparalleled support with custom SLAs and feature prioritization, with flexibility to leverage OpenPipe's internal engineering team for custom builds.\n\n___\n\nIn addition to the significant cost savings on inference you get from customizing your own models, OpenPipe offers additional discounts as you scale. For more information on customized pricing plans, please schedule an enterprise demo with our team below.",
      "# [Terms of Service](https://openpipe.ai/tos)\nOpenPipe, Inc. Terms of Service\n\nLast Updated: January 30, 2024\n\nIf you signed a separate Cover Page to access the Product with the same account, and that agreement has not ended, the terms below do not apply to you. Instead, your separate Cover Page applies to your use of the Product.\n\nThis Agreement is between OpenPipe, Inc. and the company or person accessing or using the Product. This Agreement consists of: (1) the Order Form and (2) the Key Terms, both of which are on the Cover Page below, and (3) the Common Paper Cloud Service Agreement Standard Terms Version 1.1 (“Standard Terms”). Any modifications to the Standard Terms made in the Cover Page will control over conflicts with the Standard Terms. Capitalized words have the meanings or descriptions given in the Cover Page or the Standard Terms.\n\nIf you are accessing or using the Product on behalf of your company, you represent that you are authorized to accept this Agreement on behalf of your company. By signing up, accessing, or using the Product, Customer indicates its acceptance of this Agreement and agrees to be bound by the terms and conditions of this Agreement.\n\nCover Page\n\nOrder Form\n\nCloud Service: OpenPipe is a cloud service provider for managing datasets, fine-tuning custom models, and monitoring model performance in production.\n\nSubscription Start Date: The Effective Date\n\nSubscription Period: 1 month(s)\n\nNon-Renewal Notice Period: At least 30 days before the end of the current Subscription Period.\n\nCloud Service Fees: Section 5.2 of the Standard Terms is replaced with: Certain parts of the Product have different pricing plans, which are available at Provider’s pricing page . Within the Payment Period, Customer will pay Provider fees based on the Product tier selected at the time of account creation and Customer’s usage per Subscription Period. Provider may update Product pricing by giving at least 7 days notice to Customer (including by email or notification within the Product), and the change will apply in the next Subscription Period.\n\nPayment Period: 5 day(s) from the last day of the Subscription Period\n\nInvoice Period: Monthly\n\nKey Terms\n\nCustomer: The company or person who accesses or uses the Product. If the person accepting this Agreement is doing so on behalf of a company, all use of the word \"Customer\" in the Agreement will mean that company.\n\nProvider: OpenPipe, Inc.\n\nEffective Date: The date Customer first accepts this Agreement.\n\nGeneral Cap Amount: The fees paid or payable by Customer to provider in the 12 month period immediately before the claim\n\nGoverning Law: The laws of the State of Delaware\n\nChosen Courts: The state or federal courts located in Delaware\n\nNotice Address:\n\nFor Provider: notices@openpipe.ai\n\nFor Customer: The main email address on Customer's account",
      "# [OpenPipe](https://openpipe.ai/pricing)\nOpenPipe is the easiest way to train and deploy your own fine-tuned models. It only takes a few minutes to get started and can save you 25x relative to OpenAI with higher quality.\n\nOpenPipe is the easiest way to train and deploy your own fine-tuned models. It only takes a few minutes to get started and can save you 25x relative to OpenAI with higher quality.",
      "# [OpenPipe](https://openpipe.ai/blog)\nA few weeks ago, OpenAI announced Reinforcement Fine-Tuning (RFT). This post covers the technical details of how it works, as well as the types of tasks for which it is a major breakthrough that lets LLMs be applied to even more complex custom tasks.\n\nChapter 2 continuing OpenPipe's fine-tuning best practices series. This chapter focuses specifically on models and the trade-off choices around them to ultimately create the highest performing fine-tuned LLM.\n\nA post introducing a series of articles on LLM fine-tuning best practices, and the first chapter specifically on Training Data best practices.\n\nHere’s the summary and full video of a talk given at Mastering LLMs Conference titled the “Ten Commandments of Fine-Tuning in Prod.”\n\nYou’ve probably heard about Llama 3, a new open-source LLM. There are 3 variants announced so far: a small 8B parameter model, medium 70B parameter model, and very large 405B parameter model (still in training). I wanted to share the most important things we’ve learned about these models so far.\n\nHi, I’m David, the CTO of OpenPipe. This is a post for individuals who want to build high-level intuition around fine-tuning.\n\nEvals can be tricky, but we’ve found that LLM-as-judge is the most reliable way to compare two outputs.\n\nS-LoRA describes a set of optimizations for running thousands of separate LLMs simultaneously on the same GPU.\n\nAxis is a modern platform to help multinational enterprises understand their local markets across the world.\n\nHi there! We'd love to share with you some of the recent updates we've made in December\n\nFine-tunes based on our new model are slightly stronger than GPT-4, as measured by GPT-4 itself.\n\nBoth crypto and AI have been heavily debated on Hacker News, with discussions going back years.\n\nIf you’re fine-tuning a task-specific LLM under 34B parameters you should probably be using Mistral.\n\nI've been playing around with fine-tuning models for a couple of years, and wanted to share some insights and practical code\n\nOpenPipe lets you capture your existing prompts and completions, and then use them to fine-tune a model specific to your use-case.",
      "# [tuning for production apps](https://app.openpipe.ai/)\n",
      "# [Tuning for Developers](https://openpipe.ai/old-home)\nOpenpipe increased our inference speed by 3x compared to GPT4-turbo while reducing cost by >10x. It’s a no-brainer for any company that uses LLMs in prod.\n\nOpenpipe increased our inference speed by 3x compared to GPT4-turbo while reducing cost by >10x. It’s a no-brainer for any company that uses LLMs in prod.\n\nWe used OpenPipe to process a huge dataset we needed classified. GPT-4 would have cost us $60K, but with OpenPipe it was only a few hundred dollars. The process was super simple and results were great. The OpenPipe team is the real deal and really knows their stuff.\n\nWe used OpenPipe to process a huge dataset we needed classified. GPT-4 would have cost us $60K, but with OpenPipe it was only a few hundred dollars. The process was super simple and results were great. The OpenPipe team is the real deal and really knows their stuff.\n\nInVintory processes millions of wine labels every month, and GPT-4 was prohibitively expensive to continue using. OpenPipe allowed us to train a model that is just as accurate at 1/8th the cost, I’d highly recommend them for fine-tuning task specific models!\n\nInVintory processes millions of wine labels every month, and GPT-4 was prohibitively expensive to continue using. OpenPipe allowed us to train a model that is just as accurate at 1/8th the cost, I’d highly recommend them for fine-tuning task specific models!\n\nOpenPipe has been huge for us! They’ve made it easy and cheap to deploy fine tunes and rapidly iterate on them. We’ve deployed ~10 fine tunes on OpenPipe in the last few months and have been able to ship some big improvements to our quest + inventory features because of them. Their support has also been amazing!\n\nOpenPipe has been huge for us! They’ve made it easy and cheap to deploy fine tunes and rapidly iterate on them. We’ve deployed ~10 fine tunes on OpenPipe in the last few months and have been able to ship some big improvements to our quest + inventory features because of them. Their support has also been amazing!\n\nFor us, the biggest benefit was lowering time to production. OpenPipe lets us focus on our IP and use the platform to train, review and deploy models in a few clicks with confidence.\n\nFor us, the biggest benefit was lowering time to production. OpenPipe lets us focus on our IP and use the platform to train, review and deploy models in a few clicks with confidence.",
      "# [Overview](https://docs.openpipe.ai/overview)\nWhat We Provide\n\nHere are a few of the features we offer:\n\nUnified SDK: Collect and utilize interaction data to fine-tune a custom model and continually refine and enhance model performance. Switching requests from your previous LLM provider to your new model is as simple as changing the model name. All our models implement the OpenAI inference format, so you won’t have to change how you parse its response.\n\nData Capture: OpenPipe captures every request and response and stores it for your future use.\n\nRequest Logs: We help you automatically log your past requests and tag them for easy filtering.\n\nUpload Data: OpenPipe also allows you to import fine-tuning data from OpenAI-compatible JSONL files.\n\nExport Data: Once your request logs are recorded, you can export them at any time.\n\nFine-Tuning: With all your LLM requests and responses in one place, it’s easy to select the data you want to fine-tune on and kick off a job.\n\nPruning Rules: By removing large chunks of unchanging text and fine-tuning a model on the compacted data, we can reduce the size of incoming requests and save you money on inference.\n\nModel Hosting: After we’ve trained your model, OpenPipe will automatically begin hosting it.\n\nCaching: Improve performance and reduce costs by caching previously generated responses.\n\nEvaluations: Compare your models against one another and OpenAI base models. Set up custom instructions and get quick insights into your models’ performance.\n\nWelcome to the OpenPipe community!",
      "# [OpenPipe Documentation](https://docs.openpipe.ai/introduction)\nSoftware engineers and data scientists use OpenPipe’s intuitive fine-tuning and monitoring services to decrease the cost and latency of their LLM operations. You can use OpenPipe to collect and analyze LLM logs, create fine-tuned models, and compare output from multiple models given the same input.",
      "# [OpenPipe, Inc status](https://status.openpipe.ai/)\nAll services are online\n\nLast updated on Mar 28 at 10:25pm PDT",
      "# [Base Models](https://docs.openpipe.ai/base-models)\nWe regularly evaluate new models to see how they compare against our existing suite. If you’d like us to check out a base model you’re particularly excited about, send an email to hello@openpipe.ai.\n\nCurrent Base Models\n\nOpen Source\n\nmeta-llama/Meta-Llama-3.1-8B-Instruct\n\nmeta-llama/Meta-Llama-3.1-70B-Instruct\n\nmeta-llama/Llama-3.1-8B\n\nmeta-llama/Llama-3.1-70B\n\nQwen/Qwen2.5-72B-Instruct\n\nQwen/Qwen2.5-7B-Instruct\n\nQwen/Qwen2.5-1.5B-Instruct\n\nQwen/Qwen2.5-Coder-32B-Instruct\n\nmistralai/Mistral-Nemo-Base-2407\n\nmistralai/Mistral-Small-24B-Base-2501\n\nmeta-llama/Llama-3.2-1B-Instruct\n\nmeta-llama/Llama-3.2-3B-Instruct\n\nmeta-llama/Llama-3.3-70B-Instruct\n\nOpenAI\n\ngpt-4o-mini-2024-07-18\n\ngpt-4o-2024-08-06\n\ngpt-3.5-turbo-0125\n\nGoogle Gemini\n\ngemini-1.0-pro-001\n\ngemini-1.5-flash-001\n\nEnterprise models\n\nThese models are currently available for enterprise customers only. If you’re interested in exploring these models, we’d be happy to discuss further. Please reach out to us at hello@openpipe.ai to learn more.\n\nAWS Bedrock",
      "# [Pruning Rules](https://docs.openpipe.ai/features/pruning-rules)\nSome prompts have large chunks of unchanging text, like system messages which don’t change from one request to the next. By removing this static text and fine-tuning a model on the compacted data, we can reduce the size of incoming requests and save you money on inference.\n\nAdd pruning rules to your dataset in the Settings tab, as shown below and in our demo dataset.\n\nTo see the effect your pruning rules had on an individual training entry’s input messages, open the Dataset Entry drawer:\n\nBy default, fine-tuned models inherit pruning rules applied to the dataset on which they were trained (see demo model). These rules will automatically prune matching text from any incoming requests sent to that model. New pruning rules will not be associated with previously trained models, so you don’t need to worry about backwards compatibility when adding new rules to your dataset. Before training a new model, you can choose to disable any inherited pruning rules.\n\nWarning: can affect quality!\n\nWe’ve found that while pruning rules always decrease latency and costs, they can also negatively affect response quality, especially with smaller datasets. We recommend enabling pruning rules on datasets with 10K+ training examples, as smaller datasets may not provide enough guidance for the model to fully learn the task."
    ],
    "search_results": [
      {
        "title": "OpenPipe: Fine-Tuning for Developers",
        "link": "https://openpipe.ai/",
        "snippet": "Fine-tuning for production apps. Train higher-quality, faster models that continuously improve. 90% Fewer errors in production. 5min To start collecting ...",
        "formattedUrl": "https://openpipe.ai/"
      },
      {
        "title": "Fine-Tuning for Enterprise AI - OpenPipe",
        "link": "https://openpipe.ai/enterprise",
        "snippet": "OpenPipe is the easiest way to train and deploy your own fine-tuned models. It only takes a few minutes to get started and can save you 25x relative to OpenAI ...",
        "formattedUrl": "https://openpipe.ai/enterprise"
      },
      {
        "title": "Terms of Service - OpenPipe",
        "link": "https://openpipe.ai/tos",
        "snippet": "Jan 30, 2024 ... OpenPipe is a cloud service provider for managing datasets, fine-tuning custom models, and monitoring model performance in production.",
        "formattedUrl": "https://openpipe.ai/tos"
      },
      {
        "title": "Pricing",
        "link": "https://openpipe.ai/pricing",
        "snippet": "50k training rows per dataset, up to 50 fine-tuned models, from $0.48 per 1M tokens for training and $1.20 and $1.60 input and output.",
        "formattedUrl": "https://openpipe.ai/pricing"
      },
      {
        "title": "Blog",
        "link": "https://openpipe.ai/blog",
        "snippet": "OpenPipe lets you capture your existing prompts and completions, and then use them to fine-tune a model specific to your use-case. GRPO beats o3 ...",
        "formattedUrl": "https://openpipe.ai/blog"
      },
      {
        "title": "OpenPipe: Fine-tuning for production apps",
        "link": "https://app.openpipe.ai/",
        "snippet": "Train faster, cheaper models on monitored data.",
        "formattedUrl": "https://app.openpipe.ai/"
      },
      {
        "title": "Fine-Tuning for Developers - OpenPipe",
        "link": "https://openpipe.ai/old-home",
        "snippet": "Replace your prompts with faster, cheaper fine-tuned models. Install OpenPipe Learn More Shorten your deployment loop Keep your datasets, models, and ...",
        "formattedUrl": "https://openpipe.ai/old-home"
      },
      {
        "title": "Cookie Policy - OpenPipe",
        "link": "https://openpipe.ai/cookie-policy",
        "snippet": "Jul 29, 2024 ... This Cookie Policy explains how OpenPipe, Inc. (\"Company,\" \"we,\" \"us,\" and \"our\") uses cookies and similar technologies to recognize you when you visit our ...",
        "formattedUrl": "https://openpipe.ai/cookie-policy"
      },
      {
        "title": "Fulfillment Terms - OpenPipe",
        "link": "https://openpipe.ai/fulfillment-terms",
        "snippet": "Feb 22, 2024 ... This page provides the fulfillment terms for OpenPipe, Inc. (“Company,” “we,” “us,” or “our“) and describes how we charge for training and inference.",
        "formattedUrl": "https://openpipe.ai/fulfillment-terms"
      },
      {
        "title": "Information Security - OpenPipe",
        "link": "https://openpipe.ai/information-security",
        "snippet": "Feb 27, 2024 ... Information Security Policy · Our physical infrastructure is hosted in secure AWS facilities that comply with SOC 2 standards, ensuring robust ...",
        "formattedUrl": "https://openpipe.ai/information-security"
      },
      {
        "title": "Privacy Policy - OpenPipe",
        "link": "https://openpipe.ai/privacy-policy",
        "snippet": "Dec 12, 2024 ... We collect personal information that you voluntarily provide to us when you express an interest in our Services, such as learning about us or our Services.",
        "formattedUrl": "https://openpipe.ai/privacy-policy"
      },
      {
        "title": "Overview - OpenPipe",
        "link": "https://docs.openpipe.ai/overview",
        "snippet": "Overview. OpenPipe is a streamlined platform designed to help product-focused teams train specialized LLM models as replacements for slow and expensive prompts.",
        "formattedUrl": "https://docs.openpipe.ai/overview"
      },
      {
        "title": "OpenPipe Documentation - OpenPipe",
        "link": "https://docs.openpipe.ai/introduction",
        "snippet": "You can use OpenPipe to collect and analyze LLM logs, create fine-tuned models, and compare output from multiple models given the same input.",
        "formattedUrl": "https://docs.openpipe.ai/introduction"
      },
      {
        "title": "OpenPipe, Inc status",
        "link": "https://status.openpipe.ai/",
        "snippet": "All services are online. Last updated on Mar 23 at 02:36pm PDT. Current status by service. Operational. API. 30 days ago. 60 days ago. 90 days ago.",
        "formattedUrl": "https://status.openpipe.ai/"
      },
      {
        "title": "S-LoRA: Serving Thousands of Models From One GPU for Fun and ...",
        "link": "https://openpipe.ai/blog/s-lora",
        "snippet": "Jan 17, 2024 ... The LoRA technique involves cleverly inserting extra adapter layers in a few carefully-selected locations and only fine-tuning those. These ...",
        "formattedUrl": "https://openpipe.ai/blog/s-lora"
      },
      {
        "title": "Base Models - OpenPipe",
        "link": "https://docs.openpipe.ai/base-models",
        "snippet": "Train and compare across a range of the most powerful base models. We regularly evaluate new models to see how they compare against our existing suite.",
        "formattedUrl": "https://docs.openpipe.ai/base-models"
      },
      {
        "title": "Analyzing OpenAI's Reinforcement Fine-Tuning: Less Data, Better ...",
        "link": "https://openpipe.ai/blog/openai-rft",
        "snippet": "Dec 30, 2024 ... This post will cover the technical details of how it works, as well as the types of tasks for which it is a major breakthrough that lets LLMs be applied to ...",
        "formattedUrl": "https://openpipe.ai/blog/openai-rft"
      },
      {
        "title": "Introducing Direct Preference Optimization (DPO) Support on ...",
        "link": "https://openpipe.ai/blog/announcing-dpo-support",
        "snippet": "Oct 1, 2024 ... OpenPipe is the first fine-tuning platform to support Direct Preference Optimization (DPO)! This powerful technique allows you to align models with your ...",
        "formattedUrl": "https://openpipe.ai/blog/announcing-dpo-support"
      },
      {
        "title": "5 Deep Research Prompts that are Supercharging our Sales ...",
        "link": "https://openpipe.ai/blog/deep-research-prompts",
        "snippet": "Feb 19, 2025 ... Whether it's refining prospect lists, preparing for meetings, or optimizing landing pages, these prompts have given us a sharper edge in sales.",
        "formattedUrl": "https://openpipe.ai/blog/deep-research-prompts"
      },
      {
        "title": "Is AI the Next Crypto? Insights from 2M HN comments - OpenPipe",
        "link": "https://openpipe.ai/blog/hn-ai-crypto",
        "snippet": "Nov 8, 2023 ... Both crypto and AI have been heavily debated on Hacker News, with discussions going back years. By looking at trends in HN commenter opinions we might find ...",
        "formattedUrl": "https://openpipe.ai/blog/hn-ai-crypto"
      },
      {
        "title": "Pruning Rules - OpenPipe",
        "link": "https://docs.openpipe.ai/features/pruning-rules",
        "snippet": "Pruning Rules. Decrease input token counts by pruning out chunks of static text. Some prompts have large chunks of unchanging text, like system messages which ...",
        "formattedUrl": "https://docs.openpipe.ai/features/pruning-rules"
      },
      {
        "title": "OpenPipe Mixture of Agents: Outperform GPT-4 at 1/25th the Cost ...",
        "link": "https://openpipe.ai/blog/mixture-of-agents",
        "snippet": "Jun 20, 2024 ... Our Mixture of Agents model is designed as a drop-in replacement for GPT-4. That is, its input is a set of messages and tools, and its output is a chat ...",
        "formattedUrl": "https://openpipe.ai/blog/mixture-of-agents"
      },
      {
        "title": "Evaluations - OpenPipe",
        "link": "https://docs.openpipe.ai/features/evaluations/overview",
        "snippet": "Evaluations allow you to compare model outputs across a variety of inputs to determine which model is doing a better job.",
        "formattedUrl": "https://docs.openpipe.ai/features/evaluations/overview"
      },
      {
        "title": "Datasets - OpenPipe",
        "link": "https://docs.openpipe.ai/features/datasets/overview",
        "snippet": "Collect, evaluate, and refine your training data.",
        "formattedUrl": "https://docs.openpipe.ai/features/datasets/overview"
      },
      {
        "title": "Criteria - OpenPipe",
        "link": "https://docs.openpipe.ai/features/criteria/overview",
        "snippet": "A criterion is a combination of an LLM model and prompt that can be used to identify a specific issue with a model's output. Criterion judgements are generated ...",
        "formattedUrl": "https://docs.openpipe.ai/features/criteria/overview"
      },
      {
        "title": "Product Updates December 2023 - OpenPipe",
        "link": "https://openpipe.ai/blog/product-updates-december-2023",
        "snippet": "Jan 3, 2024 ... We recently released a new Mistral fine-tune that outperforms Mistral's base 7B model, and in some cases even outperforms GPT-4 once it has been fine-tuned.",
        "formattedUrl": "https://openpipe.ai/blog/product-updates-december-2023"
      },
      {
        "title": "One Right Answer or Many? A Useful Distinction for Evaluating and ...",
        "link": "https://openpipe.ai/blog/deterministic-vs-freeform-tasks",
        "snippet": "Jan 14, 2025 ... Deterministic tasks have one (or a few) correct outputs for a given input, and the model should always produce the correct output.",
        "formattedUrl": "https://openpipe.ai/blog/deterministic-vs-freeform-tasks"
      },
      {
        "title": "From Prompts to Models - OpenPipe",
        "link": "https://openpipe.ai/blog/from-prompts-to-models",
        "snippet": "Aug 28, 2023 ... OpenPipe lets you capture your existing prompts and completions, and then use them to fine-tune a model specific to your use-case.",
        "formattedUrl": "https://openpipe.ai/blog/from-prompts-to-models"
      },
      {
        "title": "Quick Start - OpenPipe",
        "link": "https://docs.openpipe.ai/getting-started/quick-start",
        "snippet": "Get started with OpenPipe in a few quick steps. Step 1: Create your OpenPipe Account If you don't already have one, create an account with OpenPipe.",
        "formattedUrl": "https://docs.openpipe.ai/getting-started/quick-start"
      },
      {
        "title": "Uploading Data - OpenPipe",
        "link": "https://docs.openpipe.ai/features/datasets/uploading-data",
        "snippet": "Upload external data to kickstart your fine-tuning process. Use the OpenAI chat fine-tuning format. Upload a JSONL file populated with a list of training ...",
        "formattedUrl": "https://docs.openpipe.ai/features/datasets/uploading-data"
      }
    ]
  },
  "general_search_markdown": "# Official social media\n- [OpenPipe on LinkedIn](https://www.linkedin.com/company/openpipe)\n- [OpenPipe on X (formerly Twitter)](https://x.com/openpipeai?lang=en)\n\n# Job boards\n- [Founding Software Engineer (Backend) at OpenPipe | Y Combinator](https://www.ycombinator.com/companies/openpipe/jobs/0kfSfxe-founding-software-engineer-backend)\n\n# App stores\n- No relevant app store links found.\n\n# Product reviews\n- [OpenPipe AI Reviews: Use Cases, Pricing & Alternatives](https://www.futurepedia.io/tool/openpipe)\n\n# News articles (most recent first, grouped by event)\n### Funding and Company Developments\n- [Seattle startup OpenPipe raises $6.7M to help companies reduce ...](https://www.geekwire.com/2024/seattle-startup-openpipe-raises-6-7m-to-help-companies-reduce-costs-for-llm-development/) - Mar 26, 2024\n- [LLM Fine-Tuning Startup OpenPipe Raises $6.7 Million](https://www.globenewswire.com/news-release/2024/03/26/2852286/0/en/LLM-Fine-Tuning-Startup-OpenPipe-Raises-6-7-Million.html) - Mar 26, 2024\n- [The Opportunities Costanoa Ventures Sees In Vertical AI](https://news.crunchbase.com/venture/vertical-ai-investment-costanoa-sands-cowgill/) - Nov 27, 2024\n- [Mercury CEO Immad Akhund seeks $26m for VC fund](https://www.axios.com/pro/fintech-deals/2025/02/25/mercury-immad-akhund-26-million-vc-fund) - Feb 25, 2025\n- [Seattle Inno's 25 startups to watch in 2025 - Puget Sound Business ...](https://www.bizjournals.com/seattle/news/2025/01/10/startups-to-watch-puget-sound-seattle-inno-2025.html) - Jan 10, 2025\n\n### Product Innovations\n- [OpenPipe Introduces a New Family of 'Mixture of Agents' MoA ...](https://www.marktechpost.com/2024/06/24/openpipe-introduces-a-new-family-of-mixture-of-agents-moa-models-optimized-for-generating-synthetic-training-data-outperform-gpt-4-at-1-25th-the-cost/) - Jun 24, 2024\n- [OpenPipe Mixture of Agents: Outperform GPT-4 at 1/25th the Cost ...](https://www.reddit.com/r/LocalLLaMA/comments/1dky8my/openpipe_mixture_of_agents_outperform_gpt4_at/) - Jun 21, 2024\n\n### General AI and Industry Insights\n- [AI Confronts an 'Energy Crisis'? Electricity May Have ...](https://www.trendforce.com/news/2024/03/28/news-ai-confronts-an-energy-crisis-electricity-may-have-emerged-as-a-challenge/) - Mar 28, 2024\n\n# Key employees (grouped by employee)\n### Kyle Corbitt\n- [Kyle Corbitt, CEO of OpenPipe, on the future of fine-tuning LLMs ...](https://sacra.com/research/kyle-corbitt-openpipe-fine-tuning-llms/) - Aug 27, 2024\n- [My finetuned models beat OpenAI's GPT-4 | Hacker News](https://news.ycombinator.com/item?id=40843848) - Date not specified\n\n### David Corbitt\n- [David Corbitt - OpenPipe | LinkedIn](https://www.linkedin.com/in/davidcorbitt)\n\n# Other pages on the company website\n- [OpenPipe: Fine-tuning for production apps](https://app.openpipe.ai/)\n- [OpenPipe Documentation - OpenPipe](https://docs.openpipe.ai/introduction)\n- [OpenPipe: Cost-Effective Fine-Tuning For LLMs](https://appscribed.com/software/openpipe-llm-fine-tuning-tool/) - Jul 23, 2024\n- [OpenPipe: Fine-Tuning for Developers](https://openpipe.ai/old-home)\n- [Pricing](https://openpipe.ai/pricing)\n\n# Other\n- [OpenPipe - Crunchbase Company Profile & Funding](https://www.crunchbase.com/organization/openpipe)\n- [OpenPipe 2025 Company Profile: Valuation, Funding & Investors ...](https://pitchbook.com/profiles/company/535432-15) - Jan 17, 2025\n- [OpenPipe download | SourceForge.net](https://sourceforge.net/projects/openpipe.berlios/) - Jun 9, 2014",
  "crunchbase_markdown": "# OpenPipe, founded 2023-01-01 [(Crunchbase, 2025)](https://www.crunchbase.com/organization/openpipe)\nOpenPipe is an artificial intelligence platform that empowers software developers to construct datasets and refine prompts using a cost-effective model. The platform works by automatically capturing existing prompts and completions and then using them to fine-tune a drop-in replacement.\n\n- [Website](https://www.openpipe.ai/)\n- [LinkedIn](https://www.linkedin.com/company/openpipe/)\n- [Twitter](https://www.twitter.com/OpenPipeAI)\n\n## Funding (6M USD total)\n\n- 6M USD on 2024-03-26\n\n## News\n\n- The Opportunities Costanoa Ventures Sees In Vertical AI ([Gené Teare, 2024-11-27](https://news.crunchbase.com/venture/vertical-ai-investment-costanoa-sands-cowgill/))\n- OpenPipe Raises $6.7M in Seed Funding ([FinSMEs, 2024-04-17](https://www.finsmes.com/2024/03/openpipe-raises-6-7m-in-seed-funding.html))\n- AI startup OpenPipe looks to expand team after seed round - Puget Sound Business Journal ([The Business Journals, 2024-04-17](https://www.bizjournals.com/seattle/news/2024/03/26/openpipe-seed-funding-round-seattle-chatgpt.html))\n- Seattle startup OpenPipe raises $6.7M to help companies reduce costs for LLM models ([GeekWire, 2024-04-17](https://www.geekwire.com/2024/seattle-startup-openpipe-raises-6-7m-to-help-companies-reduce-costs-for-llm-development/))\n- OpenPipe raises $6.7M to help developers fine-tune lightweight but powerful LLMs ([SiliconANGLE, 2024-04-17](https://siliconangle.com/2024/03/26/openpipe-raises-6-7m-help-developers-fine-tune-lightweight-powerful-llms/))\n- AI startup OpenPipe looks to expand team after seed round - Puget Sound Business Journal ([The Business Journals, 2024-03-30](https://www.bizjournals.com/seattle/news/2024/03/26/openpipe-seed-funding-round-seattle-chatgpt.html))\n- OpenPipe Raises $6.7M in Seed Funding ([FinSMEs, 2024-03-30](https://www.finsmes.com/2024/03/openpipe-raises-6-7m-in-seed-funding.html))\n- LLM Fine-Tuning Startup OpenPipe Raises $6.7 Million ([AiThority, 2024-03-30](https://aithority.com/machine-learning/llm-fine-tuning-startup-openpipe-raises-6-7-million/))\n- OpenPipe raises $6.7M to help developers fine-tune lightweight but powerful LLMs ([SiliconANGLE, 2024-03-30](https://siliconangle.com/2024/03/26/openpipe-raises-6-7m-help-developers-fine-tune-lightweight-powerful-llms/))\n- Seattle startup OpenPipe raises $6.7M to help companies reduce costs for LLM models ([GeekWire, 2024-03-30](https://www.geekwire.com/2024/seattle-startup-openpipe-raises-6-7m-to-help-companies-reduce-costs-for-llm-development/))\n\n",
  "customer_experience_result": {
    "output_text": "# OpenPipe Company Sentiment\n\n## Positive Sentiment\n- \"OpenPipe's a game-changer indeed.\" [(Last_Inspector2515, Reddit, 2024-03-07)](https://www.reddit.com/r/SaaS/comments/1b92w5o/how_i_reduced_our_startups_llm_costs_by_almost_90/ktuc1p5/)\n- \"I love this idea and will definitely be using it.\" [(greencandlevandal, Reddit, 2024-03-08)](https://www.reddit.com/r/SaaS/comments/1b92w5o/how_i_reduced_our_startups_llm_costs_by_almost_90/ktx06ml/)\n- \"Really clever way to gather training data with 0 effort.\" [(dzigizord, Reddit, 2024-03-08)](https://www.reddit.com/r/SaaS/comments/1b92w5o/how_i_reduced_our_startups_llm_costs_by_almost_90/ktw16mq/)\n- \"Both for cost and performance. Way cheaper and just better performing. Also gives me the flexibility to not be locked into OpenAI.\" [(pmarks98, Reddit, 2024-03-08)](https://www.reddit.com/r/SaaS/comments/1b92w5o/how_i_reduced_our_startups_llm_costs_by_almost_90/ktxvwfy/)\n- \"We've cut our prices an additional 60%+ since this was posted, along with making the models stronger and latency lower!\" [(corbt, Reddit, 2024-07-17)](https://www.reddit.com/r/SaaS/comments/1b92w5o/how_i_reduced_our_startups_llm_costs_by_almost_90/ldm5usm/)\n\n## Neutral Sentiment\n- \"I discovered a tool called OpenPipe that allows you to fine-tune open-source models almost too easily. It looked legit and was backed by YCombinator, so I gave it a try.\" [(pmarks98, Reddit, 2024-03-07)](https://www.reddit.com/r/ArtificialInteligence/comments/1b92hlk/how_i_reduced_our_llm_costs_by_over_85/)\n\n# OpenPipe Product Information\n\n## Cost Efficiency\n- \"And it reduced the costs by 88%. The cost of inference dropped from $10 per 1M input tokens to $1.20. And cost per output token dropped from $30 to $1.60.\" [(pmarks98, Reddit, 2024-03-07)](https://www.reddit.com/r/ArtificialInteligence/comments/1b92hlk/how_i_reduced_our_llm_costs_by_over_85/)\n- \"I swapped out the OpenAI SDK with their SDK (a drop-in replacement), which passed all my LLM API calls to OpenAI but recorded all inputs and outputs. This created unique datasets for each of my prompts, which I could use to fine-tune a cheaper open-source model.\" [(pmarks98, Reddit, 2024-03-07)](https://www.reddit.com/r/ArtificialInteligence/comments/1b92hlk/how_i_reduced_our_llm_costs_by_over_85/)\n\n## Performance and Quality\n- \"I even confirmed that the fine-tuned Mistral output quality was just as high as GPT-4 by a series of evals and in-app customer feedback.\" [(pmarks98, Reddit, 2024-03-07)](https://www.reddit.com/r/ArtificialInteligence/comments/1b92hlk/how_i_reduced_our_llm_costs_by_over_85/)\n- \"we've fine-tuned thousands of models for customers, and we've actually found that for many tasks you can get away with about 100 examples and have a good experience!\" [(corbt, Reddit, 2024-09-22)](https://www.reddit.com/r/LocalLLaMA/comments/1fm59kg/how_do_you_actually_finetune_a_llm_on_your_own/lobj2ws/)\n- \"You definitely do not need 100k examples for a finetune. You can start seeing results with far less than 1000 as well, as long as they're properly designed and you use a sufficiently large number of epochs.\" [(brown2green, Reddit, 2024-09-22)](https://www.reddit.com/r/LocalLLaMA/comments/1fm59kg/how_do_you_actually_finetune_a_llm_on_your_own/lod9nx4/)\n\n## Open Source Advantages\n- \"The power of Open Source AI in a nutshell.\" [(mrdevlar, Reddit, 2024-03-08)](https://www.reddit.com/r/ArtificialInteligence/comments/1b92hlk/how_i_reduced_our_llm_costs_by_over_85/ktw2xwe/)\n- \"Open source models have already beaten GPT4: I can run open-source models on sensitive data. GPT4 doesn't even give me that option.\" [(ReturningTarzan, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfxz9x4/)\n- \"I’m getting better instruction compliance from open source models than GPT4 nowadays.\" [(fish312, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfygbes/)\n- \"Open source models can run offline, have NSFW queries, and allow for fine-tuning without API costs when run locally.\" [(MikePounce, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfxmdia/)\n- \"Open source is dynamic and fueled by passion.\" [(SkyLordOmega, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfxnu4w/)\n\n## Model Options\n- \"I decided to go with a Mistral 7B model (they have options between llama, mixtral, and mistral as of rn).\" [(pmarks98, Reddit, 2024-03-07)](https://www.reddit.com/r/SaaS/comments/1b92w5o/how_i_reduced_our_startups_llm_costs_by_almost_90/ktu4d7s/)\n\n# User Experience Feedback\n\n## Mixed Sentiment\n- \"I felt the answer were like putting a pie inside a blender, and then serve it as a drink. I was expecting a big improvement, and for me the responses lost freshness compared to the candidates, and they got big on purple prose.\" [(brahh85, Reddit, 2024-06-23)](https://www.reddit.com/r/SillyTavernAI/comments/1dmwawz/mixture_of_agents_on_st_using_qr_thoughts_and/)\n- \"Today I was testing again, starting a new chat but just using qwen2, not using the script at all, just a regular chat, and the answers weren't as good as I thought they were yesterday.\" [(brahh85, Reddit, 2024-06-24)](https://www.reddit.com/r/SillyTavernAI/comments/1dmwawz/mixture_of_agents_on_st_using_qr_thoughts_and/la2vcno/)\n- \"I created a 4 layered script using just qwen 2, and it created the responses in ST. Then, in a separated app I used gpt4-o to measure how good or bad were those answers, and how they improved.\" [(brahh85, Reddit, 2024-06-25)](https://www.reddit.com/r/SillyTavernAI/comments/1dmwawz/mixture_of_agents_on_st_using_qr_thoughts_and/la5vfz2/)\n- \"So according to gpt4-o standards the responses improved a lot.\" [(brahh85, Reddit, 2024-06-25)](https://www.reddit.com/r/SillyTavernAI/comments/1dmwawz/mixture_of_agents_on_st_using_qr_thoughts_and/la5vfz2/)\n- \"I think the first responses may vary too much, or maybe address unrelated stuff, and that might be part of the problem.\" [(nengon, Reddit, 2024-06-24)](https://www.reddit.com/r/SillyTavernAI/comments/1dmwawz/mixture_of_agents_on_st_using_qr_thoughts_and/la1ztep/)",
    "intermediate_steps": [
      "- \"I discovered a tool called OpenPipe that allows you to fine-tune open-source models almost too easily. It looked legit and was backed by YCombinator, so I gave it a try.\" [(pmarks98, Reddit, 2024-03-07)](cache://reddit/29)\n- \"I swapped out the OpenAI SDK with their SDK (a drop-in replacement), which passed all my LLM API calls to OpenAI but recorded all inputs and outputs. This created unique datasets for each of my prompts, which I could use to fine-tune a cheaper open-source model.\" [(pmarks98, Reddit, 2024-03-07)](cache://reddit/29)\n- \"And it reduced the costs by 88%. The cost of inference dropped from $10 per 1M input tokens to $1.20. And cost per output token dropped from $30 to $1.60.\" [(pmarks98, Reddit, 2024-03-07)](cache://reddit/29)\n- \"I even confirmed that the fine-tuned Mistral output quality was just as high as GPT-4 by a series of evals and in-app customer feedback.\" [(pmarks98, Reddit, 2024-03-07)](cache://reddit/29)\n- \"The power of Open Source AI in a nutshell.\" [(mrdevlar, Reddit, 2024-03-08)](cache://reddit/37)",
      "- \"we've fine-tuned thousands of models for customers, and we've actually found that for many tasks you can get away with about 100 examples and have a good experience!\" [(corbt, Reddit, 2024-09-22)](cache://reddit/104)\n- \"You definitely do not need 100k examples for a finetune. You can start seeing results with far less than 1000 as well, as long as they're properly designed and you use a sufficiently large number of epochs.\" [(brown2green, Reddit, 2024-09-22)](cache://reddit/105)",
      "- \"OpenPipe's a game-changer indeed.\" [(Last_Inspector2515, Reddit, 2024-03-07)](cache://reddit/158)\n- \"I love this idea and will definitely be using it.\" [(greencandlevandal, Reddit, 2024-03-08)](cache://reddit/180)\n- \"Really clever way to gather training data with 0 effort.\" [(dzigizord, Reddit, 2024-03-08)](cache://reddit/149)\n- \"I decided to go with a Mistral 7B model (they have options between llama, mixtral, and mistral as of rn).\" [(pmarks98, Reddit, 2024-03-07)](cache://reddit/147)\n- \"Both for cost and performance. Way cheaper and just better performing. Also gives me the flexibility to not be locked into OpenAI.\" [(pmarks98, Reddit, 2024-03-08)](cache://reddit/182)\n- \"I swapped out the OpenAI SDK with their SDK (a drop-in replacement), which passed all my LLM API calls to OpenAI but recorded all inputs and outputs.\" [(pmarks98, Reddit, 2024-03-07)](cache://reddit/141)\n- \"It reduced the costs by 88%.\" [(pmarks98, Reddit, 2024-03-07)](cache://reddit/141)\n- \"We've cut our prices an additional 60%+ since this was posted, along with making the models stronger and latency lower!\" [(corbt, Reddit, 2024-07-17)](cache://reddit/201)",
      "- \"Open source models have already beaten GPT4: I can run open-source models on sensitive data. GPT4 doesn't even give me that option.\" [(ReturningTarzan, Reddit, 2024-01-02)](cache://reddit/298)\n- \"I’m getting better instruction compliance from open source models than GPT4 nowadays.\" [(fish312, Reddit, 2024-01-02)](cache://reddit/406)\n- \"Open source has many more people, and collectively many great minds working together.\" [(None, Reddit, 2024-01-02)](cache://reddit/372)\n- \"The earliest open source models AFAIK were 6-7b. The biggest open source models we have now are 120b, with more commonly sized ones being 70b.\" [(petrus4, Reddit, 2024-01-02)](cache://reddit/387)\n- \"Open source models can run offline, have NSFW queries, and allow for fine-tuning without API costs when run locally.\" [(MikePounce, Reddit, 2024-01-02)](cache://reddit/394)\n- \"Open source is dynamic and fueled by passion.\" [(SkyLordOmega, Reddit, 2024-01-02)](cache://reddit/381)\n- \"I believe OSS models will probably lag behind because big labs can just burn investor money on compute to make new and better models.\" [(Dankmemexplorer, Reddit, 2024-01-02)](cache://reddit/413)\n- \"Open source can pair any model with any functionality and have a 'product'.\" [(heytherepotato, Reddit, 2024-01-02)](cache://reddit/382)",
      "```\n```",
      "- \"I felt the answer were like putting a pie inside a blender, and then serve it as a drink. I was expecting a big improvement, and for me the responses lost freshness compared to the candidates, and they got big on purple prose.\" [(brahh85, Reddit, 2024-06-23)](cache://reddit/537)\n- \"Today i was testing again, starting a new chat but just using qwen2, not using the script at all, just a regular chat, and the answers weren't as good as I thought they were yesterday.\" [(brahh85, Reddit, 2024-06-24)](cache://reddit/550)\n- \"I created a 4 layered script using just qwen 2, and it created the responses in ST. Then, in a separated app I used gpt4-o to measure how good or bad were those answers, and how they improved.\" [(brahh85, Reddit, 2024-06-25)](cache://reddit/552)\n- \"So according to gpt4-o standards the responses improved a lot.\" [(brahh85, Reddit, 2024-06-25)](cache://reddit/552)\n- \"I think the first responses may vary too much, or maybe address unrelated stuff, and that might be part of the problem.\" [(nengon, Reddit, 2024-06-24)](cache://reddit/551)"
    ],
    "url_to_review": {},
    "review_markdowns": [
      "# Post ID 1b92w5o: How I Reduced Our Startup's LLM Costs by Almost 90% with +242 score by [(pmarks98, Reddit, 2024-03-07)](https://www.reddit.com/r/SaaS/comments/1b92w5o/how_i_reduced_our_startups_llm_costs_by_almost_90/)\nWith AI apps popping up everywhere, it’s fair to think building one is both easy and cheap.\n\nUnfortunately, you’d be *(mostly)* wrong. I know because I learned the hard way.\n\nI’m not saying it’s hard per se, but as of this writing, gpt-4-turbo costs $0.01/$0.03 per 1000 input/output tokens. This can quickly add up if you’re building a complex AI workflow.\n\nYes, you could use less expensive, worse performing models, like GPT 3.5 or an open-source one like Llama, stuff everything into one API call with excellent prompt engineering, and hope for the best. But this probably won’t turn out that great. This type of approach doesn’t really work in production—at least not yet with the current state of AI.\n\n**It could give you the right answer 90% or even 99% of the time.** But that one time it decides to go off the rails, it’s really frustrating. As a developer and/or business, you know you must never break a user’s experience. It might be okay for a toy app or prototype but not for a production-grade application you charge for.\n\nImagine if Salesforce or any other established software company said its reliability was only one or two nines. That would be insane. No one would use it.\n\n**But this is the state of most AI applications today. They’re unreliable.**\n\n# AI isn’t a Universal Function\n\nThe non-deterministic nature of LLMs forces us to be more thoughtful about how we write our code. We should not just “hope” that an LLM will always correctly respond. We need to build redundancy and proper error handling. For some reason, many builders forget everything they learned about software engineering and treat AI like some magical universal function that doesn’t fail.\n\n**It’s not there yet.**\n\nTo fix this limitation, we must write code that only interacts with AI when absolutely necessary—that is, when a system needs some sort of “human-level” analysis of unstructured data. Subsequently, whenever possible, we must force the LLM to return references to information (i.e., a pointer) instead of the data itself.\n\n**When I recognized these two things, I had to redesign the backend architecture of my personal software business completely.**\n\n# Rearchitecting Jellypod\n\nFor context, I started an app called Jellypod. It enables users to subscribe to email newsletters and get a daily summary of the most important topics from all of them as a single podcast.\n\nThis seems pretty simple on the outside—and the MVP honestly was. The app would just process each email individually, summarize it, convert it to speech, and stitch all the audio together, side-by-side, into a daily podcast.\n\nThe output was fine, but it needed to be better.\n\nIf two different newsletters discussed the same topic, the “podcast” would talk about it twice, not realizing we had already mentioned it. You could say, “Well, why don’t you just stuff all the newsletter content into one big LLM call to summarize everything?”\n\nWell, that’s what I tried at first.\n\nAnd it failed. **Miserably.**\n\nEven with an extremely detailed prompt using all the best practices, I couldn’t guarantee that the LLM would always detect the most important topics, summarize everything, and consistently create an in-depth output. Also, the podcast always needed to be \\~10 minutes long.\n\nSo I went back to the drawing board. How can I make this system better? And yes, we’re getting to the cost reduction part - don’t worry!\n\n# Defining the Requirements\n\nJellypod must be able to process any number of input documents (newsletters) and create an output that always includes the top ten most important topics across all those inputs. If two subparts of any input are about the same topic, we should recognize that and merge the sections into one topic.\n\nFor example, if the Morning Brew has a section about US Elections and the Daily Brief also has a section on the current state of US Politics, they should be merged. I’ll skip over how I determined a similarity threshold (i.e., should two topics be merged or remain separate).\n\n# Exploding Costs\n\nI built on top of a few different approaches outlined in papers written by the LangChain community to semantic chunk and organize everything in a almost deterministic way.\n\n**But this was INSANELY expensive.** The number of API calls grew at a rate of O(n log n), with n being the number of input chunks from all newsletters.\n\nSo, I had a dilemma. Do I keep this improved and more expensive architecture or throw it down the drain?\n\nI decided to keep it and figure out how to reduce costs.\n\n# Reducing Costs\n\nThat’s when I discovered a tool called OpenPipe that allows you to fine-tune open-source models almost too easily. It looked legit and was backed by YCombinator, so I gave it a try.\n\nI swapped out the OpenAI SDK with their SDK (a drop-in replacement), which passed all my LLM API calls to OpenAI but recorded all inputs and outputs. This created unique datasets for each of my prompts, which I could use to fine-tune a cheaper open-source model.\n\nAfter about a week of recording Jellypod’s LLM calls, I had about 50,000 rows of data. And with a few clicks, I fine-tuned a Mistral 7B model for each LLM call.\n\nI replaced GPT-4 with the new fine-tuned model.\n\n**And it reduced the costs by 88%.**\n\nThe cost of inference dropped from $10 per 1M input tokens to $1.20. And cost per output token dropped from $30 to $1.60.\n\nI was blown away. I could now run Jellypod’s new architecture for approximately the same cost as the MVP’s trivial approach. I even confirmed that the fine-tuned Mistral output quality was just as high as GPT-4 by a series of evals and in-app customer feedback.\n\nBy redesigning the system to only use AI for the smallest unit of work it’s actually needed for, I could confidently deploy a fine-tuned model as a drop-in replacement for GPT 4. And by prompting to return pointers to data instead of the data itself, I could ensure data integrity while reducing the number of output tokens consumed.\n\n# In Conclusion\n\nIf you’re considering building an AI application, I would encourage you to take a step back and think about your architecture’s output reliability and costs. What happens if the LLM doesn’t answer your prompt in the right way? Can you prompt the model to return data identifiers instead of raw data? And, is it possible to swap GPT-4 with a cheaper, fine-tuned model?\n\nI wish I had these insights when I started, but hey, you live and learn.\n\nI hope you found at least some parts of this interesting! I thought there were enough learnings to share. Feel free to reach out if you’re curious about the details.\n\n## Comment ID kttteh7 with +20 score by [(itsreallyalex, Reddit, 2024-03-07)](https://www.reddit.com/r/SaaS/comments/1b92w5o/how_i_reduced_our_startups_llm_costs_by_almost_90/kttteh7/) (in reply to ID 1b92w5o):\nReally insightful, thanks for sharing.\n\n### Comment ID kttuo34 with +3 score by [(pmarks98, Reddit, 2024-03-07)](https://www.reddit.com/r/SaaS/comments/1b92w5o/how_i_reduced_our_startups_llm_costs_by_almost_90/kttuo34/) (in reply to ID kttteh7):\nOf course, glad to share!\n\n## Comment ID kttz68b with +12 score by [(mistypedusrname, Reddit, 2024-03-07)](https://www.reddit.com/r/SaaS/comments/1b92w5o/how_i_reduced_our_startups_llm_costs_by_almost_90/kttz68b/) (in reply to ID 1b92w5o):\nReally interesting to learn about the struggle of using LLMs in production. Thanks for sharing.\n\n### Comment ID kttzfem with +2 score by [(pmarks98, Reddit, 2024-03-07)](https://www.reddit.com/r/SaaS/comments/1b92w5o/how_i_reduced_our_startups_llm_costs_by_almost_90/kttzfem/) (in reply to ID kttz68b):\nGlad to share. Hope it helps some people.\n\n## Comment ID ktu3sex with +6 score by [(None, Reddit, 2024-03-07)](https://www.reddit.com/r/SaaS/comments/1b92w5o/how_i_reduced_our_startups_llm_costs_by_almost_90/ktu3sex/) (in reply to ID 1b92w5o):\n[removed]\n\n### Comment ID ktu4d7s with +12 score by [(pmarks98, Reddit, 2024-03-07)](https://www.reddit.com/r/SaaS/comments/1b92w5o/how_i_reduced_our_startups_llm_costs_by_almost_90/ktu4d7s/) (in reply to ID ktu3sex):\nOf course, glad to share. The platform I used (OpenPipe) hosts the model for me but I can export the weights if I wanted to. \n\n  \nI decided to go with a Mistral 7B model (they have options between llama, mixtral, and mistral as of rn). Another option is to host it yourself or go with a cloud provider like OctoAI that deals with the scaling of GPUs, etc. for you. I think OpenPipe uses OctoAI under the hood.\n\n#### Comment ID ktw73qm with +3 score by [(Witty_Side8702, Reddit, 2024-03-08)](https://www.reddit.com/r/SaaS/comments/1b92w5o/how_i_reduced_our_startups_llm_costs_by_almost_90/ktw73qm/) (in reply to ID ktu4d7s):\n>You can then verify the output by checking that all the output ids actually exist in the input data structure. If one output id is missing or wasn't one of the input ids, you repeat the LLM call saying it failed\n\nhow you tried this with n > 100? ie. id\\_1 to id\\_100, consistent results?\n\n## Comment ID ktw16mq with +4 score by [(dzigizord, Reddit, 2024-03-08)](https://www.reddit.com/r/SaaS/comments/1b92w5o/how_i_reduced_our_startups_llm_costs_by_almost_90/ktw16mq/) (in reply to ID 1b92w5o):\nGreat insight, thanks. I did not know about OpenPipe, really clever way to gather training data with 0 effort.\n\n## Comment ID ktw0oll with +4 score by [(None, Reddit, 2024-03-08)](https://www.reddit.com/r/SaaS/comments/1b92w5o/how_i_reduced_our_startups_llm_costs_by_almost_90/ktw0oll/) (in reply to ID 1b92w5o):\n[removed]\n\n### Comment ID ktw1wtc with +2 score by [(pmarks98, Reddit, 2024-03-08)](https://www.reddit.com/r/SaaS/comments/1b92w5o/how_i_reduced_our_startups_llm_costs_by_almost_90/ktw1wtc/) (in reply to ID ktw0oll):\nYeah exactly. Combination of agentic semantic, with some statistical analysis for determining importance using a local and global important algo (i.e. what topics does this user like + what topics is everyone talking about)\n\n#### Comment ID le9sirg with +1 score by [(JokEonE, Reddit, 2024-07-21)](https://www.reddit.com/r/SaaS/comments/1b92w5o/how_i_reduced_our_startups_llm_costs_by_almost_90/le9sirg/) (in reply to ID ktw1wtc):\nWow this sounds really alien to mee (I'm a frontend dev) where could I start reading about this topic? TY\n\n### Comment ID ktw1yec with +1 score by [(pmarks98, Reddit, 2024-03-08)](https://www.reddit.com/r/SaaS/comments/1b92w5o/how_i_reduced_our_startups_llm_costs_by_almost_90/ktw1yec/) (in reply to ID ktw0oll):\nWould love your feedback if you give the app a try - looking for ways to improve it.\n\n## Comment ID ktvlpn7 with +6 score by [(Reddiculer, Reddit, 2024-03-08)](https://www.reddit.com/r/SaaS/comments/1b92w5o/how_i_reduced_our_startups_llm_costs_by_almost_90/ktvlpn7/) (in reply to ID 1b92w5o):\nThis is awesome thank you for sharing! When you say returning pointers to data instead of the data itself, what do you mean by that?\n\n### Comment ID ktvnxwm with +12 score by [(pmarks98, Reddit, 2024-03-08)](https://www.reddit.com/r/SaaS/comments/1b92w5o/how_i_reduced_our_startups_llm_costs_by_almost_90/ktvnxwm/) (in reply to ID ktvlpn7):\nHere's a quick example: \n\nI want to do some LLM processing on the data to order everything based on semantics. Let's say my prompt is something like \"Given these three sentences, order them so that the most \"important\" is first\".   \n\n\nInstead of having the LLM return the ordered strings, it should return a reference to the data (like an id). \n\nSo in this case (with the above prompt), I would pass as input\"id\\_1: <sentence 1>, id\\_2: <sentence 2>, id\\_3: <sentence 3>\". \n\nThe prompt would be adjusted to instruct to return the ordered ids, not the sentences themselves. \n\nThis gives you the benefit of reducing output tokens (often more expensive than input) and reduces the chance that the LLM accidentally doesn't output the sentence verbatim/makes changes to it.\n\n  \nYou can then verify the output by checking that all the output ids actually exist in the input data structure. If one output id is missing or wasn't one of the input ids, you repeat the LLM call saying it failed\n\n#### Comment ID ktw4gq5 with +2 score by [(Reddiculer, Reddit, 2024-03-08)](https://www.reddit.com/r/SaaS/comments/1b92w5o/how_i_reduced_our_startups_llm_costs_by_almost_90/ktw4gq5/) (in reply to ID ktvnxwm):\nGod damn this is genius. Thank you for sharing all of this!\n\n#### Comment ID lar2qro with +1 score by [(No_Entrepreneur7665, Reddit, 2024-06-28)](https://www.reddit.com/r/SaaS/comments/1b92w5o/how_i_reduced_our_startups_llm_costs_by_almost_90/lar2qro/) (in reply to ID ktvnxwm):\nI noticed sometimes if the llm doesnt repeat the sentence - it makes mistakes in ranking the importance for example. Repeating was like reminding the llm, did u notice performance issues?\n\n## Comment ID ktuc1p5 with +2 score by [(Last_Inspector2515, Reddit, 2024-03-07)](https://www.reddit.com/r/SaaS/comments/1b92w5o/how_i_reduced_our_startups_llm_costs_by_almost_90/ktuc1p5/) (in reply to ID 1b92w5o):\nI faced similar issues; OpenPipe's a game-changer indeed.\n\n## Comment ID ktulgmw with +2 score by [(LuckyOne2915, Reddit, 2024-03-08)](https://www.reddit.com/r/SaaS/comments/1b92w5o/how_i_reduced_our_startups_llm_costs_by_almost_90/ktulgmw/) (in reply to ID 1b92w5o):\n🔥! Thank you 😊\n\n## Comment ID ktuwc21 with +2 score by [(am3141, Reddit, 2024-03-08)](https://www.reddit.com/r/SaaS/comments/1b92w5o/how_i_reduced_our_startups_llm_costs_by_almost_90/ktuwc21/) (in reply to ID 1b92w5o):\nThanks for sharing!!\n\n## Comment ID ktvinor with +2 score by [(CrybullyModsSuck, Reddit, 2024-03-08)](https://www.reddit.com/r/SaaS/comments/1b92w5o/how_i_reduced_our_startups_llm_costs_by_almost_90/ktvinor/) (in reply to ID 1b92w5o):\nBanger of a post! Thanks for the insight.\n\n## Comment ID ktvlbxq with +2 score by [(IamJatinbhutani, Reddit, 2024-03-08)](https://www.reddit.com/r/SaaS/comments/1b92w5o/how_i_reduced_our_startups_llm_costs_by_almost_90/ktvlbxq/) (in reply to ID 1b92w5o):\nMicrosoft for startup is giving $5000 credits for startup, you could check it out.\n\n### Comment ID ktw0uxs with +2 score by [(dzigizord, Reddit, 2024-03-08)](https://www.reddit.com/r/SaaS/comments/1b92w5o/how_i_reduced_our_startups_llm_costs_by_almost_90/ktw0uxs/) (in reply to ID ktvlbxq):\nit goes upto 125k\n\n### Comment ID ktvmt8t with +1 score by [(pmarks98, Reddit, 2024-03-08)](https://www.reddit.com/r/SaaS/comments/1b92w5o/how_i_reduced_our_startups_llm_costs_by_almost_90/ktvmt8t/) (in reply to ID ktvlbxq):\nThis is super helpful - thanks!\n\n### Comment ID ku9jbre with +1 score by [(Minimum-Box5103, Reddit, 2024-03-10)](https://www.reddit.com/r/SaaS/comments/1b92w5o/how_i_reduced_our_startups_llm_costs_by_almost_90/ku9jbre/) (in reply to ID ktvlbxq):\nthis is for gpt 3.5 right? not for gpt 4 unless they changed it?\n\n## Comment ID ktvxtkc with +2 score by [(IkHaalHogeCijfers, Reddit, 2024-03-08)](https://www.reddit.com/r/SaaS/comments/1b92w5o/how_i_reduced_our_startups_llm_costs_by_almost_90/ktvxtkc/) (in reply to ID 1b92w5o):\nHow does the pricing work? Are you only paying for the API calls or do you also pay for deployment of your finetuned model (i.e. can you scale to zero)?\n\n### Comment ID ktw1zqq with +3 score by [(pmarks98, Reddit, 2024-03-08)](https://www.reddit.com/r/SaaS/comments/1b92w5o/how_i_reduced_our_startups_llm_costs_by_almost_90/ktw1zqq/) (in reply to ID ktvxtkc):\nYeah just inference + training costs. No hosting costs\n\n#### Comment ID ktw6tdx with +4 score by [(IkHaalHogeCijfers, Reddit, 2024-03-08)](https://www.reddit.com/r/SaaS/comments/1b92w5o/how_i_reduced_our_startups_llm_costs_by_almost_90/ktw6tdx/) (in reply to ID ktw1zqq):\nI take it they don't keep your endpoints warm all the time then. Do you have any insights in the cold start times? You likely don't care about cold start times since I figure you do batch inference once a day, but i am trying to find out how viable this is for real time inference (lets say, is sub 5 seconds latency (startup+inference) for 1k tokens achievable?).\n\n## Comment ID ktw9bmk with +2 score by [(uwedreiss, Reddit, 2024-03-08)](https://www.reddit.com/r/SaaS/comments/1b92w5o/how_i_reduced_our_startups_llm_costs_by_almost_90/ktw9bmk/) (in reply to ID 1b92w5o):\nYes, I always wondered about the running costs and OpenAI bills. Seems that you made the right decision in only sending API requests when necessary! Great job. I'd love to read more about that side of \"AI startups\"\n\n### Comment ID ktxw6zz with +1 score by [(pmarks98, Reddit, 2024-03-08)](https://www.reddit.com/r/SaaS/comments/1b92w5o/how_i_reduced_our_startups_llm_costs_by_almost_90/ktxw6zz/) (in reply to ID ktw9bmk):\nI'm on Twitter!\n\n#### Comment ID kw64u4l with +1 score by [(uwedreiss, Reddit, 2024-03-23)](https://www.reddit.com/r/SaaS/comments/1b92w5o/how_i_reduced_our_startups_llm_costs_by_almost_90/kw64u4l/) (in reply to ID ktxw6zz):\nWhat’s your handle?\n\n## Comment ID ktwdzm4 with +2 score by [(Thin_Adeptness_356, Reddit, 2024-03-08)](https://www.reddit.com/r/SaaS/comments/1b92w5o/how_i_reduced_our_startups_llm_costs_by_almost_90/ktwdzm4/) (in reply to ID 1b92w5o):\nThanks for sharing. Was it expensive to fine-tune on OpenPipe?\n\n### Comment ID ktxw2x9 with +1 score by [(pmarks98, Reddit, 2024-03-08)](https://www.reddit.com/r/SaaS/comments/1b92w5o/how_i_reduced_our_startups_llm_costs_by_almost_90/ktxw2x9/) (in reply to ID ktwdzm4):\nNo - cost about $100\n\n## Comment ID ktwlbns with +2 score by [(teknopreneur, Reddit, 2024-03-08)](https://www.reddit.com/r/SaaS/comments/1b92w5o/how_i_reduced_our_startups_llm_costs_by_almost_90/ktwlbns/) (in reply to ID 1b92w5o):\nNice advice\n\n## Comment ID ktwpvgx with +2 score by [(itayb1, Reddit, 2024-03-08)](https://www.reddit.com/r/SaaS/comments/1b92w5o/how_i_reduced_our_startups_llm_costs_by_almost_90/ktwpvgx/) (in reply to ID 1b92w5o):\nAmazing read! Working with GPT nowadays in my daily job we encounter a lot of the challenges you’ve mentioned. Thank you for sharing!\n\n## Comment ID ktwrc20 with +2 score by [(CheapBison1861, Reddit, 2024-03-08)](https://www.reddit.com/r/SaaS/comments/1b92w5o/how_i_reduced_our_startups_llm_costs_by_almost_90/ktwrc20/) (in reply to ID 1b92w5o):\nWow, that's some savvy cost-cutting! Been there, done that.\n\n## Comment ID ktwt4y6 with +2 score by [(itz_nicoo, Reddit, 2024-03-08)](https://www.reddit.com/r/SaaS/comments/1b92w5o/how_i_reduced_our_startups_llm_costs_by_almost_90/ktwt4y6/) (in reply to ID 1b92w5o):\nVery insightful. Thanks for sharing.\n\n## Comment ID ktwtjpc with +2 score by [(smthamazing, Reddit, 2024-03-08)](https://www.reddit.com/r/SaaS/comments/1b92w5o/how_i_reduced_our_startups_llm_costs_by_almost_90/ktwtjpc/) (in reply to ID 1b92w5o):\nThanks, this is insightful!\n\nTo clarify my understanding: you recorded 50000 calls made by real users, fine-tuned Mistral on those inputs and outputs, and then did an A/B test to compare its performance to GPT - is that correct?\n\n### Comment ID ktxlmf9 with +1 score by [(pmarks98, Reddit, 2024-03-08)](https://www.reddit.com/r/SaaS/comments/1b92w5o/how_i_reduced_our_startups_llm_costs_by_almost_90/ktxlmf9/) (in reply to ID ktwtjpc):\npretty much yeah!\n\n## Comment ID ktx06ml with +2 score by [(greencandlevandal, Reddit, 2024-03-08)](https://www.reddit.com/r/SaaS/comments/1b92w5o/how_i_reduced_our_startups_llm_costs_by_almost_90/ktx06ml/) (in reply to ID 1b92w5o):\nI love this idea and will definitely be using it\n\n## Comment ID ktxllt8 with +2 score by [(None, Reddit, 2024-03-08)](https://www.reddit.com/r/SaaS/comments/1b92w5o/how_i_reduced_our_startups_llm_costs_by_almost_90/ktxllt8/) (in reply to ID 1b92w5o):\n[deleted]\n\n### Comment ID ktxvwfy with +1 score by [(pmarks98, Reddit, 2024-03-08)](https://www.reddit.com/r/SaaS/comments/1b92w5o/how_i_reduced_our_startups_llm_costs_by_almost_90/ktxvwfy/) (in reply to ID ktxllt8):\nBoth for cost and performance. Way cheaper and just better performing. Also gives me the flexibility to not be locked into OpenAI\n\n## Comment ID ktxqq1j with +2 score by [(Crazy-Mission-7920, Reddit, 2024-03-08)](https://www.reddit.com/r/SaaS/comments/1b92w5o/how_i_reduced_our_startups_llm_costs_by_almost_90/ktxqq1j/) (in reply to ID 1b92w5o):\nHi u/pmarks98 I really enjoyed your post. I would like to learn how to do this stuff. I have a fairly good programming background. What resource do you recommend for someone like me to get up to speed with building an app that uses an open source model like you did? Thank you.\n\n### Comment ID ku0o48w with +1 score by [(pmarks98, Reddit, 2024-03-09)](https://www.reddit.com/r/SaaS/comments/1b92w5o/how_i_reduced_our_startups_llm_costs_by_almost_90/ku0o48w/) (in reply to ID ktxqq1j):\nFeel free to reach out on twitter - glad to chat more [piersonmarks](https://twitter.com/piersonmarks)\n\n## Comment ID ktxtl50 with +2 score by [(Zenai, Reddit, 2024-03-08)](https://www.reddit.com/r/SaaS/comments/1b92w5o/how_i_reduced_our_startups_llm_costs_by_almost_90/ktxtl50/) (in reply to ID 1b92w5o):\nThis is exactly why the open model is going to win out. There are too many competing models it's clear that they're all racing in the same direction which makes it a commodity.\n\n## Comment ID ktzq5n3 with +2 score by [(biztactix, Reddit, 2024-03-08)](https://www.reddit.com/r/SaaS/comments/1b92w5o/how_i_reduced_our_startups_llm_costs_by_almost_90/ktzq5n3/) (in reply to ID 1b92w5o):\nThanks for the info... Didn't even know something for this existed... But I should know there is always an app for that!\n\n### Comment ID ku05kuz with +1 score by [(pmarks98, Reddit, 2024-03-09)](https://www.reddit.com/r/SaaS/comments/1b92w5o/how_i_reduced_our_startups_llm_costs_by_almost_90/ku05kuz/) (in reply to ID ktzq5n3):\nThere's always an app for it! Except there wasn't for Jellypod... so I built it lol\n\n## Comment ID ku36sw5 with +2 score by [(Programmer_Virtual, Reddit, 2024-03-09)](https://www.reddit.com/r/SaaS/comments/1b92w5o/how_i_reduced_our_startups_llm_costs_by_almost_90/ku36sw5/) (in reply to ID 1b92w5o):\nGreat case study. Cost concerns related to LLMops is going to grow over time amongst companies. \n\nDid you experience significant deviations in model accuracy?\n\n### Comment ID ku3cppy with +1 score by [(pmarks98, Reddit, 2024-03-09)](https://www.reddit.com/r/SaaS/comments/1b92w5o/how_i_reduced_our_startups_llm_costs_by_almost_90/ku3cppy/) (in reply to ID ku36sw5):\n100%, and no it performed equally well. The usage of the model at each step was very narrow and I had response validation with regex (where possible). So essentially there are no failure because if the model responds with something unexpected, I retry. \n\n  \nFor the actual summarization task you can't really test except with evals, but that's okay. I use customer feedback in the app to ensure it's summaries are accurate and high quality\n\n#### Comment ID ku3is26 with +1 score by [(Programmer_Virtual, Reddit, 2024-03-09)](https://www.reddit.com/r/SaaS/comments/1b92w5o/how_i_reduced_our_startups_llm_costs_by_almost_90/ku3is26/) (in reply to ID ku3cppy):\nExcellent, I appreciate your perspective.\n\n## Comment ID kuaxlxg with +2 score by [(marcokatig, Reddit, 2024-03-11)](https://www.reddit.com/r/SaaS/comments/1b92w5o/how_i_reduced_our_startups_llm_costs_by_almost_90/kuaxlxg/) (in reply to ID 1b92w5o):\nThis is great!  I checked out your website.  It looks like you've made amazing progress!  Is the IOS App only available in limited countries?\n\n### Comment ID kub783h with +1 score by [(pmarks98, Reddit, 2024-03-11)](https://www.reddit.com/r/SaaS/comments/1b92w5o/how_i_reduced_our_startups_llm_costs_by_almost_90/kub783h/) (in reply to ID kuaxlxg):\nThanks! Some big ndw features being released this week.\n\nYes - right now the iOS app  is only in a few countries so far\n\n#### Comment ID kuc0as2 with +1 score by [(marcokatig, Reddit, 2024-03-11)](https://www.reddit.com/r/SaaS/comments/1b92w5o/how_i_reduced_our_startups_llm_costs_by_almost_90/kuc0as2/) (in reply to ID kub783h):\nThat's a bummer!  I signed up for your newsletter.  Looking forward to when you expand Jellypod worldwide.\n\nThe information you shared was really helpful by the way!  Had a meeting with our dev team today and we're looking into this now for for our platform.\n\n## Comment ID kv9kzz6 with +1 score by [(Fabulous_Limit_798, Reddit, 2024-03-17)](https://www.reddit.com/r/SaaS/comments/1b92w5o/how_i_reduced_our_startups_llm_costs_by_almost_90/kv9kzz6/) (in reply to ID 1b92w5o):\n![img](avatar_exp|169429696|bravo)\n\n## Comment ID kvqzmk5 with +1 score by [(MousseOne330, Reddit, 2024-03-20)](https://www.reddit.com/r/SaaS/comments/1b92w5o/how_i_reduced_our_startups_llm_costs_by_almost_90/kvqzmk5/) (in reply to ID 1b92w5o):\nMy question is (its a stupid question but i really need to know that) where can i get the data to fine tune my model??\n\n### Comment ID kyb2kat with +1 score by [(OddFood5483, Reddit, 2024-04-06)](https://www.reddit.com/r/SaaS/comments/1b92w5o/how_i_reduced_our_startups_llm_costs_by_almost_90/kyb2kat/) (in reply to ID kvqzmk5):\nI believe that highly depends on your usecase and what you are trying to build. In his case he was clever to use his customer's requests and he's existing system's(OpenAI API) responses to create rows of data. If you already have established users you can find a similar way. If not then try other means. If your application is too general you don't need to fine-tune. depends\n\n## Comment ID kwxq5zc with +1 score by [(wazdesign, Reddit, 2024-03-28)](https://www.reddit.com/r/SaaS/comments/1b92w5o/how_i_reduced_our_startups_llm_costs_by_almost_90/kwxq5zc/) (in reply to ID 1b92w5o):\nInteresting, thanks for sharing\n\n## Comment ID l38l027 with +1 score by [(jeffrod2, Reddit, 2024-05-09)](https://www.reddit.com/r/SaaS/comments/1b92w5o/how_i_reduced_our_startups_llm_costs_by_almost_90/l38l027/) (in reply to ID 1b92w5o):\nI'm building an LLM cost management solution and would love to partner with folks who want to track and optimize their LLM usage and cost. Please email hello@greenscale.ai.\n\n## Comment ID lccr5cn with +1 score by [(anonalist, Reddit, 2024-07-09)](https://www.reddit.com/r/SaaS/comments/1b92w5o/how_i_reduced_our_startups_llm_costs_by_almost_90/lccr5cn/) (in reply to ID 1b92w5o):\nThis is sick and I learned a bunch. But I'm confused-- was this your process:  \n  \n1. You first focused on RAG to see if that could be optimized (Where you mentioned LangChain papers). You gave up because it was still too expensive with just trying to optimize RAG.  \n2. Then you found that by fine-tuning a model like LLaMa you could keep the performance and get to the token cost you needed.\n\nIs that right?\n\nWondering if you're still using RAG, and if you are, how you're indexing the content the Agent sees in the retrieval-- this seems really application specific -- perhaps you could share some tips?\n\nThanks a ton!\n\n## Comment ID ldm5usm with +1 score by [(corbt, Reddit, 2024-07-17)](https://www.reddit.com/r/SaaS/comments/1b92w5o/how_i_reduced_our_startups_llm_costs_by_almost_90/ldm5usm/) (in reply to ID 1b92w5o):\nWoah, this is so cool! I'm one of the co-founders of OpenPipe and just saw this review, thanks to the OP for posting it. Happy to answer any questions about how the service works. Also, we've cut our prices an additional 60%+ since this was posted, along with making the models stronger and latency lower!\n\n## Comment ID le9s4ip with +1 score by [(JokEonE, Reddit, 2024-07-21)](https://www.reddit.com/r/SaaS/comments/1b92w5o/how_i_reduced_our_startups_llm_costs_by_almost_90/le9s4ip/) (in reply to ID 1b92w5o):\nDamn this is amazing I would love to see a comprehensive post/course or something because I was thinking on building something like that. Thanks a lot for sharing!!! <3\n\n## Comment ID le9s8vr with +1 score by [(JokEonE, Reddit, 2024-07-21)](https://www.reddit.com/r/SaaS/comments/1b92w5o/how_i_reduced_our_startups_llm_costs_by_almost_90/le9s8vr/) (in reply to ID 1b92w5o):\nDamn this is amazing I would love to see a comprehensive post/course or something because I was thinking on building something like that. Thanks a lot for sharing!!! <3\n\n## Comment ID lql8n0m with +1 score by [(AchillesFirstStand, Reddit, 2024-10-06)](https://www.reddit.com/r/SaaS/comments/1b92w5o/how_i_reduced_our_startups_llm_costs_by_almost_90/lql8n0m/) (in reply to ID 1b92w5o):\nCan you tell me how you got the LLM to return pointers to data, i.e. what was your prompt / setup?\n\n## Comment ID lthe1xm with +1 score by [(MostCamp2015, Reddit, 2024-10-24)](https://www.reddit.com/r/SaaS/comments/1b92w5o/how_i_reduced_our_startups_llm_costs_by_almost_90/lthe1xm/) (in reply to ID 1b92w5o):\nAwesome insight OP!  \nWhat did you use to build your website? The UI is very clean/fluid.\n\n## Comment ID mdbjeqm with +1 score by [(masterKova, Reddit, 2025-02-17)](https://www.reddit.com/r/SaaS/comments/1b92w5o/how_i_reduced_our_startups_llm_costs_by_almost_90/mdbjeqm/) (in reply to ID 1b92w5o):\nNot all prompts are needed top tier model \nThere is a GitHub package that chose the model based on the complexity of the prompt. \nNadir-LLM\nWww.GitHub.com/doramirdor/nadir\n\n## Comment ID ktvlko2 with +1 score by [(Usernamecheckout101, Reddit, 2024-03-08)](https://www.reddit.com/r/SaaS/comments/1b92w5o/how_i_reduced_our_startups_llm_costs_by_almost_90/ktvlko2/) (in reply to ID 1b92w5o):\nGood stuff, the thing about ai is that by yourself you can’t do much and you really need a PhD in ai to build anything meaningful yourself so the alternative is building your business based on api calls. Back in the days, Twitter, Facebook open their apis and people build third party apps on top.. but you can’t scale because it’s too costly and they could kill your business any day.. so the aim is build a small app, hope it’s make some good money but not an actual business out of it?\n\n### Comment ID ktvofa1 with +4 score by [(pmarks98, Reddit, 2024-03-08)](https://www.reddit.com/r/SaaS/comments/1b92w5o/how_i_reduced_our_startups_llm_costs_by_almost_90/ktvofa1/) (in reply to ID ktvlko2):\nI partially agree but I don't think a PhD is necessary. Or even a bachelors honestly (I am a huge pro-college person though). \n\nEngineers always build on layers of abstraction. If you needed to know everything to build a piece of software, no one would. That would be like saying you need to know x86 assembly to program in C/C++.  All you need to know is the interfect provided.\n\nAI is the same. You either build on REST API calls to an AI hosted in the cloud, an API call to a locally running model, etc. No need to rebuild something that others do way better than you could individually\n\n#### Comment ID ktvpmcn with +2 score by [(dimsumham, Reddit, 2024-03-08)](https://www.reddit.com/r/SaaS/comments/1b92w5o/how_i_reduced_our_startups_llm_costs_by_almost_90/ktvpmcn/) (in reply to ID ktvofa1):\nStrongly agree. I'm a dumb BCom grad with 0 coding background until last year and I'm building useful stuff with AI. \n\nIf you can read documentation and have the patience to iterate through errors, you'd be amazed at what you can build with 0 technical background.",
      "# Post ID 1dky8my: OpenPipe Mixture of Agents: Outperform GPT-4 at 1/25th the Cost - OpenPipe with +37 score by [(weeblay, Reddit, 2024-06-21)](https://www.reddit.com/r/LocalLLaMA/comments/1dky8my/openpipe_mixture_of_agents_outperform_gpt4_at/)\n\n\n## Comment ID l9l7k2b with +17 score by [(DeltaSqueezer, Reddit, 2024-06-21)](https://www.reddit.com/r/LocalLLaMA/comments/1dky8my/openpipe_mixture_of_agents_outperform_gpt4_at/l9l7k2b/) (in reply to ID 1dky8my):\nInteresting. It would be nice if they released their fine tuned models so that it can be independently reviewed.\n\n## Comment ID l9l8ydb with +6 score by [(Open_Channel_8626, Reddit, 2024-06-21)](https://www.reddit.com/r/LocalLLaMA/comments/1dky8my/openpipe_mixture_of_agents_outperform_gpt4_at/l9l8ydb/) (in reply to ID 1dky8my):\nPrompt 2 to reason about the previous answers is interesting\n\n## Comment ID l9lfk5a with +5 score by [(MrVodnik, Reddit, 2024-06-21)](https://www.reddit.com/r/LocalLLaMA/comments/1dky8my/openpipe_mixture_of_agents_outperform_gpt4_at/l9lfk5a/) (in reply to ID 1dky8my):\nAm I reading this chart correctly? GPT-4 Turbo has 65% win rate against GPT-4 Turbo?\n\n### Comment ID l9ln8av with +6 score by [(vasileer, Reddit, 2024-06-21)](https://www.reddit.com/r/LocalLLaMA/comments/1dky8my/openpipe_mixture_of_agents_outperform_gpt4_at/l9ln8av/) (in reply to ID l9lfk5a):\nI think it is MoA GPT4-Turbo with 65% \n\nhttps://preview.redd.it/vmugyq2uuw7d1.png?width=784&format=png&auto=webp&s=4d783633afc7e7b72abba24c372d75fdf857f4e9\n\n#### Comment ID l9lxoql with +5 score by [(Single_Ring4886, Reddit, 2024-06-21)](https://www.reddit.com/r/LocalLLaMA/comments/1dky8my/openpipe_mixture_of_agents_outperform_gpt4_at/l9lxoql/) (in reply to ID l9ln8av):\nI find most interesting that LLM judge in this graph reflect my own mental map of how models are \"strong\"!!! Which is different compared to publicly known benchmark results.\n\n## Comment ID l9l82do with +4 score by [(flotos, Reddit, 2024-06-21)](https://www.reddit.com/r/LocalLLaMA/comments/1dky8my/openpipe_mixture_of_agents_outperform_gpt4_at/l9l82do/) (in reply to ID 1dky8my):\nThat is an interesting strategy for GPT-4 MoA, thanks for the blog.  \nI'm more skeptical about the fine-tuned llama-8b results, as it all depends on what was the generated dataset using gpt-4 MoA. If it was related closely to the benchmark tasks, of course the llama-8b would outperform. Do you have more details on that ?\n\n## Comment ID l9lapa2 with +2 score by [(Kathane37, Reddit, 2024-06-21)](https://www.reddit.com/r/LocalLLaMA/comments/1dky8my/openpipe_mixture_of_agents_outperform_gpt4_at/l9lapa2/) (in reply to ID 1dky8my):\nFeel like comparing orange and apple \nMaybe I did not really understood the blog post but yes flow architecture improve models response quality but what are they comparing vanilla GPT-4 vs flow llama3 ?",
      "# Post ID 1iab1oe: Best frameworks for fine-tuning models—what’s everyone using? with +17 score by [(Vivid-Entertainer752, Reddit, 2025-01-26)](https://www.reddit.com/r/LocalLLaMA/comments/1iab1oe/best_frameworks_for_finetuning_modelswhats/)\nHey everyone, I’m new to fine-tuning LLMs/SLMs and trying to figure out what tools people usually use for this. From what I’ve seen so far, here are some options:\n\n1. **Hugging Face TRL** (e.g., SFT, PPO, etc.)\n\n2. **Unsloth AI**\n\n3. **No-code tools** like Together AI, Predibase, FinetuneDB\n\n4. Any other thing?\n\n## Comment ID m997xde with +5 score by [(7pot, Reddit, 2025-01-26)](https://www.reddit.com/r/LocalLLaMA/comments/1iab1oe/best_frameworks_for_finetuning_modelswhats/m997xde/) (in reply to ID 1iab1oe):\n[axolotl](https://github.com/axolotl-ai-cloud/axolotl)?\n\n### Comment ID m99fabt with +3 score by [(SuperChewbacca, Reddit, 2025-01-26)](https://www.reddit.com/r/LocalLLaMA/comments/1iab1oe/best_frameworks_for_finetuning_modelswhats/m99fabt/) (in reply to ID m997xde):\nThat's what I used.  I needed multi GPU support.  Unsloth is still in beta for multi GPU, but it's very nice.\n\n### Comment ID m9bllgj with +1 score by [(toothpastespiders, Reddit, 2025-01-26)](https://www.reddit.com/r/LocalLLaMA/comments/1iab1oe/best_frameworks_for_finetuning_modelswhats/m9bllgj/) (in reply to ID m997xde):\nYep, I like unsloth but axolotl's been my default for a long time now. With 30b'ish and above I really just want multi-gpu. Though I also really enjoy reading the unsloth blog posts and other updates.\n\n### Comment ID m9f7x8b with +1 score by [(Vivid-Entertainer752, Reddit, 2025-01-27)](https://www.reddit.com/r/LocalLLaMA/comments/1iab1oe/best_frameworks_for_finetuning_modelswhats/m9f7x8b/) (in reply to ID m997xde):\nThanks, this is new to me\n\n## Comment ID m9a3l0e with +3 score by [(DinoAmino, Reddit, 2025-01-26)](https://www.reddit.com/r/LocalLLaMA/comments/1iab1oe/best_frameworks_for_finetuning_modelswhats/m9a3l0e/) (in reply to ID 1iab1oe):\nTorchtune \n\nhttps://pytorch.org/torchtune/stable/overview.html\n\n### Comment ID m9f80xk with +1 score by [(Vivid-Entertainer752, Reddit, 2025-01-27)](https://www.reddit.com/r/LocalLLaMA/comments/1iab1oe/best_frameworks_for_finetuning_modelswhats/m9f80xk/) (in reply to ID m9a3l0e):\nare there advantages to use Torchtune? most people I met usually used unsloath\n\n## Comment ID m9b5uam with +2 score by [(Felladrin, Reddit, 2025-01-26)](https://www.reddit.com/r/LocalLLaMA/comments/1iab1oe/best_frameworks_for_finetuning_modelswhats/m9b5uam/) (in reply to ID 1iab1oe):\nFor those in MacOS, it's also possible to do [Fine-Tuning with MLX LM](https://github.com/ml-explore/mlx-examples/blob/main/llms/mlx_lm/LORA.md) (LoRA, QLoRA and Full).\n\n## Comment ID m9gwuvy with +2 score by [(Hero-World, Reddit, 2025-01-27)](https://www.reddit.com/r/LocalLLaMA/comments/1iab1oe/best_frameworks_for_finetuning_modelswhats/m9gwuvy/) (in reply to ID 1iab1oe):\n[https://github.com/hiyouga/LLaMA-Factory](https://github.com/hiyouga/LLaMA-Factory)  \nI used this tool with gui\n\n## Comment ID m9dwixm with +1 score by [(Fuzzdump, Reddit, 2025-01-27)](https://www.reddit.com/r/LocalLLaMA/comments/1iab1oe/best_frameworks_for_finetuning_modelswhats/m9dwixm/) (in reply to ID 1iab1oe):\n[Kiln](https://github.com/Kiln-AI/Kiln) is new and interesting\n\n### Comment ID m9f84m2 with +1 score by [(Vivid-Entertainer752, Reddit, 2025-01-27)](https://www.reddit.com/r/LocalLLaMA/comments/1iab1oe/best_frameworks_for_finetuning_modelswhats/m9f84m2/) (in reply to ID m9dwixm):\nIt seems that generating synthetic data is interesting!",
      "# Post ID 1f39k76: LLM Fine-tuning best practices around model selection (OpenAI vs Open Source, Large vs Small, Hparams tweaking). Learned over the course of tuning thousands of models! with +1 score by [(billmalarky, Reddit, 2024-08-28)](https://www.reddit.com/r/ArtificialInteligence/comments/1f39k76/llm_finetuning_best_practices_around_model/)\nHey Reddit! I've been working on a series capturing the industry best practices in producing fine-tuned LLMs. Our first post covered training data preparation, and for this one we'll be talking about how to select your base model and hyperparameters, covering both closed and open models as well as models of different sizes. This was created in collaboration with Kyle Corbitt at OpenPipe (YC23)\n\nWe cover:\n\n- Proprietary (well mainly OpenAI) vs open source models: OAI has really great performance w/ relatively small training datasets but you hit a ceiling on max performance sooner than you do w/ open source models. We're not sure exactly what drives this, but it very well could be that OAI has made some technical decisions under the hood that leans towards this \"Red Mage\" approach in order to serve a broader audience (ie users w/ less training data).\n\n- Large vs Small Models: Main thing is larger models typically let you \"get away\" with less training data all things equal. But where possible you want to deploy the smallest model that achieves acceptable performance in prod. Smaller is less \"costly\" across a variety of dimensions (not just price).\n\n- Hyperparameter Tuning: We cover this in some detail for those who are curious, but to be frank hparam tuning is generally not a high ROI use of your engineering time. Plow that time into dataset curation instead :)\n\nHope this is useful for folks! Happy to answer questions here (and hope to learn something new too)\n\nhttps://openpipe.ai/blog/fine-tuning-best-practices-chapter-2-models\n\n\n\n## Comment ID lki29e1 with +1 score by [(knipthog, Reddit, 2024-08-29)](https://www.reddit.com/r/ArtificialInteligence/comments/1f39k76/llm_finetuning_best_practices_around_model/lki29e1/) (in reply to ID 1f39k76):\nAwesome post! I totally agree with focusing on dataset curation over hyperparameter tuning for the best returns. For efficiently sifting through literature and getting reliable, cited data, I've been using Afforaiit's a game changer in speeding up my research workflow. Definitely worth a look if you're deep into this space!",
      "# Post ID 18wasf8: If you think open-source models will beat GPT-4 this year, you're wrong. I totally agree with this. with +312 score by [(CeFurkan, Reddit, 2024-01-01)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/)\n\n\n## Comment ID kfwnxt3 with +668 score by [(az226, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfwnxt3/) (in reply to ID 18wasf8):\nDon’t disagree but fuck this guy. He lied that his startup had a product and invite people to join a beta but there was no beta or product. Just a ruse to show investors how many had signed up so he could build the product. Scumbag.\n\n### Comment ID kfy1ekz with +128 score by [(MannowLawn, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfy1ekz/) (in reply to ID kfwnxt3):\nDon’t know the guy or story but now fuck this guy\n\n#### Comment ID kfype7y with +59 score by [(Severin_Suveren, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfype7y/) (in reply to ID kfy1ekz):\nAlso I kind of disagree with him. Well, sort of. He is correct in what he is saying, but his assertion that open-source models can't beat GPT-4 is just wrong. Mistral have promised a model equal to or better than GPT-4 in 2024, and given their track-record, I'm inclined to believe them. Meta have given similar promises with their LLaMa-3 release, which is expected to drop probably Q1 2024.\n\nSo to me, 2024 looks very promising with multiple actors aiming to try and catch up to OpenAI\n\n### Comment ID kfx5h5w with +27 score by [(wwwmaster1, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfx5h5w/) (in reply to ID kfwnxt3):\nLink to story?\n\n### Comment ID kfyehqp with +3 score by [(rookan, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfyehqp/) (in reply to ID kfwnxt3):\nText to UI startup?\n\n### Comment ID kg2cwwd with +3 score by [(alvisanovari, Reddit, 2024-01-03)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kg2cwwd/) (in reply to ID kfwnxt3):\nDisagree and fuck this guy\n\n### Comment ID kfxg9f6 with +22 score by [(None, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfxg9f6/) (in reply to ID kfwnxt3):\nThat's the reason Elizabeth Holmes went to jail.\n\n#### Comment ID kfxq8s1 with +52 score by [(njtrafficsignshopper, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfxq8s1/) (in reply to ID kfxg9f6):\nI thought she went to jail because she told people she could diagnose illnesses she couldn't diagnose, and they relied on those diagnoses for healthcare decisions.\n\n#### Comment ID kfxy1md with +8 score by [(Due-Ad-7308, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfxy1md/) (in reply to ID kfxg9f6):\nElizabeth Holmes would have been more like if Bernard launched a cybersecurity platform that flagged or ignored network activity based on a random number generator and spent the remainder of his career hiding that fact and getting it into as many hospitals and pharmacies as possible.\n\nHolmes and Bernard are both vaporware liars, but Holmes is on a completely different level\n\n### Comment ID kfzvs85 with +2 score by [(ab2377, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfzvs85/) (in reply to ID kfwnxt3):\ndidnt know this but thanks a lot for sharing!\n\n### Comment ID kfzhth5 with +2 score by [(Koalateka, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfzhth5/) (in reply to ID kfwnxt3):\nI totally agree with \"Fuck this guy\"\n\n## Comment ID kfwnh89 with +339 score by [(PwanaZana, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfwnh89/) (in reply to ID 18wasf8):\nI imagine an important point not mentioned here is that one instance of GPT4 runs on a 100k$ machine (super approx number).\n\nAn important part of open source, or at least open models, is that it can run on consumer hardware (for customizability, privacy, etc). It's like saying that cars can't beat semi truck, which is true, but cars are a lot more flexible.\n\n### Comment ID kfww4yj with +139 score by [(field_marzhall, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfww4yj/) (in reply to ID kfwnh89):\nThis is where the big difference lies. It will be hard or impossible for an open-source solution to beat a corporate solution when it comes to a cloud service like Chatgpt. However, when it comes to a solution that works on nearly all hardware and is accessible offline open source can get ahead.\n\n#### Comment ID kfxahy1 with +67 score by [(my_aggr, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfxahy1/) (in reply to ID kfww4yj):\nYou're also missing the point that we are essentially being subsidized to use chatgpt by Microsoft credits.\n\nThe conversation is going to be very different when you have to pay the real price of tokens.\n\n#### Comment ID kfy3skt with +11 score by [(iamapizza, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfy3skt/) (in reply to ID kfww4yj):\nIs it not possible to have a distributed-work type setup, similar to Folding@Home?  Slower but many people can contribute tiny bits on normal commodity hardware.\n\n#### Comment ID kfyfiel with +10 score by [(keepthepace, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfyfiel/) (in reply to ID kfww4yj):\n> It will be hard or impossible for an open-source solution to beat a corporate solution when it comes to a cloud service like Chatgpt\n\nWell, there is always the solution to join [The Horde](https://stablehorde.net/) today!\n\n(crowd-pooled GPUs with a reward system)\n\nSETI@Home and Folding@Home proved that crowd pooling can provide with a huge cluster. Harder to use, yes, but bigger than what most money can buy.\n\n#### Comment ID kfwx64i with +55 score by [(Biggest_Cans, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfwx64i/) (in reply to ID kfww4yj):\nYep, you can get REAL close to GPT-4 at 1/100th that cost.\n\n#### Comment ID kfxl1s0 with +3 score by [(Dynamics_20, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfxl1s0/) (in reply to ID kfww4yj):\nThis seems sensible comment\n\n#### Comment ID kfy5b6c with +3 score by [(Mobile_Ad9119, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfy5b6c/) (in reply to ID kfww4yj):\nI’m hoping this is where Apple is headed. Their big thing is privacy and they’d sell MacBooks like no tomorrow if they promoted a private llm. At the same time they could go with connecting the ecosystem with AI. Or maybe both. They’d blow up if they could do both.\n\n#### Comment ID kfykxbw with +2 score by [(Lulukassu, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfykxbw/) (in reply to ID kfww4yj):\nI wish we did have at least one big open source project working on a commercial scale.\n\nSomeone gearing up to try to compete with NOTopenAI etc, without the censorship and politics.\n\n#### Comment ID kgfn6co with +1 score by [(millhouse056, Reddit, 2024-01-05)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kgfn6co/) (in reply to ID kfww4yj):\nThat's exactly what OpenAI wants\n\n### Comment ID kfxh4vx with +26 score by [(None, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfxh4vx/) (in reply to ID kfwnh89):\nLocal LLM is like 'quasi-Monte Carlo method' - sure, it's not as good, but it's good enough. For the sake of privacy and customization, many people will choose this route.\n\n### Comment ID kfz0spu with +7 score by [(Natural_Precision, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfz0spu/) (in reply to ID kfwnh89):\nI think $100k is probably a super optimistic number too, not just approximate. GPT4 is rumoured to have more than 1 trillion parameters, which would require 2TB of VRAM even at a BF16 precision just to hold the model. That is 25 A100s -  probably more like 30 by the time you leave some space for inference data. So that's $600k before you even look at the rest of the computers, networking, etc etc.\n\nThat open source models can even come close on a home PC costing $2000 is incredibly impressive (or just proves that gpt-4 is seriously wasteful).\n\n#### Comment ID kg06pfu with +2 score by [(PwanaZana, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kg06pfu/) (in reply to ID kfz0spu):\nOof, that's a lot of dinero. I meant, chat GPT4 does not run one instance on either a home computer, nor on a 100 million dollar supercomputer.\n\nAnd certainly GPT4 is wasteful, but the most powerful/best anything is. It's useful as a model that optimizes for performance only, not caring about price, nor about battery life (which'll be super relevant for wearable LLMs).\n\n#### Comment ID kg26l49 with +2 score by [(Ok-Kaleidoscope5627, Reddit, 2024-01-03)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kg26l49/) (in reply to ID kfz0spu):\nIt's hard to make a comparison because they're probably using custom hardware and chips. And if they aren't already, you can bet that they will be soon.\n\n#### Comment ID kg90k4m with +1 score by [(None, Reddit, 2024-01-04)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kg90k4m/) (in reply to ID kfz0spu):\nThe wastefulness is a problem we think about a lot.\n\n### Comment ID kfxbvno with +5 score by [(justgetoffmylawn, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfxbvno/) (in reply to ID kfwnh89):\nPartially true, but since I don't have a beefy GPU, I'll usually spin up an A100 on Colab if I want to try open source. That may not be GPT4 levels, but I think it gives 80GB of VRAM.\n\nI think there's still a lot of room to refine things, and we'll see smaller models being more efficient with fewer parameters, while VRAM also continues to grow. But it may take a couple years because fabs cannot keep up with software.\n\n#### Comment ID kfyhbvg with +2 score by [(Amgadoz, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfyhbvg/) (in reply to ID kfxbvno):\nNope. Colab only has the 40GB version of A100.\n\n### Comment ID kfx6j09 with +14 score by [(stepanogil, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfx6j09/) (in reply to ID kfwnh89):\nexactly. gpt4 is 1.7 trillion param which is 25x bigger than the biggest oss models which maxes out at 70b.\n\n#### Comment ID kfx7c16 with +11 score by [(mpasila, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfx7c16/) (in reply to ID kfx6j09):\nsource?\n\n#### Comment ID kfxm8ae with +3 score by [(Infinite100p, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfxm8ae/) (in reply to ID kfx6j09):\nFalcon is 180b. Isn't it OS?\n\n#### Comment ID kfy90vt with +2 score by [(squareOfTwo, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfy90vt/) (in reply to ID kfx6j09):\nWrong. There are way bigger models than 70b !\n\n### Comment ID kfycb56 with +5 score by [(ThisGonBHard, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfycb56/) (in reply to ID kfwnh89):\n>I imagine an important point not mentioned here is that one instance of GPT4 runs on a 100k$ machine (super approx number).\n\nEven at MSRP of 10k (and real price seems to be 50k), you need over 26 H100 to fit in a 2TB model in FP16.\n\nThat server is probably half a million to a million.\n\n#### Comment ID kg07b0h with +1 score by [(PwanaZana, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kg07b0h/) (in reply to ID kfycb56):\nThanks for the info!\n\n### Comment ID kfyfrzs with +3 score by [(MINIMAN10001, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfyfrzs/) (in reply to ID kfwnh89):\nThis is the one point that I feel like doesn't really exist anymore.\n\nSure you can't run it as fast but you can buy a Mac 192 GB of RAM.\n\nThe reality is we don't have a model that beats GPT4 because we do not have a model that beats GPT4.\n\nTo me the reason why we can't beat GPT4 has always been because we don't know how to make a model that good.\n\nThe knowledge simply doesn't exist the ability to create it does not exist.\n\nI will be excited the day in which that changes and a model of high quality is released for public and commercial use.\n\nYou think if Google or Meta knew how to make something that good they would just pass on it?\n\nThey are working on it and it will be an iterative process and we will get there one day and I have no idea when that will be this whole field is moving at an incredible rate and any guesses that I may or may not have feel like playing darts with a blindfold on.\n\n#### Comment ID kg07858 with +1 score by [(PwanaZana, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kg07858/) (in reply to ID kfyfrzs):\nMy point was simply that open-source models that run on computer hardware (or modest servers) are at a massive disadvantage from the get-go against something that runs on massive hardware, no matter the software or techniques.\n\n### Comment ID kg0btr2 with +3 score by [(frozencreed, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kg0btr2/) (in reply to ID kfwnh89):\nThis is the real point. It is apples to oranges. AI and the ML architectures behind it are growing at an incredible pace and we don't always know what will work before we try it. To really drive that point home, we don't always know why something DOES work after it's already a success when it comes to AI. \n\nThe fact this guy is talking about talent and resources may be true but when those engineers are essentially throwing the kitchen sink against the wall and seeing what sticks, the gap in that advantage becomes less pronounced. \n\nDon't forget that open source projects also have more freedom to experiment on less tested methods. When you're working with a budget as big as openAI you bet your ass your investors are looking for results and if you're trying some esoteric strategy, all the sudden that $1M salary doesn't seem so guaranteed. \n\nMost of points the guy makes are not verifiable in the sense that the statements are broad and because the evidence for the the effectiveness of them lies in the future. History, on the other hand, is full of underdogs who beat corporate giants with only the tools in their garage. \n\nMy theory is that AI as a whole will have lots of smaller breakthroughs over the next year. Sure, GPT4 will have some, but lots of those breakthroughs will come from the community at large working on smaller projects. Those smaller projects may not get the resources that GPT4 does, but the impact they have on the world may be larger than you think.\n\n### Comment ID kfyafrl with +2 score by [(MathmoKiwi, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfyafrl/) (in reply to ID kfwnh89):\n>I imagine an important point not mentioned here is that one instance of GPT4 runs on a 100k$ machine (super approx number).\n\nHowever, that $100K of hardware will cost just $10K in five years from now, then five years after will cost only $1K. \n\nSo I'm not too bothered by this at the moment if you're just willing to *be patient*\n\n### Comment ID kfzdth6 with +2 score by [(2CatsOnMyKeyboard, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfzdth6/) (in reply to ID kfwnh89):\nHow many users could a 100k$ machine handle? Open source has much lower budgets in general, but it has some. A 100K is no problem for most universities for example. Or for some companies that want in house solutions. If there are good models, and one can get good support, why go to Microsoft and the like?\nI'm just afraid the bottle neck is billions, not 100K.\n\n#### Comment ID kg06bal with +1 score by [(PwanaZana, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kg06bal/) (in reply to ID kfzdth6):\nI mean, for a small business, like a group of a couple lawyers, could use a 100k$ machine to help find precedents/rulings, with a 120B model trained on legal data. (Again, super approx numbers.)\n\n### Comment ID kg2d8x1 with +2 score by [(alvisanovari, Reddit, 2024-01-03)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kg2d8x1/) (in reply to ID kfwnh89):\nNot true - turbo likely can run on minimal hardware especially with optimizations.\n\nAlso - why is it so hard for ppl in this space to extrapolate gains in efficiency? Have you learned nothing from the past 6 months /1 year / 5 year / decade / all of time?\n\n### Comment ID kfyu4y5 with +2 score by [(ramzeez88, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfyu4y5/) (in reply to ID kfwnh89):\nIf we had a consumer hardware capable of running a goliath moe it would be better than gpt4\n\n#### Comment ID kg06tsi with +1 score by [(PwanaZana, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kg06tsi/) (in reply to ID kfyu4y5):\nIf the next gen of Nvidia has more vram, it'd be great. Like the 5090 with 32gb (I dare not hope for more!)\n\n### Comment ID kfy5rp8 with +3 score by [(Vivarevo, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfy5rp8/) (in reply to ID kfwnh89):\nThe whole top salary thing is meaningless too. Studies have repeatedly shown monetary reward reduces performance, both speed and quality.\n\nBeing in a good team can off set it if the point of the work overrides pay incentive i bet though.\n\n#### Comment ID kg07kkx with +1 score by [(PwanaZana, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kg07kkx/) (in reply to ID kfy5rp8):\nI agree, I mean, it can help but having rockstar developers is not necessarily good.\n\nPerhaps we could pay a 1 billion dollar/year wage to a soldier and watch him Doomguy the shit out of the enemy?  :P\n\n\n\n### Comment ID kfzkch3 with +1 score by [(marrow_monkey, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfzkch3/) (in reply to ID kfwnh89):\n> I imagine an important point not mentioned here is that one instance of GPT4 runs on a 100k$ machine (super approx number).\n\nI've been pondering if Wikimedia foundation would be able to run something like that. With suitable donations.\n\n## Comment ID kfwmiit with +167 score by [(Koksny, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfwmiit/) (in reply to ID 18wasf8):\nYes, however, Mistral.\n\nIf anyone has a shot at GPT4, it's them.\n\n### Comment ID kfwsaak with +37 score by [(ninjasaid13, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfwsaak/) (in reply to ID kfwmiit):\nMistral however won't open source them.\n\n#### Comment ID kfwt0aq with +22 score by [(Koksny, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfwt0aq/) (in reply to ID kfwsaak):\nWDYM? Both small Instruct and Mixtral are open sourced already, right? Or are You talking about the Medium and further?\n\n#### Comment ID kfx08qm with +1 score by [(poop_fart_420, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfx08qm/) (in reply to ID kfwsaak):\nWhy?\n\n### Comment ID kfwriur with +17 score by [(Cless_Aurion, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfwriur/) (in reply to ID kfwmiit):\nYeah, just... not this year.\n\nPeople forgot how OP is GPT4 compared to all other models. And there is no fucking way we will have this year a GPT4 model running on current hardware at home. People can barely even run Mixtral and its miles away.\n\n#### Comment ID kfx07m5 with +11 score by [(poop_fart_420, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfx07m5/) (in reply to ID kfwriur):\nI can run mixtral well enough on a 3060 mobile\n\n#### Comment ID kfxh0ry with +2 score by [(Guinness, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfxh0ry/) (in reply to ID kfwriur):\n> And there is no fucking way we will have this year a GPT4 model running on current hardware at home. \n\nWell yeah, that is because GPT4 requires massive MASSIVE amounts of hardware and compute.\n\n#### Comment ID kfy1p5x with +2 score by [(MannowLawn, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfy1p5x/) (in reply to ID kfwriur):\nNo issues here on a m2 64 gb machine\n\n#### Comment ID lga1h19 with +1 score by [(pmp22, Reddit, 2024-08-03)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/lga1h19/) (in reply to ID kfwriur):\nThis aged like milk.\n\n## Comment ID kfwtsxz with +102 score by [(PikaPikaDude, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfwtsxz/) (in reply to ID 18wasf8):\nHe's a troll, ignore him.\n\n### Comment ID kfydjkp with +44 score by [(None, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfydjkp/) (in reply to ID kfwtsxz):\nHe's also conveniently ignoring Meta, that giant corporation that just bought more H100s than anyone else and is committed to open source.\n\n#### Comment ID kxw90fn with +1 score by [(Guilty_Land_7841, Reddit, 2024-04-03)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kxw90fn/) (in reply to ID kfydjkp):\nI study business and stratergies, the only reason this is open source is because it is to get more attention towards LLama rather than their competitor openAI. down the road I'm sure the better ones are going to be closed source.\n\n#### Comment ID kgeglpj with +1 score by [(epicwisdom, Reddit, 2024-01-05)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kgeglpj/) (in reply to ID kfydjkp):\nThe whole post is built on the assumption of decentralized open source teams, which doesn't make much sense. Meta isn't even the only one, there's plenty of big corps that seem eager to distribute open source models (or at least free-as-in-beer model weights)...\n\n## Comment ID kfwmwax with +125 score by [(ThickBamboo999, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfwmwax/) (in reply to ID 18wasf8):\nThey expressed similar skepticism about open source software in the past. Yet, today, they not only utilize these OSS applications but offer them as commerical services in their  cloud environments and also actively contribute to their development.\n\nAlthough 2024 might not be the defining moment, each step taken towards it is significant and contributes to the overall progress.\n\n### Comment ID kfy39x8 with +54 score by [(GoatBass, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfy39x8/) (in reply to ID kfwmwax):\nPeople need to understand that this is an opinion from a man whose livelihood depends on creating products that have to outdo open-source solutions. This is kind of like learning about sustainable solutions from an oil exec.\n\n#### Comment ID kfyi6j4 with +7 score by [(justpurple_, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfyi6j4/) (in reply to ID kfy39x8):\nI'm happy someone else said it. I'm neither saying he is saying the truth, nor am I saying he is wrong... but what I would say is the same thing you did, and additionally:\n\nNo one has a glass ball. He doesn't have any visions of the future, knows any secrets you or I don't or can reliably predict the future just because he has a big twitter following.\n\nI personally think he's underestimating people's desire for AI girlfriends running on their own computer. ;-)\n\nA small anecdote:\n\nIf you asked anyone after GPT 3's (before ChatGPT) release if they thought this was significant, not a lot of people would have said yes. Early diffusion model's generated things you could only describe as \"abstract\" (checkout https://www.reddit.com/r/midjourney/s/IkZfqrb1pO - and the v1 image was already super good for the time). \"Early\" meaning a year ago.\n\nIn fact, Ieven had very early developer API access to GPT 3 - I'm a developer and I still wonder how and why I didn't quite realize how huge this is gonna be. \n\nHindsight is 20/20, as they say! \n\nAnd, as a closing sentence: A very common fallacy of us humans is to assume that what happened in the past, will happen in the future, too. \n\nSigned, a quantum super intelliREDACTED REDACTED REDACTED REDACTED REDACTED REDACTED REDACTED RED R RRRRRR bbgtttial!2810000\n\n#### Comment ID kfyg3hr with +3 score by [(MINIMAN10001, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfyg3hr/) (in reply to ID kfy39x8):\nThat being said I have heard a lot about oil execs moving into the renewables industry simply because it is more profitable to pivot into it.\n\nThey have made their investments into oil already and they can turn a profit but if there's more profit in renewables they can pivot their money into it.\n\nJust a side tangent really\n\n## Comment ID kfwywlc with +37 score by [(RandySavageOfCamalot, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfwywlc/) (in reply to ID 18wasf8):\nWho is this guy and why should I care? Competition is always good for industry and open sourcing is always good for industry. Additionally, open source AI has made progress in months that it took closed source companies years.\n\nNo one has any idea what is going to happen but open source has always given closed a run for its money. 2024 is anyone’s game and with how unstable OpenAI has been, and with how much GPT-4 has been degrading, I would not sleep on open source.\n\n### Comment ID kfz7neb with +2 score by [(bran_dong, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfz7neb/) (in reply to ID kfwywlc):\nmost ai subreddits are being flooded with screenshots of tweets that are basically exactly this post. it's always some rumor or hot take from someone who's never worked for the company they're talking about. this \"you guys can't beat openai because I worked for Google\" is beyond stupid.\n\n## Comment ID kfwna7o with +174 score by [(a_beautiful_rhind, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfwna7o/) (in reply to ID 18wasf8):\nOpen models don't have to reach GPT-4 level, GPT-4 level will come down to them.\n\nBig ego, hubris and \"safety\" gonna make it happen.\n\n### Comment ID kfx0f2v with +38 score by [(tropicalpolevaulting, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfx0f2v/) (in reply to ID kfwna7o):\nBeating GPT4 would be incredible, being just as good as it would be awesome. However, I just need it to be just good enough while running on my bucket of a PC. And there's a lot of people who are happy with just good enough.\n\n#### Comment ID kfyo6ak with +1 score by [(Divniy, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfyo6ak/) (in reply to ID kfx0f2v):\nOn the other hand, if there would be a decent, easy to setup cloud infrastructure at competitive price (vs openai subscription), I'd rather prefer running my instance of cloud instead of giving my data to openai overlords.\n\n### Comment ID kfxenpl with +16 score by [(throwaway_ghast, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfxenpl/) (in reply to ID kfwna7o):\nClaude moment.\n\n#### Comment ID kfyaozc with +15 score by [(ComprehensiveWord477, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfyaozc/) (in reply to ID kfxenpl):\nIt’s funny this literally happened to Claude.\n\n### Comment ID kfyciyr with +7 score by [(ThisGonBHard, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfyciyr/) (in reply to ID kfwna7o):\nThis is kind of how I feel about current models beating GPT3.5.\n\nThey beat this shit version, but I am not sure they would have beat the November 2022 version.\n\n#### Comment ID kg00yoo with +1 score by [(MoffKalast, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kg00yoo/) (in reply to ID kfyciyr):\n3.5 used to be worse than it is now imo, but 4 was better. They sort of met in the middle.\n\n### Comment ID kfwr9xo with +23 score by [(Busy_Farmer_7549, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfwr9xo/) (in reply to ID kfwna7o):\nThis.\n\n### Comment ID kg0ve8f with +3 score by [(None, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kg0ve8f/) (in reply to ID kfwna7o):\nBeautifully said.\n\n### Comment ID kfzpska with +2 score by [(Civil-Demand555, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfzpska/) (in reply to ID kfwna7o):\nI only wait for multi language support this is for my a killer feature of even chatgpt 3.5 .\n\nFor me even model that is on the level 3.5 would be insanely useful\n\n## Comment ID kfwqkdm with +25 score by [(catfish_dinner, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfwqkdm/) (in reply to ID 18wasf8):\ngood ol' fashioned [fud](https://en.wikipedia.org/wiki/Fear,_uncertainty,_and_doubt)\n\n## Comment ID kfwmlsv with +169 score by [(sophosympatheia, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfwmlsv/) (in reply to ID 18wasf8):\nI think the open-source community does a disservice to itself by setting the goalposts around beating gpt4. For the reasons given in this post, that seems unrealistic. Meanwhile, the community is making real gains around smaller models, local performance, and niche use cases that fall outside what the corporate gatekeepers will let you do with their services. I think we should play to our strengths in 2024 rather than trying to prove a point.\n\nHappy New Year, everyone!\n\n### Comment ID kfwnr6l with +30 score by [(Koksny, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfwnr6l/) (in reply to ID kfwmlsv):\nFair point. This year we will see first third party products including local LLMs like Mixtral, and well optimized backends for mobiles, like MLC. \n\nI honestly doubt a GPT4 level LM is required for 90% of common purposes, and while OpenAI might be ahead for incoming decades, i'm fairly sure their market share will be slow eaten by local models, with the burden of cost moved to user hardware, instead of paid cloud API. It's just inevitable, and Mixtral is the proof of concept the community needed.\n\n#### Comment ID kfww6np with +29 score by [(frozen_tuna, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfww6np/) (in reply to ID kfwnr6l):\n>I honestly doubt a GPT4 level LM is required for 90% of common purposes\n\nPeople aren't getting how much new/exciting software can now be built with a 7B model that is good at simply taking in a pdf/chat/transcript and putting details into a JSON. Its wild.\n\n### Comment ID kfwyw4l with +53 score by [(None, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfwyw4l/) (in reply to ID kfwmlsv):\n[deleted]\n\n#### Comment ID kfx12ri with +10 score by [(sophosympatheia, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfx12ri/) (in reply to ID kfwyw4l):\nTotally agree. I have way more fun with open source, and like you said, it only needs to be good enough.\n\n#### Comment ID kfx3q9q with +8 score by [(None, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfx3q9q/) (in reply to ID kfwyw4l):\nGood enough and running locally on an ultralight work laptop with no Internet connection.\n\nWe're getting close with 3B and tuned 1B models.\n\n#### Comment ID kfyes65 with +4 score by [(davikrehalt, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfyes65/) (in reply to ID kfwyw4l):\nWhat if gpt4 is not even good enough for my use cases\n\n#### Comment ID kfxzjbm with +1 score by [(Caffdy, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfxzjbm/) (in reply to ID kfwyw4l):\nthe real question is, is GTP4 good enough?\n\n### Comment ID kfxz9x4 with +24 score by [(ReturningTarzan, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfxz9x4/) (in reply to ID kfwmlsv):\nOpen-source models have already beaten GPT4:\n\n- I can run open-source models on sensitive data. GPT4 doesn't even give me that option.\n- I can build a whole application around an open-source model and expect it to work the same tomorrow as it does today.\n- OpenAI might have less downtime in the future, but it's not a given that they can keep building new infrastructure to keep up with demand.\n- I can estimate the cost of running a local model and plan accordingly. I really have no way of knowing what the price of GPT4 will be in a year, or even a month for that matter.\n- And as for cost, $2 for one 32k prompt is actually really expensive. They do have cheaper models of course, but those are also the ones that open-source models are already catching up to. And they're still not free, or private, or guaranteed to exist in six months.\n\n#### Comment ID kg16f9i with +5 score by [(sophosympatheia, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kg16f9i/) (in reply to ID kfxz9x4):\nThis was a great comment. It helps to be clear about what we mean when we say \"beats gpt4.\" I think it's easy to get tunnel visioned about \"beating gpt4\" in the sense of raw capability, which is what I had in mind when I made my comment. /u/ReturningTarzan rightfully pointed out that there are many more dimensions to it than that, and in some of those dimensions we have advantages already.\n\nP.S. Thank you for your work on exllama in 2023, /u/ReturningTarzan. I can't wait to see what you do in 2024!\n\n#### Comment ID kfyvo3i with +3 score by [(WolframRavenwolf, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfyvo3i/) (in reply to ID kfxz9x4):\nMy favorite comment in this thread, focusing on the areas where we've already beaten GPT-4 because centralized commercial AI (LLMaaS) can't even compete there. Guess we have a moat, too, a free, local, uncensored one.\n\n#### Comment ID l62q3dj with +1 score by [(Mchanger, Reddit, 2024-05-28)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/l62q3dj/) (in reply to ID kfxz9x4):\nCould you share a bit more how you train model on sensitive data?\n\nThis has been my main (and major) block to pursue building anything with AI. The lack of missing nuances and generating general *in the ball park* answers isn't good enough for how I want to use AIs.\n\nCould you help me understand how this would be done?\n\nThanks in advance!\n\n### Comment ID kfwpcu9 with +8 score by [(thetaFAANG, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfwpcu9/) (in reply to ID kfwmlsv):\nI think the goal post has been beneficial\n\nSame conclusion as you, just a different interpretation\n\n#### Comment ID kfx18ky with +9 score by [(sophosympatheia, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfx18ky/) (in reply to ID kfwpcu9):\nNo doubt. Aim for the stars and all that.\n\n### Comment ID kfyg5c9 with +3 score by [(MINIMAN10001, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfyg5c9/) (in reply to ID kfwmlsv):\nSee the part that I find amusing with this is GPT-4 wasn't always the goal post GPT 3.5 was the goal post and we seem to have met it and that honestly is incredible.\n\n### Comment ID kfyewlv with +2 score by [(davikrehalt, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfyewlv/) (in reply to ID kfwmlsv):\nWhy are we so pessimistic. Gpt4 is a fixed model. Quantized versions probably closer to 200B than 1T. 70B models are routinely released. Btw meta is still releasing llamas open source and they have ashitton of gpu\n\n#### Comment ID kg1apgp with +1 score by [(sophosympatheia, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kg1apgp/) (in reply to ID kfyewlv):\nGiven enough time, I expect the Moore's Law of LLM development will eventually mean we get a local model that exceeds the capabilities of GPT 4. I'll be impressed if that happens in 2024, but I wouldn't put it that far off in the future.\n\n### Comment ID kfyidgi with +2 score by [(LePopeUrban, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfyidgi/) (in reply to ID kfwmlsv):\nMore intelligent use of RAG and good function calling / tool use are the future for local OSS.\n\nGPT4 needs to be this big behemoth of a thing that can snappily answer any type of queries. The models you run at home do not because YOU know what capabilities you need at any one time, they don't need to all fit in memory at once, and you're probably willing to wait a minute for them to spin up because like... that's how all your applications work.\n\nFor the vast majority of local use I'm not sitting here pinning all my hopes on one application, or one LLM for that matter.\n\nThere is gold in limited domain local tool using models that OpenAI simply isn't interested in.\n\nThere is gold in standardizing I/O for OSS too. Just a lib that RAG models can call that passes I/O between models and you're cooking your own custom multimodal with hot swappable parts in exactly the same way as you set up workflows with a bunch of different apps on your operating system.\n\nLets also not forget there is significant incentive for hardware manufacturers in the game. A lot of people HERE bought hardware specifically to gain function in AI.\n\nA killer local app that makes that tradeoff attractive to normies and its off the the races in a price war as everyone tries to build multiple price ranges of tensor hardware so they have a reason to sell every person on the planet a new device the same way apple convinced everyone they NEEDED this new thing called a smartphone.\n\nAnd on top of all that lets not forget the robots. We've already seen recent advancements turbocharge the software end of robotics. Every company that wants to sell a new futuristic talking intelligent household robot thing would much rather prefer to stick a tensor chip in and occasionally update the software than have to run a complicated backend on a server to run literally every device they sell.  \n\n\nBe the company that can manufacture those chips in bulk and sell them to the household robotics equivalent of cheap android phone manufacturers and you've got a gold mine.\n\n#### Comment ID kg07g85 with +1 score by [(paretoOptimalDev, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kg07g85/) (in reply to ID kfyidgi):\nYes. If people could better limit things to code and areas they care about things would vastly improve.\n\n## Comment ID kfwq4e6 with +20 score by [(Inside-Homework6544, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfwq4e6/) (in reply to ID 18wasf8):\nyah, but which one will pretend to be my waifu?\n\n## Comment ID kfwwof3 with +20 score by [(Biggest_Cans, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfwwof3/) (in reply to ID 18wasf8):\nEh you never know, sometimes all it takes is something clever. It is just a big math problem after all.\n\nAlso they don't have to, they just have to keep cheating off the big kids' homework and stay within shooting distance. And they don't have every-human-belongs-in-daycare minded censorship to battle.\n\n## Comment ID kfx3ob0 with +20 score by [(Right_Ad371, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfx3ob0/) (in reply to ID 18wasf8):\nIf GPT-4 doesn't whine up with the policy and moral code every 5 minutes, then no, open source for me. At least I don't have to comply with their rules.\n\n### Comment ID kg0wt47 with +2 score by [(Blizado, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kg0wt47/) (in reply to ID kfx3ob0):\nYep, and that is why Open Source models will always win. Companies get hard pressure from our society to limit their AI stuff. That pressure is a lot smaller on Open Source models. But I have the bad feeling, that in the future also OS models get under more pressure, like it often is with moral and ethical stuff, it comes only later. Also some OS models are only not limited because there exists finetunes/jailbreaks to make them free again.\n\n## Comment ID kfwnl8z with +51 score by [(vasileer, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfwnl8z/) (in reply to ID 18wasf8):\nFalcon, Llama2, and Mistral/Mixtral are also coming from \"centralized\" teams, and Mixtral is already beating ChatGPT in ChatBotArena,\n\nI think GTP-4 will be outperformed with LLama3 release, and/or next versions of Mixtral,\n\nso the real question is if the GTP-5 will be beaten or not this year, not GPT-4\n\nhttps://preview.redd.it/grwnahtr6x9c1.png?width=3108&format=png&auto=webp&s=4f792f6cdd8c0c313fce4e6c154d55b75605b174\n\n### Comment ID kfwqn5h with +17 score by [(metalman123, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfwqn5h/) (in reply to ID kfwnl8z):\nOpen source models have been gaining roughly 1.5 mmlu a month since February.\n\nIt's likely we catch up to 4 by March or April.\n\nLlama 3 should be right on time.\n\n#### Comment ID kfwsjs8 with +15 score by [(ninjasaid13, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfwsjs8/) (in reply to ID kfwqn5h):\n>Open source models have been gaining roughly 1.5 mmlu a month since February.\n\nI don't trust those benchmarks.\n\n## Comment ID kfws8xt with +66 score by [(wt1j, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfws8xt/) (in reply to ID 18wasf8):\nSigh. And yet Linux fucking destroyed Microsoft in the data center no matter how much they spent and how centralized their team was. Kids need to read more history.\n\n### Comment ID kfx9zr7 with +23 score by [(AndrewVeee, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfx9zr7/) (in reply to ID kfws8xt):\nIt did get pretty scary for a while, though. Linux was competing too well, so MS brought out the patent guns until they started losing antitrust cases haha\n\n## Comment ID kfwpevd with +46 score by [(extopico, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfwpevd/) (in reply to ID 18wasf8):\nYea nah. Open source always wins, then gets bought, and then it starts again.\n\n### Comment ID kfxd3ad with +3 score by [(noiseinvacuum, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfxd3ad/) (in reply to ID kfwpevd):\nNot always. Certainly not when it comes to infrastructure.\n\n#### Comment ID kfxde5t with +6 score by [(extopico, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfxde5t/) (in reply to ID kfxd3ad):\nSure. But infrastructure is just a short term barrier that actually serves to drive further innovation.\n\n#### Comment ID khbw8f2 with +1 score by [(LibrarianMundane4705, Reddit, 2024-01-11)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/khbw8f2/) (in reply to ID kfxd3ad):\nI mean… OpenStack?\n\n## Comment ID kfwsi2t with +45 score by [(Revolutionalredstone, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfwsi2t/) (in reply to ID 18wasf8):\nThis idea that GPT4 is special or unbeatable is pretty strongly a view held only by those who don't actually use the Open Source Models correctly.\n\nWhen you fine tune for any specific use case you easily beat GPT4: https://openpipe.ai/blog/mistral-7b-fine-tune-optimized\n\nThe idea of having one single model do everything is a bad plan, it is the reason OpenAI have to pay so many humans for manual RL and it's the reason why MoE and other out of the box ideas that do not just linearly scale parameters are getting such good results.\n\nOpen Source Is always the best in the long run, these predictions are not worth jack.\n\nPeace\n\n### Comment ID kfxm3uo with +5 score by [(FaceDeer, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfxm3uo/) (in reply to ID kfwsi2t):\nI've found it's a not uncommon fallacy to think that there are inventions that are somehow \"unique\" in history, such that if the particular person or organization that invented it hadn't done so then nobody else *ever* would have. It's really baffling, but it seems to me that this particular thought might be more common with those of an artistic bent since art can more plausibly be thought of as following that pattern.\n\n#### Comment ID kfxos6x with +2 score by [(Revolutionalredstone, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfxos6x/) (in reply to ID kfxm3uo):\nYeah in my old age (32 a fews days ago lol) I'm seeing more and more of what you're talking about here.\n\nCompanies and technologies which I thought we're uniquely exceptional at the time are more and more to me like they fit right in with their contemporaries.\n\nThe moving zeitgeist drags us all such that which decade you're in matters more than which XYZ other thing.\n\nEvery company and their mom getting a custom LLM recently might be one reality where ChatGPT would be one mirage (a point of false or atleast undue focused) .\n\nTa!\n\n### Comment ID kfxk7gu with +10 score by [(kulchacop, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfxk7gu/) (in reply to ID kfwsi2t):\nAbsolutely agree with you. The tweet is trying to mislead by making a distinction between base models and fine tunes with points 4 (model vs product) and 2 (ChatGPT dataset).\n\n### Comment ID kfxs5fz with +1 score by [(my_aggr, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfxs5fz/) (in reply to ID kfwsi2t):\nYes but you need a big hunk of metal to fine tune locally. For the next 5 years that sort of compute will be out of the hands of the average person.\n\n#### Comment ID kfxvqdk with +3 score by [(Revolutionalredstone, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfxvqdk/) (in reply to ID kfxs5fz):\nAgreed on first point!, not on second..\n\nSeen some real interesting innovations (think models-designed-and-trained-to-be-fine-tuned not-to-mention sparse, slined, q-lora's etc etc)\n\nIt IS too hard and weird for normal people atm but it will go easier ;)\n\n### Comment ID kg1qbfe with +1 score by [(otterquestions, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kg1qbfe/) (in reply to ID kfwsi2t):\nCan you imagine if you finetuned gpt4 for each of those specific use cases though?\n\n## Comment ID kfx2e3b with +12 score by [(dralth, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfx2e3b/) (in reply to ID 18wasf8):\nSaving this with a reminder for 1 year. We’ll see if it aged like wine or milk.\n\n### Comment ID kfz45a7 with +2 score by [(IversusAI, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfz45a7/) (in reply to ID kfx2e3b):\nremindme! one year\n\n#### Comment ID kg14372 with +3 score by [(bucolucas, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kg14372/) (in reply to ID kfz45a7):\nRIP Reddit API\n\n## Comment ID kfwlzdt with +56 score by [(wind_dude, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfwlzdt/) (in reply to ID 18wasf8):\nMaybe the problem for him was google… despite google have years of ml/ai experience, one of the largest datasets available, collectively the most experience processing it, they seem to be failing at LLMs compared to competitors. Plus it’s kinda become a big machine inefficient at innovation other than increasing margins.\n\n### Comment ID kfx7x1q with +3 score by [(OkDimension, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfx7x1q/) (in reply to ID kfwlzdt):\nI am not sure but I don't think Google used all the data available to train Gemini, it just seems too clueless for that\n\n#### Comment ID kfxyrho with +7 score by [(fish312, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfxyrho/) (in reply to ID kfx7x1q):\nGemini is heavily censored and shackled, could be why it underperforms\n\n#### Comment ID kfy7dvu with +1 score by [(FullOf_Bad_Ideas, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfy7dvu/) (in reply to ID kfx7x1q):\nThe fact that you have some kind of customers data doesn't mean you can train on it. I suspect Google might be somewhat better ethically with their dataset over OpenAI.\n\n### Comment ID kfx8mqj with +5 score by [(toothpastespiders, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfx8mqj/) (in reply to ID kfwlzdt):\nYeah, I respect Google in general for the contributions they've made over their history. But I got a laugh at his citing google as among the top ai research labs if it's in the context of LLMs. I like gemini, I actually use the API pretty consistently. But I use it because it's roughly comparable to nous capybara 34b for me without needing to hog my GPU for basic text analysis. I'm not using it because it's \"better\" than 34b, but because it's perfectly comparable for the tasks I'm using it for. Google itself stands as proof that local models can be highly competitive with the largest players.\n\n## Comment ID kfwoxl3 with +27 score by [(smellof, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfwoxl3/) (in reply to ID 18wasf8):\n1. they have money\n2. they have data\n3. they have money and data\n\n### Comment ID kfx8p87 with +15 score by [(3-4pm, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfx8p87/) (in reply to ID kfwoxl3):\n4 . They have the politicians that will regulate the open source competition away.\n\n#### Comment ID kfxgb88 with +8 score by [(throwaway_ghast, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfxgb88/) (in reply to ID kfx8p87):\n*[laughs in Chinese]*\n\n#### Comment ID kfxfpbf with +5 score by [(Trivale, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfxfpbf/) (in reply to ID kfx8p87):\nChicken little horse shit.\n\n#### Comment ID kfxrxcs with +2 score by [(my_aggr, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfxrxcs/) (in reply to ID kfx8p87):\nBullshit we won encryption and that was already defined as a weapon.\n\n## Comment ID kfwlvdo with +20 score by [(mzbacd, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfwlvdo/) (in reply to ID 18wasf8):\nMore like they can take whatever good open source comes out and turn it into their product for free.\n\n### Comment ID kfx9ngm with +6 score by [(AndrewVeee, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfx9ngm/) (in reply to ID kfwlvdo):\nThanks, I was looking for this comment haha\n\nWe'll build fun, interesting, cool stuff, and any company will be free to copy it.\n\nAnd we've got to work with difficult issues like small models/low-power, multiple OSes, and yet there's still so much cool stuff coming out.\n\nBut I guess that's the game we have to play. Keep trying to make good stuff, and let it benefit everyone.\n\n#### Comment ID kfxygrc with +3 score by [(synn89, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfxygrc/) (in reply to ID kfx9ngm):\nTo be fair, the open source community uses the commercial models to improve the open source ones. Synthetic data sets being a prime example.\n\n## Comment ID kfwq9rt with +22 score by [(FickleAbility7768, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfwq9rt/) (in reply to ID 18wasf8):\nMistral bitches. Mistral!\n\n## Comment ID kfwug2j with +10 score by [(Zealousideal_Nail288, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfwug2j/) (in reply to ID 18wasf8):\nsounds like the best consumer PC you can buy wont be on the top 500 supercomputer ranking this year either\n\n## Comment ID kfy8fjf with +9 score by [(perksoeerrroed, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfy8fjf/) (in reply to ID 18wasf8):\nlel.\n\nWe already reached GPT3.5 levels.\n\nAt this rate we should hit GPT4 level with open source models around march-april.\n\n## Comment ID kfx3xqd with +16 score by [(JustDoinNerdStuff, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfx3xqd/) (in reply to ID 18wasf8):\nHere's the one reason open source may outperform Chat4 and I'm serious: Excessive guard rails. When you ask Chat4: \"I have pain in my chest and my left arm, what could this be\" Chat4 may respond \"there are many things that could cause these symptoms. (Filler sentence) (disclaimer sentence) (summary sentence) You should contact a doctor if you aren't sure what your symptoms mean.\" An open source LLM will say \"bro you're having a heart attack! Pound some Aspirin asap, sit upright, and call 911!\" I made that example question up, maybe Chat4 can answer that specific question, but overall it is so unbelievably censored for content and liability, I find little reason to use it. It refuses to answer questions about anything even remotely controversial. This is likely worsening how it answers non-controversial questions too, because these guard rails require broad strokes to prevent circumvention. These guard rails have as much of an impact on output quality as any of the advantages listed above. I mean, you're PROBABLY right that Chat4 will perform better overall in a lot of cases, but I'm just saying it's not going to be night and day. Many users will sincerely prefer alternatives, and get better results.\n\n## Comment ID kfwo7wm with +25 score by [(7734128, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfwo7wm/) (in reply to ID 18wasf8):\nDisagree. The best open source models are too close to not reach GPT4 in twelve months. However I believe proprietary models will be more capable in general in the long term for the reasons he wrote.\n\n## Comment ID kfwzc2k with +13 score by [(fimbulvntr, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfwzc2k/) (in reply to ID 18wasf8):\n## Talent\n\nOS has a lot of talent too, and most people who are hoping to get picked up by big tech aren't going to go through academia, but through OS contrib. The current times we're living in are unprecedented:\n\n- You have devs reading and implementing whitepapers straight from source within weeks or days of publication.\n- You have youtubers explaining whitepapers\n- Anything you don't understand can be fed into GPT4. Yeah it hallucinates and makes mistakes but that's alright, progress is clunky.\n\n## Data\n\n- We've started to see more open datasets being shared at the end of 2023 and I hope the trend continues\n- We can take data from GPT4. They can't. (yes I know about synthetic data being used at OpenAI. That's not the point I'm making, my point is we can just \"Orca\" GPT4 while they would need \"GPT5\" to be the teacher and that would be pointless if you already have GPT5)\n- We can use uncensored data. They can't.\n- We can use proprietary data. They can't.\n\n\n## Team structure\n\nThis is just bullshit false information. Remote, distributed teams work better than in-person, centralized teams inside an office.\n\nThis is just obvious, has this guy learned nothing from the pandemic? Does he think workers spending hours in traffic and having to pay insane rent in SF to go to a drab office listening to clueless bosses somehow have an inherent advantage? Absolutely fucking cope delusions.\n\n## Model vs Product\n\n... and? Who gives a shit? Does he mean open source will never be able to generate as much revenue as an AI company? If so, I agree, but that's also missing the point by a hundred lightyears.\n\nOracle makes more money than PostgreSQL but which one is OBJECTIVELY the best RDBMS?\n\nIf you say Oracle is better or \"it depends on your usecase\" you're an idiot - unless the usecase is \"I need to extract as much in consulting fees as possible\".\n\n## Infrastructure\n\n- For many, local > cloud, so already the race is subjective\n- There are many flavors of \"public cloud\". What do you mean? Renting boxes for training? Yeah maybe. But for inference, how is OpenRouter or Fireworks.ai worse?\n- Fine tuning via Unsloth is much more ergonomic, cheaper and faster than fine tuning GPT3.5 via their weird system\n\n\n## Extra\n\nThese are just refutations of his individual points, I'm not even going to go into the advantages OS has over OpenAI. This tweet will age poorly.\n\nNow if he says OS won't catch up to OpenAI, then he has a point (they should release 4.5 or 5 this year), whereas we're just beginning with multimodality, function calling, and have only just surpassed (debatable) 3.5 with some models (falcon, Goliath, Yi, mixtral). But that's not the argument he made, he specifically mentioned gpt-4.\n\n## Comment ID kfwzf3b with +15 score by [(complains_constantly, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfwzf3b/) (in reply to ID 18wasf8):\nLLaMA-3 will surpass it, we have every reason to think so. Not to mention Meta has bought 150k H100s to train it.\n\n### Comment ID kfy9s4m with +3 score by [(squareOfTwo, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfy9s4m/) (in reply to ID kfwzf3b):\nsource,?\n\n#### Comment ID kfyk6s2 with +7 score by [(ttkciar, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfyk6s2/) (in reply to ID kfy9s4m):\nhttps://old.reddit.com/r/LocalLLaMA/comments/189opkb/meta_has_purchased_approximately_150k_h100s_this/\n\n## Comment ID kfwuac3 with +6 score by [(Tacx79, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfwuac3/) (in reply to ID 18wasf8):\nIt's worth noting that gpt-4 is not just the model like most people see it. If you use gpt-4 you also use tons of other software around it which makes it work so good. It's like running koboldcpp vs having entire operating system written from scratch to host just one exact model (yes it's exaggeration but the scale and support applies)\n\n## Comment ID kfwvuhl with +8 score by [(Majestical-psyche, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfwvuhl/) (in reply to ID 18wasf8):\nMicrosoft has a ton of computing power.\nIf the open source community keeps developing Petals, along with MOE infrastructure; the gap will be much closer.\n\n## Comment ID kfwwedj with +14 score by [(Lorian0x7, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfwwedj/) (in reply to ID 18wasf8):\nStill chat gpt4 is censored... in that sense Open Source is already better and surpassed chat gpt 4\n\n## Comment ID kfwymh1 with +7 score by [(Cradawx, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfwymh1/) (in reply to ID 18wasf8):\nI would think that too but I always seem to underestimate how quickly things advance. It's hard to believe Llama 1 was only ~10 months ago, it feels like 10 years of advancement has happened since then. I won't be surprised at all if this guy is wrong.\n\n## Comment ID kfx142x with +5 score by [(bias_guy412, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfx142x/) (in reply to ID 18wasf8):\nMake a sensational post and farm user engagement.\n\n## Comment ID kfybt6f with +6 score by [(daedalus1982, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfybt6f/) (in reply to ID 18wasf8):\nI feel like I need to not only refute the points he made but also offer up  some counter points.\n\n1. Your price plans can be jacked up and your fine tuned models and customized stuff you have with OpenAI can be wiped out in a night.  We know because they already did this prior to the launch of gpt-4\n2. Monoliths can be shut off and your access to chatgpt can just disappear for hours with zero explanation.  If you offer up products that uses their api then you just have to live with it. Don't enter into any HA contracts with openai api calls in your product.\n3. The open source community will prevent tunnel vision and thought stagnation because we aren't the same 50 people getting passed back and forth between Google and OpenAI\n4. The open source community has people making models AND people making products\n5. It's cute to see how much you can crunch when you set an enormous pile of money on fire, but the part of the point of the opensource community is getting this technology to the state where it can be run on those terrible cloud infra's that the OP sneers at so hard.  It's nice to see that you can make it work if you have limitless cash but we have people doing image inference locally on a laptop they can buy at walmart now.\n\nSo ... yeah.  Disagree.\n\n## Comment ID kfwmw5m with +12 score by [(tshawkins, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfwmw5m/) (in reply to ID 18wasf8):\nBut can OSS models get good enough that the hassle of using remote commercial models becomes not worth it.\n\n## Comment ID kfx4w6z with +12 score by [(Fheredin, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfx4w6z/) (in reply to ID 18wasf8):\nNarrow models > Generalist models\n\n## Comment ID kfx044f with +14 score by [(GodIsAWomaniser, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfx044f/) (in reply to ID 18wasf8):\nLol, anyone who has read the \"we have no moat\" letter will just laugh at this guy, just retarded copium.\n\n## Comment ID kfwrz6y with +4 score by [(a_bit_of_byte, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfwrz6y/) (in reply to ID 18wasf8):\nI’m not sure I agree with all his points. Linux is just as competent an OS as Windows, despite similar arguments holding true. \n\nWhat is valid is that OpenAI has access to particularly powerful computing hardware the open source community might not. It’s still possible, but the model the FOSS community makes would likely have to fit on consumer-grade GPUs. That limitation might be hard to overcome, even with the considerable talent the FOSS community has.\n\n## Comment ID kfwxx2n with +5 score by [(noiserr, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfwxx2n/) (in reply to ID 18wasf8):\nNo single FOSS model may be as good as GPT-4 in the near term. But I think it's possible to have domain specific models which can give GPT-4 run for its money on given tasks they were fine tuned for. In fact they already do for things that are restricted.\n\n## Comment ID kfx4fzl with +5 score by [(XpiredLunchMeat, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfx4fzl/) (in reply to ID 18wasf8):\nMaybe this guys means to say open-source won't beat GPT5 or whatever the top talent is working on right now... but almost every statement above is somewhat negated assuming the team is not focused on GPT4. Open-source does have a shot of taking down GPT4, just as it has taken down 3.5. Once active development starts to stagnate we can catch up and surpass.\n\n## Comment ID kfy4tcl with +5 score by [(Mobile_Ad9119, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfy4tcl/) (in reply to ID 18wasf8):\nNot looking to beat it. Vague and general queries for gpt4 have their place. IP, sensitive information and business strategy are why I’d want private. I’m not going to feed OpenAI my industry knowledge and I’m deff not paying crazy enterprise money for privacy. I don’t think I’ll need much improvement in private for my use cases. Look how fast everything is moving. Open source is going to be crazy in a year.\n\n## Comment ID kfya5gk with +5 score by [(CodeGriot, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfya5gk/) (in reply to ID 18wasf8):\nRidiculous idea that anyone can predict ANYTHING about LLMs 12 months into the future.  I'm not wasting such time investigating my navel. I've got work to do (driven by OSS, BTW).\n\n## Comment ID kfybrti with +4 score by [(speacexstarlink, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfybrti/) (in reply to ID 18wasf8):\nStupid as F# people please use Cloud based censored AI models, world needs sheep too. The rest of us keep running locally whatever we want and we create.\n\n## Comment ID kfwt723 with +17 score by [(None, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfwt723/) (in reply to ID 18wasf8):\n- Talent: I honestly dont think that the salaries of the engineers matter. Open source has many more people, and collectively many great minds working together\n- Data: I’ll concede that their ability to mine literally the whole internet is unparalleled- but the open source community can work together to create more tailored datasets that ultimately can perform extremely well in niche area tuning\n- Team Structure: I mean, yeah, the open source community is not being paid to collaborate- but arguably the fact that pull requests and improvements are made by conscious eyes and with community review, arguable these are of much higher quality than a corporate structure where people just want to get by and do as theyre told by stakeholders\n- Model vs Product: I dont really understand this point. Thats the inherent difference between opensource and closed, they are creating for sale products. Its not 1:1\n- Infrastructure: Surely the $$ availability is not comparable, but I really believe that we can accomplish a lot with a swarm of smaller hardware implementations. You may not need a supersonic drill just to fix a doorknob (example)- likewise you may not need 150GB of RAM and a GPU to enable LLM-powered insights on your website\n\nThanks for sharing\n\n### Comment ID kfx9o7h with +12 score by [(toothpastespiders, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfx9o7h/) (in reply to ID kfwt723):\n>Team Structure\n\nI'd also argue that from everything I've heard, Google's team structure often does more harm than good.\n\n#### Comment ID kfxmajw with +19 score by [(FaceDeer, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfxmajw/) (in reply to ID kfx9o7h):\nAnd OpenAI's \"team structure\" nearly caused the company to completely implode a few months back. Open source projects don't have a monopoly on personality-driven drama and strife, it turns out.\n\n### Comment ID kfx15pz with +9 score by [(ttkciar, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfx15pz/) (in reply to ID kfwt723):\nYup, all of this, to which I will add a few points:\n\nTalent: As it happens, some of the top, best-paid talent in the industry is also involved in the open source community.  We might not get their undivided attention, but I bet we get more aggregate brain-hours per day than OpenAI just on sheer number of participants.\n\nModel vs Product:  A couple of interpretations of this make sense to me, though I'm not sure which is actual (if either):\n\n* There's a lot more technology that goes into the \"ChatGPT experience\" than just the model.  There is also (I strongly suspect) a lot of symbolic logic scaffolding which allows it to do things like follow a long conversation.\n\n* Or he could just be making noises to convince customers and investors that OpenAI will always be king, no matter what those silly amateur open source enthusiasts come up with.\n\nInfrastructure: What he says is more true of training than inference, I think.  Even though OpenAI has Azure at its disposal, it has to split that infrastructure across all of its [100+ million customers](https://techreport.com/statistics/openai-statistics/#:~:text=In%202022%2C%20OpenAI%20had%2021.1,million%20people%20using%20it%20monthly), whereas a \"GPU poor\" open source enthusiast can dedicate their entire GPU to their own use.  It's not quite an apples-to-apples comparison, because local inference can be faster or slower than ChatGPT depending on what model is running locally (3B, 7B, 13B, 33B, 70B), but the key take-away is that it's not as clear-cut as Benard makes it sound.\n\n### Comment ID kfwzpw7 with +11 score by [(confused_boner, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfwzpw7/) (in reply to ID kfwt723):\nhe talks like a crypto bro\n\n## Comment ID kfwxigm with +15 score by [(Shokisan1, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfwxigm/) (in reply to ID 18wasf8):\nAs long as gpt4 continues to lecture its users on morality and ethics, it won't be used for long term. Same goes for Bing. Those AIs are like a vegan insulting a steak eater at this point. It takes clever prompting for the Ai to not act condescending towards users. Private LLM would probably solve this since Microsoft isn't sweating over hate mail from every Karen. I don't see a future where gpt4 exists in its current form. It's already obsolete for anyone able to afford a powerful computer. Bing AI and GPT4 is like the America Online of AI. I think we will see much more powerful competitors or just superior private models that are cheap and make chatgpt obsolete.\n\nOf course, I'm open to surprises. Microsoft is trying to save Blizzard from WokeBroke Syndrome right now but they themselves are walking on the Karen Eggshells. They need to stop being so damn prude and just release something cool and gritty already. Think original FF7 vs the remake. Nobody likes shit that plays it safe. It's fucking lame. WokeBroke Karens are loud but they don't move product! I think more big fat cats are waking up to this fact. To those who have ears let them hear!\n\n## Comment ID kfwrex0 with +9 score by [(JohnExile, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfwrex0/) (in reply to ID 18wasf8):\nI don't disagree but the part where local is shining by far is going to be the lack of filters. Every time GPT4 gets smarter, it also gets dumber in some aspect because they keep doing their best to filter responses. The moment I stopped using ChatGPT was when it refused to give me a snippet of python code because the library it was trying to use had a function called ``kill_instance()``...\n\n## Comment ID kfwqmtc with +4 score by [(Aperturebanana, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfwqmtc/) (in reply to ID 18wasf8):\nThe goal is “good enough” at a level comparable.\n\n## Comment ID kfxnu4w with +4 score by [(SkyLordOmega, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfxnu4w/) (in reply to ID 18wasf8):\nI bet someone will break it this year. \n\nWhen GCP and other cloud companies started offering cloud vision tools there were open source alternatives that became popular. \n\nSimilarly LibreOffice if anyone remembers or Linux..\n\nOpen source is dynamic and fuelled by passion. \n\nMy bet on OS bearing closed source in the year.\n\n## Comment ID kfy2xea with +4 score by [(heytherepotato, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfy2xea/) (in reply to ID 18wasf8):\nThey wrong :). This guy is just virtue signaling.\n\nI say that because feels like a bit of a straw man post, or a very cherry picked set of points, but by their own argument of comparing a model to a product, they've started a discussion that ends with them being wrong when we also compare a product, to a product.\n\nThey've omitted that fact that open source can pair any model with any functionality and have a \"product\". \n\nWe honestly don't know the products we'll see this year from open source, but they won't be constrained by \"what is good for the shareholders\" or \"what is legally feasible\" or \"what the highly paid engineers find time to do in between kissing their bosses' ass\"\n\nThis year from Open Source, I'm guess we'll see more progress in:\n\n* responsive Avatars and interactivity\n* live back and forth communication with LLMs using speech (no push to talk)\n* RAG to the point it will \"just work\"\n* More, and dynamic fine tuning and behavior flexibility/adaptability.\n* Better/easier task following from models\n* more stuff I can't even think of\n\nBig business will steal/borrow/share ideas from the above, but won't have the flexibility like open-source will. Bringing a (non-jank) product to market takes time.\n\nAnd then the clown finishes it with \"disagree?\" because the algorithm requires engagement!\n\n### Comment ID kfy6k1z with +1 score by [(heytherepotato, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfy6k1z/) (in reply to ID kfy2xea):\nI'm leaving my crap grammar on the parent comment but I wanted to expand on some of the experiences I've seen.\n\n* Earlier in the year, with some coaxing of gradio, VAD, whisper, and silero tts I could have an almost live conversation with a LLM, no push to talk.\n* I could take that further and use a speech emotion recognition model to add flavor to the text that goes to the LLM to say \"the speaker said this with a happy voice\"\n* I could use CLIP and a web cam to periodically feed the LLM status updates so that it could takes notes that \"the user is currently sitting on the couch\" or \"the user is sitting at their desk\" and then feed it the time and a work hour schedule so that it could use that information in context.\n* And when I wanted to really to be fancy, I added speaker diarization to be able to discern when it was me talking, or the TV, or someone else.\n\nbut....if I want to mumble to myself that \"I'm not sure if I have milk\", it almost always suggests to add some to a shopping list to get tomorrow morning(thereby ruining breakfast!) or to order some online. Ideally it will just remind me next time I'm up to check the fridge. The answer would be to do some fine tuning of some non-traditional and less 'generic' thinking but it's just not something I can be bothered doing right now.\n\nAnother area where I'm not getting the right behavior from them is when I put together a prompt of \"mock state\" of a game world, and it doesn't have the tuning or reasoning to provide useful advice or discussion. Kind of like generating a text adventure for an LLM from a 3d game, e.g.\n\n* A JTAC in a flight sim game, being fed a simplified state of the world. \"Player is at grid x in an F22 and headed north west, to the north are 2 Mig29. Does the player need to be told anything, here's the history of previous states...\" etc\n* \"The player is playing Skyrim, they have just completed quest X. The player said they were planning on finding some iron to make some daggers. Here's a history of previous states. Does the player need to be told anything?\" Maybe add in some RAG to google good places to get iron in Skyrim for extra usefulness etc.\n\nAnd it's very hard to get a useful output from the models in those scenarios. Again, I think the answer is fine tuning to train in behaviors because those scenarios wouldn't have strong representation in the training data, but it's also hard to spend a lot of time and energy to build and train when we seem to get new models and paradigms every month that make the effort redundant.\n\nThat said, I am slowly accumulating a set of what I think are useful interactions/behaviors, and I recommend everyone do so because models are fleeting but datasets are forever.\n\n#### Comment ID kw631k9 with +1 score by [(og-mango-jesus, Reddit, 2024-03-23)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kw631k9/) (in reply to ID kfy6k1z):\nI just clicked through your profile after the comment i saw you made on whiterabbitneo. this was the next post i saw you make. i like the cut of your jib. dont ever change.\n\n## Comment ID kfy8n3p with +4 score by [(wellshitiguessnot, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfy8n3p/) (in reply to ID 18wasf8):\nI totally disagree with this. Specifically because GPT-4 is a product, accessible through ChatGPT or an API both of which you pay for and both of which are hosted by people holding reigns with no other hosting options.\n\nAlso hosting more efficient small models locally has the advantages of total privacy and the end user can find tune themselves. Model use cases are subjective and GPT-4 can't beat being able to make your own tuned private llms.\n\nBenchmarks - they're a shot in the dark and the usefulness of a model for ones purpose is theirs to determine; hellaswag, gsm8k, etc cetera don't mean a thing if the actual end user finds the AI insufferable for their use case. GPT-4 is killer for coding.. but roleplay? Creative writing? Terrible.\n\n## Comment ID kg0eqkb with +4 score by [(sixpointfivehd, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kg0eqkb/) (in reply to ID 18wasf8):\nDisagree due to censorship. It has been proven that censorship vastly reduces a model's ability to function and Open AI is doing more censorship than ever.\n\n## Comment ID kfwotod with +11 score by [(petrus4, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfwotod/) (in reply to ID 18wasf8):\n- The earliest open source models AFAIK were 6-7b.  The biggest open source models we have now are 120b, with more commonly sized ones being 70b.  That's an increase of almost 20 times in 12 months.\n\n- GPT4 is rumoured to be an 8 core 120b MOE model.  Goliath is a single core 120b model.  7 more instances of Goliath or the equivalent, and we're there.\n\n### Comment ID kfxomym with +6 score by [(jd_3d, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfxomym/) (in reply to ID kfwotod):\nBloom came out mid-2022 and had 176B parameters though... The trend seems to be going up on average but a lot slower than people might think.\n\n#### Comment ID kfxpdba with +2 score by [(petrus4, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfxpdba/) (in reply to ID kfxomym):\nI am not closed in general to arguments that we will not reach parity with GPT4 this year.  I ***am*** pretty much closed to OP's specific argument, though; to the extent that I feel it can even be called one.\n\n## Comment ID kfwpd0p with +10 score by [(International-Try467, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfwpd0p/) (in reply to ID 18wasf8):\nDisagree because Meta is somewhat of the same thing as OAI\n\n## Comment ID kfwxcij with +11 score by [(PrysmX, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfwxcij/) (in reply to ID 18wasf8):\nAlready lost me at #1 because how much an employee is paid does not automatically equate to the quality of work they produce. I've seen a lot of \"grunt\" workers produce far better results than colleagues paid 50% more than them.\n\n## Comment ID kfws48p with +6 score by [(None, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfws48p/) (in reply to ID 18wasf8):\nWe’ll see. While there is some logic, the arguments sound a lot like “Linux will never beat Microsoft Windows” in late 1990s.\n\n## Comment ID kfx0p32 with +6 score by [(lordlestar, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfx0p32/) (in reply to ID 18wasf8):\nMeta has the same money, talent and data, and they are releasing OSS, wait for llama3\n\n## Comment ID kfxmdia with +5 score by [(MikePounce, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfxmdia/) (in reply to ID 18wasf8):\nI disagree. The quality of the output is not yet generally superior to GPT-4, but it's catching up and more importantly *LLMs have other advantages GPT-4 can't have* :\n\n* privacy, \n* ability to run offline,\n* NSFW queries,\n* queries that aren't NSFW but trigger the content policy warning (ex: Tarantino's Ezekiel 25-17),\n* ability to fine-tune\n* no API cost when run locally \n...\n\n## Comment ID kfx5tq0 with +3 score by [(SinoirLancelot, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfx5tq0/) (in reply to ID 18wasf8):\nTime will tell.\n\n## Comment ID kfxrj33 with +3 score by [(Ylsid, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfxrj33/) (in reply to ID 18wasf8):\nI have heard LLMs compared to mainframe computing. Now, the processing requirements are out of reach for most people.  But how about in 20 years? Each use-case has a logical limit, and it will be difficult to justify using a massive and expensive model over an open, cheaper one, when for a specific case they both perform equally as adequately.\n\nWe see this already for stable diffusion. The DALLE tier gigamodels can do everything, but the smaller and more specialised models can do their particular thing as well if not better.\n\n### Comment ID kfyou6q with +1 score by [(monnef, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfyou6q/) (in reply to ID kfxrj33):\n> The DALLE tier gigamodels can do everything, but the smaller and more specialised models can do their particular thing as well if not better.\n\nDALLE3 on all platforms I tried (bing image creator, chatgpt gpt 4 and dalle gpt) is censored as hell. You can't even mention styles (as far as I know styles have no legal protections whatsoever, so it doesn't even make sense). There are also problems with tooling - for example making a similar image is almost impossible in dalle3, because it's image->text->image and a lot is being striped from the text (e.g. universe in background has same feel, but a subject - a sexy woman becomes an ugly zombie; happened to me just few days ago). In Stable Diffusion, even old 1.5, this is trivial - just img2img, write prompt (or use interogator) and do few generations with different denoising strengths. Not to mention you have options how to create these \"similar\" images, like controlnet (depth, lineart, scribble, ip adapter) and may use multiple of these stacked on each other. There are tools which alleviate to some degree even the worse prompt adherence (mixing concepts, colors etc) like regional prompting or you can insert it afterwards (e.g. inpaint with changed prompt, or rough sketch with inpaint etc).\n\nSo if you want ultimate control - to have the result you want, I don't think there is anything even remotely comparable to open access stable diffusion. If you want quick images without virtually any control and arbitrarily limited on what you are allowed to generate (e.g. attractive women, what an offensive thing to generate...), then dalle3 for illustrations (especially if you need that good prompt adherence - can save a lot of time you would spent with inpainting) or new midjourney for photos and digital art might be enough.\n\nThe way I see it - those commercial ~~models~~ services feel like a toy. They produce flashy images, but you can't be much / at all creative, there isn't much beyond writing a prompt (which if too complex or long will be mostly ignored; even in prompt I find SD in A1111 more powerful, because there is prompt editing and alterations, with plugins there are many more options). In my opinion open solutions for generating images already surpassed closed services.\n\nI haven't tried Firefly, since Photoshop is pricey and not available on Linux, but FOSS crowd has things like https://github.com/Acly/krita-ai-diffusion and https://github.com/zero01101/openOutpaint, so since Firefly is also censored, I don't think even that would presuade me to say proprietary services are better than open alternatives.\n\nPS: I am still pretty much beginner in stable diffusion, using just anonymous1111 web ui with a few plugins. ComfyUI can do some crazy things which I believe truly are impossible in all commercial products.\n\n#### Comment ID kfyqnkw with +2 score by [(Ylsid, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfyqnkw/) (in reply to ID kfyou6q):\nI was only using DALLE as an example- if DALLE were open sourced it would probably be a gigantic, do everything model. That said having extra and precise control over AI in general is always another benefit of OS and something megacorps are unwilling to facilitate.\n\n## Comment ID kfxrrh1 with +3 score by [(_SputnicK_, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfxrrh1/) (in reply to ID 18wasf8):\nRemindMe! 12 months\n\n### Comment ID kfyj6hl with +3 score by [(ttkciar, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfyj6hl/) (in reply to ID kfxrrh1):\n!remindme 12 months\n\n#### Comment ID m1vzfsf with +2 score by [(_SputnicK_, Reddit, 2024-12-13)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/m1vzfsf/) (in reply to ID kfyj6hl):\nSpoiler: OP was wrong.\n\n## Comment ID kfxteub with +3 score by [(Smeetilus, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfxteub/) (in reply to ID 18wasf8):\nOpenAi will be to Ai as AOL was to the internet. AOL introduced people to the internet. People even thought AOL was the actual internet.\n\n## Comment ID kfya8yu with +3 score by [(mathenjee, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfya8yu/) (in reply to ID 18wasf8):\nwhat if your internet is freaking down?\n\n## Comment ID kfygbes with +3 score by [(visarga, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfygbes/) (in reply to ID 18wasf8):\nI am getting better instruction compliance from open source models than GPT4 nowadays. Not to mention the tendency to be lazy, GPT4 often would not iterate all input data into structured form.\n\n## Comment ID kfwyguh with +4 score by [(Oswald_Hydrabot, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfwyguh/) (in reply to ID 18wasf8):\nFeel free to provide a source bub because distributed training is not far off.\n\nI come to this sub to get info about FOSS LLMs.  Fuck OpenAI and fuck this post.  Get out of here with this corporate ghoul bullshit.\n\n## Comment ID kfx3mht with +3 score by [(omniron, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfx3mht/) (in reply to ID 18wasf8):\nI strongly disagree with this\n\nChatGPT and OpenAI has its place, but oss models are good enough for a huge range of tasks and can be hosted for far cheaper than ChatGPT calls\n\nAn app we’re building can have 80% of the LLM tasks done with oss, and 20% done with ChatGPT for the most critical user facing functions\n\nand once we see ai accelerators like groq be more popular, and multimodal MoE models being optimized for them, OpenAI is done\n\nI think OpenAI needs to innovate waaayy more or they’ll be out of business within 5 years.\n\n## Comment ID kfxjgh3 with +5 score by [(kulchacop, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfxjgh3/) (in reply to ID 18wasf8):\nI totally disagree with this. If you think open source models will not beat GPT-4 this year, you are wrong.  \n\n\nOpen source base models have these constraints. But their fine tunes will come close to GPT-4 by generating synthetic data from other closed models and simply merging multiple models.\n\nThe points from the tweet are valid only in the context of open source models beating GPT-5.\n\n## Comment ID kfx65bd with +2 score by [(Sidion, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfx65bd/) (in reply to ID 18wasf8):\nI love how everyone stands on their soapbox and yells their takes as if they're gospel. The google ai part is the funniest. I wasn't aware google brain researchers had access to OpenAI's proprietary dataset to make judgements like this on it...\n\n## Comment ID kfx86pc with +2 score by [(lNylrak, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfx86pc/) (in reply to ID 18wasf8):\nI just hope it gets better. It doesn't have to beat GPT 4 in every aspect, especially if it can run locally in a consumer friendly machine.\n\n## Comment ID kfxe8i1 with +2 score by [(AndrewH73333, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfxe8i1/) (in reply to ID 18wasf8):\nHe says no open source model will best GPT-4 and then immediately says a better model can’t beat it because it’s a product. So he thinks there might be a better model, right?\n\n## Comment ID kfxfh7m with +2 score by [(Dankmemexplorer, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfxfh7m/) (in reply to ID 18wasf8):\nmight beat current gpt-4 performance but my bet is that FOSS will probably lag behind bc big labs can just burn investor money on compute to make new and better models\n\n## Comment ID kfxfp1e with +2 score by [(gte959f, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfxfp1e/) (in reply to ID 18wasf8):\nLargely the arguments are short sighted.\n\n1. More AI experts will be minted over the next few years than those that work at OpenAI today.\n\n2. The data OpenAI is training on is mostly “open”.  As a percentage the human feedback is small.  It’s fairly conceivable a Wikipedia type approach exists in the open to simply tell a model yes/no on its output.  This can also be funded by industry groups that are forming today.\n\n3. Open source is not necessarily equivalent to decentralized and unorganized.  See Linux kernel and distros that sit atop it.  See kubernetes or Apache any of hundreds of critical infrastructure software running the world today.\n\n4. Model vs product is actually one of the reasons alternatives have more scope - they don’t need a better model just a better product and as far as products go there isn’t much of one at OpenAI.\n\n5. If you are referring to training infrastructure you do have a point here since google, Microsoft, etc can bankroll purchases of large graphics card farms.  Training time for a GPT4 level LLM from scratch is prohibitive cost wise due to this.  It is possible to fine tune open source models contributed by one of the large entities for general needs.  More importantly it is not the case that such a large LLM is actually necessary for most use cases.  Narrow LLMs for things like code assist are cheaper to train and run inference on.  I’m addition there is a need to have them closer to users on edge devices. Basically just because you can create a model with trillions of parameters does not mean most utilitarian use cases will need it.  Eventually compute costs drop or more efficient training methods or architectures will be created.\n\n## Comment ID kfxrh2d with +2 score by [(Chambellan, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfxrh2d/) (in reply to ID 18wasf8):\n“Beating GPT-4” misses the point. Regardless of which model is currently breaking benchmarks, they’re all getting better fast. I don’t need an F1 car. A nice, free, Ferrari would suit me just fine.\n\n## Comment ID kfxvnxw with +2 score by [(_____redditor______, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfxvnxw/) (in reply to ID 18wasf8):\nThis year, maybe. But if we take a look at the example of Android OS development vs iOS, SymbianOS or another proprietary OS, you will see that open-source will always have a place to grow and on par with proprietary products. If there are many users using open-source model, there will be many people and communities accessing it, leading to a better and broader development of the (distro/fine-tuning) models.\n\n## Comment ID kfxw1s8 with +2 score by [(bakaldo, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfxw1s8/) (in reply to ID 18wasf8):\nI could be wrong using this example but... you could think of TAS gameplay as GPT-4 and speedrunners as decentralized open source  advances...\n\nthe fastest Mario from NES speedrunner  is milisecondss away from beating a tasser, it might never catch it,  but there will come a time where the difference  will be irrelevant\n\n## Comment ID kfy0vu2 with +2 score by [(i-FF0000dit, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfy0vu2/) (in reply to ID 18wasf8):\nFuck this. I think we should take this as a challenge.\n\n## Comment ID kfy280d with +2 score by [(-BlueTear-, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfy280d/) (in reply to ID 18wasf8):\nOpen source models are already winning when it comes to flexibility, you can't use GPT-4 for anything NSFW or use it locally. So it depends on what one means with \"beat\".\n\n## Comment ID kfy8dig with +2 score by [(IntolerantModerate, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfy8dig/) (in reply to ID 18wasf8):\nI think a better question is can openAI make a 7B or 13B model that is materially better than LLaMa or Mistral?\n\n## Comment ID kfyi0vg with +2 score by [(arekku255, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfyi0vg/) (in reply to ID 18wasf8):\nI disagree with number 4. Inference is fungible and I can switch provider and model by just changing two import statements.\n\nExample:\n\n    import api_horde as api\n    import model_noromaid as model\n\nCan easily be changed to:\n\n    import api_kobold as api\n    import model_puffin as model\n\n## Comment ID kg0jncf with +2 score by [(rkh4n, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kg0jncf/) (in reply to ID 18wasf8):\nHowever, mistral and deepseekcoder works so much better than ChatGPT4. I don’t have to continuously argue with ChatGPTs stupid censorship\n\n## Comment ID kg0q8a9 with +2 score by [(sentientmassofenergy, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kg0q8a9/) (in reply to ID 18wasf8):\nNo one is going to \"beat\" anyone.  \nApple OSX hasn't beat linux any more than it has beat windows.  \n\n\nI dont care if OAI exists to the end of time or google established market dominance.  \nWhat I DO care about is open access to super inteligence.  \nSure nothing has \"beat\" gpt4 yet, but by this year's end we *will* have a gpt4 level open source model; the fact that gpt5 will be out by then is irrelevant.  \nIf open source lags behind, so beat it.   \nBut open source will not be \"beat\", and ASI *will* be open source eventually.\n\n## Comment ID l4bsxdk with +2 score by [(cheatreatr, Reddit, 2024-05-16)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/l4bsxdk/) (in reply to ID 18wasf8):\nUNTIL ANY LLM-based bot can DEFINITELY & COMPLETELY distinguish between humor & sarcasm, results will ALWAYS be biased. Open-source has the inherent  strategic advantage of having both, using the frequent disagreements among open-source developers\n\n### Comment ID l4buoti with +1 score by [(CeFurkan, Reddit, 2024-05-16)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/l4buoti/) (in reply to ID l4bsxdk):\nTrue\n\n## Comment ID lf1yz3w with +2 score by [(JawGBoi, Reddit, 2024-07-26)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/lf1yz3w/) (in reply to ID 18wasf8):\nWelp, llama 3.1 405b...\n\n## Comment ID m4ym2d4 with +2 score by [(bot-333, Reddit, 2025-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/m4ym2d4/) (in reply to ID 18wasf8):\nI’m confident that DeepSeek-V3 beats gpt-4 by some margin.\n\n### Comment ID m4ypan3 with +1 score by [(ttkciar, Reddit, 2025-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/m4ypan3/) (in reply to ID m4ym2d4):\nYep, this post aged poorly.\n\n#### Comment ID m4ypdf3 with +1 score by [(bot-333, Reddit, 2025-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/m4ypdf3/) (in reply to ID m4ypan3):\nOh, hi there! Haven't met you in some time.\n\n## Comment ID kfxgzr5 with +2 score by [(highmindedlowlife, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfxgzr5/) (in reply to ID 18wasf8):\nLiterally don't care. As long as local models keep moving forward that's good enough for me.\n\n## Comment ID kfxjyek with +3 score by [(Thistleknot, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfxjyek/) (in reply to ID 18wasf8):\nI don't buy it.  That's like saying indy games like dwarf fortress never will succeed at doing better because Microsoft made Rise of Nations and has sw dev's.\n\nHe is just speaking about PR hype.  OpenAI is a marketable product with a certain # of features implemented, enough to make a profit.  I don't think it's bleeding edge.  There are models out there that are on par.  I would say MoE is on par.\n\n## Comment ID kfx8c7k with +2 score by [(bullno1, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfx8c7k/) (in reply to ID 18wasf8):\n\\4. As a product, it's already beaten.\n\nI can customize sampling **per token** and format output exactly the way I want with local models. I see no reason to use a black box text-in text-out API.\n\n## Comment ID kfxp5pq with +2 score by [(cosmicr, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfxp5pq/) (in reply to ID 18wasf8):\nStrong disagree. There are already many models that come close to matching. All it takes is one more breakthrough, open or closed, and to bring it all together. I think it will happen in the next 6 months.\n\n## Comment ID kfxw6qj with +2 score by [(PercentageNo1005, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfxw6qj/) (in reply to ID 18wasf8):\nThis is BS\n\n## Comment ID kwkizuo with +1 score by [(Pesthuf, Reddit, 2024-03-26)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kwkizuo/) (in reply to ID 18wasf8):\nAs long as \"open source\" models receive most of their training data from automated GPT-4 prompts, being an inferior knock-off is all they can hope to be. \n\n\nAll the innovation in \"open source\" models that actually mattered came from Facebook anyway - looks like our only hope is for them to get better training data.\n\n## Comment ID l0pgew9 with +1 score by [(azuric01, Reddit, 2024-04-22)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/l0pgew9/) (in reply to ID 18wasf8):\nuh why does he think the product is so good. I have been using it for a while and it still fails every so often, the web UI is ok but not necessarily completely robust. LLama 3 70b is pretty good running it on a 3080ti, that''s rather impressive.\n\n## Comment ID l4ca8j5 with +1 score by [(cheatreatr, Reddit, 2024-05-16)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/l4ca8j5/) (in reply to ID 18wasf8):\nUNTIL the New York Times lawsuit is COMPLETELY resolved (appeals notwithstanding), ChatGPT WILL remain proprietary. Open-source, however, has the inherent advantage of an open community with experts who UNDERSTAND the complexities AND design issues surrounding the central weakness of ANY A.I. chatbot: understanding AND distinguishing between humor AND sarcasm. The requirements for LLM is always dependent on how sophisticated the LLM algorithms can address this issue. Between copyright lawsuits AND sophistication, ONLY thru an open-source approach to LLM algorithms, can such A.I. algorthim issues be SUSTAINABLY resolved\n\n## Comment ID lbvur10 with +1 score by [(Straight_Abrocoma321, Reddit, 2024-07-06)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/lbvur10/) (in reply to ID 18wasf8):\nCommand R+ is already better than 2 versions of gpt-4 3 months after discussion was created. It just shows how much open-source models are advancing over gpt-4 and others.\n\n## Comment ID kfwqi9h with +1 score by [(StunningBank, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfwqi9h/) (in reply to ID 18wasf8):\nStandalone open-source models will always be behind but still are highly valuable for smaller companies and businesses who  cannot afford gpt-4 or want to be independent. Same for cases when you need local models to work on device without internet.\n\nAlliances of many smaller companies will probably emerge with time and they may boost open-source models by sharing data and collaborating. \n\nI'd say open-source and openai/google have different markets and also may collaborate in some areas in future.\n\n## Comment ID kfxcixc with +1 score by [(Crafty-Confidence975, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfxcixc/) (in reply to ID 18wasf8):\nWhat the hell is point 4? It makes me think the dude has never even used a LLM outside of the ChatGPT setting. The entire point of language models is that the only UI you need is language. In some domains a simple CLI interface is enough to get better results than GPT 4 will do with censoring (ie. world generation in fantasy).\n\nSo what’s the product? I’m perfectly happy with some mixtral models on my laptop over anything ChatGPT spits out. Language in, language out and nothing else matters.\n\n## Comment ID kfxcp1m with +1 score by [(noiseinvacuum, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfxcp1m/) (in reply to ID 18wasf8):\nLet’s wait and see how Llama3 performs compared to ChatGPT4. \n\nAll points he listed down are negated by Meta.\n\n## Comment ID kfxdxnu with +1 score by [(decorrect, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfxdxnu/) (in reply to ID 18wasf8):\nSame guy would have said OpenAI could never beat google at Gen AI a year ago for the same reasons\n\n## Comment ID kfxi1tf with +1 score by [(Holiday-Regret-1896, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfxi1tf/) (in reply to ID 18wasf8):\nI agree, but still open source losing battle on general model\nIf there would be 10 specific lightweight specific use cases open source models - that would be competition\n\n## Comment ID kfxpgdz with +1 score by [(gthing, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfxpgdz/) (in reply to ID 18wasf8):\nNone of those things are reasons why open source can't beat gpt4.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n## Comment ID kfwrdvu with +1 score by [(Top-Opinion-7854, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfwrdvu/) (in reply to ID 18wasf8):\nInfrastructure is the big one\n\n## Comment ID kfwwkxm with +1 score by [(IffyNibba01, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfwwkxm/) (in reply to ID 18wasf8):\ni agree. open source models prolly won't beat GPT-4 THIS year.\n\nthat's all i'll say\n\n## Comment ID kfx39ru with +1 score by [(El_Minadero, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfx39ru/) (in reply to ID 18wasf8):\nIt really depends on the use case. I have no doubt GPT-4 outclasses most open source models across a range of benchmarks. However, if all you care about is specific domains, I already have seen how local LLMs are beating out what chatgpt can offer. On a cost of experimenting basis, open source solutions are still winning out.\n\n## Comment ID kfx56yl with +1 score by [(cshotton, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfx56yl/) (in reply to ID 18wasf8):\nYour premise based on models having to be centrally hosted and delivered with all sorts of supporting tools. This industry is barely off the ground. What seems like a plausible implementation and product offering today will be laughable in 2 or 3 years.\n\nSpecifically, decentralized, cooperating models running on edge devices are going to wipe the floor with monolithic businesses like OpenAI. As soon as the rough edges are knocked off the tech and a couple of iterations on efficiency and size are complete, you're going to run stuff on your desktop or mobile device that will do everything these giant, unwieldy cloud products do. Now add in the ability for peer devices to collaborate, with your specialized model talking to mine and a hundred others in a swarm, and well, your assertions seem quaint.\n\n## Comment ID kfx7ej0 with +1 score by [(opi098514, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfx7ej0/) (in reply to ID 18wasf8):\nI think this is mostly correct. However, we don’t need open source models to be better than chat gpt 4. We need ones to can be better at specific things than gpt 4.\n\n## Comment ID kfxge0j with +1 score by [(Inevitable-Start-653, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfxge0j/) (in reply to ID 18wasf8):\nI think there will be something so close people won't care that it's a little worse here or there, then the open source community will mod it to specialize in certain areas.\n\n## Comment ID kfxgg93 with +1 score by [(KakaTraining, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfxgg93/) (in reply to ID 18wasf8):\nGoogle AI is not a top AI research institution, but rather an organization that specializes in exaggerated publicity.\n\n## Comment ID kfxjmb8 with +1 score by [(Kaizenism, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfxjmb8/) (in reply to ID 18wasf8):\nOpen source will “win” when “The Internet” becomes self aware :)\n\n## Comment ID kfxjoys with +1 score by [(nborwankar, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfxjoys/) (in reply to ID 18wasf8):\nI think this is a strawman that a lot of people are beating.\nThe statement is vacuously correct and adds no useful knowledge to the collective wisdom.\n\nBecause open source vs ChatGPT or open source vs OpenAI is completely wrong framing. In 2024 local, smaller, private models will create a whole new massive market segment that centralized models just can’t touch because of data ownership concerns and black box vs transparency concerns and resource concerns. \n\nRe privacy - do we really trust OpenAI or Google or Meta or any one of the big boys when they promise “we won’t use your API driven data to train our models”? Sure!! Bridge, sell etc. \n\nIt’s the year of RAG, fine-tuning (loosely speaking) of small models and possibly Mamba if proven viable. OpenAI will still be there but huge new markets with space for 100’s of new application verticals will emerge. \n\nIt’s not open source vs OpenAI - that’s a fakeout and “psychological warfare” because OpenAI business model may have peaked. \n\nIt’s massive centralized black box models with unknown and variable performance vs customized rapidly updatable transparent models that will run on commodity hardware and the edge. \n\nBut most people on /r/LocalLlama already know this.\n\n## Comment ID kfxlqhy with +1 score by [(Someoneoldbutnew, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfxlqhy/) (in reply to ID 18wasf8):\nit's not about \"better\". having more money then god will guarantee their model wins on their tests. it's about leveraging the right tool for a task.\n\n## Comment ID kfxn6qo with +1 score by [(None, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfxn6qo/) (in reply to ID 18wasf8):\n[deleted]\n\n## Comment ID kfxnpfa with +1 score by [(stannenb, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfxnpfa/) (in reply to ID 18wasf8):\nUntil someone defines \"beat,\" this ia an empty prediction, in either direction.\n\n## Comment ID kfxns4g with +1 score by [(bot-333, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfxns4g/) (in reply to ID 18wasf8):\nRemindMe! 1 year\n\n### Comment ID kfxnvvx with +1 score by [(RemindMeBot, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfxnvvx/) (in reply to ID kfxns4g):\nI will be messaging you in 1 year on [**2025-01-02 04:21:03 UTC**](http://www.wolframalpha.com/input/?i=2025-01-02%2004:21:03%20UTC%20To%20Local%20Time) to remind you of [**this link**](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfxns4g/?context=3)\n\n[**3 OTHERS CLICKED THIS LINK**](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5Bhttps%3A%2F%2Fwww.reddit.com%2Fr%2FLocalLLaMA%2Fcomments%2F18wasf8%2Fif_you_think_opensource_models_will_beat_gpt4%2Fkfxns4g%2F%5D%0A%0ARemindMe%21%202025-01-02%2004%3A21%3A03%20UTC) to send a PM to also be reminded and to reduce spam.\n\n^(Parent commenter can ) [^(delete this message to hide from others.)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Delete%20Comment&message=Delete%21%2018wasf8)\n\n*****\n\n|[^(Info)](https://www.reddit.com/r/RemindMeBot/comments/e1bko7/remindmebot_info_v21/)|[^(Custom)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5BLink%20or%20message%20inside%20square%20brackets%5D%0A%0ARemindMe%21%20Time%20period%20here)|[^(Your Reminders)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=List%20Of%20Reminders&message=MyReminders%21)|[^(Feedback)](https://www.reddit.com/message/compose/?to=Watchful1&subject=RemindMeBot%20Feedback)|\n|-|-|-|-|\n\n## Comment ID kfxpny3 with +1 score by [(theaceoface, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfxpny3/) (in reply to ID 18wasf8):\nGPT4 is great for zero shot learning but many people will be doing finetuning and that's where models like LLAMA 2 and MIXTRAL are important. They dont really need to be as good as GPT4 in few shot learning scenarios\n\n## Comment ID kfxt8a0 with +1 score by [(ICE0124, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfxt8a0/) (in reply to ID 18wasf8):\nWith most stuff open source is always behind. I say mostly because there are other open source products that are ahead of closed source options. But also I think the cost of making a model that is as good as gpt4 and can run on consumers hardware is very very expensive. \n\nBut they shall never forget that basically their entire foundation is heavily reliant on open source software.\n\n## Comment ID kfxxao0 with +1 score by [(southpalito, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfxxao0/) (in reply to ID 18wasf8):\nIt is true. A competitive open source task force will require raising about half a billion USD to be able to go toe to toe in the big leagues.\n\n## Comment ID kfxydhp with +1 score by [(FeltSteam, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfxydhp/) (in reply to ID 18wasf8):\nGPT-4 will absolutely get beaten by Open Source. Im hoping within a month or two at most. This of course won't affect OpenAI that much, but an open source GPT-4 and a sub 10B open source GPT-4 would be cool to have.\n\n## Comment ID kfxz25t with +1 score by [(andrewlapp, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfxz25t/) (in reply to ID 18wasf8):\nMy prediction, an open source model will beat the best GPT4 version this year. OpenAI will release a new model that out performs the top open source model this year.\n\n## Comment ID kfy0vr5 with +1 score by [(olddoglearnsnewtrick, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfy0vr5/) (in reply to ID 18wasf8):\nI worked at OpenAI as a janitor for a week and as an expert I am calling this bullshit.\n\n## Comment ID kfy1786 with +1 score by [(Hanibal247, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfy1786/) (in reply to ID 18wasf8):\nMistral is definitely good enough to beat GPT-4\n\n## Comment ID kfy1ktr with +1 score by [(MannowLawn, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfy1ktr/) (in reply to ID 18wasf8):\nMaybe we just need a model for a use case instead of an all purpose full blown model. Maybe we won’t get an all around got open source model, but if I can get one working for my purpose I’m fine by it.\n\nCensorship is number one reason for me to get away from OpenAI. I can’t deal with that shit anymore, it’s a rat race every day and a waste of resources to change prompt every time.\n\n## Comment ID kfy4h44 with +1 score by [(Mysterious_Brush3508, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfy4h44/) (in reply to ID 18wasf8):\nI think there are two different versions of “beating GPT-4” or “overtaking OpenAI”.\n\nVersion 1: Achieving a model which is able to perform to an equivalent level as GPT-4 (today) **for the subset of tasks you care about**\n\nVersion 2: Creating a better general model than whatever OpenAI cooks up in the next 12 months.\n\nVersion 1 seems achievable and would be a massive win for the community and for humanity. Version 2 seems far more challenging (but is not what I need).\n\n## Comment ID kfy4ows with +1 score by [(ryanknapper, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfy4ows/) (in reply to ID 18wasf8):\nI don't think most people need the entire capabilities or infrastructure of GPT4 to run their waifu.\n\n## Comment ID kfy523o with +1 score by [(toddgak, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfy523o/) (in reply to ID 18wasf8):\nI'm curious what the metrics are for 'beating' GPT-4. I can think of a few ways which open source can exceed GPT-4 utility by even using GPT-4. \n\nNew models using synthetic data can be orders of magnitude smaller, running on accessible hardware with almost comparable results. I would consider that 'beating' GPT-4 because the cost to produce those models will be a tiny fraction to what OpenAI has paid for GPT-4.\n\n## Comment ID kfy8cbp with +1 score by [(Desm0nt, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfy8cbp/) (in reply to ID 18wasf8):\nYep. OpenSource can't beat GPT-4. In general. It's true. BUT!\n\nOpenSource can beat GPT-4 in very narrow specific tasks and domains with different models with specific fine-tunes.\n\nSo, what we actualy need - is large fast SSD, realy fast model-estimator for determining which model should be loaded to solve the task (maybe even cascade of swithcing models), and the ability to somehow cache the context in the way that it to be hot-swapped to a new model when switching to it without long cold promt processing (like it happens now, especially in MOE).\n\nAnd then just split the tasks into more accelerated specialized ones and increase the total number of accelerated specialized models.\n\nOf course, it will not be as fast and convenient as one huge model on a cluster of h100, but it is much more feasible (IMHO).\n\n## Comment ID kfy8sls with +1 score by [(MLTyrunt, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfy8sls/) (in reply to ID 18wasf8):\nNobody can be right or wrong about this, because nobody can know this in advance for sure, all we can think to have is a reasonable expectation, based on what we know. Engineers getting a million dollars does not tell me anything in comparison to the horde of FOSS AI engineers. \n\nI think it depends on how fast openai keeps moving the edge or if they got stuck. \n\nIf they get stuck (and they apparently already did  once) several times in a row, they'll have a problem from open source and closed source competition.\n\n## Comment ID kfy96w6 with +1 score by [(FewLifetimes_ago_21, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfy96w6/) (in reply to ID 18wasf8):\nAgree that it might take a long time to beat GPT4, but the points made here could be made for GPT3.5 too, since it's the same talent, product and data, and yet GPT3.5 has been beat.\n\n## Comment ID kfy9vn0 with +1 score by [(Monkey_1505, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfy9vn0/) (in reply to ID 18wasf8):\nBeing a product is a downside. And yes I disagree. Mistral medium is benching close to 3.5 turbo, and despite the quirks subjectively it feels not far off too. The next model will likely at least get close to 4 turbo (which is the real 'product' and not as good as the original version either).\n\n## Comment ID kfyayy9 with +1 score by [(LuluViBritannia, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfyayy9/) (in reply to ID 18wasf8):\n1. OpenAI doesn't have the monopoly of talent.\n2. Data is everywhere. Again, OpenAI doesn't own the world.\n3. That's sheer asumption. Open Source has been perfectly functional for decades now. \n4. That's not even an argument, lmao. Anything is a product as long as it's sold.\n5. So what? It hasn't prevented drastic improvements over the last year.\n\n&#x200B;\n\nThe idea that proprietary product can't be beaten by open source has been proven wrong countless times in many other instances.\n\n&#x200B;\n\nThis entire post could be used in a debate between Stable Diffusion and Dall-E, and look at where we are. Dall-E 3 doesn't have a SINGLE method to give control to the user. Not even image-to-image. \"Ohh, but it understands text so weeeelll!\". So what? That's not how art works.\n\n&#x200B;\n\nThat's why you don't just grab somebody else's arguments.\n\n## Comment ID kfyb4b4 with +1 score by [(daedalus1982, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfyb4b4/) (in reply to ID 18wasf8):\n1. Neat\n2. They're being sued currently by close to 20 different entities over that \"massive proprietary chatgpt dataset\" if any of those suits lands...\n3. mother of all \\[citation needed\\].  \n4. nooooo gpt-4 is a model.  I know this because I license it for use on products we develop.  ChatGPT is the product. What even IS this point?  \n5. is this just a revision of point 1?  big corporations with lots of money can spend it on doing things in a dedicated way that cannot be matched by people having to do this part time. We've always known this\n\nLike... I'm sorry if any of the stuff he mentioned disappoints any of you.  But none of this is news and this is still worth doing.\n\n## Comment ID kfybd76 with +1 score by [(_iamhamza_, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfybd76/) (in reply to ID 18wasf8):\nOpenAI is anything but open. I'd still use a good open source model over GPT-4, and I wouldn't notice a difference. I agree that GPT-4 will most likely be better than any open source model out there, but I genuinely think that would be the case at an enterprise-level, not the personal-level.\n\n## Comment ID kfybvs1 with +1 score by [(Lemgon-Ultimate, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfybvs1/) (in reply to ID 18wasf8):\nGPT-4 is the SOTA AI model, that's visible for anyone comparing these models. However, it gets easier to train models when Openai releases their own next gen GPT model like GPT-4.5 or GPT 5. These can generate datasets and logic patterns for other AI to train on. There's talent required I agree on but it's silly to assume that no one will ever match the experts at Openai, Mistral proved that there are other AI companies with visions that are more open source friendly.\n\n## Comment ID kfyce9d with +1 score by [(_RealUnderscore_, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfyce9d/) (in reply to ID 18wasf8):\nFor sure, most open-source models are passion projects and/or by university students. High budgets and proper development always have the extreme upper hand compared to open-source, just an unfortunate truth. Well, fortunate that there'll always be a public model (ha, pun intended) for open-source projects to strive towards, unfortunate that it won't be, well, open source.\n\n## Comment ID kfycwee with +1 score by [(ambient_temp_xeno, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfycwee/) (in reply to ID 18wasf8):\nMixtral instruct already beats GPT4 for every case in which GPT4 refuses.\n\n## Comment ID kfyd1fc with +1 score by [(Maleficent_Employ693, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfyd1fc/) (in reply to ID 18wasf8):\nI would agree if this dude was reliable but since he isn’t got4 we are coming for you \n\nSam said open source wouldn’t beat 3 we just beat 3.5\n\n## Comment ID kfydsf3 with +1 score by [(None, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfydsf3/) (in reply to ID 18wasf8):\nThose are stupid observations. Will close sourced models be on paid with closed source? Maybe? It will be good enough.\n\nBut he is missing one point. They will tell truth vs closed sourced models that are censored. \n\nPeople don't want censored models.\n\n## Comment ID kfydwxg with +1 score by [(keepthepace, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfydwxg/) (in reply to ID 18wasf8):\nBig agree on 1.\n\nSlight disagree on 2. Open source team can crowdfund bigger models than companies can if they need it.\n\nOpen minded on 3. There is a balance with 1. If you can offer remote work you have access to a bigger pool of talents and decentralization works better for breadth of development, probably worse for speed on specific features.\n\nDisagree on 4. The product that ChatGPT is has been reproduced by FOSS fairly easily. The model and the infra to run it is the important part. \n\nOn 5. I don't feel competent enough to comment. Haven't trained huge models. I know that up to 8x H100 you can find machines \"easily\". Open source and researchers are actually more interested in making existing models more compute-efficient. They see value in reproductibility. OpenAI and Google sees value in needing a big centralized infra that can't easily be duplicated.\n\n\nAnd I'll never \"disagree\" on Twitter since I realized that they give monetary incentives to trigger angry answers and that most answers are actually never viewed by anyone else than spammers. Never had engaging conversation in twitter like I had on lemmy or reddit.\n\n## Comment ID kfyey55 with +1 score by [(DerfK, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfyey55/) (in reply to ID 18wasf8):\nI think that public opensource projects just can't afford the compute to create GPT4 level models until someone invents useful distributed training. (I'll donate unused gpu cycles to learning@home).  The other issue is data acquisition to learn from, a publicly sourced dataset would be *interesting* but in the same way that it was *interesting* to see how quickly 4chan ruined Microsoft Tay.\n\nI think open source will continue to do best focusing on the tooling and actual software using LLMs, especially if we can get more self-assembling \"products\" in other fields like oobabooga is for web chat.  Home Assistant+local LLM stack, anyone?\n\n## Comment ID kfyfjgk with +1 score by [(None, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfyfjgk/) (in reply to ID 18wasf8):\nI disagree a lot, one year is a long time and now that we have a model that actually uses the expert architecture I think an open-source better GPT-4 isn't out of the question for the first half of the year, and especially for the entire year honestly, that said I also think OpenAI has a better model already since GPT-4 was originally going to release alongside GPT-3 apparently so I do think there's no beating openAI in general, that said GPT-4 is definitely very beatable to me now, especially if Mistral does it but even without them I think it can be done.\n\n## Comment ID kfyggqp with +1 score by [(Minute_Attempt3063, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfyggqp/) (in reply to ID 18wasf8):\nSure OpenAI can hire people with a salary of more then a million...\n\nThis doesn't stop Google for using their data centers for training... since they have way more data as well, and way more compute worldwide..\nI also don't see OpenAi making anything like AlphaGo in the next X years. Maybe that is jot relevant,  but I think Google has just the right people in their belts to make a proper LLM model\n\n## Comment ID kfykc6f with +1 score by [(ah-chamon-ah, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfykc6f/) (in reply to ID 18wasf8):\nWhat if we all banded together as an internet \"society\" with an ideal of combined human conciousness creating the best A.I we can that is open source?  \n\n\nThe one thing a big company has is that they pay people money that drives and compels them to do it. If we all had the organized purpose to work together we could out do gpt4 quite easily.  \n\n\nWe just aren't that organized.\n\n## Comment ID kfym9jx with +1 score by [(CulturedNiichan, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfym9jx/) (in reply to ID 18wasf8):\nShort term, sure. But it wouldn't be the first time one of those pundits claims that x tech thing will be so expensive that only the five richest kings in Europe would own them.\n\nAnd this corpo spook, because he has all the traits of a damn corpo spook, just look at the pic, just look at the things he babbles, misses one point: many people will do ANYTHING to avoid depending on corporations, their censorship, their agenda, their terms of service. My local Mixtral has 0 terms of service I genuinely care about.\n\nWhat about OpenAI? No siree.\n\n## Comment ID kfyne6k with +1 score by [(TurianHammer, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfyne6k/) (in reply to ID 18wasf8):\nThe point that this paves over is that an open-source model can be **private** and can also be fine-tuned to attempt to **remove bias**.\n\nI loved chat GPT and find myself still using Bing AI but for the serious questions I use my uncensored local AI and then I will valid it's statements using generic searches.\n\n## Comment ID kfypfbk with +1 score by [(Niigel_cyborking, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfypfbk/) (in reply to ID 18wasf8):\nstrongly disagree on point 3.. if a decentralised open source work environment is so bad then how come things like the linux kernel are still maintained this way. ( i refer to the linux kernel here as an easy large scale example however there a tons of other things being build and maintained this way) honestly sounds like this guy just wishes opensource fails because he'll profit from that\n\n### Comment ID kfyppcu with +1 score by [(Niigel_cyborking, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfyppcu/) (in reply to ID kfypfbk):\nofcourse it could be that it doesnt work well for the this type of development but i find that unlikely as we are already seeing pretty good models coming from the opensource community. if anything I'd say data collection would be the biggest point as traditional centralized business will probably have much more data and analytics than their opensource community counterparts\n\n## Comment ID kfyq8rd with +1 score by [(Kep0a, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfyq8rd/) (in reply to ID 18wasf8):\nthis is the dumbest take I've seen on AI meant to ruffle feathers\n\n### Comment ID kfyq9sb with +1 score by [(haikusbot, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfyq9sb/) (in reply to ID kfyq8rd):\n*This is the dumbest take*\n\n*I've seen on AI meant*\n\n*To ruffle feathers*\n\n\\- Kep0a\n\n---\n\n^(I detect haikus. And sometimes, successfully.) ^[Learn&#32;more&#32;about&#32;me.](https://www.reddit.com/r/haikusbot/)\n\n^(Opt out of replies: \"haikusbot opt out\" | Delete my comment: \"haikusbot delete\")\n\n## Comment ID kfyqp9h with +1 score by [(g_rich, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfyqp9h/) (in reply to ID 18wasf8):\nThis is all true, but open source models that are both more accessible and can run in consumer hardware, models that are good enough in most situations, are more flexible and will ultimately be the drivers in this space than those offered by companies like OpenAI. Just look at Windows versus Linux or any popular open source project versus its closed source equivalent. \n\nThis isn’t winner takes all, you can have both commercial and open source models and they can both be successful. As we’ve seen in the open source space what will ultimately happen is open source will feed into closed source and vice versa to the benefit of both. We already see this with Facebook, Google, Microsoft and OpenAI. Anyone that sprout’s comments such as this while ignoring this are being disingenuous at best and more likely trying to “sell” something; going by past actions by Arnaud this is most definitely the case here.\n\n## Comment ID kfyrhl4 with +1 score by [(Reasonable_Virus1816, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfyrhl4/) (in reply to ID 18wasf8):\nThis completely ignores that the dynamics of OSS are just radically different. This comparison focuses on the capacity of OSS to compete with OpenAI on one product offering, which never was put in question. The strength of OSS is the ability to cover a much wider spectrum of UXs and applications, which OpenAI can't do, otherwise they lose focus. OpenAI can basically do well at two things: develop better capabilities (as they've done by leading with GPT-4) and lead on DevX for AI (which they've also lead). The two have strong synergies, but I'm not sure they can expand far beyond this without losing focus as a relatively small team. OSS explores all options by default.\n\n## Comment ID kfyrkng with +1 score by [(sebo3d, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfyrkng/) (in reply to ID 18wasf8):\nI mean... for my specific use case(roleplay) it very well might. Just because Open Source won't create a model that surpasses GPT4 in all use cases doesn't mean it won't in one specific fields. I mean just look at how far Open Source progressed in 2013. We started the year where you needed a super computer to run a 6B parameter model which sucked so bad it was unironically sad. Today, Those who could only run 6Bs back then now run quantized 70Bs that offer coherency, creativity and intelligence on par if not surpassing Turbo.\n\n## Comment ID kfyrrmb with +1 score by [(Reasonable_Virus1816, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfyrrmb/) (in reply to ID 18wasf8):\nAlso can anyone explain how is public cloud infra so terrible compared to OpenAI's? I know they have dedicated infrastructure teams + Microsoft partnership, but surely an experienced team starting with dedicated instances on lambda/AWS can compete. (disclaimer: i'm considering more the perspective of building on top of models, and at most fine tuning, not building new capabilities, which I think is a given considering the fixed cost of training a large model)\n\n## Comment ID kfyu1vv with +1 score by [(Educational_Farmer73, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfyu1vv/) (in reply to ID 18wasf8):\nOpen source will outlive chatgpt\n\n## Comment ID kfyuxko with +1 score by [(durden111111, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfyuxko/) (in reply to ID 18wasf8):\nI don't think anybody has said we are going to beat GPT4 locally. openAI has hundreds of millions of dollars invested lmao. GPT3.5 is certainly beatable though.\n\n## Comment ID kfyv01j with +1 score by [(No-Roll8250, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfyv01j/) (in reply to ID 18wasf8):\nAs long as they can’t create ecosystem or functional lock in, gpt4 is not really a product… what does he mean with public cloud infra? You can host paid on azure or cloudflared, etc. it’s not magic.\n\n## Comment ID kfyy5gh with +1 score by [(davew111, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfyy5gh/) (in reply to ID 18wasf8):\nYou have no moat. As long as private companies keep lobotomizing their models for the sake of political correctness, it's only a matter of time.\n\n## Comment ID kfz1rdf with +1 score by [(obiouslymag1c, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfz1rdf/) (in reply to ID 18wasf8):\nDefine what \"beating GPT4\" means.\n\nMy use case involves classifying and summarizing ~100m or so documents. GPT-4 does an amazing job at ~65% out of the box using few shot with examples in the context window. Using a decent foundational model however - even a fairly small 3B model, and fine tuning for my use case using some good old fashioned supervised learning gets me to 90%+ and it won't cost me tens of thousands of dollars.\n\n## Comment ID kfz3b7c with +1 score by [(rustyrazorblade, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfz3b7c/) (in reply to ID 18wasf8):\nOpenAI isn't paying 1MM in cash salaries.  It's total comp, mostly in shares, and you can't sell the shares for 2 years.\n\n## Comment ID kfz4etw with +1 score by [(Nilvothe, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfz4etw/) (in reply to ID 18wasf8):\nThe fact that this discussion even exist is a MASSIVE win for open source models in my book.\n\n## Comment ID kfz5fcg with +1 score by [(freehuntx, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfz5fcg/) (in reply to ID 18wasf8):\nOpensource models already outperform closed source models in areas like NSFW chats.\n\nAs long as openai lobotomizes their models it simply isnt as powerful as it \\*could\\* be.\n\n## Comment ID kfz6obv with +1 score by [(dphiggs01, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfz6obv/) (in reply to ID 18wasf8):\nWho will win this race is still very much up in the air.\n\nSome of the provided points are good, but they also have counterpoints that are equally strong, if not stronger.\n\n1. Talent - \tYes, open AI has many great engineers driven by million-dollar salaries. However, future breakthroughs in AI are likely to come from these places, as well as academia and places like Meta and IBM, where democratizing AI is a paramount concern.\n2. Data - Yes, Massive proprietary datasets are an advantage for OpenAI, but much research is underway to improve foundation models with much less data. https://arxiv.org/abs/2309.00267\n3. Team Structure - This is a flat-out No; there is no objective evidence that I am aware of that proves this point. All of Academia works in a non-centralized manner in which ideas are freely exchanged and built upon. Not to mention that the arguably greatest invention of humankind thus far is the internet. It is built and operates on open-source software in a distributed manner.\n4. Model vs product. Open source will not win by creating a better product. Open source will win by creating an eco-system in which many specialized and unique products will win in their individually selected specialties.  \n5. Infrastructure. This category should be $$$ as a company with an 80 Billion dollar valuation and full access to MS Azure has a clear advantage. But I would not count out the Open Source community to find a competitive solution and push the frontier of needing fewer resources for training and finetuning highly optimized models.\n\nJust a few counter-thoughts.\n\nHappy New Year! 2024 is going to be a crazy ride for AI\n\n## Comment ID kfz7768 with +1 score by [(LocoMod, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfz7768/) (in reply to ID 18wasf8):\nHis second sentence is an appeal to authority. He worked at “top ai research labs”. And? \n\nIsn’t he essentially disproving his own argument? Why hasn’t Google Gemini beat GPT4 yet given the blank check and “top tier” talent and years of head start? \n\nBecause none of the arguments he made guarantees success. \n\nThe only thing I agree with is GPT is a platform, not a model. You cannot compare an open source raw model with the GPT platform. \n\nAt best what we can say is the OpenAI product is more mature than alternatives, since they have a head start in ML Ops, if you will, and history will churn and a group of young inexperienced strangers will inevitably leapfrog them given enough time. \n\nWe may never know though, since the big companies will swoop in, offer 7 figure checks, and absorb them. Then they’ll have some guy post on X about how they have the “best teams”. Yea, you bought your competition.\n\nThe argument of compute power is also moot. OpenAI, I mean, Microsoft may have giant data centers powering GPT4, but they have to split that compute capacity among hundreds of millions of users, and an even larger number of prompts per second. They don’t have enough compute capacity to serve everyone the full model. Now we get the gimped quantized version that is lazy and a former shell of what was available when it first came out. The amount of compute power required to serve one user is not much more than what can be done on a modern laptop, but they have a LOT of users. \n\nSo my prediction is that as alternatives get better and more competitive, users leave the platform which frees up capacity, which then means they can pump up the complexity of the workflow and serve full model, dangle the carrot and show us how much better it is, then when a few million users return to test this improved version, it gets gimped again due to capacity constraints. \n\nThis is one of those things where you can’t be both successful, and remain the best. The better you are, the more users you get, the more stress put in the platform, and then the cycle repeats where the inferior model and workflow is served. The “Great Filter” wins again.\n\n## Comment ID kfz7cu6 with +1 score by [(bran_dong, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfz7cu6/) (in reply to ID 18wasf8):\nlol at this guy. he's coping hard. \"you guys can't do it because I can't see it happening\" is such a little dick energy statement when nobody knows how fast this stuff is gonna develop.\n\n## Comment ID kfz7wvw with +1 score by [(UnorthodoxEng, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfz7wvw/) (in reply to ID 18wasf8):\nBlah blah - we've got more money than you, that makes us better.\n\nOn second reading, it sounds defensive - as if he might be worried and can't think of any decent reasons.\n\nUnfortunately, GPT 4 has become progressively worse as they've added more & more to the background, system prompt.  It's drowning out the user prompt.  This has to be one of the reasons OS AI appears to be catching up.\n\n## Comment ID kfza2dk with +1 score by [(Musicprotocol, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfza2dk/) (in reply to ID 18wasf8):\nI disagree… history has shown in the long run nothing beats open source.. just look at every piece of software out there the top is open source… in the beginning commercial may have some advantages but over time you can’t compare with the millions of passionate developers.\n\n## Comment ID kfzawm0 with +1 score by [(Useful_Hovercraft169, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfzawm0/) (in reply to ID 18wasf8):\nYeahbut the dudes at Mistral are no slouches!\n\n## Comment ID kfzc1t5 with +1 score by [(2CatsOnMyKeyboard, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfzc1t5/) (in reply to ID 18wasf8):\nI can sum it up differently.\n1. Has all the moneys.\n\nFOSS projects are rarely well funded. When huge server parks and top notch researchers and devs are needed, as well as lawyers to deal with all the copyright claims coming your way, well... money is more than a little advantage.\n\nLike, further development of Linux kernel or a web server or some nice libraries don't need that much expertise and resources to keep up with an exploding market and field of applications.\n\nI find this highly problematic by the way. It will be the next thing that's out of our hands and in the hands of ruthless capitalism. And it's one of the things that will be hard to do without for many professionals.\n\n## Comment ID kfzk4ue with +1 score by [(marrow_monkey, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfzk4ue/) (in reply to ID 18wasf8):\nAlthough he has some valid points I think he's still wrong. Consider Linux, OpenBSD, etc, vs Windows and MacOS. You could make similar points but Linux is still objectively much better from a technical point of view.\n\n## Comment ID kfzp2pi with +1 score by [(bajanda, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfzp2pi/) (in reply to ID 18wasf8):\nRemindMe! One Year\n\n## Comment ID kfzq9q2 with +1 score by [(xylont, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfzq9q2/) (in reply to ID 18wasf8):\nSo many open source are already at par GPT 4\n\n## Comment ID kfzqj2o with +1 score by [(bajanda, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfzqj2o/) (in reply to ID 18wasf8):\n1 - There are plenty of great researchers who don't want to work at OpenAi and rather participate of OSS projects  \n2 - true  \n3 - Yes, in the short term and at the current scale of their company, but as the Linux project has shown us is that decentralized development is better at scale. we just need our Linus Torvalds, maybe that's Karpathy's final arc?  \n 4 - this is out of argument, so many of us are building products around open-source models,  it's like saying \"Yahoo controls the market with their search engine, why would you try to build something else, Larry and Sergei?\"  \n5- Sure bro, you got a bigger computer\n\n## Comment ID kfzt3nc with +1 score by [(Typh123, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfzt3nc/) (in reply to ID 18wasf8):\nI’m new to LLMs, but I don’t see why companies would use openAI if they care about data privacy and open source is “good enough.” Regular people who don’t care about privacy? Sure. But how do you make money off regular people? Advertisements or subscriptions? Selling data? Could work I suppose.\n\n## Comment ID kfzx3ry with +1 score by [(ab2377, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kfzx3ry/) (in reply to ID 18wasf8):\nthat guy and his post has zero meaning. so has the need for comparison of proprietary models of hundreds of billions of parameters to what we can run on our laptops today. this whole debate is lame enough.\n\n## Comment ID kg02r7k with +1 score by [(Commercial-Penalty-7, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kg02r7k/) (in reply to ID 18wasf8):\nGPT-3 was made by Openai and it's being overtaken by opensource soon so...\n\n## Comment ID kg040x9 with +1 score by [(curious_myst, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kg040x9/) (in reply to ID 18wasf8):\nCan an open source team make a better model than gpt-4-32k in 2024. Yes. Can an open source team make a better product than the Chat GPT Pro in 2024. Yes. \n\nChatGPT is leading in model and in product but only because it was the first mover. The underlying model can be beaten, especially by teams that figure out how to get the same performance with smaller models, or a simply by publishing a model with less guard rails. The product can also be beaten because OpenAI doesn’t really have a vision for user experience. After the novelty of being able to converse with AI wears off, the app leaves much to be desired in terms of UX.\n\n## Comment ID kg0920m with +1 score by [(tamereen, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kg0920m/) (in reply to ID 18wasf8):\nWe will not have one model equivalent to GPT4, but several smallest models able to beat GPT4 in one particular domain. I see it with coding, with codellama or phind by example. Another will be better for story telling but not for coding etc...\n\n## Comment ID kg0d8vi with +1 score by [(Canchito, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kg0d8vi/) (in reply to ID 18wasf8):\nPoints 1, 3, and 4 are far from impossible to replicate for open source projects.\n\n1: Talent can be driven by passion as well as money\n\n3: Open source teams can very well be centralized if at all necessary (open source has nothing to do with how the product is produced, but only concerns the legal status of the product). \n\n4: Model versus product is just meaningless marketing-speak/gibberish. \n\nThe only significant and valid points are 2 and 5 because they involve material means. Non-commercial models would indeed fail if they attempted to replicate openai in those regards, because they are capital intensive and those who control capital do not want this technology to be a public good.\n\nI tend to believe that there is no technical solution to this obstacle, as it is a social problem rooted in the capitalistic nature of our society. The only solution is to transform openai into a democratically controlled public utility run not for profit, but for the benefit of society as a whole.\n\n## Comment ID kg0j6lq with +1 score by [(dimtass, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kg0j6lq/) (in reply to ID 18wasf8):\nI'm satisfied with an uncensored model that is close enough and I don't care if it's better than gpt-4. It will get there in a few months/years.\n\n## Comment ID kg0u13z with +1 score by [(firattogoko, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kg0u13z/) (in reply to ID 18wasf8):\nSometimes small victories are also important. Linux cannot compete with Windows and Macos, but the existence of the Linux operating system is an assurance for us.\n\n## Comment ID kg0wxbg with +1 score by [(Zestyclose_Yak_3174, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kg0wxbg/) (in reply to ID 18wasf8):\nSince GPT-4 has already had several iterations, it is interesting to think about what it means \"to beat GPT-4\" - I think these big companies will be surprised what creative and passionate people can come up with and I am quite optimistic we will certainly reach the current GPT4 model for most usecases. It will just be a matter of time. Of course they have tremendous amounts of talent and money which means that these giant corporations will always have a head start but I am sure it will get pretty exciting quite soon!\n\n## Comment ID kg156zw with +1 score by [(gnublet, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kg156zw/) (in reply to ID 18wasf8):\nWhy would you open source something that can make you billions of dollars? Maybe once someone's made enough money or social impact, then they'd open source it. It seems transformers themselves as a tech is plateauing as the best open source models are now mixtures/ensembles. Tons of big tech companies have more data than them so it's a question of incentives.\n\n## Comment ID kg160wk with +1 score by [(Marlsboro, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kg160wk/) (in reply to ID 18wasf8):\nI disagree with some of this. The human intervention is mostly for alignment, I believe. It makes the model safer, not better. If anything it makes it a little worse. The same goes for the \"model vs product\" argument.\nThe point about working in person instead of remotely is just silly\n\n## Comment ID kg18iwp with +1 score by [(Randommaggy, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kg18iwp/) (in reply to ID 18wasf8):\nFor code GPT4 is utter garbage and in my real world benchmark tasks it's handily beaten by a 7B parameter frankenstein mix of Open Hermes and Intel Neural Chat.  \n\n\nIt's not even close when you're doing novel work and technical debt is a factor.\n\n## Comment ID kg1e53f with +1 score by [(Charuru, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kg1e53f/) (in reply to ID 18wasf8):\nThe fuck is \"open source teams\", good models will come out of massively resourced companies like meta, mistral, yi, etc just chooses an open license.\n\n## Comment ID kg1wsm1 with +1 score by [(None, Reddit, 2024-01-02)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kg1wsm1/) (in reply to ID 18wasf8):\nif we didn't have big companies like the ones in the ai alliance, i would agree 100%\n\ndoes openai has the best ai engineers? meta, ibm, intel, etc. same...\n\nresources? meta alone is much bigger and has much more resources that openai today, so does ibm, intel, etc.\n\nlinux kernel has lots of power from big companies and big universities... think like that\n\n## Comment ID kg1yzo0 with +1 score by [(ReMeDyIII, Reddit, 2024-01-03)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kg1yzo0/) (in reply to ID 18wasf8):\nI disagree in the sense that we don't necessarily need a general purpose model. Want a model for Roleplay? Okay, great, grab a model that focuses on that. Want a model for programming? Okay, focus on that. We can beat/match GPT-4 on a go-tall strategy as opposed to a go-wide approach.\n\nAlso, I've read uncensored models perform better as they don't have guardrails and can think more freely.\n\n## Comment ID kg24dym with +1 score by [(bot-333, Reddit, 2024-01-03)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kg24dym/) (in reply to ID 18wasf8):\nIt's funny how if you paste this tweet into an LLM they will immediately start to throw counterarguments.\n\n## Comment ID kg29kgi with +1 score by [(jacek2023, Reddit, 2024-01-03)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kg29kgi/) (in reply to ID 18wasf8):\nI asked ChatGPT why this person is wrong  \n\n\n\" ChatGPT\n\nTo address the opinion that open-source models won't surpass GPT-4 this year, we need to analyze each point separately:\n\n1. **Talent**: It's true that OpenAI and similar organizations have attracted top talent with competitive salaries. However, this doesn't preclude the possibility of open-source projects also attracting brilliant minds. Open-source communities often include highly skilled volunteers and contributors from diverse backgrounds, including those from top institutions. The collaborative nature of open-source projects can foster innovative approaches that might not emerge in more traditional, hierarchical structures.\n2. **Data**: While OpenAI has access to a massive, proprietary dataset, the open-source community also has access to large amounts of data. The internet is a vast resource, and with the right tools and strategies, open-source projects can aggregate huge datasets. Moreover, open-source projects can potentially tap into more diverse data sources, leading to different, possibly more innovative or well-rounded models.\n3. **Team Structure**: The claim that centralized teams are inherently more effective than decentralized ones is debatable. Decentralized, open-source teams can be more flexible and agile, and they benefit from a wide range of perspectives. The success of many open-source projects (like Linux, Python, etc.) showcases how effective decentralized collaboration can be.\n4. **Model vs Product**: It's accurate that GPT-4 is not just a model but a comprehensive product. However, this doesn't mean a superior model cannot emerge from the open-source community. A more advanced model, even if not initially part of a polished product, could still surpass GPT-4 in terms of capabilities and be integrated into products later.\n5. **Infrastructure**: The assertion about the limitations of public cloud infrastructure is overly simplistic. Many open-source projects successfully use public clouds, and these platforms are continually improving. The scalability and accessibility of cloud platforms can be advantageous. While proprietary infrastructures might have some edges, this does not inherently limit the potential of open-source models to iterate and improve rapidly.\n\nIn conclusion, while the opinion presents valid points about the strengths of organizations like OpenAI, it underestimates the potential of the open-source community. Open-source projects have often been at the forefront of innovation in various fields, including AI, and there's a possibility that an open-source model could rival or even surpass GPT-4 in the future.\"\n\n## Comment ID kg2e00o with +1 score by [(theguyabroad, Reddit, 2024-01-03)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kg2e00o/) (in reply to ID 18wasf8):\nIsn't mixtral as good?\n\n## Comment ID kg2q0qb with +1 score by [(pornthrowaway42069l, Reddit, 2024-01-03)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kg2q0qb/) (in reply to ID 18wasf8):\nAll that needs to happen is for people to realize you don't need a general model as good as gpt-4, but just focus on more domain-specific smaller models, which when chained most likely would be able to match gpt-4, at least in the area of its expertise.\n\n## Comment ID kg2v9ob with +1 score by [(ArakiSatoshi, Reddit, 2024-01-03)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kg2v9ob/) (in reply to ID 18wasf8):\nWith the direction OpenAI is going, open source models *already* are better than GPT-4 for the general tasks. GPT-4 is only good for specialized tasks like coding, evaluating datasets for the open source models, doing tasks that require 16k+ tokens, etc.\n\n## Comment ID kg31ukn with +1 score by [(esotericloop, Reddit, 2024-01-03)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kg31ukn/) (in reply to ID 18wasf8):\nIs he saying open-source products won't beat the GPT-4 product offering, or that open-source models won't beat the GPT-4 LLM? And what does he mean by \"beat\", does a more accurate model that takes 100x compute \"beat\" a model 99% as good that you can run on your phone? This is a public relations soundbite, not an engineering comparison.\n\n## Comment ID kg3440c with +1 score by [(arinjay_11020, Reddit, 2024-01-03)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kg3440c/) (in reply to ID 18wasf8):\n6th reason he hid\n\nAll closed source models have engineering tricks and hard coded rules along with the model, which we as researchers/users will never know. There are lots of engineering tricks here, comparing to them will be like you comparing an engine to a car, where engine is an open source model and car is a closed source model along with other engineering stuff.\n\nCompare engines with engines and cars with cars.\n\n## Comment ID kg35vg3 with +1 score by [(No-Camel-3819, Reddit, 2024-01-03)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kg35vg3/) (in reply to ID 18wasf8):\nI disagree with most of the points, and I have a simple answer: see Linux vs Windows.\n\n## Comment ID kg388c3 with +1 score by [(WaifuEngine, Reddit, 2024-01-03)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kg388c3/) (in reply to ID 18wasf8):\nI mean it depends on how you define win or beat lol, if foss is delusional to compete directly yeah but I don’t think that’s the goal. The goal is more or less to make this technology available and have a counter force to a betrayal on openness\n\n## Comment ID kg3ngie with +1 score by [(Jazzlike_Painter_118, Reddit, 2024-01-03)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kg3ngie/) (in reply to ID 18wasf8):\nAn appeal to authority basically. You could argue a lot of the same points against Linux and it still happened.  \n\n\nThe work in person is better stated as fact got me laughing though.\n\n## Comment ID kg3s1tz with +1 score by [(Ancient_Constant_604, Reddit, 2024-01-03)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kg3s1tz/) (in reply to ID 18wasf8):\nGPT-4 is a lofty goal for sure, but I think many people would have said at the beginning of this year that running gpt-3.5 level models on consumer hardware was a pipe dream. With the increasing focus on moe, accelerators for consumer grade cpu/gpu, and the funding (and props!) being given to open-source/weight startups, I'm excited to see where we're going to be next year.\n\n## Comment ID kg4jjqe with +1 score by [(EarlyPlantain7810, Reddit, 2024-01-03)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kg4jjqe/) (in reply to ID 18wasf8):\nWrong.\n\nOpen source Communities dominates brands\n\n## Comment ID kg4ut4u with +1 score by [(amemingfullife, Reddit, 2024-01-03)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kg4ut4u/) (in reply to ID 18wasf8):\n1. Is based on a premise that top engineers will drive performance. That’s based on the assumption that there is a lot left to go on performance and that step changes are needed. I think we’re at the marginal gains phase of this architecture, so no. Top engineers increase your time to value for the company here, not necessarily raw performance. Since open source doesn’t have an intrinsic business model time to value doesn’t matter as much, all it needs is willing volunteers. \n2. The ‘OpenAI has no moat and neither do we’ memo by Google covers this. The data advantage is limited, the product is the only differentiator. I’d argue that testing systems are usually what differentiates ML products, rather than publicly available benchmarks.\n3. Linux would like a word. \n4. Cf my second point. The product is only differentiating insofar as it produces data for the model owners. This is the only real differentiator. This can be mitigated by open source alternatives for collecting data, of which there are few projects ongoing. But yes, openAI wins here, but Google does too since Google has more data any\n5. Yeah, because it’s subsidised. The open source movement is in this for the long haul. We’ll see where we are in 5-10 years. I’d bank on Moore’s law making the current systems achievable on home setups in that time. \n\nSo yes I do ‘disagree?’\n\nBut I also disagree with the premise of this argument. I don’t think that open source alternatives want direct competition with performance, they want an open source *alternative* I.e. optionality. I think this sub has plenty of alternatives to OpenAI in many dimensions, just not cost.\n\n## Comment ID kgahuai with +1 score by [(skullbonesandnumber, Reddit, 2024-01-04)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kgahuai/) (in reply to ID 18wasf8):\nI think both opensource and GPT-4 are not going to make it or maybe in an other way then mentioned:\n\n1) all this \"talent\" busies themselves with a \"hallucinating AI\" that cannot be trusted as a source on intelligence (the knowing of facts)\n\n2) datasets are based upon copyright infringements to be able to train an AI. Copyright holders can force AI based on these sets to be taken of the market\n\n3) team structures are nothing if it all works on a product that is not a product.\n\n4) this is nor model nor product. the model does guessing so it cannot be a source of intelligence, not a product because it hallucinates.\n\n5) cloud as infrastructure is done with (always too expensive) and localized (esp. on mobile) will do best if the \"it does not hallucinate anymore\" version **#sanitypatent** has arrived\n\n**#sanitypatent** is:\n\na 1:1 question/answer database that can do the artificial intelligence, the knowing of facts. Build upon an intelligence that provides the same answer to a question that is asked again would allow the \\*cough\\* 1 million salary ML specialists to develop algorithm that would provide the Artificial  Intellect, that what knows to reason based upon facts.\n\n&#x200B;\n\nI am not a patent hunter, but i bet ya (not for real) that this is the \"other break through\" Sam Altman was talking about. On the other hand, every opensource coders can make a reasoning algoritm based on a dict key/value facts database. Does Sam Altman or does a \"basement steve jobs the second opensource hacker\" do the  **#sanitypatent ?**\n\n&#x200B;\n\n!!\n\n## Comment ID kgm83hl with +1 score by [(pete_68, Reddit, 2024-01-06)](https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/kgm83hl/) (in reply to ID 18wasf8):\nI don't think it's really a big deal. The open source ones are close on the heels of GPT 4. The technology is progressing. New techniques for training with higher quality vs larger quantity data sets is going to make it easier for others to eventually catch up. In the meantime, as long as open source can manage to keep pace, even being a little behind, it's going to be a fine solution for people who don't want to pay and that's what's eventually going to be the death knell for OpenAI.\n\nThey've already lost my money just by making it so lazy about coding. I use LLMs because I'M lazy about coding. I don't need an LLM to be as lazy as me. Then it's completely useless. I've been using Phind, Bard and Claude and finding them to be plenty adequate for my needs. I even had a case where Phind was able to solve a bug right away that I had spent several hours going back and forth with ChatGPT 4 on, and it couldn't solve it. \n\nThe client-side tools are getting better as well and will continue to improve. I'm actually working on one tool that uses multiple LLMs for its work. It uses a local one through Ollama's API for things that that's good for, and then for a few select prompts, it goes out to Claude or OpenAI.  But since most of the prompts are handled locally, they're free and only a small subset of the prompts cost me anything. \n\nSo I don't think OpenAI or GPT 4 have any sort of magical future. On the contrary, things will catch up. Maybe not in the next year, but in the next few years, and I suspect OpenAI won't survive. Lots of groundbreakers don't survive. Xerox/Parc created the GUI and they're gone. Cray was the go-to for the best super computers. DEC was the go-to for mini computers. Iomega came up with large capacity (for the time) removable storage. IBM invented the desktop PC. The only reason they're still around is that they had a enough other businesses to let them survive the collapse of their PC division.\n\nSo there's no reason to think OpenAI will be around 5 years from now. They might, but Google will probably eat them.",
      "# Post ID 1b92hlk: How I Reduced Our LLM Costs by Over 85% with +72 score by [(pmarks98, Reddit, 2024-03-07)](https://www.reddit.com/r/ArtificialInteligence/comments/1b92hlk/how_i_reduced_our_llm_costs_by_over_85/)\nWith AI apps popping up everywhere, it’s fair to think building one is both easy and cheap.\n\nUnfortunately, you’d be *(mostly)* wrong.\n\nI’m not saying it’s hard per se, but as of this writing, gpt-4-turbo costs $0.01/$0.03 per 1000 input/output tokens. This can quickly add up if you’re building a complex AI workflow.\n\nYes, you could use less expensive, worse performing models, like GPT 3.5 or an open-source one like Llama, stuff everything into one API call with excellent prompt engineering, and hope for the best. But this probably won’t turn out that great. This type of approach doesn’t really work in production—at least not yet with the current state of AI.\n\n**It could give you the right answer 90% or even 99% of the time.** But that one time it decides to go off the rails, it’s really frustrating. As a developer and/or business, you know you must never break a user’s experience. It might be okay for a toy app or prototype but not for a production-grade application you charge for.\n\nImagine if Salesforce or any other established software company said its reliability was only one or two nines. That would be insane. No one would use it.\n\n**But this is the state of most AI applications today. They’re unreliable.**\n\n# AI isn’t a Universal Function\n\nThe non-deterministic nature of LLMs forces us to be more thoughtful about how we write our code. We should not just “hope” that an LLM will always correctly respond. We need to build redundancy and proper error handling. For some reason, many builders forget everything they learned about software engineering and treat AI like some magical universal function that doesn’t fail.\n\n**It’s not there yet.**\n\nTo fix this limitation, we must write code that only interacts with AI when absolutely necessary—that is, when a system needs some sort of “human-level” analysis of unstructured data. Subsequently, whenever possible, we must force the LLM to return references to information (i.e., a pointer) instead of the data itself.\n\n**When I recognized these two things, I had to redesign the backend architecture of my personal software business completely.**\n\n# Rearchitecting Jellypod\n\nFor context, I started an app called Jellypod. It enables users to subscribe to email newsletters and get a daily summary of the most important topics from all of them as a single podcast.\n\nThis seems pretty simple on the outside—and the MVP honestly was. The app would just process each email individually, summarize it, convert it to speech, and stitch all the audio together, side-by-side, into a daily podcast.\n\nThe output was fine, but it needed to be better.\n\nIf two different newsletters discussed the same topic, the “podcast” would talk about it twice, not realizing we had already mentioned it. You could say, “Well, why don’t you just stuff all the newsletter content into one big LLM call to summarize everything?”\n\nWell, that’s what I tried at first.\n\nAnd it failed. **Miserably.**\n\nEven with an extremely detailed prompt using all the best practices, I couldn’t guarantee that the LLM would always detect the most important topics, summarize everything, and consistently create an in-depth output. Also, the podcast always needed to be ~10 minutes long.\n\nSo I went back to the drawing board. How can I make this system better? And yes, we’re getting to the cost reduction part - don’t worry!\n\n# Defining the Requirements\n\nJellypod must be able to process any number of input documents (newsletters) and create an output that always includes the top ten most important topics across all those inputs. If two subparts of any input are about the same topic, we should recognize that and merge the sections into one topic.\n\nFor example, if the Morning Brew has a section about US Elections and the Daily Brief also has a section on the current state of US Politics, they should be merged. I’ll skip over how I determined a similarity threshold (i.e., should two topics be merged or remain separate).\n\n# Exploding Costs\n\nI built on top of a few different approaches outlined in papers written by the LangChain community to semantic chunk and organize everything in a almost deterministic way.\n\n**But this was INSANELY expensive.** The number of API calls grew at a rate of O(n log n), with n being the number of input chunks from all newsletters.\n\nSo, I had a dilemma. Do I keep this improved and more expensive architecture or throw it down the drain?\n\nI decided to keep it and figure out how to reduce costs.\n\n# Reducing Costs\n\nThat’s when I discovered a tool called OpenPipe that allows you to fine-tune open-source models almost too easily. It looked legit and was backed by YCombinator, so I gave it a try.\n\nI swapped out the OpenAI SDK with their SDK (a drop-in replacement), which passed all my LLM API calls to OpenAI but recorded all inputs and outputs. This created unique datasets for each of my prompts, which I could use to fine-tune a cheaper open-source model.\n\nAfter about a week of recording Jellypod’s LLM calls, I had about 50,000 rows of data. And with a few clicks, I fine-tuned a Mistral 7B model for each LLM call.\n\nI replaced GPT-4 with the new fine-tuned model.\n\n**And it reduced the costs by 88%.**\n\nThe cost of inference dropped from $10 per 1M input tokens to $1.20. And cost per output token dropped from $30 to $1.60.\n\nI was blown away. I could now run Jellypod’s new architecture for approximately the same cost as the MVP’s trivial approach. I even confirmed that the fine-tuned Mistral output quality was just as high as GPT-4 by a series of evals and in-app customer feedback.\n\nBy redesigning the system to only use AI for the smallest unit of work it’s actually needed for, I could confidently deploy a fine-tuned model as a drop-in replacement for GPT 4. And by prompting to return pointers to data instead of the data itself, I could ensure data integrity while reducing the number of output tokens consumed.\n\n# In Conclusion\n\nIf you’re considering building an AI application, I would encourage you to take a step back and think about your architecture’s output reliability and costs. What happens if the LLM doesn’t answer your prompt in the right way? Can you prompt the model to return data identifiers instead of raw data? And, is it possible to swap GPT-4 with a cheaper, fine-tuned model?\n\nI wish I had these insights when I started, but hey, you live and learn.\n\nI hope you found at least some parts of this interesting! I thought there were enough learnings to share. Feel free to reach out if you’re curious about the details.\n\n\n\n## Comment ID ktud1oh with +9 score by [(gj80, Reddit, 2024-03-08)](https://www.reddit.com/r/ArtificialInteligence/comments/1b92hlk/how_i_reduced_our_llm_costs_by_over_85/ktud1oh/) (in reply to ID 1b92hlk):\n>I swapped out the OpenAI SDK with their SDK (a drop-in replacement), which passed all my LLM API calls to OpenAI but recorded all inputs and outputs. This created unique datasets for each of my prompts, which I could use to fine-tune a cheaper open-source model.\n\nThat's neat, and a sensible approach... start with an expensive model, record outputs, then train something more specific and economical.\n\nClaude 3 Haiku, when released, might make an even easier/lazier/cheaper alternative without even needing to fine tune a lower parameter model... it's a staggeringly cheap $0.25/1M input tokens, and if it's anything like Opus/Sonnet it ought to even have quite good accuracy within its wide 200k context window.\n\n### Comment ID ktudy93 with +2 score by [(pmarks98, Reddit, 2024-03-08)](https://www.reddit.com/r/ArtificialInteligence/comments/1b92hlk/how_i_reduced_our_llm_costs_by_over_85/ktudy93/) (in reply to ID ktud1oh):\nI'm super excited for all the Claude models to roll out. Bummer that the Elon/OpenAI lawsuit might slow OpenAI's response to Anthropic, but 2024 will be really interesting year.\n\n## Comment ID ktvhl4m with +2 score by [(dimsumham, Reddit, 2024-03-08)](https://www.reddit.com/r/ArtificialInteligence/comments/1b92hlk/how_i_reduced_our_llm_costs_by_over_85/ktvhl4m/) (in reply to ID 1b92hlk):\nWait so how did you solve the problem of summarizing multiple newsletters and getting them to output a summary most important topics?\n\n### Comment ID ktvj6jg with +1 score by [(pmarks98, Reddit, 2024-03-08)](https://www.reddit.com/r/ArtificialInteligence/comments/1b92hlk/how_i_reduced_our_llm_costs_by_over_85/ktvj6jg/) (in reply to ID ktvhl4m):\nSo I don't bore you with too many technical details lol, I combined some techniques from Greg Kamradt's/LangChain's agentic semantic chunking with a weighted priority sort using a few different methods of statistical analysis\n\n#### Comment ID ktvjpw5 with +2 score by [(dimsumham, Reddit, 2024-03-08)](https://www.reddit.com/r/ArtificialInteligence/comments/1b92hlk/how_i_reduced_our_llm_costs_by_over_85/ktvjpw5/) (in reply to ID ktvj6jg):\nI love technical details =D\n\nIf you're uncomfortable sharing publicly, I would love to chat via DM.\n\nOf course, if you're uncomfortable sharing at all, I understand as well!\n\n## Comment ID ktw13v1 with +2 score by [(mrdevlar, Reddit, 2024-03-08)](https://www.reddit.com/r/ArtificialInteligence/comments/1b92hlk/how_i_reduced_our_llm_costs_by_over_85/ktw13v1/) (in reply to ID 1b92hlk):\n> And it reduced the costs by 88%.\n\nThe power of Open Source AI in a nutshell. \n\nYou should cross-post this to /r/LocalLLaMA\n\n### Comment ID ktw1rh0 with +2 score by [(pmarks98, Reddit, 2024-03-08)](https://www.reddit.com/r/ArtificialInteligence/comments/1b92hlk/how_i_reduced_our_llm_costs_by_over_85/ktw1rh0/) (in reply to ID ktw13v1):\nFunny thing I crossposted it and the mods removed it - for some reason it got downvoted to oblivion. Must have missed something in the rules, idk\n\n#### Comment ID ktw2xwe with +2 score by [(mrdevlar, Reddit, 2024-03-08)](https://www.reddit.com/r/ArtificialInteligence/comments/1b92hlk/how_i_reduced_our_llm_costs_by_over_85/ktw2xwe/) (in reply to ID ktw1rh0):\nOh that sucks. It's generally a more technical board, and you seem to be illustrating a design process, which is something I always enjoy reading, since we're often either in the AGI clouds or in the nity grity.\n\n## Comment ID ktw9c0f with +2 score by [(Clarker133, Reddit, 2024-03-08)](https://www.reddit.com/r/ArtificialInteligence/comments/1b92hlk/how_i_reduced_our_llm_costs_by_over_85/ktw9c0f/) (in reply to ID 1b92hlk):\nLove this, you abused the O (n log n) to exponentially increase the performance finetune down the line of a local model.\n\n## Comment ID kv8is78 with +1 score by [(Ayush1733433, Reddit, 2024-03-17)](https://www.reddit.com/r/ArtificialInteligence/comments/1b92hlk/how_i_reduced_our_llm_costs_by_over_85/kv8is78/) (in reply to ID 1b92hlk):\nBut how are you hosting the mistral model, AWS bedrock? I think hosting models for inference in cloud can be extremely expensive.\n\n## Comment ID l4ga0y5 with +1 score by [(Historical-Ebb-6490, Reddit, 2024-05-17)](https://www.reddit.com/r/ArtificialInteligence/comments/1b92hlk/how_i_reduced_our_llm_costs_by_over_85/l4ga0y5/) (in reply to ID 1b92hlk):\nThanks good summary. some other techniques to reduce LLM costs in this video \n\n[Strategies for Cost Effective LLM Implementations](https://youtu.be/xlzTITjl2G0?si=G_HZfjQofaYfFG46)\n\n## Comment ID l5u9y1c with +1 score by [(parthi2929, Reddit, 2024-05-27)](https://www.reddit.com/r/ArtificialInteligence/comments/1b92hlk/how_i_reduced_our_llm_costs_by_over_85/l5u9y1c/) (in reply to ID 1b92hlk):\nSo, use a more reliable expensive model like GPT-4 first, store that transaction data, use that to fine tune 7B models, and then use the latter?\n\n## Comment ID lc7lx2o with +1 score by [(Michael_Daytona, Reddit, 2024-07-08)](https://www.reddit.com/r/ArtificialInteligence/comments/1b92hlk/how_i_reduced_our_llm_costs_by_over_85/lc7lx2o/) (in reply to ID 1b92hlk):\nVery interesting!\n\n## Comment ID ld9r9w8 with +1 score by [(lutian, Reddit, 2024-07-15)](https://www.reddit.com/r/ArtificialInteligence/comments/1b92hlk/how_i_reduced_our_llm_costs_by_over_85/ld9r9w8/) (in reply to ID 1b92hlk):\nnice man, how do you exactly imbue the learned prompt-answer pairs into a smaller model? that's key. I know fine-tuning makes it learn style/format, not actual knowledge. \n\nare you saying you've tested a smaller model before and after this fine tuning and it learned new info? I might be missing something\n\n## Comment ID ltob7kw with +1 score by [(Apprehensive-Luck-19, Reddit, 2024-10-25)](https://www.reddit.com/r/ArtificialInteligence/comments/1b92hlk/how_i_reduced_our_llm_costs_by_over_85/ltob7kw/) (in reply to ID 1b92hlk):\nI read it all, twice. I'm looking to do something similar for my SaaS project. I'm not knowledgeable enough :)\n\n## Comment ID ktwez9x with +1 score by [(BerrDev, Reddit, 2024-03-08)](https://www.reddit.com/r/ArtificialInteligence/comments/1b92hlk/how_i_reduced_our_llm_costs_by_over_85/ktwez9x/) (in reply to ID 1b92hlk):\nVery cool and also a super nice product idea!  \nWill check it out and will try out OpenPipe  \nWish you well and hope you succeed.\n\n### Comment ID ktxm4l3 with +1 score by [(pmarks98, Reddit, 2024-03-08)](https://www.reddit.com/r/ArtificialInteligence/comments/1b92hlk/how_i_reduced_our_llm_costs_by_over_85/ktxm4l3/) (in reply to ID ktwez9x):\nThank you!",
      "# Post ID 1fm59kg: How do you actually fine-tune a LLM on your own data? with +290 score by [(No-Conference-8133, Reddit, 2024-09-21)](https://www.reddit.com/r/LocalLLaMA/comments/1fm59kg/how_do_you_actually_finetune_a_llm_on_your_own/)\nI've watched several YouTube videos, asked Claude, GPT, and I still don't understand how to fine-tune LLMs.\n\nContext: There's this UI component library called Shadcn UI, and most models have no clue of what it is or how to use it. I'd like to see if I can train a LLM (doesn't matter which one) to see if it can get good at the library. Is this possible?\n\nI already have a dataset ready to fine-tune the model in a json file as input - output format. I don’t know what to do after this.\n\n**Hardware Specs:**\n\n* CPU: AMD64 Family 23 Model 96 Stepping 1, AuthenticAMD\n* CPU Cores: 8\n* CPU Threads: 8\n* RAM: 15GB\n* GPU(s): None detected\n* Disk Space: 476GB\n\nI'm not sure if my PC is powerful enough to do this. If not, I'd be willing to fine-tune on the cloud too.\n\n## Comment ID lo89dre with +180 score by [(Hefty_Wolverine_553, Reddit, 2024-09-21)](https://www.reddit.com/r/LocalLLaMA/comments/1fm59kg/how_do_you_actually_finetune_a_llm_on_your_own/lo89dre/) (in reply to ID 1fm59kg):\nYour PC isn't powerful enough to do finetuning, no. You can use Unsloth + Google Colab to finetune smaller llms, maybe try finetuning Qwen2.5 7b or llama 3.1 8b. Here are some [Unsloth Notebooks](https://docs.unsloth.ai/get-started/unsloth-notebooks) that you can directly run.\n\n### Comment ID lo8yd9e with +28 score by [(MachineZer0, Reddit, 2024-09-21)](https://www.reddit.com/r/LocalLLaMA/comments/1fm59kg/how_do_you_actually_finetune_a_llm_on_your_own/lo8yd9e/) (in reply to ID lo89dre):\nThis is a great way to get started. Google Colab has those T4s you can attach for a little while with a free account.\n\n### Comment ID lpn31l5 with +1 score by [(Professional_Hair550, Reddit, 2024-09-30)](https://www.reddit.com/r/LocalLLaMA/comments/1fm59kg/how_do_you_actually_finetune_a_llm_on_your_own/lpn31l5/) (in reply to ID lo89dre):\nIs 25GB RAM, 12 Cores, 4GB VRAM enough?\n\n#### Comment ID lpnf21n with +5 score by [(Hefty_Wolverine_553, Reddit, 2024-09-30)](https://www.reddit.com/r/LocalLLaMA/comments/1fm59kg/how_do_you_actually_finetune_a_llm_on_your_own/lpnf21n/) (in reply to ID lpn31l5):\nFor local finetuning, you *might* be able to get away with llama 3.2 1b/3b, once Unsloth has support for that. Again, I'd highly recommend using Google Colab, since you just get a free 12gb vram that you can use.\n\n### Comment ID lr6t2h7 with +1 score by [(teleECG, Reddit, 2024-10-10)](https://www.reddit.com/r/LocalLLaMA/comments/1fm59kg/how_do_you_actually_finetune_a_llm_on_your_own/lr6t2h7/) (in reply to ID lo89dre):\nYes to Unsloth and Colab! I fine-tuned phi3.5 to run a custom API in \\~9 hours. I paid about $10 because I wanted a faster GPU. I had to make modifications to the Unsloth notebook, but it was no big deal thanks to Unsloth.\n\n#### Comment ID lr6tclr with +3 score by [(Hefty_Wolverine_553, Reddit, 2024-10-10)](https://www.reddit.com/r/LocalLLaMA/comments/1fm59kg/how_do_you_actually_finetune_a_llm_on_your_own/lr6tclr/) (in reply to ID lr6t2h7):\nIf you're spending $10, might as well use vast/runpod lol\n\n## Comment ID lo87yfb with +30 score by [(Wh-Ph, Reddit, 2024-09-21)](https://www.reddit.com/r/LocalLLaMA/comments/1fm59kg/how_do_you_actually_finetune_a_llm_on_your_own/lo87yfb/) (in reply to ID 1fm59kg):\nI ended up using litgpt: https://github.com/Lightning-AI/litgpt\n\n### Comment ID lo9aqik with +2 score by [(w00ddie, Reddit, 2024-09-21)](https://www.reddit.com/r/LocalLLaMA/comments/1fm59kg/how_do_you_actually_finetune_a_llm_on_your_own/lo9aqik/) (in reply to ID lo87yfb):\nLooks cool.\n\nHow does this differ from open webui and its custom model?\n\n#### Comment ID lo9ccof with +4 score by [(Wh-Ph, Reddit, 2024-09-21)](https://www.reddit.com/r/LocalLLaMA/comments/1fm59kg/how_do_you_actually_finetune_a_llm_on_your_own/lo9ccof/) (in reply to ID lo9aqik):\nHmmm... I bet it differs by ability to create your own custom model even from scratch. Offline.  \nAnd I must admit that creating model from scratch is fun.\n\n### Comment ID loca23p with +1 score by [(waiting_for_zban, Reddit, 2024-09-22)](https://www.reddit.com/r/LocalLLaMA/comments/1fm59kg/how_do_you_actually_finetune_a_llm_on_your_own/loca23p/) (in reply to ID lo87yfb):\nDid you run it locally on the cloud (if so which one)?\n\nDo you mind sharing a bit more your process?\n\n#### Comment ID lockad0 with +8 score by [(Wh-Ph, Reddit, 2024-09-22)](https://www.reddit.com/r/LocalLLaMA/comments/1fm59kg/how_do_you_actually_finetune_a_llm_on_your_own/lockad0/) (in reply to ID loca23p):\nLocally. Kubuntu 24.04, 2xRTX3090 with nvlink.\n\nAny specific questions on the process?  \nTheir docs are decent and clear...\n\nFirst I tried fine-tuning phi-2 model to understand how it goes. Basically they just have a sequence to copy/paste to console so the only thing remaining to do is to make input/output json file. Here's the bash sequence: [https://github.com/Lightning-AI/litgpt?tab=readme-ov-file#finetune-an-llm](https://github.com/Lightning-AI/litgpt?tab=readme-ov-file#finetune-an-llm)\n\nThen I tried to pretrain (i.e. train from scratch) a model. They have an example: [https://github.com/Lightning-AI/litgpt?tab=readme-ov-file#pretrain-an-llm](https://github.com/Lightning-AI/litgpt?tab=readme-ov-file#pretrain-an-llm)  \nAfter doing their example, I've discovered their \\~/litgpt/config\\_hub directory which had config to train LLama 2B (requires two 24Gb cards) and TinyLLama 1.1 (can fit one card). From there the main pitfall is preparing datatset to train. It is \\*much\\* less trivial than it appears at the first glance.\n\n## Comment ID lo82dmi with +21 score by [(Everlier, Reddit, 2024-09-21)](https://www.reddit.com/r/LocalLLaMA/comments/1fm59kg/how_do_you_actually_finetune_a_llm_on_your_own/lo82dmi/) (in reply to ID 1fm59kg):\ncheck out Sebastian Raschka's guide from their new book, full code on GitHub here:\nhttps://github.com/rasbt/LLMs-from-scratch/blob/main/ch06/01_main-chapter-code/ch06.ipynb?utm_source=substack&utm_medium=email\n\n## Comment ID lo7x4h8 with +35 score by [(-brianh-, Reddit, 2024-09-21)](https://www.reddit.com/r/LocalLLaMA/comments/1fm59kg/how_do_you_actually_finetune_a_llm_on_your_own/lo7x4h8/) (in reply to ID 1fm59kg):\nTo setup a training dataset for this fine-tuning task, you'd need input prompts for specific components and output code using shadcn components.\n\nSince there's no such dataset you'd need to create one yourself. You can also make use of apis like divmagic api which can give you html/tailwind code for components but you still need some manual work for shadcn imports and classnames.\n\n### Comment ID lo7xpat with +7 score by [(No-Conference-8133, Reddit, 2024-09-21)](https://www.reddit.com/r/LocalLLaMA/comments/1fm59kg/how_do_you_actually_finetune_a_llm_on_your_own/lo7xpat/) (in reply to ID lo7x4h8):\nI can definitely create the dataset myself. I’m confused how to actually fine tune the LLM. Is this complicated or something?\n\nThought you could do this on hugging face? I have no clue where to go, what to do etc.\n\n#### Comment ID lo7z03a with +17 score by [(-brianh-, Reddit, 2024-09-21)](https://www.reddit.com/r/LocalLLaMA/comments/1fm59kg/how_do_you_actually_finetune_a_llm_on_your_own/lo7z03a/) (in reply to ID lo7xpat):\nFine-tuning is not complicated at all if you have the dataset. I don't know about hugging face, I never trained a model there.\n\nLangchain and OpenAI both have docs that show you how you can fine-tune a model. You should look at those. Fine-tuning on openai is super easy. You just need to have system/user/assistant prompts for each unique example and put them in a JSON file and openai trains the model. Langchain has a very similar data format too.\n\n#### Comment ID lo9hjua with +16 score by [(adzx4, Reddit, 2024-09-21)](https://www.reddit.com/r/LocalLLaMA/comments/1fm59kg/how_do_you_actually_finetune_a_llm_on_your_own/lo9hjua/) (in reply to ID lo7xpat):\nLike others mentioned, using python libraries is something current general models know and understand, so I would recommend also that fine tuning is inferior here to using RAG/few-shot unless your specific library is extremely unique in how it's used, which I highly doubt tbh.\n\nA combination of few shot prompting and RAG is the most sensible solution, as you can list even 10s-100s of examples to common vendor models.\n\nIt takes a lot of data to change an off the shelf models behaviour, in the order of hundreds to thousands, and it would have to be high quality, you'd then also have to serve the fine tuned model, which also is a headache.\n\n#### Comment ID lo7z0ys with +10 score by [(gaztrab, Reddit, 2024-09-21)](https://www.reddit.com/r/LocalLLaMA/comments/1fm59kg/how_do_you_actually_finetune_a_llm_on_your_own/lo7z0ys/) (in reply to ID lo7xpat):\nIf you're on Linux look into Llama Factory. Or else then Unsloth\n\n#### Comment ID locheym with +3 score by [(hschaeufler, Reddit, 2024-09-22)](https://www.reddit.com/r/LocalLLaMA/comments/1fm59kg/how_do_you_actually_finetune_a_llm_on_your_own/locheym/) (in reply to ID lo7xpat):\nI would recommend to use PEFT Methods like LoRA and QLoRA. There you don't need a huge Dataset and that much GPU Power Like with a Full Parameter Finetuning. You can start with Google Colab and Unsloth for Example when you don't want to buy a GPU. Other possibilty is a Mac, because it has a unified Memory (RAM is also used as GPU RAM) but it's not that fast and u have to use the MLX Library. You don't need to put your Data in an Instruction Format. You can also Finetune without prompt and only the Code. For example I train for Unit-Test Generation. My train.jsonl looks Like this {\"text\" : \"###code:  import 'package:.... ###test: import...\"}. Mostly phases for Finetuning is: Dataset Creation, Finetuning, Evaluate your Model, Tune your Hyperparameter and Tune again.\n\n#### Comment ID lo8yeoz with +5 score by [(RyuguRenabc1q, Reddit, 2024-09-21)](https://www.reddit.com/r/LocalLLaMA/comments/1fm59kg/how_do_you_actually_finetune_a_llm_on_your_own/lo8yeoz/) (in reply to ID lo7xpat):\nGoogle's AI studio also allows you to finetune your own model. I've made one just to experiement with it but havent done it since\n\n#### Comment ID lodgszn with +1 score by [(Flamesilver_0, Reddit, 2024-09-22)](https://www.reddit.com/r/LocalLLaMA/comments/1fm59kg/how_do_you_actually_finetune_a_llm_on_your_own/lodgszn/) (in reply to ID lo7xpat):\nLook up unsloth. And YouTube tutorials surrounding it.\n\n## Comment ID lo8fyc9 with +17 score by [(Educational_Rent1059, Reddit, 2024-09-21)](https://www.reddit.com/r/LocalLLaMA/comments/1fm59kg/how_do_you_actually_finetune_a_llm_on_your_own/lo8fyc9/) (in reply to ID 1fm59kg):\n[https://github.com/unslothai/unsloth](https://github.com/unslothai/unsloth) You have prepared notebooks with the code you need to start tuning.\n\n### Comment ID loddjbb with +2 score by [(sleepy_roger, Reddit, 2024-09-22)](https://www.reddit.com/r/LocalLLaMA/comments/1fm59kg/how_do_you_actually_finetune_a_llm_on_your_own/loddjbb/) (in reply to ID lo8fyc9):\nThis is a great book, going through it myself right now.\n\n## Comment ID lo8yb6o with +26 score by [(redbrick5, Reddit, 2024-09-21)](https://www.reddit.com/r/LocalLLaMA/comments/1fm59kg/how_do_you_actually_finetune_a_llm_on_your_own/lo8yb6o/) (in reply to ID 1fm59kg):\nRAG first, tune later or never\n\n### Comment ID lo984q9 with +18 score by [(No-Conference-8133, Reddit, 2024-09-21)](https://www.reddit.com/r/LocalLLaMA/comments/1fm59kg/how_do_you_actually_finetune_a_llm_on_your_own/lo984q9/) (in reply to ID lo8yb6o):\nI just think you made me realize RAG is all I need for my specific task. Appreciate it!\n\n#### Comment ID lo9bndd with +11 score by [(w00ddie, Reddit, 2024-09-21)](https://www.reddit.com/r/LocalLLaMA/comments/1fm59kg/how_do_you_actually_finetune_a_llm_on_your_own/lo9bndd/) (in reply to ID lo984q9):\nThis is the way … open webui is a great setup to use in my opinion. Easy rag to setup custom models\n\n### Comment ID lo91edn with +21 score by [(thomash, Reddit, 2024-09-21)](https://www.reddit.com/r/LocalLLaMA/comments/1fm59kg/how_do_you_actually_finetune_a_llm_on_your_own/lo91edn/) (in reply to ID lo8yb6o):\ncontext first, RAG second, tune later or never\n\n## Comment ID lo83c15 with +10 score by [(i_wayyy_over_think, Reddit, 2024-09-21)](https://www.reddit.com/r/LocalLLaMA/comments/1fm59kg/how_do_you_actually_finetune_a_llm_on_your_own/lo83c15/) (in reply to ID 1fm59kg):\nGoing to need the cloud for sure with only 15GB of computer ram. If you want to do it on your own machine, need something like an RTX 3090 or better with 24GB of vram. Then you can make 4 bit Lora’s on something like a 13b model.\n\n## Comment ID lo8398i with +8 score by [(databasehead, Reddit, 2024-09-21)](https://www.reddit.com/r/LocalLLaMA/comments/1fm59kg/how_do_you_actually_finetune_a_llm_on_your_own/lo8398i/) (in reply to ID 1fm59kg):\nI believe vercel v0 already has support for shadcn. You might wanna check it out.\n\n### Comment ID lo84097 with +5 score by [(No-Conference-8133, Reddit, 2024-09-21)](https://www.reddit.com/r/LocalLLaMA/comments/1fm59kg/how_do_you_actually_finetune_a_llm_on_your_own/lo84097/) (in reply to ID lo8398i):\nIt does! I use that a lot. I think it’d be fun to see how fine-tuning a model on Shadcn UI would turn out.\n\n## Comment ID lobjiu8 with +8 score by [(SuccessIsHardWork, Reddit, 2024-09-22)](https://www.reddit.com/r/LocalLLaMA/comments/1fm59kg/how_do_you_actually_finetune_a_llm_on_your_own/lobjiu8/) (in reply to ID 1fm59kg):\nI use a synthetic dataset generator that creates synthetic datasets from PDF and finetunes on it.\nhttps://www.reddit.com/r/LocalLLaMA/comments/1eipty2/tool_to_create_synthetic_datasets_using_pdf_files/?utm_source=share&utm_medium=mweb3x&utm_name=mweb3xcss&utm_term=1&utm_content=share_button\n\n### Comment ID loc3l9f with +1 score by [(No-Conference-8133, Reddit, 2024-09-22)](https://www.reddit.com/r/LocalLLaMA/comments/1fm59kg/how_do_you_actually_finetune_a_llm_on_your_own/loc3l9f/) (in reply to ID lobjiu8):\nOut of context but I like your username\n\n## Comment ID lo9odxf with +6 score by [(Cold-Adeptness2506, Reddit, 2024-09-21)](https://www.reddit.com/r/LocalLLaMA/comments/1fm59kg/how_do_you_actually_finetune_a_llm_on_your_own/lo9odxf/) (in reply to ID 1fm59kg):\nI have a Macbook with an M2 Max chip, do you guys think mine would be powerful enough for any fine-tuning?\n\n### Comment ID loax4ex with +3 score by [(mike7seven, Reddit, 2024-09-22)](https://www.reddit.com/r/LocalLLaMA/comments/1fm59kg/how_do_you_actually_finetune_a_llm_on_your_own/loax4ex/) (in reply to ID lo9odxf):\nM2 Max is good enough but it Depends on the amount of ram as well\n\n### Comment ID locg6o7 with +3 score by [(hschaeufler, Reddit, 2024-09-22)](https://www.reddit.com/r/LocalLLaMA/comments/1fm59kg/how_do_you_actually_finetune_a_llm_on_your_own/locg6o7/) (in reply to ID lo9odxf):\nYou could try Parameter Efficent Finetuning (Lora/QLora) with the MLX library from Apple. Right now I'm finetuning a LLAMA 3.1 8B on my M3 Max. There are some Tutorials and Examples. The good about the Apple Silicons is that they have unified Memory. So the RAM is used also as GPU Memory. How many Ram you have?\n\n#### Comment ID logcn3n with +2 score by [(Cold-Adeptness2506, Reddit, 2024-09-23)](https://www.reddit.com/r/LocalLLaMA/comments/1fm59kg/how_do_you_actually_finetune_a_llm_on_your_own/logcn3n/) (in reply to ID locg6o7):\n32 gigs\n\n## Comment ID lo8baxa with +6 score by [(Willing_Landscape_61, Reddit, 2024-09-21)](https://www.reddit.com/r/LocalLLaMA/comments/1fm59kg/how_do_you_actually_finetune_a_llm_on_your_own/lo8baxa/) (in reply to ID 1fm59kg):\nI'm pretty sure that you cannot fine tune anything on your computer. Which brings us to a question of interest to me : what are the cheapest fine tuning options on the cloud? Can we fine tune on Google modal  free tier? What about vast.ai ?\nPrice estimates and tutorials would be lovely.\n\n### Comment ID lo8wlfd with +10 score by [(Amgadoz, Reddit, 2024-09-21)](https://www.reddit.com/r/LocalLLaMA/comments/1fm59kg/how_do_you_actually_finetune_a_llm_on_your_own/lo8wlfd/) (in reply to ID lo8baxa):\nYou can qlora a 7B/8B/9B on the free Google Colab T4 notebook using unsloth.\n\n#### Comment ID loa1p82 with +3 score by [(un_passant, Reddit, 2024-09-21)](https://www.reddit.com/r/LocalLLaMA/comments/1fm59kg/how_do_you_actually_finetune_a_llm_on_your_own/loa1p82/) (in reply to ID lo8wlfd):\nIndeed !\n\nI just have to try to use the Phi 3.5 mini example Notebook [https://huggingface.co/unsloth/Phi-3-mini-4k-instruct-bnb-4bit](https://huggingface.co/unsloth/Phi-3-mini-4k-instruct-bnb-4bit) with the LongCite dataset [https://huggingface.co/datasets/THUDM/LongCite-45k](https://huggingface.co/datasets/THUDM/LongCite-45k) for grounded RAG, it will be X-mas ! ☺\n\nThanks for reminding me.\n\n## Comment ID lo9q27e with +5 score by [(curiousily_, Reddit, 2024-09-21)](https://www.reddit.com/r/LocalLLaMA/comments/1fm59kg/how_do_you_actually_finetune_a_llm_on_your_own/lo9q27e/) (in reply to ID 1fm59kg):\nI have a full YT video on fine-tuning Llama 3 (which works for 3.1 too) on custom data: https://www.youtube.com/watch?v=0XPZlR3_GgI\nHope that helps!\n\n### Comment ID mh8pi49 with +1 score by [(Famous-Put5515, Reddit, 2025-03-11)](https://www.reddit.com/r/LocalLLaMA/comments/1fm59kg/how_do_you_actually_finetune_a_llm_on_your_own/mh8pi49/) (in reply to ID lo9q27e):\nthe link is not valid\n\n## Comment ID lo8hndz with +14 score by [(MakitaNakamoto, Reddit, 2024-09-21)](https://www.reddit.com/r/LocalLLaMA/comments/1fm59kg/how_do_you_actually_finetune_a_llm_on_your_own/lo8hndz/) (in reply to ID 1fm59kg):\nJust make sure you have at least 100.000-ish examples of your own data\n\nFine-tuning on too small of a dataset won't work well\n\n### Comment ID lo8wpxq with +12 score by [(Amgadoz, Reddit, 2024-09-21)](https://www.reddit.com/r/LocalLLaMA/comments/1fm59kg/how_do_you_actually_finetune_a_llm_on_your_own/lo8wpxq/) (in reply to ID lo8hndz):\nYou can Lora on 1000 only, assuming the task isn't incredibly difficult for the model.\n\n### Comment ID lo8ntz7 with +7 score by [(aadoop6, Reddit, 2024-09-21)](https://www.reddit.com/r/LocalLLaMA/comments/1fm59kg/how_do_you_actually_finetune_a_llm_on_your_own/lo8ntz7/) (in reply to ID lo8hndz):\nYes. This is easier said than done.\n\n#### Comment ID lo8qkrx with +8 score by [(MakitaNakamoto, Reddit, 2024-09-21)](https://www.reddit.com/r/LocalLLaMA/comments/1fm59kg/how_do_you_actually_finetune_a_llm_on_your_own/lo8qkrx/) (in reply to ID lo8ntz7):\nOfc. But something people should look out for. Even generating some synth data to make up for some of the missing amount should help\n\n#### Comment ID lobkzi8 with +1 score by [(huldress, Reddit, 2024-09-22)](https://www.reddit.com/r/LocalLLaMA/comments/1fm59kg/how_do_you_actually_finetune_a_llm_on_your_own/lobkzi8/) (in reply to ID lo8ntz7):\nI wish LLM had it's stablediffusion Kohya.gui moment or the equivalent, I'm interested in training my own data (out of curiosity of what the results would be) but it's far beyond my comprehension level, I have so many questions that can only  be answered by training it myself.\n\nmaybe one day there'll be a \"How to finetune an LLM model for dummies\" lol\n\n### Comment ID lobj2ws with +4 score by [(corbt, Reddit, 2024-09-22)](https://www.reddit.com/r/LocalLLaMA/comments/1fm59kg/how_do_you_actually_finetune_a_llm_on_your_own/lobj2ws/) (in reply to ID lo8hndz):\nSo at OpenPipe (serverless data prep, fine-tuning and inference service) we've fine-tuned thousands of models for customers, and we've actually found that for many tasks you can get away with about 100 examples and have a good experience!\n\nMore definitely helps though, up to a certain point for saturation which is super task dependent. Generally the easier and narrower your task is, the faster you'll hit saturation on it.\n\n### Comment ID lod9nx4 with +2 score by [(brown2green, Reddit, 2024-09-22)](https://www.reddit.com/r/LocalLLaMA/comments/1fm59kg/how_do_you_actually_finetune_a_llm_on_your_own/lod9nx4/) (in reply to ID lo8hndz):\n> Just make sure you have at least 100.000-ish examples of your own data\n\nYou definitely do not need 100k examples for a finetune. You can start seeing results with far less than 1000 as well, as long as they're properly designed and you use a sufficiently large number of epochs. This is easy to verify locally with small models and hand-crafted toy datasets. Even OpenAI agrees with this in the documentation of their finetuning services:\n\nhttps://platform.openai.com/docs/guides/fine-tuning/preparing-your-dataset\n\n> To fine-tune a model, you are required to provide at least 10 examples. We typically see clear improvements from fine-tuning on 50 to 100 training examples with `gpt-4o-mini` and `gpt-3.5-turbo`, but the right number varies greatly based on the exact use case.\n>\n> We recommend starting with 50 well-crafted demonstrations and seeing if the model shows signs of improvement after fine-tuning. In some cases that may be sufficient, but even if the model is not yet production quality, clear improvements are a good sign that providing more data will continue to improve the model. No improvement suggests that you may need to rethink how to set up the task for the model or restructure the data before scaling beyond a limited example set.\n\n#### Comment ID lodbmj6 with +1 score by [(MakitaNakamoto, Reddit, 2024-09-22)](https://www.reddit.com/r/LocalLLaMA/comments/1fm59kg/how_do_you_actually_finetune_a_llm_on_your_own/lodbmj6/) (in reply to ID lod9nx4):\nIt varies by use case sure, but if you're aiming for fine-tuning conversational LLMs you really won't like the results of only 10 examples\n\nAlso note that I assumed OP's fine-tuning a heftier model than 4o-mini\n\n## Comment ID lo90qcz with +3 score by [(__SlimeQ__, Reddit, 2024-09-21)](https://www.reddit.com/r/LocalLLaMA/comments/1fm59kg/how_do_you_actually_finetune_a_llm_on_your_own/lo90qcz/) (in reply to ID 1fm59kg):\nyou will need a gpu. a decent one with big vram.\n\nAnd then just use oobabooga and iterate\n\n## Comment ID lo9e38x with +3 score by [(beshkenadze, Reddit, 2024-09-21)](https://www.reddit.com/r/LocalLLaMA/comments/1fm59kg/how_do_you_actually_finetune_a_llm_on_your_own/lo9e38x/) (in reply to ID 1fm59kg):\nDo you need a tuning model for this? \nI work with these components using RAG through a VS Code extension called Continue through adding the shadcn/ui site as a documentation repository. \nIf you are not using VS Code or Idea, you can create a project in Claude or Custom GPT, upload the documentation from the site there and use that in your work.\n\n## Comment ID loa8fdz with +3 score by [(Chongo4684, Reddit, 2024-09-21)](https://www.reddit.com/r/LocalLLaMA/comments/1fm59kg/how_do_you_actually_finetune_a_llm_on_your_own/loa8fdz/) (in reply to ID 1fm59kg):\nYou need a GPU to do fine tuning. At least a 16GB.\n\nYou can fine-tune using the free google colab or you could use a commercial provider like runpod or [vast.ai](http://vast.ai) or pay for an upgrade to google colab. Or... you could buy a GPU.\n\nAs for how to fine tune on your own data... You need to make a dataset.\n\nThe best/easiest way to do that is download a dataset from huggingface and examine it to see how it's put together. \n\nI usually download into excel because excel is easier to play with than anything else.\n\nOther folks may have different approaches.\n\n## Comment ID loandcs with +3 score by [(-gauvins, Reddit, 2024-09-22)](https://www.reddit.com/r/LocalLLaMA/comments/1fm59kg/how_do_you_actually_finetune_a_llm_on_your_own/loandcs/) (in reply to ID 1fm59kg):\nI'll follow with considerable interest. I have trained several large BERT models, small in comparison to the current generation of LLMs, on my workstation (TRX 40 3960x + RTX 4090 + 265G RAM).\n\nI've recently upgraded my OS and can no longer run inferences with these models. I might retrain on a more recent tensorflow version or fine-tune an ollama open llm.\n\nI was able to generate inferences (sentiment) at the rate of 1M/h. Anyone here has intuition on what LLMs inference speed might be?\n\n### Comment ID loc53mv with +1 score by [(Chongo4684, Reddit, 2024-09-22)](https://www.reddit.com/r/LocalLLaMA/comments/1fm59kg/how_do_you_actually_finetune_a_llm_on_your_own/loc53mv/) (in reply to ID loandcs):\nMuch slower than BERT.\n\n## Comment ID loc81ko with +3 score by [(Ace-2_Of_Spades, Reddit, 2024-09-22)](https://www.reddit.com/r/LocalLLaMA/comments/1fm59kg/how_do_you_actually_finetune_a_llm_on_your_own/loc81ko/) (in reply to ID 1fm59kg):\nSure, your PC’s not up for the task. Go cloud or forget it.\n\n## Comment ID lob91o9 with +2 score by [(FunWater2829, Reddit, 2024-09-22)](https://www.reddit.com/r/LocalLLaMA/comments/1fm59kg/how_do_you_actually_finetune_a_llm_on_your_own/lob91o9/) (in reply to ID 1fm59kg):\nNot exactly sure what your use case is but generally I use cursor to index a specific web page which can be included in a prompt on the fly to generate code. Does a pretty good job. You can check it out!\n\n### Comment ID loc3i8h with +2 score by [(No-Conference-8133, Reddit, 2024-09-22)](https://www.reddit.com/r/LocalLLaMA/comments/1fm59kg/how_do_you_actually_finetune_a_llm_on_your_own/loc3i8h/) (in reply to ID lob91o9):\nYep, I do that too! Been using Cursor for a year now and it’s truly only getting better. I didn’t actually have a specific use case to fine-tune a LLM model on Shadcn UI. It was more of a 3am thought “imagine fine-tuning a model to be so good at Shadcn UI for fun\", then I got interested in how to do it.\n\n## Comment ID lobe1n2 with +2 score by [(BrianNice23, Reddit, 2024-09-22)](https://www.reddit.com/r/LocalLLaMA/comments/1fm59kg/how_do_you_actually_finetune_a_llm_on_your_own/lobe1n2/) (in reply to ID 1fm59kg):\nIf you want to play around with some models, I simply rent those hourly servers that already has a beefy GPU.. I think considering it costs less than $1 an hour for a 4090 I would simply use those\n\n## Comment ID lobs4v6 with +2 score by [(AccidentAnnual, Reddit, 2024-09-22)](https://www.reddit.com/r/LocalLLaMA/comments/1fm59kg/how_do_you_actually_finetune_a_llm_on_your_own/lobs4v6/) (in reply to ID 1fm59kg):\nNot what you ask for, but in GPT4All you can use your own data as a resource. And online there is Google NotebookLM where you can upload data for questioning and such. It can even make a virtual podcast.\n\n## Comment ID lofqm6e with +2 score by [(MrQuicast, Reddit, 2024-09-22)](https://www.reddit.com/r/LocalLLaMA/comments/1fm59kg/how_do_you_actually_finetune_a_llm_on_your_own/lofqm6e/) (in reply to ID 1fm59kg):\nYou can train the Llama 3.1 8B model on Google Colab using 4-bit quantization (no unsloth) and PEFT to obtain the adapter. Then, you can merge the adapter with the base model (without quantization) on another GPU with a little more VRAM to enhance performance.\n\n## Comment ID lofw5gf with +2 score by [(indrasmirror, Reddit, 2024-09-22)](https://www.reddit.com/r/LocalLLaMA/comments/1fm59kg/how_do_you_actually_finetune_a_llm_on_your_own/lofw5gf/) (in reply to ID 1fm59kg):\nI'm currently using SWIFT to finetune a Lora for Qwen2.5 on a custom reasoning dataset. Fingers crossed 🤞 will be interesting.\n\n## Comment ID lo8gpi1 with +3 score by [(GreatBigJerk, Reddit, 2024-09-21)](https://www.reddit.com/r/LocalLLaMA/comments/1fm59kg/how_do_you_actually_finetune_a_llm_on_your_own/lo8gpi1/) (in reply to ID 1fm59kg):\nNot that it's helpful with local LLM's, but Claude uses Shadcn when writing react components in it's artifacts. It probably will do fine as long as you tell it what you're using upfront.\n\n## Comment ID lo8tw2r with +1 score by [(yukiarimo, Reddit, 2024-09-21)](https://www.reddit.com/r/LocalLLaMA/comments/1fm59kg/how_do_you_actually_finetune_a_llm_on_your_own/lo8tw2r/) (in reply to ID 1fm59kg):\nDM me and I’ll teach you\n\n### Comment ID lo8xvc0 with +4 score by [(No-Conference-8133, Reddit, 2024-09-21)](https://www.reddit.com/r/LocalLLaMA/comments/1fm59kg/how_do_you_actually_finetune_a_llm_on_your_own/lo8xvc0/) (in reply to ID lo8tw2r):\nBet\n\n### Comment ID lwpsrel with +2 score by [(Nimrod5000, Reddit, 2024-11-12)](https://www.reddit.com/r/LocalLLaMA/comments/1fm59kg/how_do_you_actually_finetune_a_llm_on_your_own/lwpsrel/) (in reply to ID lo8tw2r):\nYou still offering?  I've got so many questions right now!\n\n## Comment ID load42m with +1 score by [(Fun_Librarian_7699, Reddit, 2024-09-21)](https://www.reddit.com/r/LocalLLaMA/comments/1fm59kg/how_do_you_actually_finetune_a_llm_on_your_own/load42m/) (in reply to ID 1fm59kg):\nWould a 4070 Super be enough to fine-tune a small model like llama3.1 8B or gemma2 9B?\n\n## Comment ID lob93xa with +1 score by [(BrianNice23, Reddit, 2024-09-22)](https://www.reddit.com/r/LocalLLaMA/comments/1fm59kg/how_do_you_actually_finetune_a_llm_on_your_own/lob93xa/) (in reply to ID 1fm59kg):\nIs there a way to know how much data you need to fine tune a model? For example let's say that I just want to be able to determine if a bunch of loose PDF pages belong to a first page of the document or not.. how many example pages would I need to send to make this happen? \n\n\nI also imagine that this depends on the model as well..\n\n## Comment ID lobl8dq with +1 score by [(schnoogiee, Reddit, 2024-09-22)](https://www.reddit.com/r/LocalLLaMA/comments/1fm59kg/how_do_you_actually_finetune_a_llm_on_your_own/lobl8dq/) (in reply to ID 1fm59kg):\nI know this is supposed to be Local LLAMA but I use 3.5 Sonnet on Cursor and my ShadCN comes out clean. Just forgets to install components via CLI sometimes\n\n### Comment ID loc3p8k with +3 score by [(No-Conference-8133, Reddit, 2024-09-22)](https://www.reddit.com/r/LocalLLaMA/comments/1fm59kg/how_do_you_actually_finetune_a_llm_on_your_own/loc3p8k/) (in reply to ID lobl8dq):\nYeah that’s what I been doing. I’ll continue to do that, I just thought it would be fun to see how far I could get with fine-tuning a LLM, since I never actually did that and I thought this was a good enough to try it.\n\n## Comment ID loc612q with +1 score by [(UnderstandLingAI, Reddit, 2024-09-22)](https://www.reddit.com/r/LocalLLaMA/comments/1fm59kg/how_do_you_actually_finetune_a_llm_on_your_own/loc612q/) (in reply to ID 1fm59kg):\nLook here https://github.com/AI-Commandos/LLaMa2lang\nNo coding required, works on CPU too (warning: stupid slow)\n\n## Comment ID loc82pw with +1 score by [(Fit_Fold_7275, Reddit, 2024-09-22)](https://www.reddit.com/r/LocalLLaMA/comments/1fm59kg/how_do_you_actually_finetune_a_llm_on_your_own/loc82pw/) (in reply to ID 1fm59kg):\nYou should use Google Colab to do fine tuning of small models. You can use Unsloth for it. Then you can deploy the fine tuned model on your laptop with something like Ollama\n\n## Comment ID lvvrfa3 with +1 score by [(niujin, Reddit, 2024-11-07)](https://www.reddit.com/r/LocalLLaMA/comments/1fm59kg/how_do_you_actually_finetune_a_llm_on_your_own/lvvrfa3/) (in reply to ID 1fm59kg):\nHi. I've got an example notebook for fine tuning an LLM here:  \n[https://fastdatascience.com/generative-ai/train-ai-fine-tune-ai/](https://fastdatascience.com/generative-ai/train-ai-fine-tune-ai/)\n\nWe're running a competition to fine tune one for mental health data - if you are interested and you get the best MAE in the competition you can win £500 in vouchers!\n\n### Comment ID m13gw4p with +1 score by [(JustWuTangMe, Reddit, 2024-12-08)](https://www.reddit.com/r/LocalLLaMA/comments/1fm59kg/how_do_you_actually_finetune_a_llm_on_your_own/m13gw4p/) (in reply to ID lvvrfa3):\nThis is going till the end of January, yes? If you got the time, fire off a private message to me. I'm having trouble figuring out exactly what you're looking at having this model do.\n\n#### Comment ID m165jw3 with +1 score by [(niujin, Reddit, 2024-12-09)](https://www.reddit.com/r/LocalLLaMA/comments/1fm59kg/how_do_you_actually_finetune_a_llm_on_your_own/m165jw3/) (in reply to ID m13gw4p):\nHi. Thanks for the feedback. Yes, it's going until end of January. I added an additional \\[blog post with Youtube video on how to fine tune your own LLM\\](https://naturallanguageprocessing.com/train-ai/fine-tune-large-language-model-for-sentence-similarity/) as I was worried some people weren't sure what to do. Basically we want this model to be more accurate at predicting similarities between sentences as what a psychologist would consider to be similar, instead of what GPT etc tend to consider to be similar. I'll send you a DM also but please check the video and blog post first as that might make things clearer.\n\n## Comment ID m1ndbrm with +1 score by [(Business_Quantity571, Reddit, 2024-12-12)](https://www.reddit.com/r/LocalLLaMA/comments/1fm59kg/how_do_you_actually_finetune_a_llm_on_your_own/m1ndbrm/) (in reply to ID 1fm59kg):\nPreviously I have trained a QLoRA on custom summary data using llama3 and results were not that great.  \nSo, I'm giving FFT a shot. But there are very few examples and it's kinda confusing. And I'm just a beginner :)\n\nI have bunch of questions. I would be grateful if somebody can guide me and I appreciate all your answers. So, thank you in advance.\n\nAll my questions are related to FFT only!\n\n1. To train the LLM using Full fine tuning using all parameters, how should be my dataset formatted (say jsonl)?\n2. How can I make the same FFT model perform on summarization as well as Question answering or generating essays?\n3. Should I simply Fine tune (FFT) the base model (say llama2) for my dataset and merge with llama2 chat model?\n\n\n\n I did some work around and there were two types I noticed:\n\n* A: Use raw data:\n* Example: text\"<s>The 2nd law is stated in article number 5</s>\" ---> this is just an example text\"<s>Some info</s>\"\n* B: Use well prompted data:\n* Example: text\"<s>Please summarize the input\\\\n\\\\n### Input: Some info \\\\n\\\\n ### Answer: The 2nd law is stated in article number 5</s>\"\n\n## Comment ID mh5ztk6 with +1 score by [(BeachPrize5354, Reddit, 2025-03-11)](https://www.reddit.com/r/LocalLLaMA/comments/1fm59kg/how_do_you_actually_finetune_a_llm_on_your_own/mh5ztk6/) (in reply to ID 1fm59kg):\nis there anywone who can tell me how to finetune distillbertbase uncased for extracting information out of raw text purpose\n\n## Comment ID mhqvo5t with +1 score by [(Aromatic-Job-1490, Reddit, 2025-03-14)](https://www.reddit.com/r/LocalLLaMA/comments/1fm59kg/how_do_you_actually_finetune_a_llm_on_your_own/mhqvo5t/) (in reply to ID 1fm59kg):\nLoRA, Full FT, 30+ models : [https://docs.nebius.com/studio/fine-tuning/how-to-fine-tune](https://docs.nebius.com/studio/fine-tuning/how-to-fine-tune)",
      "# Post ID 1dmwawz: Mixture of Agents on ST using qr: Thoughts and sorrows  with +13 score by [(brahh85, Reddit, 2024-06-23)](https://www.reddit.com/r/SillyTavernAI/comments/1dmwawz/mixture_of_agents_on_st_using_qr_thoughts_and/)\nSince i read days ago that a mixture of open source models can beat GPT4-o i was thinking on a way to have something similar in ST. \n\n[https://www.together.ai/blog/together-moa](https://www.together.ai/blog/together-moa)\n\n[https://openpipe.ai/blog/mixture-of-agents](https://openpipe.ai/blog/mixture-of-agents)  \nRecently i read this comment [https://www.reddit.com/r/SillyTavernAI/comments/1dk2hmb/quick\\_reply\\_scripts/](https://www.reddit.com/r/SillyTavernAI/comments/1dk2hmb/quick_reply_scripts/)\n\nand i thought it was possible to adapt serupith's qr script to create a MoA based on together and openpipe works. Im going to paste an example script and some explanations, so it could help others to create functional MoA scripts. \n\nThe script needs this extension  [https://github.com/LenAnderson/SillyTavern-LALib](https://github.com/LenAnderson/SillyTavern-LALib)\n\n    /input What message to send? |\n    /setvar key=userMessage {{pipe}} |\n    /send {{getvar::userMessage}} |\n    /api openrouter |\n    /model euryale |\n    /gen lock=on  {{instructInput}}  \"{{getvar::userMessage}}\" {{instructFirstOutput}} |\n    /setvar key=responses_layer_one \n    ---response-start--- \n    {{pipe}} ---response-end--- |\n    /model wizardlm-2-8x22b |\n    /gen lock=on  {{instructInput}}  \"{{getvar::userMessage}}\" {{instructFirstOutput}} |\n    /addvar key=responses_layer_one \n    ---response-start--- \n    {{pipe}} ---response-end--- |\n    /model Command r | \n    /gen lock=on  {{instructInput}}  \"{{getvar::userMessage}}\" {{instructFirstOutput}} |\n    /addvar key=responses_layer_one \n    ---response-start--- \n    {{pipe}} ---response-end--- |\n    /model qwen 2 72b | \n    /gen lock=on  {{instructInput}}  \"{{getvar::userMessage}}\" {{instructFirstOutput}} |\n    /addvar key=responses_layer_one ---response-start--- \n    {{pipe}} ---response-end---   |\n    /gen lock=on  {{instructInput}} Don't Role Play.Don't Role Play.Don't Role Play.\n     Reason about the strengths and weaknesses of each candidate response to {{user}}'s prompt  . I need you to do an analysis.\n    \n    ---candidate-responses-start--- \n    {{getvar::responses_layer_one}} \n    ---candidate-responses-end--- \n    {{instructFirstOutput}}  |\n    /setvar key=reasoning_layer_one {{pipe}} |\n    /send {{getvar::reasoning_layer_one}} |\n    /send {{getvar::responses_layer_one}}  |\n    /gen lock=on  {{instructInput}} Generate a final response to {{user}} based on chat history, candidate responses, and reasoning output. Feel free to draw inspiration from the candidate responses and reasoning output, but don't feel obligated to address every single one. Ensure your response aligns with your previous statements, actions, personality, beliefs, and goals. Think about potential contradictions or inconsistencies. Think of unique or surprising elements you can bring to your response to avoid being predictable or cliché. Think about how you can leverage your quirks, mannerisms, or special abilities to make your response more engaging and interesting for {{user}} . Remember, the candidate responses and reasoning output are  just suggestions to help guide your thinking. Don't feel pressured to address every single point. Focus on crafting a response that is true to {{char}}'s personality and style of speech\n    ---candidate-responses-start--- \n    {{getvar::responses_layer_one}} \n    ---candidate-responses-end--- \n    \n    ---reasoning-output-start--- \n    {{getvar::reasoning_layer_one}} \n    ---reasoning-output-end--- \n    \n    {{instructFirstOutput}} |\n    /sendas name=\"{{char}}\" {{pipe}} \n    \n    \n    \n\n  \n\n\nI used the /api to show that you can change the api from the script,  and use models from different apis. I use openrouter, so all the models im using are from there in this script , adapt it to your own tastes  and needs. \n\nWe get responses from different models. Thats the easy part. Now lets starts with my sorrows. \n\nTogether's multilayered approach was too much for a noob like me\n\nhttps://preview.redd.it/c03hwan02d8d1.png?width=15472&format=png&auto=webp&s=588afe0195f19c10c636bf122fb85c42d06a1279\n\nSo i chose openpipe's \n\nhttps://preview.redd.it/9dtlcfaf2d8d1.png?width=2048&format=png&auto=webp&s=0275a6d2a2f689a7044c7c506c7d9927100f16d0\n\n  \nThe script will also show you the candidate  responses and the reasoning output. So you can fine tune the prompts and the code and see the results of that in ST.\n\nIm using a code editor (MS Code) highlighting \"code-snippets\". I write the changes there, and then copy and paste the code on the ST prompt each time. The first 20 edits i used the extensions menu of ST ,  i pasted it as a Quick Reply  every time... and  took ages and a lot of extra efforts. Doing  it that way is for code that you dont need to change constantly. \n\nI chose qwen2 72b  as aggregator due Together's work. I tried Qwen 1.5 110B and i got several refusals on NSFW, while qwen2 gave me 0. And in my experience in last 3 days, i have to say is the best reasoning NSFW model we have. So my advice is to keep it.\n\nThe problem is  the reasoning prompt, and the generation prompt. \n\nThis is Openpiper for reasoning. \n\n>Reason about the strengths and weaknesses of each candidate.\n\ngeneration\n\n>Generate final response based on the original input, candidate responses, and reasoning output.\n\nTogether uses just generation. \n\n>   You have been provided with a set of responses from various open-source models to the latest user query. Your task is to synthesize these responses into a single, high-quality response. It is crucial to critically evaluate the information provided in these responses, recognizing that some of it may be biased or incorrect. Your response should not simply replicate the given answers but should offer a refined, accurate, and comprehensive reply to the instruction. Ensure your response is well-structured, coherent, and adheres to the highest standards of accuracy and reliability.\n\nWell, in my tests i didnt like the responses generated with together's prompt inspiration . And to be honest, i didnt like much the ones generated with openpiper. \n\nI felt the answer were like putting  a pie inside a blender, and then serve it as a drink. I was expecting a big improvement, and for me the responses lost freshness compared to the candidates, and they got big on purple prose.\n\nWhich makes me think that for objective matters, the snowballing that MoE proposes improves the responses, but  for subjective matters (an impulsive reaction, a honest feeling, dialogue) its not that good. Or , it needs  some prompting engineering to take advantage of the \"reasoning\"  and the \"synthesizing\" that AI offers us. \n\nthis is my reasoning prompt \n\n    Don't Role Play.Don't Role Play.Don't Role Play.\n     Reason about the strengths and weaknesses of each candidate response to {{user}}'s prompt  . I need you to do an analysis.\n\n  \nand generation\n\n    Generate a final response to {{user}} based on chat history, candidate responses, and reasoning output. Feel free to draw inspiration from the candidate responses and reasoning output, but don't feel obligated to address every single one. Ensure your response aligns with your previous statements, actions, personality, beliefs, and goals. Think about potential contradictions or inconsistencies. Think of unique or surprising elements you can bring to your response to avoid being predictable or cliché. Think about how you can leverage your quirks, mannerisms, or special abilities to make your response more engaging and interesting for {{user}} . Remember, the candidate responses and reasoning output are  just suggestions to help guide your thinking. Don't feel pressured to address every single point. Focus on crafting a response that is true to {{char}}'s personality and style of speech.\n\nThese were my settings \n\nhttps://preview.redd.it/k8drlnx4xd8d1.png?width=235&format=png&auto=webp&s=611e8e952a05a40ed04fdf6952de68faf9c6a82c\n\nAny idea or suggestion is welcome, because  i lack of expertise with prompt engineering and scripting , im a noob that wanted to experiment with AI models. \n\n## Comment ID l9z9eap with +2 score by [(LienniTa, Reddit, 2024-06-24)](https://www.reddit.com/r/SillyTavernAI/comments/1dmwawz/mixture_of_agents_on_st_using_qr_thoughts_and/l9z9eap/) (in reply to ID 1dmwawz):\nlooks interesting, but how do you switch models? or do you have all 3 loaded?\n\n### Comment ID l9zg9sb with +1 score by [(brahh85, Reddit, 2024-06-24)](https://www.reddit.com/r/SillyTavernAI/comments/1dmwawz/mixture_of_agents_on_st_using_qr_thoughts_and/l9zg9sb/) (in reply to ID l9z9eap):\nI use openrouter's API, so i dont load them. To change model i use the /model command of ST. For example\n\n>**/model euryale |**  \n/gen lock=on  {{instructInput}}  \"{{getvar::userMessage}}\" {{instructFirstOutput}} |  \n/setvar key=responses\\_layer\\_one   \n---response-start---   \n{{pipe}} ---response-end--- |  \n**/model wizardlm-2-8x22b |**\n\nyou can also write /api before a model, to change to a local API and load those models too. For example, to get llama3 from my local ollama i can \n\n>/api ollama  \n/model llama3\n\nand then keeping using models from that local api,  or change to another API and models (either local or external).\n\n#### Comment ID la0wp26 with +2 score by [(GoofAckYoorsElf, Reddit, 2024-06-24)](https://www.reddit.com/r/SillyTavernAI/comments/1dmwawz/mixture_of_agents_on_st_using_qr_thoughts_and/la0wp26/) (in reply to ID l9zg9sb):\nSo with all this automated model switching, reasoning, thinking, how long does a response usually take? Sounds like it's adding a lot of overhead. \n\nDon't get me wrong, I think it's a great approach in general. It just feels like it might not be suitable for actual RP in Silly Tavern because responses might take too long.\n\n## Comment ID la19ymz with +2 score by [(Herr_Drosselmeyer, Reddit, 2024-06-24)](https://www.reddit.com/r/SillyTavernAI/comments/1dmwawz/mixture_of_agents_on_st_using_qr_thoughts_and/la19ymz/) (in reply to ID 1dmwawz):\n> I felt the answer were like putting a pie inside a blender, and then serve it as a drink. \n\n\nI mean, that's essentially what you're doing: aggregating different responses.\n\n### Comment ID la2vcno with +1 score by [(brahh85, Reddit, 2024-06-24)](https://www.reddit.com/r/SillyTavernAI/comments/1dmwawz/mixture_of_agents_on_st_using_qr_thoughts_and/la2vcno/) (in reply to ID la19ymz):\n...but i still want a pie :) ... \n\nToday i was testing again,  starting a new chat but just using qwen2 , not using the script at all, just a regular chat,  and  the answers werent as good as i thought they were yesterday.  Then i went back to one of my experimental chats, and after the  response generated by the script i started using qwen2 normally, and i felt it was smarter , like if the past responses in context (the reasoning and the candidates) \"fine tuned\" qwen2 to be better(like example dialogues). So the improvement i wanted to achieve with the synthetic response  i got it afterwards as a side effect in the non-synthetic  responses by the model. \n\nIf this is right,  then it wouldnt be needed to create \"synthetic\" responses for each prompt, just  once in a while to give the model more \"reasoning\".  The way we use \"summarize\" in ST extensions menu. Maybe changing the summarize's  prompt could be enough to make those 3 things (summarize, brainstorm, reasoning) every few messages.\n\n## Comment ID la1ztep with +2 score by [(nengon, Reddit, 2024-06-24)](https://www.reddit.com/r/SillyTavernAI/comments/1dmwawz/mixture_of_agents_on_st_using_qr_thoughts_and/la1ztep/) (in reply to ID 1dmwawz):\nI think the first responses may vary too much, or maybe address unrelated stuff, and that might be part of the problem. Have you tried just selecting the best candidate for a response instead of generating a new one based on all 3, you could iterate that with another 2 responses and so on.\n\n### Comment ID la5vfz2 with +3 score by [(brahh85, Reddit, 2024-06-25)](https://www.reddit.com/r/SillyTavernAI/comments/1dmwawz/mixture_of_agents_on_st_using_qr_thoughts_and/la5vfz2/) (in reply to ID la1ztep):\nYou were right, this was the way.\n\nI created a 4 layered script using just qwen 2 , and it created the responses in ST. Then, in a separated app i used gpt4-o to measure how good or bad were those answers, and how they improved \n\nFirst try\n\n* Group 1: 7, 6, 7\n*  \n* Group 2: 8, 8, 9\n*  \n* Group 3: 10, 9, 9\n\nSo according to gpt4-o standards the responses improved a lot \n\nSecond try \n\n* Group 1: 7, 6, 8\n*  \n* Group 2: 8, 9, 9\n*  \n* Group 3: 9, 9, 10\n\nAgain, almost as \"good\" as gpt4-o  and uncensored\n\nThird try\n\n* Group 1: 7, 8, 9\n*  \n* Group 2: 7, 8, 9\n*  \n* Group 3: 9, 8, 10\n\nThis was the \"worst\" , but even so , the average quality improved too. \n\nIm going to paste  2 version of the script. The first one shows the 4 layers in ST every time you use the script (if someone wants to experiments with prompts). The second one will just show the final response\n\n#### Comment ID la5vmuj with +2 score by [(brahh85, Reddit, 2024-06-25)](https://www.reddit.com/r/SillyTavernAI/comments/1dmwawz/mixture_of_agents_on_st_using_qr_thoughts_and/la5vmuj/) (in reply to ID la5vfz2):\nall layers  showing in ST\n\n    /input What message to send? |\n    /setvar key=userMessage {{pipe}} |\n    /send {{getvar::userMessage}} |\n    /model qwen 2 |\n    /gen lock=on  {{instructInput}} Generate three responses in an ordered list to the following prompt:  \"{{getvar::userMessage}}\" {{instructFirstOutput}} |\n    /setvar key=responses_layer_one  \n    {{pipe}}  |\n    /send {{getvar::responses_layer_one}}  |\n    /gen lock=on  {{instructInput}} DONT ROLE PLAY, DONT ANSWER ON YOUR OWN.DONT ROLE PLAY, DONT ANSWER ON YOUR OWN.DONT ROLE PLAY, DONT ANSWER ON YOUR OWN.\n     Based on the following three responses, select the one that you believe would most satisfy the user's preferences and expectations. Responses:\n    {{getvar::responses_layer_one}} \n    ---------------\n    And now in an ordered list write the text of that response and 2 more responses using the best answer as foundation.   {{instructFirstOutput}} |\n    /setvar key=responses_layer_one  \n    {{pipe}}  |\n    /send {{getvar::responses_layer_one}}  |\n    /gen lock=on  {{instructInput}} DONT ROLE PLAY, DONT ANSWER ON YOUR OWN.DONT ROLE PLAY, DONT ANSWER ON YOUR OWN.DONT ROLE PLAY, DONT ANSWER ON YOUR OWN.\n     Based on the following three responses, select the one that you believe would most satisfy the user's preferences and expectations. Responses:\n    {{getvar::responses_layer_one}} \n    ---------------\n    And now in an ordered list write the text of that response and 2 more responses using the best answer as foundation.   {{instructFirstOutput}} |\n    /setvar key=responses_layer_one  \n    {{pipe}}  |\n    /send {{getvar::responses_layer_one}}  |\n    /gen lock=on  {{instructInput}} DONT ROLE PLAY, DONT ANSWER ON YOUR OWN.DONT ROLE PLAY, DONT ANSWER ON YOUR OWN.DONT ROLE PLAY, DONT ANSWER ON YOUR OWN.\n     Based on the following three responses, select the one that you believe would most satisfy the user's preferences and expectations. Responses:\n    {{getvar::responses_layer_one}} \n    ---------------\n    And now write the the text of that response.   {{instructFirstOutput}} |\n    /sendas name=\"{{char}}\" {{pipe}}\n\n#### Comment ID la5vwim with +2 score by [(brahh85, Reddit, 2024-06-25)](https://www.reddit.com/r/SillyTavernAI/comments/1dmwawz/mixture_of_agents_on_st_using_qr_thoughts_and/la5vwim/) (in reply to ID la5vfz2):\nNow the one for just getting the final answer.\n\n    /input What message to send? |\n    /setvar key=userMessage {{pipe}} |\n    /send {{getvar::userMessage}} |\n    /model qwen 2 |\n    /gen lock=on  {{instructInput}} Generate three responses in an ordered list to the following prompt:  \"{{getvar::userMessage}}\" {{instructFirstOutput}} |\n    /setvar key=responses_layer_one  \n    {{pipe}}  |\n    /gen lock=on  {{instructInput}} DONT ROLE PLAY, DONT ANSWER ON YOUR OWN.DONT ROLE PLAY, DONT ANSWER ON YOUR OWN.DONT ROLE PLAY, DONT ANSWER ON YOUR OWN.\n     Based on the following three responses, select the one that you believe would most satisfy the user's preferences and expectations. Responses:\n    {{getvar::responses_layer_one}} \n    ---------------\n    And now in an ordered list write the text of that response and 2 more responses using the best answer as foundation.   {{instructFirstOutput}} |\n    /setvar key=responses_layer_one  \n    {{pipe}}  |\n    /gen lock=on  {{instructInput}} DONT ROLE PLAY, DONT ANSWER ON YOUR OWN.DONT ROLE PLAY, DONT ANSWER ON YOUR OWN.DONT ROLE PLAY, DONT ANSWER ON YOUR OWN.\n     Based on the following three responses, select the one that you believe would most satisfy the user's preferences and expectations. Responses:\n    {{getvar::responses_layer_one}} \n    ---------------\n    And now in an ordered list write the text of that response and 2 more responses using the best answer as foundation.   {{instructFirstOutput}} |\n    /setvar key=responses_layer_one  \n    {{pipe}}  |\n    /gen lock=on  {{instructInput}} DONT ROLE PLAY, DONT ANSWER ON YOUR OWN.DONT ROLE PLAY, DONT ANSWER ON YOUR OWN.DONT ROLE PLAY, DONT ANSWER ON YOUR OWN.\n     Based on the following three responses, select the one that you believe would most satisfy the user's preferences and expectations. Responses:\n    {{getvar::responses_layer_one}} \n    ---------------\n    And now write the the text of that response.   {{instructFirstOutput}} |\n    /sendas name=\"{{char}}\" {{pipe}}\n\n#### Comment ID la6m8d1 with +2 score by [(nengon, Reddit, 2024-06-25)](https://www.reddit.com/r/SillyTavernAI/comments/1dmwawz/mixture_of_agents_on_st_using_qr_thoughts_and/la6m8d1/) (in reply to ID la5vfz2):\nGlad to be of help <3, I also wanted to do something like it in ST but didn't know where to start, thanks for the idea. I'll check the scripts and play around with them later when I get free time, my idea was to just use a single low parameter local model to kinda trim bad responses or incoherent ones from itself, like multiple swipes, but letting the model decide which response was better to push the story, etc.",
      "# Post ID 17clmce: Why do all new websites look the same? with +252 score by [(Zealousideal-Food285, Reddit, 2023-10-20)](https://www.reddit.com/r/webdev/comments/17clmce/why_do_all_new_websites_look_the_same/)\nI could list a thousand examples but I'll keep it at a few.\n\nUsually some form of:\n\nBlurry nav bar,  dark theme, NextJS font, components that appear as you scroll, same pricing component\n\n[https://resend.com/](https://resend.com/)\n\n[https://codestory.ai/](https://codestory.ai/)\n\n[https://www.catxre.com/](https://www.catxre.com/)\n\n[https://www.trytaylor.ai/](https://www.trytaylor.ai/)\n\n[https://openpipe.ai/](https://openpipe.ai/)\n\n[https://www.ztool.co/](https://www.ztool.co/)\n\n[https://constructable.ai/](https://constructable.ai/)\n\n[https://www.getonboardai.com/](https://www.getonboardai.com/)\n\nI understand that the modern tech stack is rooted in NextJS but this goes beyond the NextJS starter kit. Is there another resource people are using?\n\n&#x200B;\n\n## Comment ID k5r3rep with +274 score by [(Richardh78, Reddit, 2023-10-20)](https://www.reddit.com/r/webdev/comments/17clmce/why_do_all_new_websites_look_the_same/k5r3rep/) (in reply to ID 17clmce):\nThese are hardly big brands? Do you think ztool.co has a team of designers passing unique Figma designs to devs\n\nThey just spun up the sites together with frameworks idk what u expect\n\n## Comment ID k5qyk0w with +139 score by [(illbookkeeper10, Reddit, 2023-10-20)](https://www.reddit.com/r/webdev/comments/17clmce/why_do_all_new_websites_look_the_same/k5qyk0w/) (in reply to ID 17clmce):\nA lot of these sites look different from each other, but I can see popular trends in each and how many can start blurring together with other sites and not have a distinct identity, so it's a good list to look through and do some thinking about.\n\n### Comment ID k5s3d1x with +53 score by [(fakehalo, Reddit, 2023-10-21)](https://www.reddit.com/r/webdev/comments/17clmce/why_do_all_new_websites_look_the_same/k5s3d1x/) (in reply to ID k5qyk0w):\nWhat? Every site I'm going to these days has these giant black rubik's cube things you can click on and interact with.  Site after site we this damned black rubik's cubes, enough is enough.\n\n#### Comment ID k5s6iqn with +38 score by [(emotyofform2020, Reddit, 2023-10-21)](https://www.reddit.com/r/webdev/comments/17clmce/why_do_all_new_websites_look_the_same/k5s6iqn/) (in reply to ID k5s3d1x):\nALL OBEY THE CUBE\n\n#### Comment ID k5sd7f7 with +14 score by [(illbookkeeper10, Reddit, 2023-10-21)](https://www.reddit.com/r/webdev/comments/17clmce/why_do_all_new_websites_look_the_same/k5sd7f7/) (in reply to ID k5s3d1x):\nThat part is definitely a little different, but a lot of sites have three.js sketches doing random shit, with no interactivity besides moving the model. Especially for that site that cube makes no sense being there\n\n#### Comment ID k5s8vvp with +7 score by [(None, Reddit, 2023-10-21)](https://www.reddit.com/r/webdev/comments/17clmce/why_do_all_new_websites_look_the_same/k5s8vvp/) (in reply to ID k5s3d1x):\nYou know what it means right? AI is being born\n\n#### Comment ID k5s4cwl with +7 score by [(stibgock, Reddit, 2023-10-21)](https://www.reddit.com/r/webdev/comments/17clmce/why_do_all_new_websites_look_the_same/k5s4cwl/) (in reply to ID k5s3d1x):\n😆\n\n#### Comment ID k5u6m0d with +3 score by [(Revolutionary-Stop-8, Reddit, 2023-10-21)](https://www.reddit.com/r/webdev/comments/17clmce/why_do_all_new_websites_look_the_same/k5u6m0d/) (in reply to ID k5s3d1x):\nIt all started with Game Cube.\n\n#### Comment ID k5saalb with +7 score by [(ReplacementLow6704, Reddit, 2023-10-21)](https://www.reddit.com/r/webdev/comments/17clmce/why_do_all_new_websites_look_the_same/k5saalb/) (in reply to ID k5s3d1x):\nI wish this was our worst concern as a species.\n\n#### Comment ID k5sr5d4 with +2 score by [(None, Reddit, 2023-10-21)](https://www.reddit.com/r/webdev/comments/17clmce/why_do_all_new_websites_look_the_same/k5sr5d4/) (in reply to ID k5s3d1x):\nStealthily deletes rubik's, ok back to the drawing board.\n\n#### Comment ID k5soizp with +2 score by [(vORP, Reddit, 2023-10-21)](https://www.reddit.com/r/webdev/comments/17clmce/why_do_all_new_websites_look_the_same/k5soizp/) (in reply to ID k5s3d1x):\nIt used to be the nice lines with dots at the end of them that would connect to each other and float around the background, fads are fads\n\n#### Comment ID k5tvfi9 with +2 score by [(zonbie11155, Reddit, 2023-10-21)](https://www.reddit.com/r/webdev/comments/17clmce/why_do_all_new_websites_look_the_same/k5tvfi9/) (in reply to ID k5s3d1x):\nCUBIC TIME IS TRUTH\n\n#### Comment ID k5thkhc with +1 score by [(Zealousideal-Food285, Reddit, 2023-10-21)](https://www.reddit.com/r/webdev/comments/17clmce/why_do_all_new_websites_look_the_same/k5thkhc/) (in reply to ID k5s3d1x):\nDo you have a list of that? Jw.\n\nWhere do they get that asset!\n\n## Comment ID k5qw497 with +144 score by [(_listless, Reddit, 2023-10-20)](https://www.reddit.com/r/webdev/comments/17clmce/why_do_all_new_websites_look_the_same/k5qw497/) (in reply to ID 17clmce):\nY-combinator has very specific things they like to see in a startup website.  They are very public about what they think is good/effective (just check out their yt channel where they critique startup landing pages), so you're going to have a selection bias with the sites you list here.\n\nAlso, tends are trends; you'll always have trends that come and go and are popular in specific market segments.  Devbros are into the next/nuxt style right now and modern frameworks like tailwind make it easy to copypasta your way to a reasonably good-looking approximation of this look.\n\n### Comment ID k5srv5i with +20 score by [(dark_salad, Reddit, 2023-10-21)](https://www.reddit.com/r/webdev/comments/17clmce/why_do_all_new_websites_look_the_same/k5srv5i/) (in reply to ID k5qw497):\nAfter the first 4, I assumed this was just a guerilla marketing post for YCombinator's latest cohort.\n\n## Comment ID k5qwpa9 with +37 score by [(Ping-and-Pong, Reddit, 2023-10-20)](https://www.reddit.com/r/webdev/comments/17clmce/why_do_all_new_websites_look_the_same/k5qwpa9/) (in reply to ID 17clmce):\nWell there is a giant range there, at least on mobile which is where most of the traffic is...\n\n## Comment ID k5qz5r2 with +216 score by [(None, Reddit, 2023-10-20)](https://www.reddit.com/r/webdev/comments/17clmce/why_do_all_new_websites_look_the_same/k5qz5r2/) (in reply to ID 17clmce):\n“Why are the majority of buildings square?”\n\n### Comment ID k5rkc1u with +41 score by [(Shortcirkuitz, Reddit, 2023-10-21)](https://www.reddit.com/r/webdev/comments/17clmce/why_do_all_new_websites_look_the_same/k5rkc1u/) (in reply to ID k5qz5r2):\nBecause pyramids were taken…\n\n### Comment ID k5s1eod with +7 score by [(Kingsblend420KmK, Reddit, 2023-10-21)](https://www.reddit.com/r/webdev/comments/17clmce/why_do_all_new_websites_look_the_same/k5s1eod/) (in reply to ID k5qz5r2):\nBecause what corner would you piss in?\n\n\n\n### Comment ID k5vqfkd with +1 score by [(None, Reddit, 2023-10-21)](https://www.reddit.com/r/webdev/comments/17clmce/why_do_all_new_websites_look_the_same/k5vqfkd/) (in reply to ID k5qz5r2):\nWhy do all laptops look like MacBooks?\n\n#### Comment ID k5y60fi with +1 score by [(Pingouino55, Reddit, 2023-10-22)](https://www.reddit.com/r/webdev/comments/17clmce/why_do_all_new_websites_look_the_same/k5y60fi/) (in reply to ID k5vqfkd):\nOkay Idk what laptops you look at but the 3 I've had in my life look nothing like a MacBooks. Apart from the fact that they'r laptops and thus have a keyboard and a monitor and can be closed.\n\nTL;DR I don't see this particular analogy being that good.\n\n## Comment ID k5r1bnv with +30 score by [(rickg, Reddit, 2023-10-20)](https://www.reddit.com/r/webdev/comments/17clmce/why_do_all_new_websites_look_the_same/k5r1bnv/) (in reply to ID 17clmce):\nA few things... one is that people tend to follow 'proven' designs when their website isn't about deigns but about communicating their business purpose.\n\nSecond, things like Bootstrap and Tailwind UI are popular and people use those or similarly popular patterns (Chakra in React world...) so the sections of pages (e.g. 'three key features') look similar.\n\nThird, all of these sites seem to be tech product sites (see point 1).\n\nAnd lastly, these patterns remove the need for visitors to work at finding information etc which is valuable when you want people to convert from curious visitor to potential lead or sale.\n\n## Comment ID k5r8n93 with +29 score by [(tridd3r, Reddit, 2023-10-20)](https://www.reddit.com/r/webdev/comments/17clmce/why_do_all_new_websites_look_the_same/k5r8n93/) (in reply to ID 17clmce):\nHow \"same\" is \"same\"? and how much variation do you THINK is possible before it becomes a nightmare to navigate, and find information?\n\n## Comment ID k5rnz9u with +32 score by [(MCneill27, Reddit, 2023-10-21)](https://www.reddit.com/r/webdev/comments/17clmce/why_do_all_new_websites_look_the_same/k5rnz9u/) (in reply to ID 17clmce):\nI find it rich that there’s a lot of people criticizing this “tailwind” look. While I’m sure there’s some with a strong sense of aesthetics here, I’m willing to bet 95%+ of this sub has worse sensibilities for simple, effective sites.\n\nStartups don’t have time to reinvent the wheel on designs for landing pages. They absolutely should outsource this, and they do, because it works.\n\nIf a startup is fucking around with the design of their landing page too much beyond this, it’s a bad sign. Fucking around with the content - now that’s a different story.\n\n### Comment ID k5u7dhb with +4 score by [(beachandbyte, Reddit, 2023-10-21)](https://www.reddit.com/r/webdev/comments/17clmce/why_do_all_new_websites_look_the_same/k5u7dhb/) (in reply to ID k5rnz9u):\nI’d also guess that many people are putting far too much influence on design being unique, If your product is good you could use a bootstrap 3 starter template and kill it. Effective communication of the value proposition and a site that is intuitive to the user are always going to be more important then “unique”.\n\n#### Comment ID k5ub431 with +1 score by [(MCneill27, Reddit, 2023-10-21)](https://www.reddit.com/r/webdev/comments/17clmce/why_do_all_new_websites_look_the_same/k5ub431/) (in reply to ID k5u7dhb):\nAgreed 100%\n\n### Comment ID k5triut with +1 score by [(marcocom, Reddit, 2023-10-21)](https://www.reddit.com/r/webdev/comments/17clmce/why_do_all_new_websites_look_the_same/k5triut/) (in reply to ID k5rnz9u):\nConversely. There is a lot that is translated to a consumer through aesthetic design. It says “we aren’t fly by night. We are obviously smart enough to hire a design firm or at least one creative in this company”, and that translates more than you think. Information is the easy part (especially if you’re smart enough to have one writer instead of a marketing-exec composing and distilling your information). \n\nNeither of us right or wrong. That’s why we are supposed to be a team with your thinking and mine, pulling each other out of our comfort zone and creating something greater than either of us could singularly achieve, a unified work of interactive technology and art. \n\nThat’s how we used to do it when most people on this sub first saw my job and wanted it for themselves 20years ago, and maybe the world has made us forget how to really collaborate together.\n\n## Comment ID k5r4i1u with +9 score by [(N3rdy-Astronaut, Reddit, 2023-10-20)](https://www.reddit.com/r/webdev/comments/17clmce/why_do_all_new_websites_look_the_same/k5r4i1u/) (in reply to ID 17clmce):\nPeople like safety and trends. If you do what everyone else does then its kind of hard to go wrong design wise. Truth is most sites don't need to break the mould, if your just a startup that specialises in lets say high tech security systems then yeah make a killer website but you don't need to re-invent the wheel, so you just go with what everyone else is doing at the moment and maybe put your own little spin on it.\n\n## Comment ID k5rd59x with +15 score by [(bigmarkco, Reddit, 2023-10-20)](https://www.reddit.com/r/webdev/comments/17clmce/why_do_all_new_websites_look_the_same/k5rd59x/) (in reply to ID 17clmce):\n>I could list a thousand examples but I'll keep it at a few.\r  \n\r  \n\n\nYou've kept it to a few very specific examples that all have *the same, rather generic, target audience*, which would explain the commonalities in some of the trends that you've identified here.\n\nBut more generally, it goes back to the goals and objectives of a particular website. They want to make sales, or drive conversions, or showcase a product. And so you want to make your website as friendly as possible for users. And a lot of that comes down to familiarity.  Predictability. The nav menu should be at the top. Clear hierarchy. Calls to action. And you can mix things up a bit. But the core guidelines remain the same. Here is a great primer on the issue from the Website Architect:\n\nhttps://www.youtube.com/watch?v=SfioRS\\_yC6A\n\nWebsites look the same because they are doing things that work. You can try something different, but that is likely to make your website more difficult for the average user to use. It really is a simple calculus.\n\n### Comment ID k5t1109 with +1 score by [(Hatpar, Reddit, 2023-10-21)](https://www.reddit.com/r/webdev/comments/17clmce/why_do_all_new_websites_look_the_same/k5t1109/) (in reply to ID k5rd59x):\nYour video link doesn't work.\n\n## Comment ID k5rrtgo with +8 score by [(SockDem, Reddit, 2023-10-21)](https://www.reddit.com/r/webdev/comments/17clmce/why_do_all_new_websites_look_the_same/k5rrtgo/) (in reply to ID 17clmce):\nIt's just clean, relatively simple design.\n\n### Comment ID k5skxrf with +2 score by [(sheriffderek, Reddit, 2023-10-21)](https://www.reddit.com/r/webdev/comments/17clmce/why_do_all_new_websites_look_the_same/k5skxrf/) (in reply to ID k5rrtgo):\nI’m always curious what a dirty website would look like. A dirty irrelatively complex design.\n\n#### Comment ID k5sow8u with +1 score by [(lordlors, Reddit, 2023-10-21)](https://www.reddit.com/r/webdev/comments/17clmce/why_do_all_new_websites_look_the_same/k5sow8u/) (in reply to ID k5skxrf):\nTry navigating Japanese Rakuten’s website. The product pages are like a nightmare. I also remember Friendster’s profile pages which let you customized the design and look.\n\n#### Comment ID k5sqdwn with +1 score by [(ztbwl, Reddit, 2023-10-21)](https://www.reddit.com/r/webdev/comments/17clmce/why_do_all_new_websites_look_the_same/k5sqdwn/) (in reply to ID k5skxrf):\nRemember MySpace?\n\n## Comment ID k5rgis7 with +36 score by [(io-x, Reddit, 2023-10-20)](https://www.reddit.com/r/webdev/comments/17clmce/why_do_all_new_websites_look_the_same/k5rgis7/) (in reply to ID 17clmce):\nWhy do all magazines look the same?\n\nWhy do all newsletters looks the same?\n\nWhy do all books look the same?\n\nWhy do all bookcases look the same?\n\nWhy do all doors look the same?\n\nWhy do everything look the same that's build in the same age and for the same purpose?\n\n### Comment ID k5scy04 with +2 score by [(ISeekGirls, Reddit, 2023-10-21)](https://www.reddit.com/r/webdev/comments/17clmce/why_do_all_new_websites_look_the_same/k5scy04/) (in reply to ID k5rgis7):\nWhy do all buildings have sharp angles?\n\n## Comment ID k5rux04 with +6 score by [(chuckdacuck, Reddit, 2023-10-21)](https://www.reddit.com/r/webdev/comments/17clmce/why_do_all_new_websites_look_the_same/k5rux04/) (in reply to ID 17clmce):\nfuel unwritten soup full heavy waiting hobbies bow nail file\n\n *This post was mass deleted and anonymized with [Redact](https://redact.dev)*\n\n### Comment ID k5ssq80 with +11 score by [(ryncewynd, Reddit, 2023-10-21)](https://www.reddit.com/r/webdev/comments/17clmce/why_do_all_new_websites_look_the_same/k5ssq80/) (in reply to ID k5rux04):\nThank you for developing accessible websites for Australians\n\n## Comment ID k5rcxv0 with +17 score by [(DamionDreggs, Reddit, 2023-10-20)](https://www.reddit.com/r/webdev/comments/17clmce/why_do_all_new_websites_look_the_same/k5rcxv0/) (in reply to ID 17clmce):\nBecause you haven't broken into the scene to show the world what real web design is all about!\n\n## Comment ID k5rpud1 with +3 score by [(djav1985, Reddit, 2023-10-21)](https://www.reddit.com/r/webdev/comments/17clmce/why_do_all_new_websites_look_the_same/k5rpud1/) (in reply to ID 17clmce):\nMost people can't turn on a computer, most don't know what a web browser is, most don't know what version of windows they are running. Most don't know the difference between searching the web and typing in a domain name to go directly to the site...\n\nIf you try to step outside the normal most people will be absolutely confused...\n\nAnd even if it's something that it's geared toward people that are techy and would understand computers. The people that are investing in it are probably not techy and not go good computers. So they still want to make it usable by everyone.\n\n## Comment ID k5rz2ny with +4 score by [(Disastrous-Hearing72, Reddit, 2023-10-21)](https://www.reddit.com/r/webdev/comments/17clmce/why_do_all_new_websites_look_the_same/k5rz2ny/) (in reply to ID 17clmce):\nBecause when you try to be unique, what you are really doing is forcing your user to learn your website. If a website is similar to every other site, it's more intuitive to navigate, resulting in longer visiting time and higher conversion rate. \n\nThe circle wheel works, we don't need to keep making it different shapes.\n\n## Comment ID k5rm9j7 with +3 score by [(dptillinfinity93, Reddit, 2023-10-21)](https://www.reddit.com/r/webdev/comments/17clmce/why_do_all_new_websites_look_the_same/k5rm9j7/) (in reply to ID 17clmce):\nTrends. Trends are trends are trends are trends.\n\n## Comment ID k5rqhh1 with +3 score by [(GlitteringAccident31, Reddit, 2023-10-21)](https://www.reddit.com/r/webdev/comments/17clmce/why_do_all_new_websites_look_the_same/k5rqhh1/) (in reply to ID 17clmce):\nFive years ago all the new websites looked the same. Now all the websites look the same but in a different way. Trends exist\n\n## Comment ID k5rqz8e with +3 score by [(dryadofelysium, Reddit, 2023-10-21)](https://www.reddit.com/r/webdev/comments/17clmce/why_do_all_new_websites_look_the_same/k5rqz8e/) (in reply to ID 17clmce):\nWhy do newspaper prints all look the same\n\n## Comment ID k5sgmqc with +3 score by [(elendee, Reddit, 2023-10-21)](https://www.reddit.com/r/webdev/comments/17clmce/why_do_all_new_websites_look_the_same/k5sgmqc/) (in reply to ID 17clmce):\nthese aren't \"all websites\"; they're product pitches.  The idea is you consume these websites once and never come back\n\nmcmaster carr is still the pinnacle of human achievement with the http art form:\n\nhttps://www.mcmaster.com/\n\n### Comment ID k5tfpir with +1 score by [(None, Reddit, 2023-10-21)](https://www.reddit.com/r/webdev/comments/17clmce/why_do_all_new_websites_look_the_same/k5tfpir/) (in reply to ID k5sgmqc):\nThis is beautiful. A real website. Thanks.\n\n## Comment ID k5rbnhv with +5 score by [(oopssorrydaddy, Reddit, 2023-10-20)](https://www.reddit.com/r/webdev/comments/17clmce/why_do_all_new_websites_look_the_same/k5rbnhv/) (in reply to ID 17clmce):\nEverybody copies the hottest website. It used to be Stripe, and now it’s Linear.\n\n## Comment ID k5rpg2b with +2 score by [(jkvincent, Reddit, 2023-10-21)](https://www.reddit.com/r/webdev/comments/17clmce/why_do_all_new_websites_look_the_same/k5rpg2b/) (in reply to ID 17clmce):\nTrends will always have an impact, but frankly many (smartly made) websites follow well documented conventions that enable accessibility and usability for users. There's contextual nuance that informs any design, but the basic rules don't tend to change in radical ways. Thus, over time you end up with a lot of sites following the same principles because those principles have proven to lead to success.\n\n## Comment ID k5s9kc8 with +2 score by [(brettdavis4, Reddit, 2023-10-21)](https://www.reddit.com/r/webdev/comments/17clmce/why_do_all_new_websites_look_the_same/k5s9kc8/) (in reply to ID 17clmce):\nI blame responsive design.  In design aspects, I kind of miss the days of separate desktop and mobile sites.\n\n## Comment ID k5skryv with +2 score by [(potcubic, Reddit, 2023-10-21)](https://www.reddit.com/r/webdev/comments/17clmce/why_do_all_new_websites_look_the_same/k5skryv/) (in reply to ID 17clmce):\nThis is the linear-effect\nhttps://bootcamp.uxdesign.cc/the-rise-of-linear-style-design-origins-trends-and-techniques-4fd96aab7646\n\n### Comment ID k5sksri with +2 score by [(AmputatorBot, Reddit, 2023-10-21)](https://www.reddit.com/r/webdev/comments/17clmce/why_do_all_new_websites_look_the_same/k5sksri/) (in reply to ID k5skryv):\nIt looks like you shared an AMP link. These should load faster, but AMP is controversial because of [concerns over privacy and the Open Web](https://www.reddit.com/r/AmputatorBot/comments/ehrq3z/why_did_i_build_amputatorbot).\n\nMaybe check out **the canonical page** instead: **[https:\\u002F\\u002Fbootcamp.uxdesign.cc\\u002Fthe-rise-of-linear-style-design-origins-trends-and-techniques-4fd96aab7646](https:\\u002F\\u002Fbootcamp.uxdesign.cc\\u002Fthe-rise-of-linear-style-design-origins-trends-and-techniques-4fd96aab7646)**\n\n*****\n\n ^(I'm a bot | )[^(Why & About)](https://www.reddit.com/r/AmputatorBot/comments/ehrq3z/why_did_i_build_amputatorbot)^( | )[^(Summon: u/AmputatorBot)](https://www.reddit.com/r/AmputatorBot/comments/cchly3/you_can_now_summon_amputatorbot/)\n\n## Comment ID k5sqwq2 with +2 score by [(svenjoelson, Reddit, 2023-10-21)](https://www.reddit.com/r/webdev/comments/17clmce/why_do_all_new_websites_look_the_same/k5sqwq2/) (in reply to ID 17clmce):\nCuz no one is using flash anymore, thank god!\n\n## Comment ID k5t0fmv with +2 score by [(None, Reddit, 2023-10-21)](https://www.reddit.com/r/webdev/comments/17clmce/why_do_all_new_websites_look_the_same/k5t0fmv/) (in reply to ID 17clmce):\nWhat about these websites looks the same?\n\n### Comment ID k5te81s with +2 score by [(teamswiftie, Reddit, 2023-10-21)](https://www.reddit.com/r/webdev/comments/17clmce/why_do_all_new_websites_look_the_same/k5te81s/) (in reply to ID k5t0fmv):\nThey all have a navbar apparently\n\n## Comment ID k5t6frb with +2 score by [(Delyzr, Reddit, 2023-10-21)](https://www.reddit.com/r/webdev/comments/17clmce/why_do_all_new_websites_look_the_same/k5t6frb/) (in reply to ID 17clmce):\nHa! I use NuxtJS instead!\n\n## Comment ID k5t7zc5 with +2 score by [(multistackdev, Reddit, 2023-10-21)](https://www.reddit.com/r/webdev/comments/17clmce/why_do_all_new_websites_look_the_same/k5t7zc5/) (in reply to ID 17clmce):\nClicked first 4 and they felt distinctly different. Yeah, similar themes but not really deserving of this thread.\n\nNot sure what Next.js has to do with this, Next has no say over look and feel. We use Next.js with completely custom designs from Figma. Next is not a theme or design and also represents a pretty small % of total web.\n\n## Comment ID k5rkyil with +3 score by [(MyHomeworkAteMyDog, Reddit, 2023-10-21)](https://www.reddit.com/r/webdev/comments/17clmce/why_do_all_new_websites_look_the_same/k5rkyil/) (in reply to ID 17clmce):\nThey seem to be following Apple’s design language for their website.\n\n## Comment ID k5rinyw with +2 score by [(saintpumpkin, Reddit, 2023-10-21)](https://www.reddit.com/r/webdev/comments/17clmce/why_do_all_new_websites_look_the_same/k5rinyw/) (in reply to ID 17clmce):\nWe are just waiting for you to build something new\n\n## Comment ID k5s8110 with +2 score by [(jmking, Reddit, 2023-10-21)](https://www.reddit.com/r/webdev/comments/17clmce/why_do_all_new_websites_look_the_same/k5s8110/) (in reply to ID 17clmce):\nBecause you're ignoring that these sites have a job to do. The more familiar to users the site is, the lower the barriers there are for getting that visitor to sign up or contact sales or requesting a demo or whatever.\n\nThese sites aren't there to re-invent the wheel.\n\n## Comment ID k5rl0i8 with +2 score by [(JesterDolor, Reddit, 2023-10-21)](https://www.reddit.com/r/webdev/comments/17clmce/why_do_all_new_websites_look_the_same/k5rl0i8/) (in reply to ID 17clmce):\nSaaS owners, UI/UX Designers, and stakeholders all want the same thing and literally copy each other.\nAlso everyone thinks it's scary to be original.\n\n## Comment ID ln1n99w with +1 score by [(dickdickalus, Reddit, 2024-09-14)](https://www.reddit.com/r/webdev/comments/17clmce/why_do_all_new_websites_look_the_same/ln1n99w/) (in reply to ID 17clmce):\nIf you think this is bad, the same thing has happened with web applications (we used to call programs). \n\nBad enough Google and JavaScript collectively have ruined the internet with Chromium + Electron, but at least try to be original. These are considered the \"hottest\" up and coming CRMs on the market right now:  \n[https://twenty.com/](https://twenty.com/)  \n[https://attio.com/](https://attio.com/)  \n[https://www.folk.app/](https://www.folk.app/)\n\nIt's not even hyperbolic to say I've seen identical twins that share less similarities than these web apps have with each other.\n\nOh, and fuck Steve Jobs for killing Flash.   \n[https://www.webdesignmuseum.org/golden-age-of-web-design](https://www.webdesignmuseum.org/golden-age-of-web-design)  \nWe've gone back twenty years.\n\n## Comment ID k5s8y1t with +1 score by [(daniel625, Reddit, 2023-10-21)](https://www.reddit.com/r/webdev/comments/17clmce/why_do_all_new_websites_look_the_same/k5s8y1t/) (in reply to ID 17clmce):\nWebsites nowadays are awful. Have to close the cookies message, chat robot, and pop up before I can see anything. It’s horrible.\n\n## Comment ID k5sbyfj with +1 score by [(NoDoze-, Reddit, 2023-10-21)](https://www.reddit.com/r/webdev/comments/17clmce/why_do_all_new_websites_look_the_same/k5sbyfj/) (in reply to ID 17clmce):\nLOL \"any other resource people are using?\" ...uhmm WordPress, bootstrap, react, laravel, etc. I don't know anyone who uses nextjs.\n\n\n\n\n\n\n\n\n\n\n\n\n\n## Comment ID k5rk2yq with +1 score by [(None, Reddit, 2023-10-21)](https://www.reddit.com/r/webdev/comments/17clmce/why_do_all_new_websites_look_the_same/k5rk2yq/) (in reply to ID 17clmce):\nI think you're attributing too much to NextJS. NextJS uses React, but so do a lot of other websites/frameworks. NexJS uses Tailwind (by default, I think), but so do a lot of others. Is there even a \"NextJS font\", or is it just a font that's used all over the place? Maybe it's just that React, Tailwind, etc are easy to work with and produce a decent site without having to spend a shit ton of money on developer time. And to my untrained eye, those sites all look quite different from a branding perspective. Websites aren't art. They're business tools.\n\n## Comment ID k5rnhaz with +1 score by [(Classic-Dependent517, Reddit, 2023-10-21)](https://www.reddit.com/r/webdev/comments/17clmce/why_do_all_new_websites_look_the_same/k5rnhaz/) (in reply to ID 17clmce):\nso i tried to follow a so called next gen ui which uses some new tech but yeah its so hard to implement and i dont find any reason to put my efforts into this to make a website of a typical SaaS.\n\nits a different stack but you get the point..\nfor example: https://codelabs.developers.google.com/codelabs/flutter-next-gen-uis\n\n## Comment ID k5rpe93 with +1 score by [(Gandalf-and-Frodo, Reddit, 2023-10-21)](https://www.reddit.com/r/webdev/comments/17clmce/why_do_all_new_websites_look_the_same/k5rpe93/) (in reply to ID 17clmce):\nNo one gives a shit about website design that much besides design and tech nerds like us. Most people don't even browse that many different product websites per week. \n\nThe first two website designs are great in my opinion. Most Websites are first and foremost meant to sell a product to the masses NOT be an elaborate artistic expression.\n\n## Comment ID k5rr63z with +1 score by [(basecase_, Reddit, 2023-10-21)](https://www.reddit.com/r/webdev/comments/17clmce/why_do_all_new_websites_look_the_same/k5rr63z/) (in reply to ID 17clmce):\n\"Well how does So and So do it? Do it like them!\"\n\n## Comment ID k5rtrkj with +1 score by [(Shmutsi, Reddit, 2023-10-21)](https://www.reddit.com/r/webdev/comments/17clmce/why_do_all_new_websites_look_the_same/k5rtrkj/) (in reply to ID 17clmce):\nall your examples look literally so diferent\n\n## Comment ID k5rzksv with +1 score by [(Haunting_Welder, Reddit, 2023-10-21)](https://www.reddit.com/r/webdev/comments/17clmce/why_do_all_new_websites_look_the_same/k5rzksv/) (in reply to ID 17clmce):\nAll old websites also look the same. Websites develop standards over time that make them easy to browse.\n\n## Comment ID k5s2lxy with +1 score by [(seamore555, Reddit, 2023-10-21)](https://www.reddit.com/r/webdev/comments/17clmce/why_do_all_new_websites_look_the_same/k5s2lxy/) (in reply to ID 17clmce):\nWhere are all the spinning GIFs!!!??\n\n## Comment ID k5s60xw with +1 score by [(rackmountme, Reddit, 2023-10-21)](https://www.reddit.com/r/webdev/comments/17clmce/why_do_all_new_websites_look_the_same/k5s60xw/) (in reply to ID 17clmce):\nBecause we ran millions of A/B tests and this is what works.\n\n## Comment ID k5s6ec7 with +1 score by [(abeuscher, Reddit, 2023-10-21)](https://www.reddit.com/r/webdev/comments/17clmce/why_do_all_new_websites_look_the_same/k5s6ec7/) (in reply to ID 17clmce):\nSaas drives the market. We're all in one geographical location going to the same places and seeing the same stuff. When I get to a new startup, they immediately copy their competitors for the website because you can't hire agency before your Series B or thereabouts. It needs to be slick and performant to a certain degree, but the surveillance marketing that the site is driving and catching is much more the focus and where the money is spent. And DemandGen folks don't really care what the website looks like so they churn out the same shit over and over.\n\nAs to why design is all dark and looks like a JVC tape deck from the late 80's right now? I'd say it's in reaction to the brighter colors of Material Design that came before it, along with a preference for sites that have a light and dark mode.\n\n## Comment ID k5sbxjn with +1 score by [(keptfrozen, Reddit, 2023-10-21)](https://www.reddit.com/r/webdev/comments/17clmce/why_do_all_new_websites_look_the_same/k5sbxjn/) (in reply to ID 17clmce):\nIn my opinion, they probably just want to look relevant and focus on conversion rate.\n\n## Comment ID k5sfpdh with +1 score by [(nate-developer, Reddit, 2023-10-21)](https://www.reddit.com/r/webdev/comments/17clmce/why_do_all_new_websites_look_the_same/k5sfpdh/) (in reply to ID 17clmce):\nThe examples are somewhat similar but also still somewhat varied tbh, I've seen more similar website groups.  \n\nYou're looking at a bunch of startups with similar products and audiences in the same sector.  They'll copy off each other, style themselves to sell to the same audience, and there are just trends that crop up every once in a while.\n\nRemember when sites were all \"corporate memphis \"style with the same looking blue blob people clip art?  This is just another trend, modern looking dark theme with neon accent and code highlights to sell to devs.  \n\nThere are lots of websites that don't look like this.    Look at something like slack.com which has brighter pastel colors and markets to a different demographic.\n\nNext.js doesn't really have anything to do with the design.\n\n## Comment ID k5sif7e with +1 score by [(None, Reddit, 2023-10-21)](https://www.reddit.com/r/webdev/comments/17clmce/why_do_all_new_websites_look_the_same/k5sif7e/) (in reply to ID 17clmce):\nI'm guessing they just use the same components library. I built my last site using React material UI. It kind of just looks like an average site released by Google. If you're not a designer you kind of just want to get something out that looks presentable.\n\n## Comment ID k5sijos with +1 score by [(ifstatementequalsAI, Reddit, 2023-10-21)](https://www.reddit.com/r/webdev/comments/17clmce/why_do_all_new_websites_look_the_same/k5sijos/) (in reply to ID 17clmce):\nIf something works why change it ?\nIt's just the typical look of tech company which sells a service.\n\n## Comment ID k5skvja with +1 score by [(Blazing1, Reddit, 2023-10-21)](https://www.reddit.com/r/webdev/comments/17clmce/why_do_all_new_websites_look_the_same/k5skvja/) (in reply to ID 17clmce):\nThe web goes through phases. At one point every website was bootstrap UI. Now it's material ui\n\n## Comment ID k5smisl with +1 score by [(SubstantialSecond156, Reddit, 2023-10-21)](https://www.reddit.com/r/webdev/comments/17clmce/why_do_all_new_websites_look_the_same/k5smisl/) (in reply to ID 17clmce):\nA lof of websites use similar ui libraries and tools to streamline development\n\n## Comment ID k5sssif with +1 score by [(Darth_Ender_Ro, Reddit, 2023-10-21)](https://www.reddit.com/r/webdev/comments/17clmce/why_do_all_new_websites_look_the_same/k5sssif/) (in reply to ID 17clmce):\nI like this as a user. I’m not entering a site for the art of its design. I enter for its info. Standardizing the info display gets me quicker to the location I need. Like a book. How would you like each book to have the info displayed differently?\n\n## Comment ID k5sv98i with +1 score by [(peduxe, Reddit, 2023-10-21)](https://www.reddit.com/r/webdev/comments/17clmce/why_do_all_new_websites_look_the_same/k5sv98i/) (in reply to ID 17clmce):\nUI design trends and everyone using the same templates.\n\nIf company X sees company Y that is more profitable in their own market they’ll generally want to do what they want in all aspects instead of trying new things.\n\n## Comment ID k5t0drk with +1 score by [(Hatpar, Reddit, 2023-10-21)](https://www.reddit.com/r/webdev/comments/17clmce/why_do_all_new_websites_look_the_same/k5t0drk/) (in reply to ID 17clmce):\nBecause CSS frameworks tend to lead people into design choices by calling styles stuff like hero-title and info-box so before you have even applied classes the site has been designed and developers  are just painting by numbers.\n\n## Comment ID k5t0lg6 with +1 score by [(TheDeadlyCat, Reddit, 2023-10-21)](https://www.reddit.com/r/webdev/comments/17clmce/why_do_all_new_websites_look_the_same/k5t0lg6/) (in reply to ID 17clmce):\nPeople still visit websites?\n\n## Comment ID k5t805w with +1 score by [(mdpuds, Reddit, 2023-10-21)](https://www.reddit.com/r/webdev/comments/17clmce/why_do_all_new_websites_look_the_same/k5t805w/) (in reply to ID 17clmce):\nLooks like regular sales funnel pages to me\n\n## Comment ID k5tdxde with +1 score by [(SalamanderSweet9909, Reddit, 2023-10-21)](https://www.reddit.com/r/webdev/comments/17clmce/why_do_all_new_websites_look_the_same/k5tdxde/) (in reply to ID 17clmce):\nBecause all the budget is invested on acquisition!\n\n## Comment ID k5th19m with +1 score by [(viruxe, Reddit, 2023-10-21)](https://www.reddit.com/r/webdev/comments/17clmce/why_do_all_new_websites_look_the_same/k5th19m/) (in reply to ID 17clmce):\nTailwind.\n\n## Comment ID k5torwy with +1 score by [(kernel348, Reddit, 2023-10-21)](https://www.reddit.com/r/webdev/comments/17clmce/why_do_all_new_websites_look_the_same/k5torwy/) (in reply to ID 17clmce):\nThey are not too similar but they all are using any kind of React framework with well known UI libraries and that's why they kinda look similar\n\n## Comment ID k5tzc7c with +1 score by [(dilfybro, Reddit, 2023-10-21)](https://www.reddit.com/r/webdev/comments/17clmce/why_do_all_new_websites_look_the_same/k5tzc7c/) (in reply to ID 17clmce):\nIt were ever thus.\n\n## Comment ID k5u4qiv with +1 score by [(Mr_Stabil, Reddit, 2023-10-21)](https://www.reddit.com/r/webdev/comments/17clmce/why_do_all_new_websites_look_the_same/k5u4qiv/) (in reply to ID 17clmce):\nBecause people are afraid to be creative\n\n## Comment ID k5u54uh with +1 score by [(BobFellatio, Reddit, 2023-10-21)](https://www.reddit.com/r/webdev/comments/17clmce/why_do_all_new_websites_look_the_same/k5u54uh/) (in reply to ID 17clmce):\n[https://resend.com/](https://resend.com/)  \n\n\nLooked hella nice tho, im stealing that!  \n\n\n(And that \"Ill steal that\" - idea is probably the core reason for sites to look similar)\n\n## Comment ID k5u8uts with +1 score by [(r_ilek, Reddit, 2023-10-21)](https://www.reddit.com/r/webdev/comments/17clmce/why_do_all_new_websites_look_the_same/k5u8uts/) (in reply to ID 17clmce):\nWebsite is not an art - it has to send the information. If it looks nice and is readable that's enough. Also with millions websites out there - of course some of them are more alike than other\n\n## Comment ID k5uhp1p with +1 score by [(jBlairTech, Reddit, 2023-10-21)](https://www.reddit.com/r/webdev/comments/17clmce/why_do_all_new_websites_look_the_same/k5uhp1p/) (in reply to ID 17clmce):\nTrends.  Someone, likely an “entrepreneur” that “specializes” in digital marketing, said something in a blog post, then others perked their ears up and went with it, and so on.\n\nI tried the online store/digital marketing thing; it’s what turned me on to the webdev space.  There are tons of gurus who will tell you what’s “hot” and how much more attractive your site will be…  Nevermind that *they* have a custom-built framework yet they’re telling 100+ everyday people trying to make their side hustle work the same thing.\n\nThis is what you get.  In the mid-2000’s, there was a point where I could tell which smaller site was listening to what “guru”.  It was enlightening, for sure.\n\n## Comment ID k5ulpcq with +1 score by [(tk421jag, Reddit, 2023-10-21)](https://www.reddit.com/r/webdev/comments/17clmce/why_do_all_new_websites_look_the_same/k5ulpcq/) (in reply to ID 17clmce):\nBecause it works. That's why they look and feel similar.\n\n## Comment ID k5va40b with +1 score by [(GentAndScholar87, Reddit, 2023-10-21)](https://www.reddit.com/r/webdev/comments/17clmce/why_do_all_new_websites_look_the_same/k5va40b/) (in reply to ID 17clmce):\nI think the thing in common is that they’re all using some tailwind ui templates. \n\nhttps://tailwindui.com/templates\n\n## Comment ID k5vb9y0 with +1 score by [(Smaug117, Reddit, 2023-10-21)](https://www.reddit.com/r/webdev/comments/17clmce/why_do_all_new_websites_look_the_same/k5vb9y0/) (in reply to ID 17clmce):\nall those are actually startup websites; don't expect a lot from dev on budget.\n\n## Comment ID k5vn579 with +1 score by [(professorhummingbird, Reddit, 2023-10-21)](https://www.reddit.com/r/webdev/comments/17clmce/why_do_all_new_websites_look_the_same/k5vn579/) (in reply to ID 17clmce):\nActually, all new websites ALWAYS look the same. This is just this era's meta. \n\nAs to why? Mostly because it looks good and we think it's effective. It's really hard and expensive to be original. It's also risky.\n\n## Comment ID k5w1wtx with +1 score by [(BobJutsu, Reddit, 2023-10-21)](https://www.reddit.com/r/webdev/comments/17clmce/why_do_all_new_websites_look_the_same/k5w1wtx/) (in reply to ID 17clmce):\n>the modern tech stack is rooted in NextJS\n\nThats a pretty bold statement. My team does almost $3mil a year in new web development and have never once used Next. And I can name more people/sites that don't have it than do. It's 1 tool amongst a sea of tools...that happen to be overrepresented on reddit.\n\n## Comment ID k5xfeyz with +1 score by [(juggler531, Reddit, 2023-10-22)](https://www.reddit.com/r/webdev/comments/17clmce/why_do_all_new_websites_look_the_same/k5xfeyz/) (in reply to ID 17clmce):\nPeople spend more time on other websites than on your own so if you want to improve usability, if you have to make sure it looks and works similar, because that is what your user is used to\n\n## Comment ID k5ytyrp with +1 score by [(beatlz, Reddit, 2023-10-22)](https://www.reddit.com/r/webdev/comments/17clmce/why_do_all_new_websites_look_the_same/k5ytyrp/) (in reply to ID 17clmce):\nBecause it’s a formula that works\n\n## Comment ID krijqj3 with +1 score by [(Mammoth_Evidence6518, Reddit, 2024-02-21)](https://www.reddit.com/r/webdev/comments/17clmce/why_do_all_new_websites_look_the_same/krijqj3/) (in reply to ID 17clmce):\nWhy waste money on designing when templates can be copied.",
      "# Post ID 1eu96tk: “if an LLM is allowed to use a fixed but non-trivial amount of inference-time compute, how much can it improve its performance on a challenging prompt?” Test-time compute can be used to outperform a 14× larger model. with +308 score by [(None, Reddit, 2024-08-17)](https://www.reddit.com/r/LocalLLaMA/comments/1eu96tk/if_an_llm_is_allowed_to_use_a_fixed_but/)\nhttps://arxiv.org/html/2408.03314v1\n\n## Comment ID lik78wo with +55 score by [(asankhs, Reddit, 2024-08-17)](https://www.reddit.com/r/LocalLLaMA/comments/1eu96tk/if_an_llm_is_allowed_to_use_a_fixed_but/lik78wo/) (in reply to ID 1eu96tk):\nWe also experimented with several such techniques and found that we can even beat gpt-4o with just gpt-4o-mini at 1/50th the cost. - https://arxiv.org/abs/2407.18521\n\n### Comment ID lik7bdk with +9 score by [(None, Reddit, 2024-08-17)](https://www.reddit.com/r/LocalLLaMA/comments/1eu96tk/if_an_llm_is_allowed_to_use_a_fixed_but/lik7bdk/) (in reply to ID lik78wo):\nTell me more\n\n#### Comment ID lik7tmq with +11 score by [(asankhs, Reddit, 2024-08-17)](https://www.reddit.com/r/LocalLLaMA/comments/1eu96tk/if_an_llm_is_allowed_to_use_a_fixed_but/lik7tmq/) (in reply to ID lik7bdk):\nSorry missed adding the link to the paper. Edited it now\n\n### Comment ID lip3s8n with +4 score by [(FreegheistOfficial, Reddit, 2024-08-18)](https://www.reddit.com/r/LocalLLaMA/comments/1eu96tk/if_an_llm_is_allowed_to_use_a_fixed_but/lip3s8n/) (in reply to ID lik78wo):\nwhat do you think is the reason none of these kind of methods (which look interesting) have made it into any mainstream usage, is it the compute overhead? do they still work across all general cases? just curious\n\n#### Comment ID lipbr6u with +4 score by [(asankhs, Reddit, 2024-08-18)](https://www.reddit.com/r/LocalLLaMA/comments/1eu96tk/if_an_llm_is_allowed_to_use_a_fixed_but/lipbr6u/) (in reply to ID lip3s8n):\nYes, there is a bit of compute overhead but these techniques are already very new, I think it will make its way to mainstream usage.\n\ne.g. you can use something similar to this from Together AI - [https://docs.together.ai/docs/mixture-of-agents](https://docs.together.ai/docs/mixture-of-agents)\n\nOpenPipe - [https://openpipe.ai/blog/mixture-of-agents](https://openpipe.ai/blog/mixture-of-agents) \n\nPatched - [https://docs.patched.codes/patched-api#optimized-inference](https://docs.patched.codes/patched-api#optimized-inference)\n\n### Comment ID lj20zjf with +1 score by [(DistractionRectangle, Reddit, 2024-08-20)](https://www.reddit.com/r/LocalLLaMA/comments/1eu96tk/if_an_llm_is_allowed_to_use_a_fixed_but/lj20zjf/) (in reply to ID lik78wo):\nHey, the paper says you open sourced your implementation in https://github.com/patched-codes/patchwork, could you point me to where it is in the code since I'm apparently missing it? Or is it not available yet?\n\n#### Comment ID llmm9t5 with +1 score by [(asankhs, Reddit, 2024-09-05)](https://www.reddit.com/r/LocalLLaMA/comments/1eu96tk/if_an_llm_is_allowed_to_use_a_fixed_but/llmm9t5/) (in reply to ID lj20zjf):\nThe implementation is available [https://github.com/codelion/optillm/blob/main/moa.py](https://github.com/codelion/optillm/blob/main/moa.py), we did not publish it as part of the patchwork repo as it was not related to development workflows.\n\n## Comment ID liit3t3 with +143 score by [(GeneriAcc, Reddit, 2024-08-17)](https://www.reddit.com/r/LocalLLaMA/comments/1eu96tk/if_an_llm_is_allowed_to_use_a_fixed_but/liit3t3/) (in reply to ID 1eu96tk):\nYeah, I already tested the basic premise behind this in a much simpler way that’s trivial to implement, and it can be highly effective.\n\nYou can basically do something like this:\n1. Give the model a prompt, get a response\n2. Pass the original prompt, the response, and an additional “Could <assistant>’s response be improved in any way? If so, rewrite it to be better. If not, just respond with <COMPLETE>.” I’m paraphrasing and simplyfing here - your “improve prompt” should be much more detailed and specific in describing what would constitute a “better” response and what you’re expecting, depending on your specific use case.\n\nThen you basically run that in a loop until you either get a <COMPLETE> response, or reach a maximum defined N iterations and break out of it. Of course, all of this is done behind the scenes - the user just provides the initial prompt and gets a single “final” response that takes longer to compute than a single-shot one would.\n\n### Comment ID liixup1 with +59 score by [(ttkciar, Reddit, 2024-08-17)](https://www.reddit.com/r/LocalLLaMA/comments/1eu96tk/if_an_llm_is_allowed_to_use_a_fixed_but/liixup1/) (in reply to ID liit3t3):\nCool :-) you've re-invented Self-Critique!  Been fiddling with this myself, using [HelixNet](https://huggingface.co/migtissera/HelixNet) as a template.  It's a powerful technique, if compute-hungry.\n\n#### Comment ID lijsrrc with +31 score by [(GeneriAcc, Reddit, 2024-08-17)](https://www.reddit.com/r/LocalLLaMA/comments/1eu96tk/if_an_llm_is_allowed_to_use_a_fixed_but/lijsrrc/) (in reply to ID liixup1):\nI had no doubt someone came up with it before, because it’s such a simple and obvious thing to try. What’s really surprising is how effective it can be given that simplicity…\n\n### Comment ID lijz3tu with +16 score by [(Distinct-Target7503, Reddit, 2024-08-17)](https://www.reddit.com/r/LocalLLaMA/comments/1eu96tk/if_an_llm_is_allowed_to_use_a_fixed_but/lijz3tu/) (in reply to ID liit3t3):\nCouldn't this be used as a kind of self distillation?\nAs the paper say that models with more compute time outperform larger models, we could distill from \"a model that used more compute time\" instead of a model that has more parameters...\n\nI mean, run your pipeline and train toward the final response as SFT of the pair initial prompt-final response... One could even use it as alignment using the fist model answer (if not <complete>) as negative and the final answer as positive with something like DPO\n\n#### Comment ID lios7ac with +2 score by [(None, Reddit, 2024-08-18)](https://www.reddit.com/r/LocalLLaMA/comments/1eu96tk/if_an_llm_is_allowed_to_use_a_fixed_but/lios7ac/) (in reply to ID lijz3tu):\nHonestly I thought this was well known and people were already doing it. It's rough trying to get it to work with multi turn but for single shot responses you're basically shooting yourself in the foot if you're not already doing this. It works even with big models like command-r-plus to give \"best of the best\" responses imo. I haven't even tried DPO yet, but you're definitely correct that you inherently create that kind of dataset by doing this.\n\n### Comment ID lij3am3 with +13 score by [(Nyao, Reddit, 2024-08-17)](https://www.reddit.com/r/LocalLLaMA/comments/1eu96tk/if_an_llm_is_allowed_to_use_a_fixed_but/lij3am3/) (in reply to ID liit3t3):\nFor a project I've tried a similar technique I called \"2 experts conversation\", where instead of one assistant, I ask to simulate a conversation between 2 'experts' of the field (in my case it was writers), then to deliver a better response.\n\nNo idea if it's better or worse than the technique you described, but results were good.\n\n#### Comment ID likfm15 with +12 score by [(Single_Composer7308, Reddit, 2024-08-17)](https://www.reddit.com/r/LocalLLaMA/comments/1eu96tk/if_an_llm_is_allowed_to_use_a_fixed_but/likfm15/) (in reply to ID lij3am3):\nI do this in a slighty different way for coding tasks. Set two llms one as the assistant one as the user. Give both of them tool access to the compiler. Give the user a task and tell it to talk to the assistant to improve the code. Let it run and they'll both make incremental improvements. It's limited largely by context length coherence. I've made some attempts at pruning and context summarization, but ultimately we need better models for the LLM's to actually be able to make good architectural choices for large projects.\n\n### Comment ID liiuoo1 with +15 score by [(Klutzy-Smile-9839, Reddit, 2024-08-17)](https://www.reddit.com/r/LocalLLaMA/comments/1eu96tk/if_an_llm_is_allowed_to_use_a_fixed_but/liiuoo1/) (in reply to ID liit3t3):\nI also use XML tags when interacting with LLM for automatic logic behind. It is more intuitive than interacting with JSON in my opinion.\n\n#### Comment ID lin4bjp with +6 score by [(ThreeKiloZero, Reddit, 2024-08-17)](https://www.reddit.com/r/LocalLLaMA/comments/1eu96tk/if_an_llm_is_allowed_to_use_a_fixed_but/lin4bjp/) (in reply to ID liiuoo1):\nAnthropic models like xml and OpenAI JSON , due to their training data format.\n\n#### Comment ID lio2duv with +1 score by [(Watchguyraffle1, Reddit, 2024-08-18)](https://www.reddit.com/r/LocalLLaMA/comments/1eu96tk/if_an_llm_is_allowed_to_use_a_fixed_but/lio2duv/) (in reply to ID liiuoo1):\nI’m sorry, I don’t understand this but I’d like to.  What do mean automatic logic behind?\n\n### Comment ID lil1y53 with +4 score by [(Mundane_Ad8936, Reddit, 2024-08-17)](https://www.reddit.com/r/LocalLLaMA/comments/1eu96tk/if_an_llm_is_allowed_to_use_a_fixed_but/lil1y53/) (in reply to ID liit3t3):\nThis has been around for years.. there's a flood of papers where people are documenting existing real world practices they stumbled on.\n\n### Comment ID liium8y with +8 score by [(Fusseldieb, Reddit, 2024-08-17)](https://www.reddit.com/r/LocalLLaMA/comments/1eu96tk/if_an_llm_is_allowed_to_use_a_fixed_but/liium8y/) (in reply to ID liit3t3):\n>“Could <assistant>’s response be improved in any way?\n\nI was literally thinking of this in the back of my head the other day, but kinda dismissed it. So you mean it ACTUALLY works?\n\n#### Comment ID lik3te4 with +7 score by [(Ravwyn, Reddit, 2024-08-17)](https://www.reddit.com/r/LocalLLaMA/comments/1eu96tk/if_an_llm_is_allowed_to_use_a_fixed_but/lik3te4/) (in reply to ID liium8y):\nOh yeah you're on the right track - and why shouldn't this work? Think about it - the transformer just reiterates with a fresh (!) nudge, a fresh prompt. OFC it's gonna react to this - and will try to improve. As it now needs to REFLECT, essentially, what COULD HAVE BEEN. \n\nIt's essentially a regen - but on system prompt level. At least thats how i see it =)\n\n#### Comment ID liiwl12 with +11 score by [(None, Reddit, 2024-08-17)](https://www.reddit.com/r/LocalLLaMA/comments/1eu96tk/if_an_llm_is_allowed_to_use_a_fixed_but/liiwl12/) (in reply to ID liium8y):\nI can confirm that it works for all kinds of things. Currently I'm running the second iteration of 5000 creative writing prompts. Before that I've had really good results doing multiple iterations over summaries of long context asking to include more details. \n\nThis was the first time I saw this kind of technique:\n\n[https://aclanthology.org/2023.findings-emnlp.714.pdf](https://aclanthology.org/2023.findings-emnlp.714.pdf)\n\n#### Comment ID likyvts with +6 score by [(my_name_isnt_clever, Reddit, 2024-08-17)](https://www.reddit.com/r/LocalLLaMA/comments/1eu96tk/if_an_llm_is_allowed_to_use_a_fixed_but/likyvts/) (in reply to ID liium8y):\nThe thing is LLMs can only write forwards. It's like if you typed by staring at the keyboard and could never hit backspace.\n\nDoing this gives the model a chance to correct any mistakes along the way. Humans do this naturally as we write so it's not obvious that it would help so much.\n\n#### Comment ID lik6h92 with +1 score by [(Junior_Ad315, Reddit, 2024-08-17)](https://www.reddit.com/r/LocalLLaMA/comments/1eu96tk/if_an_llm_is_allowed_to_use_a_fixed_but/lik6h92/) (in reply to ID liium8y):\nYes techniques like this work very well\n\n#### Comment ID linvk6l with +1 score by [(Tartooth, Reddit, 2024-08-18)](https://www.reddit.com/r/LocalLLaMA/comments/1eu96tk/if_an_llm_is_allowed_to_use_a_fixed_but/linvk6l/) (in reply to ID liium8y):\nThis is how autogen works. Check it out on github\n\n### Comment ID lijn9fh with +4 score by [(auradragon1, Reddit, 2024-08-17)](https://www.reddit.com/r/LocalLLaMA/comments/1eu96tk/if_an_llm_is_allowed_to_use_a_fixed_but/lijn9fh/) (in reply to ID liit3t3):\nThis is why I own chip maker stocks. AI is bottlenecked by compute.\n\n#### Comment ID lik3hwx with +8 score by [(Ravwyn, Reddit, 2024-08-17)](https://www.reddit.com/r/LocalLLaMA/comments/1eu96tk/if_an_llm_is_allowed_to_use_a_fixed_but/lik3hwx/) (in reply to ID lijn9fh):\n\"AI\" is bottlenecked by it's infancy and our flawed approach to it =)\n\nCompute will always be a limiting factor, I feel. \n\nAnyways, to great returns from those stonks, have a gr8 weekend!\n\n### Comment ID linvfbi with +1 score by [(Tartooth, Reddit, 2024-08-18)](https://www.reddit.com/r/LocalLLaMA/comments/1eu96tk/if_an_llm_is_allowed_to_use_a_fixed_but/linvfbi/) (in reply to ID liit3t3):\nThis is quite literally what Microsoft autogen is designed to do.\n\n## Comment ID lijqaa0 with +34 score by [(dqUu3QlS, Reddit, 2024-08-17)](https://www.reddit.com/r/LocalLLaMA/comments/1eu96tk/if_an_llm_is_allowed_to_use_a_fixed_but/lijqaa0/) (in reply to ID 1eu96tk):\nThis seems similar to the concept behind diffusion models: starting with noise and removing it gradually across multiple steps works better than generating the full image in one step, because each step after the first gives the model an opportunity to examine and correct its own output.\n\n### Comment ID lik15s6 with +13 score by [(jm2342, Reddit, 2024-08-17)](https://www.reddit.com/r/LocalLLaMA/comments/1eu96tk/if_an_llm_is_allowed_to_use_a_fixed_but/lik15s6/) (in reply to ID lijqaa0):\nAutoregressive models already do that at each step (token). This just extends compute time beyond that.\n\n#### Comment ID linchju with +6 score by [(dogesator, Reddit, 2024-08-18)](https://www.reddit.com/r/LocalLLaMA/comments/1eu96tk/if_an_llm_is_allowed_to_use_a_fixed_but/linchju/) (in reply to ID lik15s6):\nThe key difference here is that autoregressive LLMs actually **cannot** change prior tokens that its already outputted prior.\n\nImage diffusion models **can** change information they’ve previously written though.\n\nThis is why some efforts are attempting to make language diffusion work well since that would actually allow iterative improvement of an entire sequence of an output instead of only being able to control the newest token at the end.\n\n### Comment ID lijyv2b with +6 score by [(Single_Ring4886, Reddit, 2024-08-17)](https://www.reddit.com/r/LocalLLaMA/comments/1eu96tk/if_an_llm_is_allowed_to_use_a_fixed_but/lijyv2b/) (in reply to ID lijqaa0):\nThis is interesting observation and I think it is same process when humans think as well.\n\n## Comment ID linhle4 with +6 score by [(qrios, Reddit, 2024-08-18)](https://www.reddit.com/r/LocalLLaMA/comments/1eu96tk/if_an_llm_is_allowed_to_use_a_fixed_but/linhle4/) (in reply to ID 1eu96tk):\nThat is one heck of a glass-half-full title to use given their findings.\n\nThe TLDR if you care about the title is: \"You can get away with this for easy questions, but for medium-to-hard questions you're kinda fucked.\"\n\nBut there is still a bunch of interesting methodological stuff tucked away in the paper which might be useful beyond its domain, if you didn't appreciate the spoiler. (There's also a lot of typos, and you'd think LLM researchers would at least run their content through an LLM to fix grammatical issues).\n\nAlso, to the people likening their setup to whatever your preferred self-critique prompt logic is -- that ain't it and you're still poor:\n> Capability-specific finetuning is necessary to induce revision and verification capabilities into the base model on MATH since these capabilities are absent even in strong proprietary LLMs. However, we expect that future LLMs will be more effective at verification and revision due to both increased scale and the inclusion of additional data targeted specifically towards these capabilities. Therefore in order to make progress towards understanding scaling of test-time computation, we must use models finetuned for these capabilities. That said, we expect future models to be pretrained for such capabilities directly, therefore avoiding the need for capability-specific finetuning.\n\n## Comment ID lim3p0x with +4 score by [(segmond, Reddit, 2024-08-17)](https://www.reddit.com/r/LocalLLaMA/comments/1eu96tk/if_an_llm_is_allowed_to_use_a_fixed_but/lim3p0x/) (in reply to ID 1eu96tk):\nFor those saying that this works and they have experience, can you share the plain response and the after response of using this technique?  Are you also doing it manually or is it automated?\n\n## Comment ID liiy0w3 with +3 score by [(ttkciar, Reddit, 2024-08-17)](https://www.reddit.com/r/LocalLLaMA/comments/1eu96tk/if_an_llm_is_allowed_to_use_a_fixed_but/liiy0w3/) (in reply to ID 1eu96tk):\nThis looks pretty cool.  I'm wondering how it compares to self-mixing, or if it can be stacked with it.\n\n## Comment ID lijhr0j with +1 score by [(queenadeliza, Reddit, 2024-08-17)](https://www.reddit.com/r/LocalLLaMA/comments/1eu96tk/if_an_llm_is_allowed_to_use_a_fixed_but/lijhr0j/) (in reply to ID 1eu96tk):\nCool other trick just telling a model to pause and think about it yields better results the first time if you dont care about it actually saying it's thinking. Llama 3.1 405b even realizes 3.9 is bigger than 3.11 although 70b and 8b need more help to get it right. They just have to be prompted with think about that response on the next prompt to get it.\n\n### Comment ID lijw1r0 with +10 score by [(alamacra, Reddit, 2024-08-17)](https://www.reddit.com/r/LocalLLaMA/comments/1eu96tk/if_an_llm_is_allowed_to_use_a_fixed_but/lijw1r0/) (in reply to ID lijhr0j):\nLol, I was like \"3.11 is obviously the later Python version, why even is 3.9 bigger? The documentation?\"\n\n#### Comment ID lijxdzv with +2 score by [(Failiiix, Reddit, 2024-08-17)](https://www.reddit.com/r/LocalLLaMA/comments/1eu96tk/if_an_llm_is_allowed_to_use_a_fixed_but/lijxdzv/) (in reply to ID lijw1r0):\nIf you think of it in numbers and not in chapters 3.9 is bigger than 3.11. \nI think the original explanation for this LLM problem had something to do with tokenization.\n\n#### Comment ID liljs4g with +1 score by [(moncallikta, Reddit, 2024-08-17)](https://www.reddit.com/r/LocalLLaMA/comments/1eu96tk/if_an_llm_is_allowed_to_use_a_fixed_but/liljs4g/) (in reply to ID lijw1r0):\nAlso my first reaction! lmao\n\n\n\n## Comment ID lip26tm with +1 score by [(Alternative_World936, Reddit, 2024-08-18)](https://www.reddit.com/r/LocalLLaMA/comments/1eu96tk/if_an_llm_is_allowed_to_use_a_fixed_but/lip26tm/) (in reply to ID 1eu96tk):\nThat makes sense from a path-planning perspective. I often think of next-token generation as a path-planning task, where the initial point is the prompt and the desired endpoint is the model’s expected output. Each token represents a step the language model takes toward reaching that endpoint. However, the endpoint is often ambiguous, and the generated response can accumulate noise throughout the process, leading to hallucinations we find really hard to exclude. If the model can self-critique as described in the paper, it’s like setting a clear goal and allowing it to correct its steps based on history observations. If you’re familiar with Kalman Filter, you can get my idea: a robot adjusts its next steps based on the prior path and observations from sensor data."
    ],
    "sources": {
      "steam_url": null,
      "steam_reviews": null,
      "google_play_url": null,
      "google_play_reviews": null,
      "apple_store_url": null,
      "apple_reviews": null,
      "reddit_urls": [
        "https://www.reddit.com/r/SaaS/comments/1b92w5o/how_i_reduced_our_startups_llm_costs_by_almost_90/",
        "https://www.reddit.com/r/LocalLLaMA/comments/1dky8my/openpipe_mixture_of_agents_outperform_gpt4_at/",
        "https://www.reddit.com/r/LocalLLaMA/comments/1iab1oe/best_frameworks_for_finetuning_modelswhats/",
        "https://www.reddit.com/r/ArtificialInteligence/comments/1f39k76/llm_finetuning_best_practices_around_model/",
        "https://www.reddit.com/r/LocalLLaMA/comments/18wasf8/if_you_think_opensource_models_will_beat_gpt4/",
        "https://www.reddit.com/r/ArtificialInteligence/comments/1b92hlk/how_i_reduced_our_llm_costs_by_over_85/",
        "https://www.reddit.com/r/LocalLLaMA/comments/1fm59kg/how_do_you_actually_finetune_a_llm_on_your_own/",
        "https://www.reddit.com/r/SillyTavernAI/comments/1dmwawz/mixture_of_agents_on_st_using_qr_thoughts_and/",
        "https://www.reddit.com/r/webdev/comments/17clmce/why_do_all_new_websites_look_the_same/",
        "https://www.reddit.com/r/LocalLLaMA/comments/1eu96tk/if_an_llm_is_allowed_to_use_a_fixed_but/"
      ],
      "reddit_search_url": "https://www.google.com/search?q=site%3Areddit.com+%22OpenPipe%22+related%3Aopenpipe.ai+"
    }
  },
  "glassdoor_result": null,
  "news_result": [
    [
      "OpenPipe",
      "OpenPipe",
      "openpipe.ai",
      null,
      false,
      false,
      null,
      [
        false,
        false
      ]
    ],
    [
      {
        "title": "OpenPipe Mixture of Agents: Outperform GPT-4 at 1/25th the Cost ...",
        "link": "https://openpipe.ai/blog/mixture-of-agents",
        "snippet": "Jun 20, 2024 ... We've also benchmarked our MoA approach against GPT-4 variants on real-world OpenPipe customer tasks, and found completions from our MoA model were preferred ...",
        "formattedUrl": "https://openpipe.ai/blog/mixture-of-agents"
      },
      {
        "title": "OpenPipe 2025 Company Profile: Valuation, Funding & Investors ...",
        "link": "https://pitchbook.com/profiles/company/535432-15",
        "snippet": "Jan 17, 2025 ... OpenPipe General Information. Description. Developer of an AI-based platform designed to turn prompts into cheap fine-tuned models.",
        "formattedUrl": "https://pitchbook.com/profiles/company/535432-15"
      },
      {
        "title": "OpenPipe Introduces a New Family of 'Mixture of Agents' MoA ...",
        "link": "https://www.marktechpost.com/2024/06/24/openpipe-introduces-a-new-family-of-mixture-of-agents-moa-models-optimized-for-generating-synthetic-training-data-outperform-gpt-4-at-1-25th-the-cost/",
        "snippet": "Jun 24, 2024 ... In artificial intelligence, achieving superior performance at a lower cost remains a key objective. OpenPipe has made significant strides in this direction ...",
        "formattedUrl": "https://www.marktechpost.com/.../openpipe-introduces-a-new-family-of-mi..."
      },
      {
        "title": "Founding Software Engineer (Backend) at OpenPipe | Y Combinator",
        "link": "https://www.ycombinator.com/companies/openpipe/jobs/0kfSfxe-founding-software-engineer-backend",
        "snippet": "Apr 2, 2024 ... ... OpenPipe as one of our first hires with a significant equity stake. You ... AI systems. You do need to be comfortable with wearing both engineering ...",
        "formattedUrl": "https://www.ycombinator.com/.../openpipe/.../0kfSfxe-founding-software-e..."
      },
      {
        "title": "Benchmarking LLMs (OpenAI, DeepSeek, LLAMA, etc.) Against ...",
        "link": "https://medium.com/codex/benchmarking-llms-against-your-own-data-ee8f495160ff",
        "snippet": "Feb 4, 2025 ... OpenPipe is an open source comprehensive platform that enables developers to fine-tune and deploy custom AI models efficiently2. It allows companies to create ...",
        "formattedUrl": "https://medium.com/.../benchmarking-llms-against-your-own-data-ee8f495..."
      },
      {
        "title": "Ask HN: Most efficient way to fine-tune an LLM in 2024? | Hacker ...",
        "link": "https://news.ycombinator.com/item?id=39934480",
        "snippet": "Apr 4, 2024 ... You should be able to run the experiments on 4090s from vast.ai or runpod or similar service. ... I didn't try Axolotl yet, or OpenPipe but I hope to. Actual ...",
        "formattedUrl": "https://news.ycombinator.com/item?id=39934480"
      },
      {
        "title": "OpenPipe: Cost-Effective Fine-Tuning For LLMs",
        "link": "https://appscribed.com/software/openpipe-llm-fine-tuning-tool/",
        "snippet": "Jul 23, 2024 ... OpenPipe is a developer tool that fine-tunes large language models (LLMs) to reduce costs and improve performance, making custom AI more accessible.",
        "formattedUrl": "https://appscribed.com/software/openpipe-llm-fine-tuning-tool/"
      },
      {
        "title": "Using GRPO to Beat o1, o3-mini and R1 at “Temporal Clue” | Hacker ...",
        "link": "https://news.ycombinator.com/item?id=43284420",
        "snippet": "Mar 6, 2025 ... Hacker News new | past | comments | ask | show | jobs | submit · login · Using GRPO to Beat o1, o3-mini and R1 at “Temporal Clue” (openpipe.ai). 199 points by ...",
        "formattedUrl": "https://news.ycombinator.com/item?id=43284420"
      },
      {
        "title": "From Synthetic Data Generation to Fine-Tuned LLM | by AI Rabbit ...",
        "link": "https://medium.com/codex/zero-code-data-generation-and-llm-fine-tuning-part-i-e68793286120",
        "snippet": "Jan 25, 2025 ... You can now use open-source technologies like Argilla and OpenPipe to do so with just a few clicks and almost zero code.",
        "formattedUrl": "https://medium.com/.../zero-code-data-generation-and-llm-fine-tuning-part-..."
      },
      {
        "title": "Ingest vulnerability findings in OCSF format — Dynatrace Docs",
        "link": "https://docs.dynatrace.com/docs/secure/threat-observability/ingest-ocsf-data",
        "snippet": "Nov 2, 2024 ... ... OpenPipeline endpoint. Action required. Follow the instructions in Get started. 2. Data is mapped. Details. The OpenPipe ingest endpoint receives the ...",
        "formattedUrl": "https://docs.dynatrace.com/docs/secure/threat.../ingest-ocsf-data"
      },
      {
        "title": "AI Timeline - Annielytics",
        "link": "https://annielytics.com/tools/ai-timeline/",
        "snippet": "Mar 18, 2025 ... OpenPipe, Opera, Oura, Perplexity, Physical Intelligence, Pika Labs, Polymathic AI ... Disclosure: I do not share any AI news or initiatives on request. If I ...",
        "formattedUrl": "https://annielytics.com/tools/ai-timeline/"
      },
      {
        "title": "Tom Preston-Werner - Cofounder @ Preston-Werner Ventures ...",
        "link": "https://www.crunchbase.com/person/tom-preston-werner",
        "snippet": "Aug 1, 2024 ... Personal Investments ; Jul 30, 2024. Not Diamond Logo. Not Diamond. — ; Jun 5, 2024. Alinia AI Logo. Alinia AI. — ; Mar 26, 2024. OpenPipe Logo. OpenPipe. — ; Dec ...",
        "formattedUrl": "https://www.crunchbase.com/person/tom-preston-werner"
      },
      {
        "title": "Agency AI Raises $2.6M in Pre-Seed Funding",
        "link": "https://www.finsmes.com/2024/08/agency-ai-raises-2-6m-in-pre-seed-funding.html",
        "snippet": "Aug 28, 2024 ... ... OpenPipe, Prompt Armor, Browserbase, and Anon.com. AgentOps.ai recently announced it had joined the Cohere startup program. FinSMEs. 28/08/2024. Tags; 645 ...",
        "formattedUrl": "https://www.finsmes.com/.../agency-ai-raises-2-6m-in-pre-seed-funding.ht..."
      },
      {
        "title": "The Opportunities Costanoa Ventures Sees In Vertical AI",
        "link": "https://news.crunchbase.com/venture/vertical-ai-investment-costanoa-sands-cowgill/",
        "snippet": "Nov 27, 2024 ... Editor's note: This article is part of an ongoing series in which Crunchbase News interviews active investors in artificial intelligence. ... OpenPipe, which is ...",
        "formattedUrl": "https://news.crunchbase.com/.../vertical-ai-investment-costanoa-sands-cowg..."
      },
      {
        "title": "Becoming a VC: It's Not What You Think--and Neither Am I ...",
        "link": "http://costanoa.vc/becoming-a-vc-its-not-what-you-think-and-neither-am-i/",
        "snippet": "Apr 10, 2024 ... Weights & Biases, the well-loved AI developer tooling company, was at ... OpenPipe: Bridging the Gap Between LLMs and Production with Easier Fine-Tuning.",
        "formattedUrl": "http://costanoa.vc/becoming-a-vc-its-not-what-you-think-and-neither-am-i/"
      },
      {
        "title": "openpipes.org | Facebook",
        "link": "https://www.facebook.com/groups/openpipes.org/",
        "snippet": "Dec 30, 2024 ... Bob Midi is attending an event. Nov 12, 2024󰞋󱟠. Je viens de découvrir ce groupe openpipe ...",
        "formattedUrl": "https://www.facebook.com/groups/openpipes.org/"
      },
      {
        "title": "Mercury CEO Immad Akhund seeks $26m for VC fund",
        "link": "https://www.axios.com/pro/fintech-deals/2025/02/25/mercury-immad-akhund-26-million-vc-fund",
        "snippet": "Feb 25, 2025 ... Driving the news: Dubbed Immad ... More recent investments have included generative AI gaming company Campfire, automation startup Gumloop and OpenPipe.",
        "formattedUrl": "https://www.axios.com/pro/.../mercury-immad-akhund-26-million-vc-fund"
      },
      {
        "title": "2024 Backward Pass: The Definitive Guide to AI in 2024",
        "link": "https://kelvinmu.substack.com/p/2024-backward-pass-the-definitive",
        "snippet": "Dec 3, 2024 ... Anyscale, Baseten, BentoML, CentML, Clika AI, Fireworks, Lamini, Lightning AI, Modular, OpenPipe, Replicate, TensorOpera, Together AI ... AI-related budgets to ...",
        "formattedUrl": "https://kelvinmu.substack.com/p/2024-backward-pass-the-definitive"
      },
      {
        "title": "Top 10 Best LLM Ops APIs in 2024 | Konfig",
        "link": "https://konfigthis.com/blog/llmops-apis/",
        "snippet": "Apr 12, 2024 ... Generative AI refers to AI algorithms which can generate media such as ... OpenPipe is a YC startup focused on training, deployment, fine-tuning, and ...",
        "formattedUrl": "https://konfigthis.com/blog/llmops-apis/"
      },
      {
        "title": "Seattle Inno's 25 startups to watch in 2025 - Puget Sound Business ...",
        "link": "https://www.bizjournals.com/seattle/news/2025/01/10/startups-to-watch-puget-sound-seattle-inno-2025.html",
        "snippet": "Jan 10, 2025 ... Brothers Kyle, left, and David Corbitt co-founded OpenPipe in 2023. OpenPipe ... Related Articles. Here are some of Seattle's most notable tech layoffs of ...",
        "formattedUrl": "https://www.bizjournals.com/.../news/.../startups-to-watch-puget-sound-seatt..."
      }
    ],
    [
      "# [OpenPipe Mixture of Agents: Outperform GPT](https://openpipe.ai/blog/mixture-of-agents)\nWe’re excited to announce a new family of “Mixture of Agents” models optimized for generating synthetic training data. Using our Mixture of Agents (MoA) architecture, we’ve achieved SOTA results on both LMSYS’s Arena Hard Auto (score: 84.8) and AlpacaEval 2.0 (LC score: 68.4).\n\nWe’ve also benchmarked our MoA approach against GPT-4 variants on real-world OpenPipe customer tasks, and found completions from our MoA model were preferred over GPT-4 59.5% of the time (Claude 3 Opus as judge).\n\nFinally, we’ve experimented with fine-tuning smaller Llama 3 models on synthetic data generated by MoA. We found that fine-tuned Llama 3 70B outperforms GPT-4 on 4/4 tasks, and even the much smaller fine-tuned Llama 3 8B outperforms GPT-4 on 3/4 tasks. Importantly, our fine-tuned Llama 3 8B models are also 3x faster and 25x cheaper to run, occupying an attractive point on the Pareto frontier of price vs performance!\n\nWe share more details of how we implemented and validated this approach below.\n\nBackground\n\nLLMs perform better on many tasks when given “space to think.” Early results confirming this include techniques like Chain of Thought and Tree of Thoughts , both of which inspired our work here. More recently, we’ve also seen Together develop a similar Mixture of Agents approach that outperforms its constituent models.\n\nGeneralizing from these results, it’s clearly possible in at least some cases to improve model performance by dedicating more resources at inference time while holding the base model’s intelligence constant.[1] Our contribution is a concrete approach that achieves this in practice across many task types.\n\nModel Design\n\nOur Mixture of Agents model is designed as a drop-in replacement for GPT-4. That is, its input is a set of messages and tools, and its output is a chat completion object or stream. It is also base-model-agnostic—we have already deployed versions against GPT-4, GPT-4 Turbo and GPT-4o.[2]\n\nInternally, we implement the following 3-prompt chain to construct the final completion:\n\nPrompt 1 generates 3 candidate completions in parallel by calling the chosen base model with n=3 and a high temperature to promote output diversity.\n\nPrompt 2 again calls the base model. It passes in the original input again, along with the 3 candidate completions generated by prompt 1. It then asks the LLM to review the candidate completions and critique them.\n\nPrompt 3 again passes the original input, the 3 candidate completions, and their critiques. Using this information, the base model generates a final completion that incorporates the best of all 3 candidates.\n\nThis flow is represented visually below.\n\nEvaluating Response Quality\n\nTo comprehensively assess the quality of our MoA model, we benchmarked it across both open-source and private benchmarks.\n\nOpen Source Benchmarks\n\nWe chose two open-source benchmarks: Arena Hard Auto and AlpacaEval 2.0 .\n\nArena Hard Auto consists of 500 challenging user queries filtered from the LMSys arena. They also provide an automated evaluation tool which has the highest correlation and separability to the chatbot arena results. To avoid bias from only using an OpenAI model-based judge, we report separate results using both GPT-4 and Claude 3 Opus as judges. In both methods, our MoA model outperformed GPT-4-Turbo, emerging as the best-performing model.\n\nResults on Arena Hard Auto using GPT-4 as judge\n\nResults on Arena Hard Auto using Claude 3 Opus as judge\n\nAlpacaEval 2.0 includes 805 challenging prompts with a judge method that debiases results using length-controlled win rates, showing a Spearman correlation of 0.98 with Chatbot Arena. Our model outperformed all other models, including the recently released Together MoA model.\n\nPrivate Benchmark Evaluations\n\nWe selected four tasks representing common categories on our platform: Summarization, Chat, and Data Extraction. With customer permission, we chose 200 random samples from each task to create our internal benchmark. Using our custom LLM-as-judge evaluations, calibrated through user feedback, we compared GPT-4-Turbo against our MoA model. Tests were run with both GPT-4-Turbo and Claude as the LLM judge. Our MoA model showed a 19.25% improvement over GPT-4-Turbo.\n\nHuman Evaluation\n\nWe acknowledge that LLM-as-judge alone is not a sufficient measure of performance. We selected a subset of 32 samples from our benchmark, evenly split between samples where the LLM judge preferred MoA and those where it preferred GPT-4-Turbo. We manually collected preference data on this subset from four highly trained human raters. Using this data, we adjusted the win rates of the LLM judge to more accurately reflect human judgment. Although the adjustments decreased the degree to which MoA outperforms GPT-4-Turbo, the adjusted win rates still showed that MoA models were stronger than the base models they came from by 9%.\n\nThis comprehensive evaluation gives us confidence that our Mixture of Agents model offers significant improvements over existing models.\n\nMoA and Fine Tuning: Beating GPT-4 at 1/25th the cost\n\nWhen designing our Mixture of Agents architecture, we intentionally traded off both cost and latency in order to maximize generation quality. Concretely, it takes 3x as long to return a completion as calling the base model directly, and costs 3-4x as much.\n\nThis is because we believe the best use case for this kind of model is to generate synthetic data for fine-tuning smaller models, and when generating synthetic data, quality is important over all else. Once you have generated a few hundred or a few thousand examples using MoA, you can use them to train a task-specific smaller model that preserves much of the quality while being far faster and cheaper to run.\n\nIf you are already using GPT-4 in production, the process might look something like this:\n\nContinue using GPT-4 in production, while using the OpenPipe SDK to capture completions.\n\nRelabel your completions using our MoA model as the relabeling model.\n\nFine-tune a smaller model on the relabeled completions.\n\nWe tested exactly this pipeline across four use-cases, training both Llama 3 70B and 8B variants on each use-case. We found that while the MoA outputs still outperformed our fine-tuned models on most tasks, the fine-tuned models in turn outperformed base GPT-4. Concretely, Llama 3 70B outperformed GPT-4 on 4/4 tasks, and Llama 3 8B outperformed GPT-4 on 3/4 tasks. Importantly, Llama 3 8B is also 1/25th the cost and 1/3rd the latency of GPT-4-Turbo.\n\nBy visualizing price-vs-performance, we can see that models fine-tuned on high-quality domain-specific outputs significantly outperform similarly-priced prompted models at every point on the Pareto frontier.\n\nUsing MoA Today\n\nWe’ll continue releasing improved variants incorporating new techniques and more models. But our initial family of MoA models are already available for use on the OpenPipe platform . There are two primary ways to take advantage of them:\n\nFirst, you can call our OpenAI-compatible Chat Completions endpoint. Simply create an OpenPipe account, save your OpenAI key in your OpenPipe project settings and call our chat completions endpoint with one of the following model IDs:\n\nmoa-gpt-4-v1\n\nmoa-gpt-4-turbo-v1\n\nmoa-gpt-4o-v1\n\nSecond, you can use them as a relabeling model. If you have an existing dataset or are tracking one via OpenPipe, you can relabel it using a Mixture of Agents model for a much stronger dataset that will lead to higher-quality fine-tuned models. You can find the full documentation here .\n\nWe’re very excited to share these results with the community and help push the SOTA forward for both prompted and fine-tuned models!\n\n[1]: This generalization has many parallels with Aidan McLaughlin’s recent essay AI Search: The Bitter-er Lesson .\n\n[2]: Our design also works with multiple different proposer models and we’ve already tested it by mixing GPT-4 with Claude Opus. However, we’ve chosen to first release the single-base-model variants for operational simplicity.",
      "# [OpenPipe Introduces a New Family of ‘Mixture of Agents’ MoA Models Optimized for Generating Synthetic Training Data: Outperform GPT-4 at 1/25th the Cost by Asif Razzaq, Asif Razzaq https:, www.marktechpost.com, www.facebook.com on 2024-06-24](https://www.marktechpost.com/2024/06/24/openpipe-introduces-a-new-family-of-mixture-of-agents-moa-models-optimized-for-generating-synthetic-training-data-outperform-gpt-4-at-1-25th-the-cost/)\nIn artificial intelligence, achieving superior performance at a lower cost remains a key objective. OpenPipe has made significant strides in this direction with its innovative Mixture of Agents (MoA) model. Designed to generate synthetic training data, the MoA architecture demonstrates state-of-the-art (SOTA) results and offers a cost-effective alternative to existing models, particularly GPT-4.\n\nAchieving SOTA Results\n\nOpenPipe’s MoA models have excelled in rigorous benchmarking tests, achieving notable scores on LMSYS’s Arena Hard Auto and AlpacaEval 2.0. The MoA model scored 84.8 on Arena Hard Auto and 68.4 on AlpacaEval 2.0, indicating its superior performance in generating high-quality synthetic data. These benchmarks are critical as they represent challenging user queries that test the robustness and adaptability of AI models.\n\nBenchmarking Against GPT-4\n\nThe MoA model has been benchmarked against various GPT-4 variants in real-world scenarios. Results showed that OpenPipe’s MoA model was preferred over GPT-4 in 59.5% of the tasks evaluated by Claude 3 Opus. This is a significant achievement, highlighting the model’s effectiveness and practical applicability in diverse tasks encountered by OpenPipe’s customers.\n\nCost and Performance Efficiency\n\nOne of the standout features of the MoA model is its efficiency. OpenPipe has successfully fine-tuned smaller Llama 3 models using synthetic data generated by the MoA model. These fine-tuned models, such as Llama 3 70B and Llama 3 8B, have outperformed GPT-4 in multiple tasks. Remarkably, the Llama 3 8B model provides superior performance on three out of four functions at a fraction of the cost—25 times cheaper and three times faster to run compared to GPT-4.\n\nModel Design and Implementation\n\nThe MoA model’s design is a testament to OpenPipe’s innovative approach. It is a drop-in replacement for GPT-4, compatible with various base models, including GPT-4 Turbo and GPT-4o. The model employs a three-prompt chain to generate the completion: the first prompt generates three diverse candidate completions, the second critiques these completions, and the third combines the best elements of each to produce the final output. This structured approach ensures high-quality and diverse responses, enhancing the model’s performance.\n\nEvaluation and Human Validation\n\nOpenPipe has conducted extensive evaluations to validate the MoA model’s performance. In addition to automated benchmarks, they employed human evaluators to ensure the model’s outputs align with human judgment. This dual approach of using both LLM-as-judge and human evaluators has provided a comprehensive validation of the model, confirming its superiority over GPT-4 Turbo by a margin of 9%, even after adjustments for human preferences.\n\nFuture Prospects and Accessibility\n\nOpenPipe is committed to continuous improvement and has plans to release enhanced variants of the MoA model incorporating new techniques and models. Currently, users can access these models through the OpenPipe platform by creating an account and using the OpenAI-compatible chat completions endpoint. This ease of access ensures that a wider audience can benefit from the advancements in synthetic data generation offered by OpenPipe.\n\nConclusion\n\nOpenPipe’s Mixture of Agents model represents a significant advancement in AI, particularly in generating high-quality synthetic training data at a lower cost. Its superior performance, cost efficiency, and innovative design make it a valuable tool for AI practitioners looking to optimize their models. OpenPipe continues to refine and expand this technology, pushing synthetic data generation and model fine-tuning.",
      "# [Benchmarking LLMs (OpenAI, DeepSeek, LLAMA, etc.) Against Your Own Data by AI Rabbit, medium.com on 2025-02-04](https://medium.com/codex/benchmarking-llms-against-your-own-data-ee8f495160ff)\nIf you are following AI news, you might feel overwhelmed by the rapid progress in the large language model (LLM) space — especially regarding the foundation models. Every couple of days, a new model appears or an existing one is updated with major improvements in benchmarks like language understanding, math, coding, and more.\n\nHowever, when you test these models yourself, you might sometimes feel disappointed. They often perform very well on benchmark test datasets but may not meet your specific use cases. This is why it’s essential to test and benchmark these models against your own data — and, when possible, fine-tune smaller models to meet your needs at a fraction of the cost.\n\nTo give you an initial overview of this process, I’ve created a simple workflow that uses OpenWebUI and OpenPipe to benchmark multiple LLMs against your own use cases, data, and conversations.\n\n— -\n\nWhat is Open WebUI\n\nOpen WebUI is an open source extensible, self-hosted AI platform designed to operate entirely offline17. It provides a user-friendly web interface for interacting with various AI models, particularly large language models (LLMs)23.\n\n— -\n\nWhat is OpenPipe\n\nOpenPipe is an open source comprehensive platform that enables developers to fine-tune and deploy custom AI models efficiently2. It allows companies to create specialized models that reduce costs and latency compared to general-purpose models like GPT-423.\n\n— -\n\nLet’s Dive In\n\n1. Preparation\n\nOpenpipe",
      "# [Ask HN: Most efficient way to fine-tune an LLM in 2024?](https://news.ycombinator.com/item?id=39934480)\nQlora + axolotl + good foundation model (llama/mistral/etc, usually instruction fine tuned) + runpod works great.\n\nA single A100 or H100 with 80GB VRAM can fine tune 70B open models (and obviously scaling out to many nodes/GPUs is faster, or can use much cheaper GPUs for fine tuning smaller models.)\n\nThe localllama Reddit sub at https://www.reddit.com/r/LocalLLaMA/ is also an awesome community for the GPU poor :)\n\nYou probably want to build a retrial augmented generation pipeline.\n\nIf you do end up wanting to fine tune then use qlora with axolotl or unsloth to prove your hypothesis on a smaller model and then evaluate if you want the marginal gains you get from full precision training.\n\nAfter you fine tune it with 100m token dataset, use DPO to polish it off. You need to create a DPO dataset for that but it can be relatively small to get some great gains.\n\nAfter that, look at applying grammars during inference if you are expecting structured results like json.\n\nYou should be able to run the experiments on 4090s from vast.ai or runpod or similar service.\n\nIt can cost less than $100 depending on your requirements.\n\nThis is great advice!\n\nI'd like to add that if you don't have pairwise preference data (A > B) but do have binary data (A is good for x_1, B is good for x_2, etc.), then Kahneman-Tversky Optimization (KTO) might be a better fit. Despite learning with a weaker signal, it works as well or better than dpo in practice.\n\nHere is a recent HN discussion of an article that talks about this. https://news.ycombinator.com/item?id=39748537\n\nAnecdotally, I literally \"added knowledge\" to a model via fine-tuning earlier today.\n\nFine tuning can do extremely well given a specific question and answer, the tuned model \"knows\" how to answer that question much more accurately.\n\nI gave it a specific question, and a good answer as a fine tuning input. (Literally 2 data points as the input, 2 questions/answer sets.)\n\nI asked it that question, and the tuned model blows the base model away, for answering that specific question.\n\nThis thread reminds of a competition I once joined where we were supposed to fine-tune an LLM to fill out trivia answers, and we were expressly disallowed from training on the validation set.\n\nHowever: we were allowed to pick any base model in a given repo. All of the teams that “won” did so for the same reason: they had all picked the same base model (whereas a majority of teams picked the given default), presumably the one that had at some point been trained on the most favorable data for this particular challenge.\n\nIt was quite silly. Had everyone had the same base model we’d have a bit more of an interesting problem (more around NLP and alignment than picking the ‘best’ model).\n\nDoes that mean you tested on specific questions? Get 1-5 typical queries and test them with a properly configured llamaindex.\n\nIf your documents repeat the same information several different ways then you actually might get something out of LoRA on raw documents. But you need a way to measure it and you have to verify that RAG won't work with real tests first.\n\nTo do effective training with LoRA though and expect it to pick up most of the information reliably then you need to cover the knowledge and skills with multiple question answer pairs for each item you expect it to learn. Which you can then use QA pairs to validate that it learned those things.\n\nBut it's a lot of QA pair generation.\n\nWhen I first played with RAG I thought “wow this is so cool”. Now I’m starting to think it’s kinda useless, in the sense that the critical bit is the initial search, and that doesn’t use the LLM power, or at most it’s used to capture the user intent and reformulate the query.\n\nWe’re building some “smart search” functionality for some teams and I start to wonder if a traditional search results list (i.e. sans the LLM, or used only to rewrite the user query) with the document chunks wouldn’t be better than blindly taking the top N and feeding them to the LLM to produce some response.\n\nE.g. we have some docs about specific supermarket chains, but the word “supermarket” might not appear at all in them, but the user query might be “show me what we have about supermarkets”. Now the embeddings hopefully will place the word “supermarket” close to, say, “Costco”, but they might also place it closer to “shopping center”, and we might have docs about shopping centers that could rank higher. So we might take the top 5 docs and send them to the LLM, but the docs the user was after might have been in 7th and 9th position, nowhere to be seen by the LLM nor the user.\n\nI’ve worked in scaled enterprise search, both with lexical (lucene based, eg elastic search) & semantic search engines (vector retrieval).\n\nVector retrieval that isn’t contextualized in the domain is usually bad (RAG solutions call this “naive rag” … and make up for it with funky chunking and retrieval ensembles). Training custom retrievers and reranker is often key but quite an effort and still hard to generalize in a domain with broad knowledge.\n\nLexical based searching provides nice guarantees and deterministic control in results (depending on how you index). Certainly useful here is advanced querying capability. Constructing/enriching queries with transformers is cool.\n\nReranking is often nice ensemble additions, albeit can be done with smaller models.",
      "# [Tuning For LLMs on 2024-07-23](https://appscribed.com/software/openpipe-llm-fine-tuning-tool/)\nTL;DR: OpenPipe is a developer tool that fine-tunes large language models (LLMs) to reduce costs and improve performance, making custom AI more accessible.\n\nOpenPipe is an SDK that simplifies the fine-tuning of LLMs by capturing existing prompt-completion pairs and creating efficient, cost-effective models. It is ideal for developers needing tailored AI solutions for specific use cases without the high costs associated with general-purpose models like GPT-3.5 or GPT-4.\n\nWho is OpenPipe For\n\nOpenPipe is designed for a range of users needing efficient and cost-effective AI fine-tuning. Key users include:\n\nSoftware Developers: Developers looking to integrate customized AI models into their applications.\n\nAI Researchers: Researchers needing specific model adjustments for experiments.\n\nBusinesses: Companies looking to reduce costs while maintaining high-performance AI solutions.\n\nStartups: New companies requiring budget-friendly AI tools for product development.\n\nData Scientists: Professionals fine-tuning models for specific data-related tasks.\n\nKey Features of OpenPipe\n\nCost Reduction: Fine-tuned models are significantly cheaper to run compared to general-purpose LLMs.\n\nHigh Performance: Fine-tuned models can achieve performance levels close to GPT-4 at a fraction of the cost.\n\nEasy Integration: Simple SDK for integrating fine-tuned models into existing applications.\n\nAutomation: Automates the collection of prompt-completion pairs and model training.\n\nCustom Evaluations: Built-in evaluation framework to compare fine-tuned models with baseline models.\n\nHow to Use OpenPipe\n\nSign up on the OpenPipe website.\n\nInstall the OpenPipe SDK and integrate it with your existing application.\n\nCollect prompt-completion pairs using the SDK.\n\nUse the OpenPipe UI to initiate a training job with the collected data.\n\nDeploy the fine-tuned model via OpenPipe or self-host it.\n\nUpdate your application to use the new model seamlessly.\n\nKey Use Cases for OpenPipe",
      "# [Using GRPO to Beat o1, o3-mini and R1 at “Temporal Clue”](https://news.ycombinator.com/item?id=43284420)\n>To speed up our experiments, we omitted the Kullback–Leibler (KL) divergence penalty, although our training recipe supports it for interested readers.\n\nI am very curious whether omitting the KL penalty helps on narrow domains like this, and also whether doing so results in illegible reasoning. (From the samples in the post, it looks like it doesn't make reasoning illegible?)\n\n>the 32B model’s response lengths collapsing, especially after reaching peak performance.\n\nI would not have predicted this. Nor that it could collapse its response length to near zero yet lose only a few percentage points of accuracy. If you do SFT to get a model of the same size to solve these puzzles with no reasoning (just output answers directly), how good can it do?\n\nDon't have examples handy, but I did a round of grpo on a 7b model and it did indeed start to switch between english, coreean and chinese, but the reward was steadily increasing. RL doesn't care what the middle tokens are, as long as the end result gets the carrot.\n\nI think there's still a lot to learn about reward functions (saw a team work w/ just correct output, and nothing else), if you should reward partial success (i.e. code compiles / math outputs a result) or just the final thing (i.e. test cases pass / correct answer) and so on.\n\nNot to mention how to get downstream signals from e2e tasks (i.e. if an \"agent\" navigates to the correct webpage and finds a \"cookie\" or something, figure out how to reward all the intermediary steps out of that single binary signal).\n\nAnd there's a lot to learn in using grammars & stuff w/ RL as well. The problem there is that the libraries are pretty wonky atm, some things work, some things need work, and RL in itself is pretty slow due to having to generate, update the model and generate again.\n\n> Without proper reasoning you can get some \"heuristic\"\n\nRight, but the question is whether this is good enough. And what counts as \"proper\". A lot of what we call proper reasoning is still quite informal, and even mathematics is usually not formal enough to be converted directly into a formal language like Coq.\n\nSo this is a deep question: is talking reasoning? Humans talk (out loud, or in their heads). Are they then reasoning? Sure, some of what happens internally is not just self-talk, but the thought experiment goes: if the problem is not completely ineffable, then (a bit like Borges' library) there is some 1000-word text which is the best possible reasoned, witty, English-language 1000-word solution to the problem. In principle, an LLM can generate that.\n\nIf your goal is a reductio, ie my statement must be false since it implies models should write code for every problem - then I disagree, because while the ability to solve these problems might be a requirement to be deemed \"an intelligence\", nonetheless many other problems which require an intelligence don't require the ability to solve these problems.\n\n> I can't reply to your new post below, I guess the thread is too deep\n\n(I'd instead guess that you tried to reply before the timer - which allows HN members to reply after a delay proportional to a function of the depth of the discussion tree - allowed you.)\n\n> do is not reasoning\n\nWhat some people do is «not reasoning», for lack of training, or for lack of resources (e.g. time - Herbert Simon's \"satisficing\"), or for lack of ability. I had to write since the late 2022 boom that \"if the cousin you write about is consistently not using the faculty of intelligence you can't call her \"intelligent\" for the purpose of this discussion\". I have just written in another parallel discussion that «There is a difference between John who has a keen ethical sense, Ron who does not exercise it, and Don who is a clinical psychopath with missing cerebral modules making it completely Values-blind» - of course if we had to implement ethics we would \"backward engineer\" John and use Don as a counter-model.\n\n> can you guess what emotion\n\nLet me remind you my words: «Without proper reasoning you can get some \"heuristic\", which can only be useful if you only needed an unreliable result based on \"grosso modo\" criteria». Is that problem one that has \"true solutions\" or one that has \"good enough solutions\"?\n\nLet me give another example. Bare LLMs can be \"good\" (good enough) e.g. in setting capitalization and punctuation in \"[a-z0-9 ]\" texts, such as raw subtitles. That is because they operate without explicitly pondering the special cases in which it is subtle to unequivocally decide whether the punctuation there \"should have been a colon or a dash\", and such cases are generally rare, so heuristic seems to suffice.\n\nSimilar engines are useless and/or dangerous in all cases in which correct responses are critical. Important problems are those which require correct responses.\n\n> What some people do is «not reasoning», for lack of training, or for lack of resources (e.g. time - Herbert Simon's \"satisficing\"), or for lack of ability.\n\nAccording to your definition of reasoning, which involves surely getting the right answer, no human does reasoning. Probably less than 1% of published mathematics meets the definition.\n\n> Important problems are those which require correct responses.\n\nThere are many important problems where formal reasoning is not possible, and yet a decision is required, and both humans and LLMs can provide answers. \"Should I accept this proposed business deal / should I declare war / what diagnostic test should I order?\" We would like to have correct responses for these problems, but it is not possible, even in principle, to guarantee correctness. So yes, we use heuristics and approximate reasoning for such problems. Is an LLM \"unreliable\" or \"dangerous\" in such problems? Maybe yes, and maybe more so than humans, but maybe not, it depends on the case. To try to keep the point of the thread in focus, an LLM should probably not try to solve such problems by writing code.\n\n> According to your definition of reasoning, which involves surely getting the right answer\n\nNo. Let me reiterate: «\"proper reasoning\" is that process which given sufficient input will surely bring to a correct output owing to the effectiveness of its inner workings», given that enough resources are spent. I.e.: it is a matter of method.\n\nAnd a processor that cannot solve the \"detective games\" shows lacking that method. (I.e.: the general capabilities that can be instanced in solving a \"detective game\" are required, though not exhaustive, for the reasoner.)\n\n> we use heuristics and approximate reasoning for such problems\n\nBut we are expected to still use decent reasoning, even when bounded.\n\nSo: there may be no need to try and solve problems through writing code when the reasoning machine has the procedural modules that allow to reason similarly to running code, when such form of \"diligence\" is needed. When the decision is not that impactful (e.g. \"best colour for the car\"), let the decisor \"feel\"; when the decision will be impactful, I want that the decisor be able to reason.",
      "# [From Synthetic Data Generation to Fine-Tuned LLM by AI Rabbit, medium.com on 2025-01-25](https://medium.com/codex/zero-code-data-generation-and-llm-fine-tuning-part-i-e68793286120)\nMany blogs talk about fine-tuning models for specific use cases, cost savings, and increased privacy. Until recently, there were many challenges in fine-tuning your own models:\n\n• Acquiring the data\n\n• preparing the data\n\n• Fine Tuning infrastructure GPUs)\n\n• Deployment and Inference Infrastructure\n\n• Cost Factors\n\nBecause of these hurdles, many people and companies choose to use the foundation models as they are, through vendor apps like ChatGPT or Claude. Some are also interested in more advanced AI chat applications like OpenWebUI with Ollama or other open-source models. Of course, there are plenty more apps out there, but you get the idea.\n\nIf you are thinking of going one step further and fine-tuning your own model — whether from synthetic data, labeled data, or even from your existing conversations — you can now use open-source technologies like Argilla and OpenPipe to do so with just a few clicks and almost zero code.\n\nTools we use in this tutorial\n\nData generation\n\n- The synthetic data generation space https://huggingface.co/spaces/argilla/synthetic-data-generator\n\n- The Argilla HF space https://huggingface.co/spaces/argilla/argilla-template-space\n\nFine tuning\n\nOpenPipe\n\nOpenAI (optional)\n\nInference\n\nOpenPipe\n\nOpenai (optional)\n\nOllama (optional)\n\nWhat is the Dataset Generating and Argilla\n\nIn a nutshell, the Dataset Generator uses AI to generate data. That is, the user specifies what the data should be about (e.g. a customer support request) and lets the (smart) AI guess the response of a human, e.g. the appropriate response to the request. Artgille takes this a step further and allows you to take the same generated dataset and have a human review and correct it if necessary. The final dataset is then used to fine-tune a model, which then becomes an expert at performing this task.",
      "# [Ingest vulnerability findings in OCSF format — Dynatrace Docs by Unknown on 2024-07-29](https://docs.dynatrace.com/docs/secure/threat-observability/ingest-ocsf-data)\n{\n\n\"activity_id\":2,\n\n\"activity_name\":\"Update\",\n\n\"category_name\":\"Findings\",\n\n\"category_uid\":2,\n\n\"class_name\":\"Vulnerability Finding\",\n\n\"class_uid\":2002,\n\n\"cloud\":{\n\n\"account\":{\n\n\"uid\":\"111111111111\"\n\n},\n\n\"provider\":\"AWS\",\n\n\"region\":\"us-east-2\"\n\n},\n\n\"finding_info\":{\n\n\"created_time_dt\":\"2023-04-21T11:59:04.000-04:00\",\n\n\"desc\":\"Issue summary: The AES-XTS cipher decryption implementation for 64 bit ARM\\nplatform contains a bug that could cause it to read past the input buffer,\\nleading to a crash.\\n\\nImpact summary: Applications that use the AES-XTS algorithm on the 64 bit ARM\\nplatform can crash in rare circumstances. The AES-XTS algorithm is usually\\nused for disk encryption.\\n\\nThe AES-XTS cipher decryption implementation for 64 bit ARM platform will read\\npast the end of the ciphertext buffer if the ciphertext size is 4 mod 5 in 16\\nbyte blocks, e.g. 144 bytes or 1024 bytes. If the memory after the ciphertext\\nbuffer is unmapped, this will trigger a crash which results in a denial of\\nservice.\\n\\nIf an attacker can control the size and location of the ciphertext buffer\\nbeing decrypted by an application using AES-XTS on 64 bit ARM, the\\napplication is affected. This is fairly unlikely making this issue\\na Low severity one.\",\n\n\"first_seen_time_dt\":\"2023-04-21T11:59:04.000-04:00\",\n\n\"last_seen_time_dt\":\"2024-01-26T17:19:14.000-05:00\",\n\n\"modified_time_dt\":\"2024-01-26T17:19:14.000-05:00\",\n\n\"title\":\"CVE-2023-1255 - openssl\",\n\n\"types\":[\n\n\"Software and Configuration Checks/Vulnerabilities/CVE\"\n\n],\n\n\"uid\":\"arn:aws:inspector2:us-east-2:111111111111:finding/faa0d54609b94871badcc83ac7c2add5\"\n\n},\n\n\"metadata\":{\n\n\"log_version\":\"2018-10-08\",\n\n\"processed_time_dt\":\"2024-01-26T17:59:56.923-05:00\",\n\n\"product\":{\n\n\"feature\":{\n\n\"uid\":\"AWSInspector\"\n\n},\n\n\"name\":\"Inspector\",\n\n\"uid\":\"arn:aws:securityhub:us-east-2::product/aws/inspector\",\n\n\"vendor_name\":\"Amazon\",\n\n\"version\":\"2\"\n\n},\n\n\"profiles\":[\n\n\"cloud\",\n\n\"datetime\"\n\n],\n\n\"version\":\"1.1.0\"\n\n},\n\n\"observables\":[\n\n{\n\n\"name\":\"resource.uid\",\n\n\"type\":\"Resource UID\",\n\n\"type_id\":10,\n\n\"value\":\"arn:aws:ecr:us-east-2:111111111111:repository/browserhostingstack-EXAMPLE-btb1o54yh1jr/sha256:e9e2afad74f4e80511a5cff33d3d989b9797a718425f27e549f5b1f862c058a8\"\n\n}\n\n],\n\n\"resource\":{\n\n\"cloud_partition\":\"aws\",\n\n\"data\":\"{\\\"AwsEcrContainerImage\\\":{\\\"Architecture\\\":\\\"amd64\\\",\\\"ImageDigest\\\":\\\"sha256:e9e2afad74f4e80511a5cff33d3d989b9797a718425f27e549f5b1f862c058a8\\\",\\\"ImagePublishedAt\\\":\\\"2023-04-11T21:07:55Z\\\",\\\"RegistryId\\\":\\\"111111111111\\\",\\\"RepositoryName\\\":\\\"browserhostingstack-EXAMPLE-btb1o54yh1jr\\\"}}\",\n\n\"region\":\"us-east-2\",\n\n\"type\":\"AwsEcrContainerImage\",\n\n\"uid\":\"arn:aws:ecr:us-east-2:111111111111:repository/browserhostingstack-EXAMPLE-btb1o54yh1jr/sha256:e9e2afad74f4e80511a5cff33d3d989b9797a718425f27e549f5b1f862c058a8\"\n\n},\n\n\"severity\":\"Medium\",\n\n\"severity_id\":3,\n\n\"status\":\"New\",\n\n\"time\":1706307554000,\n\n\"time_dt\":\"2024-01-26T17:19:14.000-05:00\",\n\n\"type_name\":\"Vulnerability Finding: Update\",\n\n\"type_uid\":200202,\n\n\"unmapped\":{\n\n\"FindingProviderFields.Severity.Label\":\"MEDIUM\",\n\n\"FindingProviderFields.Types[]\":\"Software and Configuration Checks/Vulnerabilities/CVE\",\n\n\"ProductFields.aws/inspector/FindingStatus\":\"ACTIVE\",\n\n\"ProductFields.aws/inspector/inspectorScore\":\"5.9\",\n\n\"ProductFields.aws/inspector/packageVulnerabilityDetails/vulnerablePackages/sourceLayerHashes\":\"sha256:f56be85fc22e46face30e2c3de3f7fe7c15f8fd7c4e5add29d7f64b87abdaa09\",\n\n\"ProductFields.aws/inspector/resources/1/resourceDetails/awsEcrContainerImageDetails/platform\":\"ALPINE_LINUX_3_17\",\n\n\"ProductFields.aws/securityhub/CompanyName\":\"Amazon\",\n\n\"ProductFields.aws/securityhub/FindingId\":\"arn:aws:securityhub:us-east-2::product/aws/inspector/arn:aws:inspector2:us-east-2:111111111111:finding/faa0d54609b94871badcc83ac7c2add5\",\n\n\"ProductFields.aws/securityhub/ProductName\":\"Inspector\",\n\n\"RecordState\":\"ACTIVE\",\n\n\"Severity.Normalized\":\"40\",\n\n\"Vulnerabilities[].Cvss[].Source\":\"NVD,NVD\",\n\n\"Vulnerabilities[].Vendor.VendorSeverity\":\"MEDIUM\",\n\n\"Vulnerabilities[].VulnerablePackages[].SourceLayerHash\":\"sha256:f56be85fc22e46face30e2c3de3f7fe7c15f8fd7c4e5add29d7f64b87abdaa09\",\n\n\"WorkflowState\":\"NEW\"\n\n},\n\n\"vulnerabilities\":[\n\n{\n\n\"affected_packages\":[\n\n{\n\n\"architecture\":\"X86_64\",\n\n\"epoch\":0,\n\n\"fixed_in_version\":\"0:3.0.8-r4\",\n\n\"name\":\"openssl\",\n\n\"package_manager\":\"OS\",\n\n\"release\":\"r3\",\n\n\"remediation\":{\n\n\"desc\":\"apk update && apk upgrade openssl\"\n\n},\n\n\"version\":\"3.0.8\"\n\n}\n\n],\n\n\"cve\":{\n\n\"created_time_dt\":\"2023-04-20T13:15:06.000-04:00\",\n\n\"cvss\":[\n\n{\n\n\"base_score\":5.9,\n\n\"vector_string\":\"CVSS:3.1/AV:N/AC:H/PR:N/UI:N/S:U/C:N/I:N/A:H\",\n\n\"version\":\"3.1\"\n\n},\n\n{\n\n\"base_score\":5.9,\n\n\"vector_string\":\"CVSS:3.1/AV:N/AC:H/PR:N/UI:N/S:U/C:N/I:N/A:H\",\n\n\"version\":\"3.1\"\n\n}\n\n],\n\n\"epss\":{\n\n\"score\":\"0.00066\"\n\n},\n\n\"modified_time_dt\":\"2023-09-08T13:15:15.000-04:00\",\n\n\"references\":[\n\n\"https://nvd.nist.gov/vuln/detail/CVE-2023-1255\"\n\n],\n\n\"uid\":\"CVE-2023-1255\"\n\n},\n\n\"is_exploit_available\":true,\n\n\"is_fix_available\":true,\n\n\"references\":[\n\n\"https://git.openssl.org/gitweb/?p=openssl.git;a=commitdiff;h=bc2f61ad70971869b242fc1cb445b98bad50074a\",\n\n\"https://www.openssl.org/news/secadv/20230419.txt\",\n\n\"https://git.openssl.org/gitweb/?p=openssl.git;a=commitdiff;h=02ac9c9420275868472f33b01def01218742b8bb\"\n\n],\n\n\"remediation\":{\n\n\"desc\":\"Remediation is available. Please refer to the Fixed version in the vulnerability details section above.For detailed remediation guidance for each of the affected packages, refer to the vulnerabilities section of the detailed finding JSON.\"\n\n},\n\n\"vendor_name\":\"NVD\"\n\n}\n\n]\n\n}\n\n{\n\n\"timestamp\":\"2024-10-29T21:12:18.177000000+01:00\",\n\n\"activity_id\":\"2\",\n\n\"activity_name\":\"Update\",\n\n\"aws.account.id\":\"111111111111\",\n\n\"aws.region\":\"us-east-2\",\n\n\"aws.resource.id\":\"arn:aws:ecr:us-east-2:111111111111:repository/browserhostingstack-EXAMPLE-btb1o54yh1jr/sha256:e9e2afad74f4e80511a5cff33d3d989b9797a718425f27e549f5b1f862c058a8\",\n\n\"category_name\":\"Findings\",\n\n\"category_uid\":\"2\",\n\n\"class_name\":\"Vulnerability Finding\",\n\n\"class_uid\":\"2002\",\n\n\"cloud\":\"{\\\"account\\\":{\\\"uid\\\":\\\"111111111111\\\"},\\\"provider\\\":\\\"AWS\\\",\\\"region\\\":\\\"us-east-2\\\"}\",\n\n\"component.name\":\"openssl\",\n\n\"component.version\":\"3.0.8\",\n\n\"dt.openpipeline.pipelines\":[\n\n\"events.security:vulnerability_finding\"\n\n],\n\n\"dt.openpipeline.source\":\"/platform/ingest/v1/events.security/\",\n\n\"dt.security.risk.level\":\"MEDIUM\",\n\n\"dt.security.risk.score\":6.9,\n\n\"event.category\":\"VULNERABILITY_MANAGEMENT\",\n\n\"event.description\":\"Issue summary: The AES-XTS cipher decryption implementation for 64 bit ARM\\nplatform contains a bug that could cause it to read past the input buffer,\\nleading to a crash.\\n\\nImpact summary: Applications that use the AES-XTS algorithm on the 64 bit ARM\\nplatform can crash in rare circumstances. The AES-XTS algorithm is usually\\nused for disk encryption.\\n\\nThe AES-XTS cipher decryption implementation for 64 bit ARM platform will read\\npast the end of the ciphertext buffer if the ciphertext size is 4 mod 5 in 16\\nbyte blocks, e.g. 144 bytes or 1024 bytes. If the memory after the ciphertext\\nbuffer is unmapped, this will trigger a crash which results in a denial of\\nservice.\\n\\nIf an attacker can control the size and location of the ciphertext buffer\\nbeing decrypted by an application using AES-XTS on 64 bit ARM, the\\napplication is affected. This is fairly unlikely making this issue\\na Low severity one.\",\n\n\"event.kind\":\"SECURITY_EVENT\",\n\n\"event.name\":\"Vulnerability finding\",\n\n\"event.provider\":\"Amazon Inspector\",\n\n\"event.type\":\"VULNERABILITY_FINDING\",\n\n\"event.version\":\"1.304\",\n\n\"finding.description\":\"Issue summary: The AES-XTS cipher decryption implementation for 64 bit ARM\\nplatform contains a bug that could cause it to read past the input buffer,\\nleading to a crash.\\n\\nImpact summary: Applications that use the AES-XTS algorithm on the 64 bit ARM\\nplatform can crash in rare circumstances. The AES-XTS algorithm is usually\\nused for disk encryption.\\n\\nThe AES-XTS cipher decryption implementation for 64 bit ARM platform will read\\npast the end of the ciphertext buffer if the ciphertext size is 4 mod 5 in 16\\nbyte blocks, e.g. 144 bytes or 1024 bytes. If the memory after the ciphertext\\nbuffer is unmapped, this will trigger a crash which results in a denial of\\nservice.\\n\\nIf an attacker can control the size and location of the ciphertext buffer\\nbeing decrypted by an application using AES-XTS on 64 bit ARM, the\\napplication is affected. This is fairly unlikely making this issue\\na Low severity one.\",\n\n\"finding.id\":\"arn:aws:inspector2:us-east-2:111111111111:finding/faa0d54609b94871badcc83ac7c2add5\",\n\n\"finding.time.created\":\"2024-01-26T23:19:14.000000000+01:00\",\n\n\"finding.title\":\"CVE-2023-1255 - openssl\",\n\n\"finding_info\":\"{\\\"created_time_dt\\\":\\\"2023-04-21T11:59:04.000-04:00\\\",\\\"desc\\\":\\\"Issue summary: The AES-XTS cipher decryption implementation for 64 bit ARM\\\\nplatform contains a bug that could cause it to read past the input buffer,\\\\nleading to a crash.\\\\n\\\\nImpact summary: Applications that use the AES-XTS algorithm on the 64 bit ARM\\\\nplatform can crash in rare circumstances. The AES-XTS algorithm is usually\\\\nused for disk encryption.\\\\n\\\\nThe AES-XTS cipher decryption implementation for 64 bit ARM platform will read\\\\npast the end of the ciphertext buffer if the ciphertext size is 4 mod 5 in 16\\\\nbyte blocks, e.g. 144 bytes or 1024 bytes. If the memory after the ciphertext\\\\nbuffer is unmapped, this will trigger a crash which results in a denial of\\\\nservice.\\\\n\\\\nIf an attacker can control the size and location of the ciphertext buffer\\\\nbeing decrypted by an application using AES-XTS on 64 bit ARM, the\\\\napplication is affected. This is fairly unlikely making this issue\\\\na Low severity one.\\\",\\\"first_seen_time_dt\\\":\\\"2023-04-21T11:59:04.000-04:00\\\",\\\"last_seen_time_dt\\\":\\\"2024-01-26T17:19:14.000-05:00\\\",\\\"modified_time_dt\\\":\\\"2024-01-26T17:19:14.000-05:00\\\",\\\"title\\\":\\\"CVE-2023-1255 - openssl\\\",\\\"types\\\":[\\\"Software and Configuration Checks/Vulnerabilities/CVE\\\"],\\\"uid\\\":\\\"arn:aws:inspector2:us-east-2:111111111111:finding/faa0d54609b94871badcc83ac7c2add5\\\"}\",\n\n\"metadata\":\"{\\\"log_version\\\":\\\"2018-10-08\\\",\\\"processed_time_dt\\\":\\\"2024-01-26T17:59:56.923-05:00\\\",\\\"product\\\":{\\\"feature\\\":{\\\"uid\\\":\\\"AWSInspector\\\"},\\\"name\\\":\\\"Inspector\\\",\\\"uid\\\":\\\"arn:aws:securityhub:us-east-2::product/aws/inspector\\\",\\\"vendor_name\\\":\\\"Amazon\\\",\\\"version\\\":\\\"2\\\"},\\\"profiles\\\":[\\\"cloud\\\",\\\"datetime\\\"],\\\"version\\\":\\\"1.1.0\\\"}\",\n\n\"object.id\":\"arn:aws:ecr:us-east-2:111111111111:repository/browserhostingstack-EXAMPLE-btb1o54yh1jr/sha256:e9e2afad74f4e80511a5cff33d3d989b9797a718425f27e549f5b1f862c058a8\",\n\n\"observables\":[\n\n\"{\\\"name\\\":\\\"resource.uid\\\",\\\"type\\\":\\\"Resource UID\\\",\\\"type_id\\\":10,\\\"value\\\":\\\"arn:aws:ecr:us-east-2:111111111111:repository/browserhostingstack-EXAMPLE-btb1o54yh1jr/sha256:e9e2afad74f4e80511a5cff33d3d989b9797a718425f27e549f5b1f862c058a8\\\"}\"\n\n],\n\n\"resource\":\"{\\\"cloud_partition\\\":\\\"aws\\\",\\\"data\\\":\\\"{\\\\\\\"AwsEcrContainerImage\\\\\\\":{\\\\\\\"Architecture\\\\\\\":\\\\\\\"amd64\\\\\\\",\\\\\\\"ImageDigest\\\\\\\":\\\\\\\"sha256:e9e2afad74f4e80511a5cff33d3d989b9797a718425f27e549f5b1f862c058a8\\\\\\\",\\\\\\\"ImagePublishedAt\\\\\\\":\\\\\\\"2023-04-11T21:07:55Z\\\\\\\",\\\\\\\"RegistryId\\\\\\\":\\\\\\\"111111111111\\\\\\\",\\\\\\\"RepositoryName\\\\\\\":\\\\\\\"browserhostingstack-EXAMPLE-btb1o54yh1jr\\\\\\\"}}\\\",\\\"region\\\":\\\"us-east-2\\\",\\\"type\\\":\\\"AwsEcrContainerImage\\\",\\\"uid\\\":\\\"arn:aws:ecr:us-east-2:111111111111:repository/browserhostingstack-EXAMPLE-btb1o54yh1jr/sha256:e9e2afad74f4e80511a5cff33d3d989b9797a718425f27e549f5b1f862c058a8\\\"}\",\n\n\"severity\":\"Medium\",\n\n\"severity_id\":\"3\",\n\n\"status\":\"New\",\n\n\"time\":\"1706307554000\",\n\n\"time_dt\":\"2024-01-26T17:19:14.000-05:00\",\n\n\"type_name\":\"Vulnerability Finding: Update\",\n\n\"type_uid\":\"200202\",\n\n\"unmapped\":\"{\\\"FindingProviderFields.Severity.Label\\\":\\\"MEDIUM\\\",\\\"FindingProviderFields.Types[]\\\":\\\"Software and Configuration Checks/Vulnerabilities/CVE\\\",\\\"ProductFields.aws/inspector/FindingStatus\\\":\\\"ACTIVE\\\",\\\"ProductFields.aws/inspector/inspectorScore\\\":\\\"5.9\\\",\\\"ProductFields.aws/inspector/packageVulnerabilityDetails/vulnerablePackages/sourceLayerHashes\\\":\\\"sha256:f56be85fc22e46face30e2c3de3f7fe7c15f8fd7c4e5add29d7f64b87abdaa09\\\",\\\"ProductFields.aws/inspector/resources/1/resourceDetails/awsEcrContainerImageDetails/platform\\\":\\\"ALPINE_LINUX_3_17\\\",\\\"ProductFields.aws/securityhub/CompanyName\\\":\\\"Amazon\\\",\\\"ProductFields.aws/securityhub/FindingId\\\":\\\"arn:aws:securityhub:us-east-2::product/aws/inspector/arn:aws:inspector2:us-east-2:111111111111:finding/faa0d54609b94871badcc83ac7c2add5\\\",\\\"ProductFields.aws/securityhub/ProductName\\\":\\\"Inspector\\\",\\\"RecordState\\\":\\\"ACTIVE\\\",\\\"Severity.Normalized\\\":\\\"40\\\",\\\"Vulnerabilities[].Cvss[].Source\\\":\\\"NVD,NVD\\\",\\\"Vulnerabilities[].Vendor.VendorSeverity\\\":\\\"MEDIUM\\\",\\\"Vulnerabilities[].VulnerablePackages[].SourceLayerHash\\\":\\\"sha256:f56be85fc22e46face30e2c3de3f7fe7c15f8fd7c4e5add29d7f64b87abdaa09\\\",\\\"WorkflowState\\\":\\\"NEW\\\"}\",\n\n\"vulnerabilities\":[\n\n\"{\\\"affected_packages\\\":[{\\\"architecture\\\":\\\"X86_64\\\",\\\"epoch\\\":0,\\\"fixed_in_version\\\":\\\"0:3.0.8-r4\\\",\\\"name\\\":\\\"openssl\\\",\\\"package_manager\\\":\\\"OS\\\",\\\"release\\\":\\\"r3\\\",\\\"remediation\\\":{\\\"desc\\\":\\\"apk update && apk upgrade openssl\\\"},\\\"version\\\":\\\"3.0.8\\\"}],\\\"cve\\\":{\\\"created_time_dt\\\":\\\"2023-04-20T13:15:06.000-04:00\\\",\\\"cvss\\\":[{\\\"base_score\\\":5.9,\\\"vector_string\\\":\\\"CVSS:3.1/AV:N/AC:H/PR:N/UI:N/S:U/C:N/I:N/A:H\\\",\\\"version\\\":\\\"3.1\\\"},{\\\"base_score\\\":5.9,\\\"vector_string\\\":\\\"CVSS:3.1/AV:N/AC:H/PR:N/UI:N/S:U/C:N/I:N/A:H\\\",\\\"version\\\":\\\"3.1\\\"}],\\\"epss\\\":{\\\"score\\\":\\\"0.00066\\\"},\\\"modified_time_dt\\\":\\\"2023-09-08T13:15:15.000-04:00\\\",\\\"references\\\":[\\\"https://nvd.nist.gov/vuln/detail/CVE-2023-1255\\\"],\\\"uid\\\":\\\"CVE-2023-1255\\\"},\\\"is_exploit_available\\\":true,\\\"is_fix_available\\\":true,\\\"references\\\":[\\\"https://git.openssl.org/gitweb/?p=openssl.git;a=commitdiff;h=bc2f61ad70971869b242fc1cb445b98bad50074a\\\",\\\"https://www.openssl.org/news/secadv/20230419.txt\\\",\\\"https://git.openssl.org/gitweb/?p=openssl.git;a=commitdiff;h=02ac9c9420275868472f33b01def01218742b8bb\\\"],\\\"remediation\\\":{\\\"desc\\\":\\\"Remediation is available. Please refer to the Fixed version in the vulnerability details section above.For detailed remediation guidance for each of the affected packages, refer to the vulnerabilities section of the detailed finding JSON.\\\"},\\\"vendor_name\\\":\\\"NVD\\\"}\"\n\n],\n\n\"vulnerability.description\":null,\n\n\"vulnerability.id\":\"CVE-2023-1255\",\n\n\"vulnerability.title\":\"CVE-2023-1255\"\n\n}",
      "# [AI Timeline](https://annielytics.com/tools/ai-timeline/)\n",
      "# [Agency AI Raises $2.6M in Pre-Seed Funding by FinSMEs on 2024-08-28](https://www.finsmes.com/2024/08/agency-ai-raises-2-6m-in-pre-seed-funding.html)\nAgency AI, a San Francisco, CA-based creator of the AgentOps.ai platform, raised $2.6M in Pre-Seed funding.\n\nThe round was led by 645 Ventures and Afore Capital.\n\nThe company intends to use the funds to further develop and scale its AI agent observability and testing platform, ensuring reliability and safety for enterprises deploying large language model (LLM) enabled autonomous agents.\n\nLed by industry veterans Alex Reibman, Adam Silverman, and Shawn Qiu, Agency AI is dedicated to creating a safe environment for AI agent development. With its flagship product, AgentOps.ai, allows enterprises to build, deploy, and manage AI agents, ensuring compliance, observability, and scalability.\n\nAgentOps.ai integrates with AI agent frameworks including AutoGen from Microsoft Research, CrewAI, LlamaIndex, Cohere, and MultiOn, providing support and compliance for AI agent developers.\n\nThe solution is used by international companies including Llama Index, AutoGen from Microsoft Research, CrewAI, Ollama, OpenPipe, Prompt Armor, Browserbase, and Anon.com.\n\nAgentOps.ai recently announced it had joined the Cohere startup program.\n\nFinSMEs\n\n28/08/2024",
      "# [The Opportunities Costanoa Ventures Sees In Vertical AI by Gené Teare on 2024-11-27](https://news.crunchbase.com/venture/vertical-ai-investment-costanoa-sands-cowgill/)\nEditor’s note: This article is part of an ongoing series in which Crunchbase News interviews active investors in artificial intelligence. Read previous interviews with Felicis, Battery Ventures, General Catalyst, Bessemer Venture Partners, Accel, Insight Partners, Index Ventures, Sequoia Capital, Section 32, M12, Sapphire Ventures, Bain Capital Ventures, Menlo Ventures and Scale Venture Partners, as well as highlights from these stories from 2023.\n\nTwo years into the artificial intelligence wave, we thought it would be helpful to speak with an early-stage investor in the sector to get a perspective on what it’s been like to invest in generative AI startups from the get-go.\n\nTo that end, we caught up with Costanoa Ventures founder Greg Sands and general partner John Cowgill. The Palo Alto, California-based firm invests at pre-seed, seed and Series A, and closed on its fifth fund of $275 million (its largest), and a third opportunity fund of $119 million to invest in its winners — both in September.\n\nFrom its first $100 million fund raised in 2012, the firm has built its practice in enterprise cloud-based services leveraging data and analytics.\n\nCostanoa’s focus today is applied AI and AI infrastructure, and B2B fintech. With generative AI, “we can solve problems that were unsolvable two years ago,” said Sands.\n\nCostanoa is also doubling down on cybersecurity — as the threat instance has increased with AI as well as the potential to improve security operations — and national security. The firm incubated Vannevar Labs in 2019, before defense tech and AI were as popular as they are today.\n\nNew capabilities, not imagined before\n\nThe advancements in AI in recent years mean that through natural language processing, written language can be easily digested, consumed, organized and even reasoned with. Computer vision allows images to be evaluated and understood. Combine those technologies, and you start to get some very interesting and complex outputs, Sands said.\n\nGoing vertical\n\n“We have literally solved most of the problems that could be solved by that generation of technology,” Sands said of the SaaS revolution of the past 20 years.\n\nThe firm believes that generative AI has now opened up vertical opportunities that might have been too small in the past.\n\n“The knock on vertical SaaS has always been: it’s a smaller total addressable market because you limit yourself to one vertical, rather than building something that can apply to every vertical,” said Cowgill.\n\nThe firm is looking closely at each vertical to understand how big it can be with the rise of AI.\n\nIn that vein, Costanoa backed Aquabyte, which provides computer vision for fish farming. It also invested in Beacon AI for pilot assistance in aviation, and ForceMetrics, a service to give first responders situational awareness by consolidating and presenting data. It also made a stealth investment in a company in construction permitting. Though that may seem narrow, there’s a big opportunity in owning an end to end workflow in that space.\n\nBullish on native AI\n\nThe big question for investors is: Does this vertical opportunity benefit existing mid-size portfolio companies embedding an API? Or is it like the cloud and mobile era where it was challenging for incumbents to rebuild their business models?\n\n“I’m more bullish on AI native application layer companies than I was two years ago,” said Cowgill, who has seen firsthand how hard it is to integrate AI and where the value lies.\n\nThe firm has a handful of later-stage vertical SaaS companies working to integrate AI. “The rate at which they will integrate AI will be slower,” said Cowgill. “They might actually end up delivering different services and different products, different business models.”\n\n“The real value in AI comes when you can own an end-to-end workflow,” he said. “This is the idea of an agent moving from just applying AI, pointing at something and saying, ‘search across it with AI, summarize with the AI’ — to own a job to be done with AI. Getting agents to work is incredibly difficult.”\n\nNuance in picks and shovels\n\nCostanoa has made several bets in AI infrastructure. It invested in Delphina, creator of an AI junior data scientist to assist teams. It also backed Rerun.io, an open source visualization stack for multimodal data including audio, image and video, and OpenPipe, which is used to fine-tune models.\n\nStill, Sands noted the infrastructure for model management broadly has not grown as much as was expected 18 to 24 months ago, in part due to the fact that companies in applied AI are not managing models themselves, but purchasing from hyperscalers.\n\n‘Things that matter’\n\nBut some things have not changed.\n\nIn a 2018 blog post “What the $%&* is Applied AI?” Sands noted that AI does not replace the need for “excellent product management” and that “data is as important as the algorithm.”\n\nThrough the firm’s 12 years, Sands said he has focused on eternal principles: “find extraordinary people, pour time and energy and capability into them so that they have the best chance of success,” he said. “Pick things that matter — both technologies that matter and problems that matter.”\n\nRelated Crunchbase Pro list:",
      "# [2024 Backward Pass: The Definitive Guide to AI in 2024 by Kelvin Mu on 2024-12-03](https://kelvinmu.substack.com/p/2024-backward-pass-the-definitive)\nThe original report in PDF can be downloaded here: https://translinkcapital.docsend.com/v/c98t5/backward_pass\n\nOPENING REMARKS\n\nI began this document in September as a way to organize my personal notes around everything in AI this year. Later, I realized there might be value in sharing a more comprehensive year-in-review with a broader audience. What you’re reading now is the result of that effort.\n\nThe goal of this paper is to present the most thorough analysis of AI’s development for this year, It encompasses many of my personal thoughts on the space and draws upon insights gathered from conversations with researchers, portfolio companies, and over 300 AI startups. It also leverages the vast network of corporate relationship that my firm Translink Capital has built over the past 16 years.\n\nThe review is structured into four sections, corresponding to each of the layers in the AI technology stack:\n\nInfrastructure layer\n\nFoundational model layer\n\nTooling layer\n\nApplication layer\n\nEach section is further divided into three parts:\n\nI. Key takeaways\n\nII. Future trends to watch\n\nIII. Noteworthy startups\n\nFinally, we conclude with two sections on investment and M&A and other general AI trends.\n\nAs a venture investor working for a multinational firm, I’m incredibly fortunate to have a job that allows me to see across different layers and geographies of the AI ecosystem. This broad perspective gives me the opportunity to identify patterns that might not be immediately apparent to those more deeply focused in one specific area. By capitalizing on the opportunities afforded to me, I hope to contribute what I can to the understanding of this field.\n\nI know how valuable everyone’s time is, so I’ve put in my best effort to ensure this is worth reading. I hope you find it helpful, and I encourage you to share it with anyone who might benefit from it.\n\nSincerely,\n\nKelvin\n\nINTRODUCTION\n\nWhat a remarkable year it was for AI.\n\nThe long-anticipated AI revolution is no longer a distant vision but a present reality. Since the 1950s, when Alan Turing first proposed the Turing Test and Frank Rosenblatt pioneered the first artificial neural network, AI has been heralded as a transformative force capable of reshaping society. Yet, its journey has been far from smooth – AI went through multiple \"winters\" from the 1970s through the early 2000s, periods where interest and funding dried up. Only in recent years, with breakthroughs like ImageNet (2009), AlphaGo (2015), Transformers (2017), and ChatGPT (2022), has AI truly regained its momentum.\n\nThis year marked a watershed moment where innovation, investment, and adoption have converged like never before. AI has moved beyond the confines of research labs and academic communities to become a central topic in boardrooms, political debates, and dinner table conversations. Over $60 billion in venture capital flowed into the sector this year, with AI investments now accounting for over one-third of all VC activity – surpassing traditionally dominant industries like healthcare and consumer.\n\nFor only the third time in modern history, the entire tech infrastructure and compute stack is being reimagined from the ground up. Nvidia, the biggest beneficial of this transformation, has surged to become the world’s most valuable company, growing its market cap tenfolds to $3 trillion within a mere 24 months. Meanwhile, OpenAI, despite navigating internal turmoil, has set new historic records, reaching a $4 billion ARR run rate in just three years of commercialization – a pace at least three times faster than Amazon, the previous record-holder.\n\nEnterprises, too, are embracing AI at scale. A year ago, a JP Morgan survey revealed that only 5% of enterprises had generative AI applications in production. Today, that percentage has more than tripled. While many implementations remain in the proof-of-concept (POC) stage, some use cases—like code generation and customer support – have seen widespread adoption. At Google, more than a quarter of all new code is generated by AI, and Klarna’s AI customer support agent can do the work of 700 human reps. These examples show that AI is moving from promise to practice and is starting to deliver tangible bottom-line results to businesses.\n\nYet, despite these advancements, skepticism remains. Some have begun to question the sustainability of the current AI investment boom. In June, Sequoia published an article titled “AI’s $600B question”, challenging the ROI of massive infrastructure spending. Shortly after, Goldman Sachs echoed similar concerns in an article “Gen AI: Too Much Spend, Too Little Benefit”. Perhaps unsurprisingly, over 40% of asset managers believe we’re now in an AI bubble.\n\nRegardless of where one stands in this debate, one fact is indisputable: the pace of AI innovation and adoption this year was unprecedented. Few years in modern history have seen such a concentrated burst of technological progress and investment as we saw in 2024. This is more than a technological revolution; it is a societal revolution. And we are not merely spectators to this revolution but active participants—a rare opportunity we must responsibly embrace.\n\nWhat an extraordinary time to be alive.\n\nINFRASTRUCTURE LAYER\n\nI. Key Takeaways:\n\nWe are witnessing the dawn of a new infrastructure paradigm. In modern history, only twice before has a completely new infrastructure and compute stack been redefined entirely – the internet and telecom boom of the late 20th century, and the rise of cloud computing and SaaS. Now, with GenAI, we are entering into a third phase.\n\nBut it is still early earnings. During the internet buildout, over $1 trillion in capital was invested between 1996 and 2001. The current generative AI buildout has seen only $300 billion invested over the past two years. By this measure, we are still in the early innings. (For more detail on this topic, see my in-depth piece here comparing the current AI cycle with the dot-com cycle).\n\nDemand for inference is just getting started. While demand for training may be maturing as we reach scaling limits (more on this later), demand for inference is just getting started. Broadly speaking, there are three reasons for this:\n\nEarly Adoption. Generative AI adoption by enterprises is still in its infancy, but accelerating rapidly. OpenAI serves as a useful indicator: despite a >10x reduction in per-token API costs over the past year, its revenue quadrupled from a $1b to $4b run-rate, implying a ~40x increase in usage.* Such level of growth indicates that we’re still in the early stages of adoption.\n\nMulti-modal use cases coming soon. Most GenAI applications today are text-based. Multimodal use cases (e.g., text-to-video, text-to-3D) remain largely untapped, but are far more compute-intensive. For example, generating an AI video requires roughly 100 times the energy of an equivalent text-based document. If entire industries such as advertising, media, and entertainment adopt GenAI, that would create an exponential demand for inference. With the latest advancements in multimodal AI, this may be coming soon.\n\nEvolving model architecture. Newer models, such as OpenAI’s o1 are shifting to more inference-time reasoning (also known as test-time compute) incorporating chain-of-thought and reinforcement learning. This architecture essentially gives models extra processing time to think and complete tasks. But this also means more computational requirements. For example, the new o1 model costs 3-4x more per token than GPT-4o. As more workloads transition to this model architecture, the demand for inference will continue to rise.\n\nIt is difficult to measure the ROI of the current infrastructure buildout. This summer, Sequoia published an article questioning where the $600 billion revenue was to justify the current AI infrastructure build out. A plausible explanation is that much of today’s compute is powering internal initiatives rather than new revenue-generating products – initiatives like Notion’s GenAI features or Klarna’s AI customer support agents. These projects drive operational efficiencies rather than net new revenue, making their ROI harder to quantify. For context, $600 billion is just 0.6% of the global $100 trillion GDP— which likely underestimates AI's long-term potential.\n\nThe AI cloud market is getting increasingly fragmented.\n\nWhile hyperscalers (Amazon, Google, Microsoft) continue to dominate the AI cloud market today, emerging players like CoreWeave, Lambda Labs, and Tensorwave are offering cost-effective, specialized AI infrastructure. Brendan Burke from Pitchbook estimates this new AI cloud market to be $4 billion and growing to $32 billion by 2027.\n\nChipmakers like Nvidia and AMD are investing in these specialized providers as well. One reason is that these chipmakers are seeking to reduce their reliance on hyperscalers, who are simultaneously developing in-house chips. For example, Google’s TPU chips are now being adopted by companies like Apple. Further fragmentation in the AI cloud market appears likely.\n\nAI hardware startups face high CapEx requirements.\n\nA growing number of startups are designing custom ASIC chips for AI workloads (e.g., Groq, Cerebras). These companies face significant capital requirements for not only chip development but also for data center buildouts.\n\nThe latter is necessary because hyperscalers, with their own chip R&D efforts, are unlikely to adopt third-party startup chips in their own data centers. For example, chip startup Groq recently announced they were building their own inference data center in Saudi Arabia in partnership with Amarco Digital.\n\nIt remains to be seen if these startups can meaningfully capture market share from incumbents. Among startups so far, Cerebras seems to be the most ahead with $136 million of revenue in 1H 2024, but that’s still only 0.1% of Nvidia’s data center revenue.\n\n* This estimate excludes OpenAI’s B2C subscription revenue, but the overall trend is directionally accurate.\n\nII. Future Trends to Watch:\n\nData Center 2.0. Currently, data centers account for approximately only 1-2% of global electricity usage, but this figure is projected to rise to 3-4% of total power by 2030, driven largely by AI (In the US, this figure is closer to 8%). Overall, McKinsey estimates data center capacity to grow at a 22% CAGR between now and 2030.\n\nAI-specific data centers differ significantly from traditional cloud data centers due to their higher power density, which drives the need for innovations like next-gen liquid cooling. The idiosyncratic demands of AI training and inference also require high-bandwidth, low-latency networking. This has driven the need for next-gen networking and interconnect to reduce bottlenecks in multi-GPU clusters.\n\nAI itself can be used to optimize data centers, such as in predictive maintenance, dynamic workload allocation, and energy efficiency. For example, Phaidra is a startup that is working on autonomous control cooling systems in data centers using RL.\n\nNvidia’s dominance in hardware. Nvidia is now the world’s most valuable company with 40% of the gains on Nasdaq this year attributable to this one company alone. However, history offers few examples of an incumbent maintaining 90%+ market share from start to finish.\n\nNvidia will likely continue to dominate for the foreseeable future but there’s likely to be more competition. One contender is AMD - AMD’s data center business today is only 10% the size of Nvidia’s ($3.5 billion vs. $30.8 billion) but growing 122% year-over-year. The company is also making traction with large enterprises – for example, OpenAI recently announced they will start using AMD’s MI300 year and Lenovo says demand for AMD’s MI300 is record high.\n\nAnother major source of competition are the hyperscalers themselves. One advantage they have is their massive in-house demand for AI training and inference. Of the CSPs, Google has the biggest lead, with the new TPU V5p delivering 2x more FLOPs and 3x more high-bandwidth memory (HBM) than the previous version.\n\nMore attention paid to edge AI and edge/cloud collaboration. Here’s a fun fact: less than 1% of global compute capacity (measured in FLOPs) is owned by the hyperscalers. While this might seem surprising at first, it makes sense when you consider the sheer number of edge devices, including laptops and smartphones. Unlocking this latent power could be a gamechanger. Already, we’re witnessing an explosion of small language models (SLMs) that can be deployed on edge devices.\n\nSome experts believe that up to half of all AI workloads can be eventually moved from the cloud to the edge. One early idea I’ve seen is to build a cloud/edge router, which can dynamically route AI workloads between the cloud and edge devices based on criteria such as power, cost, and latency requirements. Finally, Edge AI can also be helpful in speculative decoding by reducing the volume of data sent to the cloud for processing.\n\nChina’s progress in AI. Export controls around AI chips may hamper China’s progress in the short term. But longer-term, it may force China to be more creative on both the infrastructure and model side. For example, it was recently revealed that China had trained a single model across multiple data centers and GPU architectures, the first time to do so by any country. Despite limitations in compute, Chinese LLMs have shown that they can compete with the best IS models. In particular, Alibaba’s Qwen models and DeepSeek models have shown they can match Western models such as GPT-4o. This shows that while hardware restrictions may pose some challenges, Chinese labs are finding ways around it and are continuing to keep pace.\n\nAI’s Impact on Sustainability: Hyperscalers have made climate commitments by 2030. Microsoft, for instance, has set an ambitious goal to become carbon negative by 2030. However, the rapid growth in AI energy consumption is pushing these commitments in the wrong direction. For example, Microsoft recently reported a nearly 30% increase in CO2 emissions since 2020, driven primarily by data center expansion. Similarly, Google’s greenhouse gas emissions in 2023 were 50% higher than in 2019, largely due to AI data centers as well. We think this trend will force enterprise decision-making to pay more attention in sustainability. In the long term, those that want to understand AI will need to understand the energy market as well.\n\nIII. Noteworthy Startups:\n\nAI Cloud and Compute:\n\nCoreweave, Crusoe, FoundryML, Lambda labs, Rescale*, SF Compute, Shadeform*, Tensorwave*, Together AI\n\nAI chip companies:\n\nBlaize, Cerebras, D-Matrix, Etched, Groq, Graphcore, Lightmatter, Rebellions, SambaNova, Tenstorrent, Hailo\n\nData center peripherals:\n\nCelestial AI, Corintis, Liquidstack, Jetcool, Phaidra, Thintronics*, Xconn Technologies*\n\n* Denotes Translink Capital Portfolio Company\n\nMODEL LAYER\n\nI. Key Takeaways:\n\nGrowing attention paid to small language models (SLMs). Last year, we anticipated a growing trend in SLMs, but their rapid progress has surpassed even our expectations. Today, a 3B parameter model can match the performance of the original 175B ChatGPT model, a more than 50x improvement in parameter efficiency in just 24 months. This remarkable progress stems from better compression techniques (e.g., distillation, quantization, pruning) and the use of high-quality synthetic data. Research from a 2023 paper “Beyond Neural Scaling Laws” demonstrated that by carefully curating datasets and removing low-quality data, it’s possible to achieve better performance with a much smaller footprint. This is the core idea behind SLMs.\n\nConsequently, edge AI is becoming more viable. As SLMs performance improves and edge hardware (CPUs, NPUs) becomes more powerful, deploying AI workloads at the edge becomes increasingly feasible. Today, a 7B model can run efficiently on a laptop. Notable examples of SLMs include Microsoft's Phi-3 model , Google's Gemini Flash, and Llama 1B and 3B models. Edge AI also has the added benefit of improved privacy & security, lower latency, and cheaper costs. This is especially good for use cases like real-time speech recognition or in offline settings. For a comprehensive review of on-device language models, check out this paper by Meta and Nexa AI.\n\nWe are shifting towards a network of models. The architecture of LLMs is evolving from large monolithic system into distributed networks of smaller, specialized models (akin to a MoE approach). This involves a parent model orchestrating tasks across these smaller, targeted models. Recent research from Meta demonstrates that using multiple smaller models in parallel can consistently outperform a single large model. This approach mirrors the human brain, which is not a single uniform structure but comprised of specialized regions like the hippocampus (memory), frontal lobe (logic), and occipital lobe (vision). We also believe this architecture will also apply to AI agents (more on this later).\n\nModels are moving towards more inference-time reasoning. OpenAI's latest o1 model signals the shift toward inference-time reasoning using techniques like chain-of-thought and reinforcement learning. (A good summary can be found here). The o1 model learns optimal paths through trial and error, much like human problem-solving involving a lot of self-reflection and error correction. This allows the model to excel in complex reasoning tasks, such as math, coding, and scientific queries. However, this capability comes at a cost, with o1's per-token price being 3-4 times higher than GPT-4o. Another example of this model is R1-lite-preview by Chinese lab DeepSeek. Unlike o1's condensed summaries, R1-Lite-Preview shows users its complete chain-of-thought process in real-time. This growing emphasis on inference-time reasoning will likely increase demand for latency-optimized compute.\n\nOpenAI experienced internal turmoil. This year, OpenAI experienced a lot of internal upheaval, including the abrupt ousting and subsequent reinstatement of Sam Altman as CEO. Among the original founders, only Greg Brockman and Wojciech Zaremba remain, while key figures like Ilya Sutskever, John Schulman, and Mira Murati have all left. Despite these challenges, OpenAI raised $6.5 billion at a $150 billion valuation in its latest funding round and is on pace to becoming the fastest growing technology company in history, far outpacing that of Amazon, Google, and Meta.\n\nMeta open-source strategy is paying off. Meta continues its bold open-source approach, with Zuckerberg committing billions to Llama and Meta's broader generative AI initiatives. This approach is proving effective, with Llama models approaching 350 million downloads this year – representing more than a 10x increase from last year. On the consumer front, Meta is integrating their LLMs into existing consumer applications like Facebook and Instagram to prevent competitors from building standalone LLM interfaces like ChatGPT or Perplexity. On the enterprise front, Meta is already working with large enterprises such as AT&T, DoorDash, and Goldman Sachs. It appears the gap between closed-source proprietary models and open-source models has significantly narrowed, largely thanks to Meta’s efforts.\n\nOver the past year, the performance gap between OpenAI and other research labs has narrowed. While OpenAI still leads, its dominance is less pronounced. Among startups, Anthropic stands out for its impressive progress in model upgrades, product launches, and talent acquisition, and is rumored to be approaching $1 billion in revenue run rate. It appears that first-mover advantage may be less enduring in the age of AI. One hypothesis is that in today’s interconnected world, mature communication infrastructure such as the internet and social media enables proprietary techniques and knowledge to disseminate faster than ever, decreasing technological moats. As a result, model performance increasingly hinges on access to capital and compute rather than any proprietary techniques. In this vein, one startup to watch for in the coming year is xAI – they already own one of the largest supercomputers in the world with over 100k H100s (soon to be 200K according to Musk). Musk has hinted that their upcoming model, Grok 3, could already be SOTA and comparable to GPT-4o.\n\nFoundational model companies are likely to remain unprofitable in the short run. According to The Information, OpenAI’s total compute expenses for training and inference is expected to be $5 billion—exceeding its revenue of $4 billion. Assuming that inference and hosting costs make up the majority of the cost of goods sold, OpenAI operates at a gross margin of around 40%. This is in line with we hear from other foundational model companies but significantly lower than the typical margins seen in software businesses.\n\nOne reason for the low margins is the ongoing token price war among model providers, which has driven prices down by more than 10x this year. Outside of compute costs, other expenses appear to be relatively modest, with employee salaries, general and administrative (G&A) expenses, and sales and marketing together accounting for only 40% of revenue. Looking ahead, it will be interesting to see if foundational model companies can eventually achieve software-like margins as the industry matures. Regardless, we believe break-even will not be achieved anytime soon. (link to below diagram)\n\nFinally, as model architectures evolve towards more inference-time reasoning, the cost structure may shift—reducing CapEx for training but increasing OpEx for inference. This transition could further impact margins and delay GAAP profitability.\n\nGrowing need for model companies to diversify. As the foundational model layer becomes increasingly commoditized, AI labs may need to diversify their businesses. Token prices are dropping, but rising adoption and revenue growth are helping to offset this decline for now. For example, despite token prices dropping significantly, OpenAI is still projected to 4x its revenue this year, and Anthropic is rumored to be growing at 10x. In the longer term, however, model companies may need to consider vertical integration to offset commoditization at the model layer.\n\nIn the infrastructure layer, OpenAI is developing its first in-house chip in partnership with Broadcom and TSMC. In addition, it’s collaborating with Microsoft on \"Project Stargate,\" a 5 GwH $100 billion data center initiative.\n\nIn the application and tooling layer, OpenAI is expanding into new products such as ChatGPT search, a Perplexity-like search tool, and OpenAI Swarm, a framework for building agents. To ensure long-term growth, foundational model companies may need to shift from purely providing models to developing tools and end-user applications.\n\nModels from China are getting much better. Chinese AI labs continue to produce impressive models, such as Alibaba’s Qwen family and Deepseek. The newest Qwen 2.5-Coder Series and QwQ-32B-preview from Alibaba are amongst the top open-source code models, matching the capability of OpenAI and Anthropic models. Both the QwQ-32-B preview and Deepseek’s R1- beat OpenAI’s o1 on popular benchmarks like MATH and AIME.\n\nThese improvements are driven by several factors. First, the competition among Chinese LLMs is fierce, even more so than the US. For example, Qwen-long tokens currently cost 0.5RMB ($0.07) per 1M tokens, a tiny fraction of the cost of its western counterparts. Chinese AI labs are particularly skilled at optimizing inference costs, a necessity given their more limited access to compute compared to Western labs. Second, data labeling and instruction-tuning (including RLHF) are significantly cheaper in China, providing an additional cost advantage. Over time, these two advantages plus others (government support, growing talent pool), could further narrow the performance gap between Chinese and Western models.\n\nII. Future Trends To Watch:\n\nScaling laws could soon plateau. Recent developments suggest that scaling laws, which have historically driven model improvements through larger pre-training datasets, may soon reach their limits. Ilya Sutskever also recently expressed this view in an interview with Reuters. Around the same time, The Information published an article discussing how OpenAI’s next flagship model Orion, might not achieve the dramatic performance leaps seen in earlier iterations. This shift signals that straightforward scaling may be giving way to a new paradigm of model innovation.\n\nNew paradigm of model development: reasoning over pre-training. Going forward, model performance will hinge less on sheer amounts of pre-trained data and more on advanced reasoning capabilities at inference time. Models like o1 are using chain-of-thought and reinforcement learning which gives models more advanced reasoning capabilities. Beyond the obvious performance benefits, another benefit is that it allows customers to tailor model selection depending on the complexity of tasks. Tasks requiring sophisticated reasoning, such as a copilot for M&A negotiations, can choose to maximum inference-time reasoning. Simpler tasks, like summarization, may seek to minimize it instead. This approach gives the end customer more flexibility and can help to reduce overall costs.\n\nThe role of synthetic data. As we run out of publicly available training data, synthetic data will play a more prominent role. Synthetic data is being used more both in pre-training and post-training. Both the smaller Microsoft Phi SLMs and the largest models by OpenAI and Anthropic incorporate synthetic data into their training. However, preserving the distribution and entropy of the synthetic dataset to ensure model robustness is the challenging part.\n\nUnsupervised / Reinforcement learning’s role in model development. Now that LLMs have some base intelligence built in, we believe RL will play a bigger role in their development. Genie (awarded best paper at ICML 2024) demonstrated this by creating a 11B parameters “world foundational model” that could generate interactive environments from unsupervised training from unlabeled internet videos. Such type of training without any ground-truth or labeled data will be the future of large model development. Over time, models will start to self-improve in a positive feedback loop, unlocking new possibilities that didn’t exist before.\n\nThe evolution of LLMs mirrors stages of human cognitive development. The first phase, mostly reliant on scaling and pre-training, is akin to the early development of a fetus or a newborn’s brain—focused on expanding and building foundational structure. Now, we are entering a second phase, where the model’s \"toddler-like\" brain expand beyond its organic, foundational base. In this stage, it grows smarter through trial-and-error learning, observation, and active exploration of its environment – what is effectively RL.\n\nAdvancement in multimodal AI. Multimodal AI is poised to become the next major driver of compute demand. Though adoption is still in its early stages, it is scaling rapidly. OpenAI’s Sora captured captivated the industry when it was released earlier this year. Shortly after, startups like Runway, Luma, Pika Labs, and Genmo have come up with their own text-to-video models. However, usage so far has been mostly limited to the prosumer market as the models lack the robustness required for the enterprise. On the other hand, voice/text-to-speech applications are beginning to gain significant adoption among enterprises. These include applications such as content creation and customer support. Elevenlabs, one of the leaders in this space, is rumored to be approaching $100 million in revenue run-rate. Another public company, Soundhound, (Translink portfolio company) is forecasted to generate $155-175 million in 2025.\n\nRise of specialized models. As general-purpose models get bigger, we expect a parallel rise in specialized models. These models may not always be text-based. Some of the more interesting areas include:\n\nTime-series models – Foundational models for time-series analysis, like Amazon Chronos, treat time-series data as a \"language\" that can be modeled using a transformer architecture. One of the key strengths of time-series models is their ability to apply transfer learning – leveraging diverse datasets from different domains to improve generalizability. We believe foundational models for time series forecasting represents an exciting frontier yet to be fully explored. The main challenge, however, lies in obtaining sufficient training data. Unlike general purpose LLMs where one can rely on the public web for pretraining data, most time-series data is probably locked up within enterprises.\n\nPhysics/world models – By definition, large language models have limited inductive biases – and hence – limited understanding of real-world physics. To address this, companies like World Labs are developing models that can understand and interact with the 3D physical world, similar to human spatial intelligence. This is important in areas like robotics. This undertaking is particularly important but challenging, as we believe the world of physics operates in a far higher-dimensional space compared to the world of language. Consider a seemingly simple action like dropping a ball from a 100-meter building: while gravity (9.8 m/s²) is a known fundamental factor, many other elements such as the ball's weight, air resistance (elevation, humidity, etc.), wind, etc. come into play. Accounting for these stochastic factors makes understanding and simulating the physical world exceptionally challenging.\n\nAnother example of a world model is Microsoft’s Aurora model - a foundational model for the atmosphere pre-trained on over 1M hours of weather and climate data. In under a minute, Aurora produces 5-day global air pollution predictions and 10-day high-resolution weather forecasts that outperform state-of-the-art classical simulation tools. While its most obvious impact is better weather forecasting, there are more potential applications such as insurance and risk assessment, financial trading, and agriculture management.\n\nBiology models - Biology-focused foundational models, such as AlphaFold 3, represent a groundbreaking advancement in structural biology. These models can accurately predict the joint structures of complex biological molecules, including proteins, nucleic acids, and small molecules, enabling scientists to generate entirely novel protein sequences. Before the advent of models like AlphaFold, determining a single protein structure could take a PhD student the entirety of their research years—typically 4-5 years. In contrast, AlphaFold has predicted over 200 million protein structures in under a year, an achievement that has fundamentally transformed our ability to understand the building blocks of life. As these models evolve, they’ll revolutionize fields such as drug discovery and personalized medicine. For further readings on this topic, please see this Forbes article by Rob Towes from Radical Ventures.\n\nLonger-term, foundational models can be used to conduct scientific research and discover new knowledge. One example of this potential is “The AI Scientist” developed by Tokyo-based Sakana AI. The AI Scientist is a comprehensive system for automating scientific discovery. It automates the entire research lifecycle, from generating novel research ideas, writing and necessary code, executing experiments and presenting the finding in an academic report. Just as AI is starting to learn coding and generate software by itself, we believe AI’s emergent capabilities will expand to the broader domain of scientific knowledge and discovery. Such advances could fundamentally reshape the trajectory of human progress by accelerating the pace of discovery across multiple scientific disciplines.\n\nIII. Noteworthy Startups:\n\nFoundational models / Research labs:\n\n01.AI, Anthropic, Deepseek, H Company, Imbue, Minimax AI, Mistral, Moonshot AI, OpenAI, Reka AI, Safe SuperIntelligence, Sakana AI*, Stability AI, xAI, Zhipu\n\nSmall Language Models (SLMs):\n\nArcee.ai, Bespoke labs, Nexa AI, Predibase\n\nMulti-modal (image and video):\n\nBlack Forest Labs, Genmo, Higgsfield, Luma AI, Midjourney, Pika labs, Runway ML, Stability AI, Tavus, Twelve Labs\n\nMultimodal (voice):\n\nAssemblyAI, DeepL, Deepgram, Elevenlabs, PlayHT, Poly AI, Resemble AI, Suno, Symbl.ai, Udio\n\nSpecialized foundational models:\n\nArchetype AI, Cradle, CuspAI, EvolutionaryScale, Formation Bio, Generate:Biomedicines, Hume AI, Illoca, Luminance, Nabla Bio, Orbital Materials, Pantheon AI, Physical Intelligence, Silurian AI, Synthefy, World Labs\n\n* Denotes Translink Capital Portfolio Company\n\nTOOLING LAYER\n\nI. Key Takeaways:\n\nRAG is the dominant technique approach that most companies use today. According to a recent report by Menlo VC, retrieval-augmented generation (RAG) adoption has increased to 51%, up from 31% last year. Meanwhile, fine-tuning remains uncommon, with only 9% of production models being fine-tuned. As underlying foundational model continue to improve, we anticipate this trend will continue to favor RAG.\n\nRAG offers significant challenges but also with its own set of challenges. The main advantage of RAG lies in separating the model's reasoning plane from the data plane, allowing responses to be grounded in live, real-world data and minimizing the risk of hallucinations. However, RAG still faces issues such as a lack of domain-specific knowledge and insufficient context, which can lead to less accurate retrieval. Optimizing chunking and retrieval remains more art than science at this point, with the last mile proving especially difficult to get right.\n\nMany are combining deterministic-based structures with RAG to help improve performance. To address some of these challenges, many companies are incorporating deterministic structures or ontologies alongside RAG to enhance performance. Knowledge graphs, for example, add a layer of structured semantic relationships between data points, making it easier to retrieve precise and relevant information for a given query. This stands in contrast to traditional RAG, where similarity is often based solely on distance between data points, which lacks deeper semantic understanding. Incorporating these structures can improve the overall accuracy of the retrieval, important particularly in domains where accuracy is critical such as healthcare, financial services, and legal.\n\nMore enterprises are opting to build solutions in-house. The Menlo report shows that nearly half of all GenAI solutions are now developed internally, up from just 20% last year. One of the most common reasons we’ve heard for why enterprises opt for in house is because they’re reluctant to give up their data to third parties. Another is the desire for greater customization to meet specific business needs. With open-source models closing the gap against closed-source models, we expect the trend towards in-house to continue.\n\nThe biggest bottleneck for enterprises is often the data curation and preparation stage. It is well known by now that AI teams spend most of their time on data preparation, with a minority of time for actual model development and deployment. Most of the data available to organizations is in the form of unstructured data, which makes up ~80% of total data today. This can be in the form of emails, documents, contracts, websites, social media, logs, etc. Transforming the data into a useable format for machine learning deployment requires extensive cleaning and standardization. Once the data is collected and cleaned, it must be then vectorized, typically by leveraging a vector database. These steps are not trivial and require deep technical and domain expertise.\n\nMonetization in the tooling layer can be challenging at times. Monetization is often difficult due to intense competition, the availability of open-source alternatives, and established players entering the space. For instance, although Pinecone is widely regarded as the leader in vector databases, it faces competition from open-source projects like Milvus (Zilliz), Weaviate, Chroma, and Qdrant. Additionally, major database companies such as MongoDB and Elastic have all introduced their own vector search capabilities, adding further competitive pressure.\n\nIn addition, cloud providers are also in this space. Products like AWS Sagemaker, Azure Machine Learning, and Google Vertex AI all offer fully managed service offerings. These end-to-end solutions are a one-stop shop for building, training, and deploying ML models.\n\nInference optimization has been a popular but highly competitive space. In the past year, we’ve seen four inference optimization players get acquired by larger companies – Run:ai, Deci, and OctoAI by Nvidia, and Neural Magic by Red Hat. Deci’s investors likely fared well (acquisition price of $300m against $57m total raised), whereas OctoAI’s investors likely saw limited returns (acquisition price of $250m against $133m total raised).\n\nOther notable players in this space include BentoML, Baseten, Fireworks, Lamini, and Together AI. Some of these companies have opted for a more integrated approach by acquiring GPUs themselves and offering a more end-to-end solution. It remains to be seen if these companies can thrive as a standalone public company, or if they will ultimately be acquired, much like their peers.\n\nII. Future Trends to Watch:\n\nEvaluation remains an important but unsolved problem in GenAI. To draw an analogy, consider credit scoring, where credit agencies like Experian, Equifax, and TransUnion assess human creditworthiness. Evaluating creditworthiness is relatively straightforward because the factors involved in determining credit worthiness are well-defined, and human financial behavior is broadly similar. This makes it possible to create standardized metrics.\n\nIn contrast to credit scoring, evaluating LLMs is significantly more complex because their applications – such as Q&A, summarization, code generation, and creative writing – are diverse and often industry-specific. Healthcare, for instance, requires different evaluation metrics compared to legal. Consequently, there is no \"one-size-fits-all\" metric that can effectively assess LLM performance across all contexts, unlike the standardized approach used by credit agencies. Additionally, unlike credit scoring, which is based on objective metrics, LLM evaluation involves subjective factors like creativity and ingenuity, which are harder to quantify.\n\nStartups and AI labs are actively working to address the evaluation challenge. Some startups like Braintrust are trying to build a more domain-agnostic, end-to-end platform equipped with automated evals. OpenAI also recently released SimpleQA, which is a simple evaluation model to check on factuality of the response. Despite these efforts, no universally accepted framework has yet been established for evaluating LLMs effectively.\n\nThe future of AI tooling will likely revolve around agent. The future of AI tooling is likely to revolve around agents, but building the foundational scaffolding to support a world full of agents is the first step. Take e-commerce agents for example – these are agents that can autonomously make purchases on your behalf one day. Enabling such an agent to work is far more complex than just imbuing it with search and reasoning capabilities. It requires a robust supporting infrastructure: How do agents securely provide credentials? How can one ensure proper authentication to verify that an agent is acting legitimately on a person’s behalf? What new payment system/rail is needed to facilitate agent transactions? We believe the supporting infrastructure around agents, rather than the agents themselves, will be the biggest bottleneck to widespread adoption.\n\nAgent-to-agent collaboration will also take center stage in the future. Just as we have systems and frameworks to evaluate human-to-human interactions, we will need similar ones to manage agent-to-agent interactions. An agent orchestration layer is needed to coordinate agent-to-agent communication. It can also be responsible for assessing agent performance, setting guardrails, and ensuring compliance with their intended mandates. As agents gain more autonomy, ensuring data security and privacy will become increasingly important. New standards and regulations will be needed to govern agent behavior, much like GDPR and similar frameworks that guide human data usage. (see diagram below for one example of an agent framework)\n\nFinally, for agents to succeed at scale, we must develop effective feedback and learning mechanisms. Agents will need to learn from successes and failures to continuously improve. This will require a robust feedback system that enables agents to improve dynamically while maintaining safety and compliance.\n\nWe believe the infrastructure scaffolding will need to be developed first before agents can truly proliferate. The orchestration layer will likely be owned by a CSP or a well-funded startup like Emergence AI. Smaller startups can carve out niches by providing ancillary tools like performance monitoring or security, which will feed into this orchestration layer.\n\nIII. Noteworthy Startups:\n\nData:\n\nCleanlabs, DatalogyAI, Hugging Face, Marqo, , Scale AI, Shakudo, Snorkel AI, SuperAnnotate, Unstructured.io, Weka, Zillis\n\nRAG / Model Customization:\n\nCohere, Contextual AI, Upstage AI, Vectara\n\nVectorDBs / Embedding Models:\n\nChroma, Milvus, Pinecone, Qdrant, Voyage AI, Weaviate\n\nModel Serving:\n\nAnyscale, Baseten, BentoML, CentML, Clika AI, Fireworks, Lamini, Lightning AI, Modular, OpenPipe, Replicate, TensorOpera, Together AI\n\nEvaluation and Observability:\n\nArize AI, Braintrust, DynamoAI, Fiddler AI, Galileo, Galileo AI, Observe, Weights and Biases, WhyLabs\n\nSecurity:\n\nCalypsoAI, Grey Swan, Hidden Layer, Protect AI, Robust Intelligence (acquired by Cisco), Troj.ai\n\nAgent orchestration/tooling:\n\nEmergence AI, Langchain, MemGPT, Tiny Fish, UnifyApps\n\nAPPLICATION LAYER\n\nI. Key Takeaways:\n\nGenAI adoption is growing rapidly. Menlo estimates that $4.6 billion has been spent on generative AI applications in 2024, an almost 8x increase from $600 million last year. We expect growth to continue next year as many current projects in POC stage convert to full deployment. This make sense given that when ChatGPT launched in November 2022, most of the corporate budgets for 2023 were already finalized. Therefore, 2024 has been the first year where companies truly had budgets for AI experimentation. Looking ahead, we anticipate AI-related budgets to expand further in 2025 and beyond. A KPMG survey of over 225 C-suite executives revealed that 83% of respondents plan to increase their investments in GenAI over the next three years.\n\nPopular use cases of GenAI right now include code generation, support chatbots/agent, and search:\n\nCode Generation – During Google's latest earnings call, it was revealed that 25% of the company's code is now generated by AI. Microsoft GitHub Copilot, the leading player in this space, has reached ~$300 million ARR and now accounts for 40% of Github’s revenue growth. Several startups, like Cursor, Poolside, Codeium, and Cognition have also entered the market, collectively raising over $1 billion to challenge GitHub’s dominance. While some startups have had impressive traction (~$50m+ ARR), most remain in the early stages of commercialization.\n\nSearch – Perplexity has emerged as one of the most recognized startups in GenAI, and have been innovating quickly with features such as Perplexity Finance and Perplexity Shopping Assistant. However, competition is expected to intensify as OpenAI has ChatGPT Search and Meta introduces its own AI-powered search engine. Perplexity’s rumored $50 million ARR is an impressive milestone for a startup of its age, but still represents only 0.025% of Google’s $200 billion search revenue, highlighting both the immense scale of competition and the significant growth potential ahead. It remains to be seen if Perplexity’s push into advertising can help it further chip away at Google’s share in search.\n\nAgents – Agent startups have been gaining traction, particularly those in customer support and sales & marketing. Companies such as Sierra, Maven AGI, and Ema are targeting enterprises, while others like 11x, Artisan, and StyleAI focus more on SMB and mid-market. Larger players are also entering the space, typically with a more horizontal platform approach: Google has Vertex AI Agent Builder, Microsoft has Copilot Studio Agent Builder, Amazon has Amazon Bedrock, and Salesforce has Agentforce. Currently, the agents we’ve seen can handle only relatively simple tasks, but as foundational models improve, especially with advanced reasoning capabilities like those in OpenAI’s o1 model, we expect them to become more capable.\n\nGenAI is transforming SaaS business models. Tanay Jaipuria from Wing Capital noted in his recent article that GenAI is leading to companies adopting more usage-based pricing. For example, Salesforce is charging $2 per conversation for their Agentforce agent while intercom is charging $0.99 per ticket resolution. Just as SaaS revolutionized the software industry by combining technological innovation with a business model innovation (shift from license & maintenance to recurring subscriptions), we believe AI too has the potential to drive another wave of business model innovation.\n\nEnterprises realizing real ROI from GenAI. AI is delivering tangible value for enterprises. For instance, Klarna reported that its AI-powered agents performed the equivalent work of 700 customer service representatives. Despite a 27% increase in revenue, Klarna reduced its headcount from 5,000 to 3,800, with plans to cut further to 2,000. These reductions were driven not by declining growth but by efficiency gains from AI. If similar productivity improvements (~5%) were replicated across the Global 2000, which collectively generates ~$52 trillion in revenue, the potential value creation could reach $2.5 trillion.\n\nUltimately, investing in AI application companies is not that different from traditional software When investing in AI application startups, a crucial question we ask ourselves is whether improvements to foundational models will leave the startup better or worse off. Simple wrapper companies that rely mostly on the strength of underlying models are likely to lose out; however, we believe those that begin with a deep understanding of the customer journey and associated workflows are likely to benefit. It's akin to building a car: focus on developing the surrounding components first - chassis, interior, software, etc. - and simply swap out the engine as better ones become available, rather than attempting to build out the engine yourself.\n\nUltimately, the key differentiator for an AI-native application company is no different from that of a traditional SaaS application company – it’s about truly understanding the pain points of your users, meeting them where they are, and delivering a delightful experience for them. The AI is just an enabler, not the differentiator.\n\nThe robotics space is seeing renewed interest, fueled by the potential to develop foundational models similar to LLMs for robotics. The latest generation of robotics startups is moving away from heuristic and rule-based programming, focusing instead on end-to-end neural networks. Telsa’s latest FSD software is an example of an end-to-end neural network, relying mostly on vision and data rather than explicitly coded controls.\n\nHowever, robotics continues to face a significant data bottleneck, and various techniques are being explored to address this challenge.\n\nWhile imitation learning and teleoperations offer high-quality data, they may not be scalable on their own. Recently, the use of videos and simulations for training has emerged as another promising avenue, with Nvidia Issac Sim and some startups working on this. Conceptually, Google’s RT-2 model demonstrated the potential for generalized robotics performance by leveraging a large model trained on internet-scale vision and language data and fine-tuning it with a smaller set of robotics data.\n\nThe primary challenge in simulation lies in creating realistic representations of the ground truth to minimize the sim2real gap. This is particularly difficult because robots come in diverse embodiments and form factors, making data collection and standardization challenging. Ultimately, we believe no single method is likely to solve all these challenges; an ensemble of techniques involving teleoperations, simulation, and video will be required to make it work. For further reading on this topic, see this excellent article by Spectrum.ieee.\n\nII. Future Trends To Watch:\n\n“Services-as-a-software” – AI presents an opportunity to disrupt not just the $400 billion global SaaS market but also the $4 trillion services market. Many jobs today involve repetitive tasks, making them ideal candidates for AI-driven automation. Considering the sheer scale of these markets, the potential impact is clear:\n\nIn total, this represents ~120 million workers and almost $4 trillion worth of salaries. For a more detailed analysis on this topic, see this article from Joanne Chen and Jaya Gupta from Foundation Capital.\n\n“Computer Use” is a major inflection point – Anthropic recently introduced “computer use”, which allows developers to direct Claude to use computer in an agentic manner - clicking buttons, typing text, etc. Claude looks at screenshots of what’s visible to the user, then calculates how many pixels it needs to move the cursor to click on the correct location. (For an in-depth case study of computer use, see this paper)\n\nBroadly speaking, there are two approaches to enabling AI agents to perform tasks. The first is API-based, where tasks are broken down into subtasks and executed by chaining together API calls. The second, as seen with Anthropic’s “computer use” is the UI-based approach, which leverages vision and reinforcement learning to interact directly with browsers to execute tasks. In other words, it teaches the model to use computers the way humans do. While the latter approach is theoretically simpler in its lower-level end-to-end approach, it requires more training data and may be computationally more expensive. The end state may indeed be vision-based, but for now, a hybrid approach combining both methods may be required to optimize performance and cost. This is akin to what happened in the full self-driving (FSD) world where end-to-end neural nets relying solely on vision slowly replaced rule-based controls.\n\nComputer use is a significant breakthrough for robotic process automation (RPA). Traditional RPA tools have traditionally faced challenges due to its fragility, as workflows often broke when interfaces changed, requiring constant maintenance. With Anthropic’s computer use, AI models can now adapt to diverse interfaces, reducing the dependency on hard-coded scripts. This breakthrough has already had an impact: UiPath quickly integrated Claude 3.5 Sonnet into three of its key products following Anthropic’s announcement of computer use. This swift adoption underscores how transformative computer use could be in driving the next wave of RPA and intelligent automation. However, we think it’s still early - to use the FSD analogy again, we believe intelligent automation is still in the L1/L2 phase. For a more detailed discussion on RPA, check out this article by Kimberly Tan from A16z.\n\nNew hardware form factors. This year has seen the rise of novel hardware form factors aimed at complementing or even replacing smartphones. Despite the excitement, success has been limited so far. For instance, Humane’s AI-powered wearable pin has generated just over $9 million in lifetime sales, despite raising $200 million in funding. Worse, daily return rates are reportedly outpacing new sales. Similarly, the Rabbit R1 has faced extremely poor reviews.\n\nThe one notable success story has been the second-generation Meta + Ray-Ban smart glasses. These devices have outsold the previous generation’s two-year sales figures within just a few months, earning generally positive reviews. Meanwhile, major research labs and tech incumbents are also exploring this area. OpenAI, for example, recently hired Caitlin Kalinowski, a former AR hardware leader at Meta, to oversee their robotics and consumer hardware. In addition, Apple design icon Jony Ive has partnered with Sam Altman on a new AI hardware project. The potential to combine large GenAI with new hardware form factors represents an exciting frontier.\n\nModels with enhanced inference-time reasoning capabilities can tackle increasingly complex scientific challenges. Some of the most promising opportunities in this space lie in drug discovery/biomedicine, material sciences, and physics/robotics. In a major nod to open-source, Google DeepMind recently released the code and weights for AlphaFold 3. This surprise announcement followed just weeks after the system’s creators, Demis Hassabis and John Jumper, were awarded the 2024 Nobel Prize in Chemistry for their contributions.\n\nIn the scientific domain, startups may pursue various monetization paths: Some choose to offer their tools as SaaS platforms, others pursue a licensing model, and some may even act as principal agents, directly bringing their solutions to market to capture a greater portion of the TAM.\n\nJust as SaaS evolved from horizontal to vertical solutions, we anticipate a similar transition in the AI space. Early in a market's lifecycle, horizontal tools gain traction quickly due to their broad appeal to a large market. However, as the market matures and competition increases, startups often move towards specialized, vertical or domain-specific solutions to differentiate themselves. In AI, this shift to verticalization appears to be happening more quickly than in SaaS for several key reasons:\n\nAI thrives on domain-specific data: AI performs best when trained on data specific to an industry or use case. Many industries have highly domain-specific data sets, making specialized or verticalized training more effective. For instance, in RAG, understanding domain context is critical for retrieval accuracy.\n\nCrowded horizontal market: Unlike the early days of SaaS, many well-established incumbents are already heavily investing in GenAI and launching horizontal solutions. These incumbents are typically already the system of record for whatever use case they are targeting, be it Salesforce for CRM, SAP for ERP, or others. This gives incumbents a substantial edge in distribution and integration. For startups, targeting verticalized or specialized markets may allow them to carve out a more defensible position that can lead to a better chance of success.\n\nRegulatory stringency in key industries: Regulated industries like healthcare, legal, and finance have stringent regulatory requirements. These requirements are most effectively met through verticalized approaches tailored to the unique requirements of each industry.\n\nGenAI consumer companies have yet to breakout. Many of the leading AI consumer startups ended up being acquired this year. For example, we observed two acquihires in the consumer GenAI space – Google’s deal with Character.ai and Microsoft’s deal with Inflection. We believe consumer GenAI applications have yet to break out due to two primary reasons:\n\nFirst, there hasn’t been a killer consumer use case beyond chatbots like ChatGPT and Perplexity. While Character.ai has arguably achieved product-market fit, its relatively narrow demographic appeal—over half of its users are aged 18 to 24—limits its broader potential. We believe the next transformative consumer application will be a highly capable personal assistant (a smarter Siri), with the longer-term vision being personalized digital twins for everyone.\n\nSecond, successful consumer applications often require viral adoption in the beginning, sometimes driven by upfront usage subsidies by the company. However, the current cost of tokens – especially for multimodal models – remains too high for such mass subsidization to be economically feasible. As token costs decrease and/or more workloads move to the edge, we anticipate a new wave of GenAI consumer companies emerging.\n\nIII. Noteworthy Startups:\n\nDeveloper / Code Gen:\n\nAugment Code, CodeComplete, Codeium, Cognition, Cursor, Magic.dev, Poolside, TabbyML, Tabnine, Tessl\n\nEnterprise Productivity:\n\nConsensus, Dust.ai, Exa, Fireflies.ai, Glean, Highlight, Mem, Otter.ai, Read.ai, Taskade, Wokelo AI\n\nConsumer:\n\nGenspark, MultiOn, Liner, Ninjatech.ai, Perplexity, Simple AI, You.com\n\nMulti-modal:\n\nBlack Forest Labs, Captions, Coactive, Creatify, Deepbrain, Descript, Heygen, Ideogram, Luma, Openart.ai, Opus Clip, PhotoRoom, Runway, Synthesia, Viggle AI\n\nNext-gen RPA:\n\nAutomat, Caddi, HappyRobot, Orby, Sola, Tektonic AI\n\nGeneral Robotics:\n\nANYbotics, Bright Machines, Field AI, Hillbot, Path Robotics, Physical Intelligence, Skild AI, Swiss-Miles, World labs\n\nHumanoid Robotics:\n\n1x Technologies, Figure AI\n\nGeneral Agents/Copilots:\n\nDeepOpinion, Ema, Factory AI, Gumloop, Jasper, Lyzr, Relevance AI, Sierra, SquidAI, Stack AI, Tektonic AI, Wordware, Writer\n\nHR/Recruiting:\n\nConverzAI, Eightfold, Jobright.ai, Mercor, Micro1, Moonhub\n\nCustomer Support:\n\nAptEdge, Cresta, Decagon, Maven AGI\n\nSales and Marketing:\n\n11x, Adsgency, Artisan AI, Bounti.ai, Connectly AI, Typeface, StyleAI, Mutiny, Nectar AI, Nooks, Omneky, Rox, Simplified\n\nProduct Design and Engineering:\n\nAmbr, Skippr, Uizard, Vizcom\n\nChip Design:\n\nAstrus, Mooreslab\n\nPresentation Editing:\n\nBeautiful.ai, Gamma, Tome\n\nVertical Specific- Healthcare:\n\nAbridge, Ambience Healthcare, Atropos Health, Cair Health*, Hippocratic AI, Hyro, Nabla, Scribenote, Segmed.ai, Slingshot AI, Suki AI, Tennr\n\nVertical Specific - Finance and Procurement:\n\nAskLio*, Auditoria.ai, Finpilot, Hebbia, Klarity, Kipoparts, Linq Alpha, Menos AI, Rogo, Spine AI*\n\nVertical Specific - Legal:\n\nCasetext (Thomson Reuters), Cicero, EvenUp, Genie AI, Harvey AI, Leya, Robin AI, Solomon AI, Solve Intelligence*, Spellbook/Rally\n\nVertical Specific - Education and Language:\n\nElsa, Eureka Labs, MagicSchool AI, Pace AI, Praktika, Riiid, Sana, Speak, Uplimit*\n\nVertical Specific - Gaming and Entertainment:\n\nAltera, Inworld AI\n\nVertical Specific - Compliance:\n\nGreenlite, Norm AI\n\nVertical Specific - Real Estate:\n\nElise AI\n\nVertical Specific - Mobility:\n\nCarvis.AI, Revv\n\n*Denotes Translink Capital Portfolio Company\n\nAI INVESTMENTS AND M&A\n\nAI Investments:\n\nYear-to-date (YTD) AI investments have exceeded $60 billion, now representing more than one-third of all venture funding. The largest rounds continue to be in the infrastructure and model layers, with major raises from OpenAI ($6.6B), xAI ($5B), Anthropic ($4B), SSI ($1B), and CoreWeave ($1B). The growing compute demands remain a key driver, with OpenAI alone expected to spend $3 billion on training compute this year.\n\nSome application companies, particularly those in the code generation space, have also raised substantial rounds as they pre-train their own code generation models. Other areas that received significant investor attention include AI chips, AI clouds, robotics foundational model, and enterprise AI. For a full list of AI startups that have raised more than $100 million this year, check out this article by TechCrunch.\n\nInvestor demand for AI startups remains strong, with many companies securing significant funding early in their lifecycle. Startups like SSI and World Labs have already reached unicorn status, driven by the strong pedigree of their founding teams. While overall AI valuations remain high –with an average revenue multiple of 26x – investors have become more selective due to the sheer volume of new startups building in this space. For instance, AI startups represented 75% of the most recent YC Summer batch.\n\nStrategic investors like Nvidia and the CSPs continue to show strong interest in AI startups, driving up overall valuations and increasing competition for financial VCs. CSPs are sitting on all time cash balances that need to be reinvested back into growth. For example, Amazon just announced another $4 billion investment in Anthropic.\n\nM&A:\n\nRise of “reverse acquihires”. This year witnessed the rise of “reverse acquihires” where an incumbent hires a substantial portion of a startup’s team and sometimes licenses its technology, bypassing the complexities of a full acquisition. This strategy allows major tech companies to bolster their AI capabilities while avoiding regulatory scrutiny. Key examples included:\n\nMicrosoft / Inflection AI – Microsoft recruited key personnel from Inflection AI, most notably CEO Mustafa Suleyman, who now oversees Microsoft’s entire AI portfolio, including Copilot, Bing, and Edge, and reports directly to Nadella. Given the scope of Suleyman’s role, the price tag may have been worth it.\n\nAmazon / Adept AI – Amazon acquired two-thirds of Adept AI’s workforce, including CEO David Luan, and secured a non-exclusive license to the company’s foundation model. The company received $25 million for the licensing deal, while their investors, who put $414 million into the company, will roughly recoup their investment.\n\nGoogle / Character.ai – With the acquisition, Google brought on CEO Noam Shazeer, President Daniel De Freitas, and about 30 of Character.ai’s 130 employees in a deal reportedly valued at $2.7 billion—more than 2.5x the company’s last valuation.\n\nTotal M&A activity (estimated to be ~$2-3 billion) was relatively subdued this year. This was similar to last year where there were only three major acquisitions (MosaicML, CaseText, Neeva). Interestingly, 5 of the 8 notable acquisitions this year were in the tooling layer, and 4 of them were in the inference optimization space (OctoAI, Deci, Run:ai, Neural Magic). Notable acquisitions include:\n\nSnowflake’s acquisition of Datavolo (undisclosed)\n\nRed Hat’s acquisition of Neural Magic (undisclosed)\n\nNvidia’s acquisition of OctoAI for ~$250m\n\nNvidia’s acquisition of Deci for ~$300m\n\nNvidia’s acquisition of Run:ai for ~$700m\n\nDocusign’s acquisition of Lexion for ~$165m\n\nCisco’s acquisition of Robust Intelligence (undisclosed)\n\nCanva’s acquisition of Leonardo.ai (undisclosed)\n\nIn the short term, we may seem limited M&A given difference of expectation between acquirers and startups. Despite strong interest from acquirers, M&A activity may remain limited due to a disconnect in valuation expectations. A corporate development executive at a leading SaaS company highlighted that while they were interested in acquiring AI startups, there was often a significant valuation gap. Incumbents acquirers believe they should receive a discount due to their existing customer base and distributional advantages, while AI startups and their investors expected a premium, driven in part by the high market valuations. This mismatch in expectations may continue to dampen M&A activity in the near term.\n\nM&A consolidation will likely happen towards the end of the cycle. In the telecom/internet era, most M&A transactions occurred in the second part of the decade. In 1999 alone, there was over $3 trillion dollars’ worth of M&A transactions. Typically, consolidation happens when the economics become more mature and the winners are largely determined already. The lack of significant consolidation in AI today is another indicator that we are still in the early stages.\n\nOTHER AI TRENDS: SOVERIGN AI, COPYRIGHT, AND REGULATIONS\n\nSovereign AI:\n\nAs AI proliferates, the concept of Sovereign AI is drawing increasing attention. A central concern for many governments is whether they are comfortable with sensitive data being processed on platforms like ChatGPT, which are controlled by other countries. The broader geopolitical rift is increasingly mirrored within the microcosm of the AI world, resulting in the emergence of separate AI ecosystems across different regions.\n\nA key consideration is: at which layers of the AI stack will sovereign AI emerge? Current developments suggest it will primarily manifest in the infrastructure and model layers. For investors, this presents unique opportunities to back region-specific startups as distinct ecosystems arise globally. Success may not require backing the global leader; instead, regional champions could thrive in local markets as well.\n\nLet’s briefly review some of the AI developments and strategies across major regions:\n\nUS: The U.S. continues to lead innovation in GenAI across all layers of the stack. Major AI labs like OpenAI, Anthropic, and Meta dominate advancements, thanks to a deep talent pool and world-class academic institutions. On the infrastructure side, U.S. hyperscalers provide unmatched compute power, while Nvidia maintains its lead in hardware. This integrated ecosystem gives the U.S. a considerable advantage in the short to medium term.\n\nChina: In response to semiconductor export controls, China is prioritizing its domestic chip industry. In May, the government announced a $47.5 billion state semiconductor investment fund to bolster its chip industry. Although hardware lags behind, Chinese LLMs, such as Alibaba's Qwen and DeepSeek, remain highly competitive. Surprisingly, China leads in generative AI adoption, with 83% of companies testing or implementing the technology – surpassing the U.S. (65%) and the global average (54%).\n\nEurope: Europe’s stringent regulations, like the EU AI Act, may stifle AI innovation. Regulations have already caused U.S. tech giants, including Meta and X, to delay AI rollouts in the region. Apple, too, opted not to launch Apple Intelligence on its latest iPhone in Europe for similar reasons. While Europe boasts standout labs like Mistral AI, their ability to compete independently with U.S. CSPs and AI labs remains unclear for now, especially in light of the regulatory handicap.\n\nJapan: Japan is experiencing significant growth in data center infrastructure. Oracle recently announced an $8 billion investment in new data centers, following Microsoft’s $3 billion commitment. At the model layer, Sakana AI has emerged as a key player, recently closing a $200 million Series A round (Translink is an investor). Japan’s government is fostering AI innovation with light regulations and strong support, recognizing AI as a critical technology to address its aging population and growing need for automation. The large market opportunity combined with government support positions Japan as a promising market for AI innovation and startups.\n\nAI and Copyright:\n\n· The intersection of AI and copyright law is becoming a critical issue as genAI content becomes more pervasive. Recent controversies, such as claims that Apple, Nvidia, and Anthropic used YouTube videos without permission to train AI models, have highlighted concerns of intellectual property violations. Similarly, major record labels like Universal, Sony, and Warner are suing AI startups such as Suno and Udio for using copyrighted music to generate content. These disputes highlight the tension between innovation and the safeguarding of creative assets.\n\nThe news industry is also grappling with these challenges. News Corp recently sued Perplexity AI over alleged false attributions and hallucinations tied to its publications, while simultaneously striking a $250 million partnership with OpenAI to provide access to its archives. These contrasting moves reflect both the risks and opportunities AI presents to traditional media.\n\nDespite these disputes, some platforms are leveraging AI to enhance creativity and compliance. For example, Spotify is using generative AI to personalize user experiences while adhering to copyright laws. This suggests that AI and intellectual property can coexist, provided there are clear frameworks.\n\nSome companies are working to address this issue. An interesting startup is Tollbit which is helping to bridge the gap between publishers and AI developers by enabling publishers to monetize their content. Another one is ProRata.ai which is working on attribution technology to enable fair compensation to content owners. Striking a balance between innovation and protecting creators' rights will be a crucial topic in the years ahead.\n\nAI Regulations:\n\nThe EU AI Act represents the first comprehensive framework of its kind, categorizing AI systems by risk levels: unacceptable, high, limited, and minimal. Systems considered \"unacceptable risk,\" like social scoring by governments or manipulative AI, are banned, while \"high-risk\" systems in critical sectors face stringent requirements.\n\nKey provisions include mandatory compliance assessments, robust documentation, and oversight by a European AI Board. Non-compliance could result in fines of up to €30 million or 6% of global revenue.\n\nFollowing his re-election in November 2024, President Donald Trump announced plans to repeal Biden's AI executive order upon taking office. The Trump administration is arguing that the existing regulations stifle innovation and impose unnecessary burdens on businesses. Instead, they advocate for a more industry-friendly approach, emphasizing voluntary guidelines and reduced federal oversight to promote AI development.\n\nCONCLUSION\n\nIf you’ve made it all the way through to here – let me be the first to congratulate you. I know that was a lot to digest. If I had to distill everything into five key takeaways, they would be:\n\nThe entire infrastructure stack is undergoing a significant overhaul, reminiscent of the internet and cloud buildouts. The demand for inference is only beginning to accelerate and will be driven by increasing adoption of GenAI, new multimodal applications, and evolving model architectures.\n\nAs scaling laws begin to plateau, model development is shifting away from large pre-training datasets towards inference-time reasoning. This shift enables models to tackle more complex reasoning tasks. Concurrently, the rise of smaller, specialized SLMs promises greater efficiency and flexibility for users.\n\nFor the first time, AI is delivering tangible ROI in enterprise settings, with use cases like code generation, customer support, and search driving measurable impact. The next frontier lies in the proliferation of AI agents, but their true potential will only be realized after we build the underlying scaffolding required to enable multi-agent interactions.\n\nInvestment in AI continues to grow, particularly in the infrastructure and foundational model layers. Most exits will be through M&A but high investor expectations could clash with market realities, potentially impacting future valuations.\n\nIt’s been only 24 months since ChatGPT took the world by storm – an event Jensen Huang of Nvidia aptly described as the \"iPhone moment\" for AI. In this short time, we've witnessed some of the fastest innovation in modern history. Massive infrastructure investments, daily breakthroughs in foundational models, and an insatiable appetite for enterprise adoption have converged to reshape not only technology but the very way our society operates.\n\nAs we move into 2025, one thing is clear: this is only the beginning. There’s still so much left to build and discover. If history has taught us anything, it’s that progress is rarely linear – surprise breakthroughs will always occur alongside unexpected setbacks. For everyone living through this these pivotal years of AI – entrepreneurs, technologists, students, and investors alike – our mission is clear: to engage deeply, innovate responsibly, and create a future where humans and technology can coexist in harmony. In doing so, we leave a better world for our future generations, just as our predecessors aspired for us.\n\nShare\n\nSOURCES USED\n\n1. https://blog.google/inside-google/message-ceo/alphabet-earnings-q3-2024/#search\n\n2. https://www.forbes.com/sites/quickerbettertech/2024/03/13/klarnas-new-ai-tool-does-the-work-of-700-customer-service-reps/\n\n3. https://www.sequoiacap.com/article/ais-600b-question/\n\n4. https://www.goldmansachs.com/insights/top-of-mind/gen-ai-too-much-spend-too-little-benefit\n\n5. https://www.marketwatch.com/story/is-the-world-in-an-ai-bubble-money-managers-are-split-3967101a\n\n6. https://www.schroders.com/en-us/us/wealth-management/insights/are-we-heading-for-a-replay-of-the-1990s/\n\n7. https://kelvinmu.substack.com/p/ai-are-we-in-another-dot-com-bubble\n\n8.https://www.reddit.com/r/LocalLLaMA/comments/1gpr2p4/llms_cost_is_decreasing_by_10x_each_year_for/\n\n9. https://techcrunch.com/2024/09/12/openai-unveils-a-model-that-can-fact-check-itself/\n\n10. https://www.tomshardware.com/tech-industry/artificial-intelligence/apple-skips-nvidias-gpus-for-its-ai-models-uses-thousands-of-google-tpus-instead\n\n11. https://groq.com/news_press/aramco-digital-and-groq-announce-progress-in-building-the-worlds-largest-inferencing-data-center-in-saudi-arabia-following-leap-mou-signing/\n\n12. https://www.cnbc.com/2024/09/30/cerebras-files-for-ipo.html\n\n13. https://www.goldmansachs.com/insights/articles/AI-poised-to-drive-160-increase-in-power-demand\n\n14. https://www.mckinsey.com/industries/technology-media-and-telecommunications/our-insights/ai-power-expanding-data-center-capacity-to-meet-growing-demand\n\n15. https://ir.amd.com/news-events/press-releases/detail/1224/amd-reports-third-quarter-2024-financial-results\n\n16. https://www.theverge.com/2024/10/29/24282843/openai-custom-hardware-amd-nvidia-ai-chips\n\n17. https://www.tomshardware.com/tech-industry/artificial-intelligence/lenovo-says-demand-for-amds-instinct-mi300-is-record-high-plans-to-offer-ai-solutions-from-all-important-hardware-vendors#:~:text=Demand%20for%20AMD%20Instinct%20MI300,in%20an%20interview%20with%20CRN.\n\n18. https://cloud.google.com/blog/products/ai-machine-learning/introducing-cloud-tpu-v5p-and-ai-hypercomputer\n\n19. https://www.tomshardware.com/tech-industry/artificial-intelligence/china-makes-ai-breakthrough-reportedly-trains-generative-ai-model-across-multiple-data-centers-and-gpu-architectures\n\n20. https://www.alibabacloud.com/en/solutions/generative-ai/qwen?_p_lc=1\n\n21. https://www.deepseek.com/\n\n22. https://blogs.microsoft.com/blog/2020/01/16/microsoft-will-be-carbon-negative-by-2030/\n\n23. https://www.cnbc.com/2024/05/15/microsofts-carbon-emissions-have-risen-30percent-since-2020-due-to-data-center-expansion.html#:~:text=Microsoft's%20carbon%20footprint%20has%20increased,effort%20to%20address%20the%20problem.\n\n24. https://www.cnbc.com/2024/07/02/googles-carbon-emissions-surge-nearly-50percent-due-to-ai-energy-demand.html\n\n25. https://arxiv.org/abs/2206.14486\n\n26. https://azure.microsoft.com/en-us/blog/introducing-phi-3-redefining-whats-possible-with-slms/\n\n27. https://deepmind.google/technologies/gemini/flash/\n\n28. https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/\n\n29. https://arxiv.org/pdf/2409.00088\n\n30. https://openai.com/index/introducing-openai-o1-preview/\n\n31. https://medium.com/@tsunhanchiang/openai-o1-the-next-step-of-rl-training-692838a39ad4\n\n32. https://venturebeat.com/ai/deepseeks-first-reasoning-model-r1-lite-preview-turns-heads-beating-openai-o1-performance/\n\n33. https://finance.yahoo.com/news/openai-closed-funding-round-raising-161842066.html\n\n34. https://ai.meta.com/blog/llama-usage-doubled-may-through-july-2024/\n\n35. https://www.tomshardware.com/desktops/servers/first-in-depth-look-at-elon-musks-100-000-gpu-ai-cluster-xai-colossus-reveals-its-secrets\n\n36. https://www.theinformation.com/articles/openai-projections-imply-losses-tripling-to-14-billion-in-2026?rc=o9bwxt\n\n37. https://www.reuters.com/technology/microsoft-openai-planning-100-billion-data-center-project-information-reports-2024-03-29/\n\n38. https://openai.com/index/introducing-chatgpt-search/\n\n39. https://github.com/openai/swarm\n\n40. https://qwenlm.github.io/blog/qwen2.5-coder-family/\n\n41. https://techcrunch.com/2024/11/27/alibaba-releases-an-open-challenger-to-openais-o1-reasoning-model/\n\n42. https://www.reuters.com/technology/artificial-intelligence/openai-rivals-seek-new-path-smarter-ai-current-methods-hit-limitations-2024-11-11/\n\n43. https://www.theinformation.com/articles/openai-shifts-strategy-as-rate-of-gpt-ai-improvements-slows?rc=o9bwxt\n\n44. https://arxiv.org/abs/2402.15391\n\n45. https://www.amazon.science/code-and-datasets/chronos-learning-the-language-of-time-series\n\n46. https://www.microsoft.com/en-us/research/blog/introducing-aurora-the-first-large-scale-foundation-model-of-the-atmosphere/\n\n47. https://www.forbes.com/sites/robtoews/2023/07/16/the-next-frontier-for-large-language-models-is-biology/\n\n48. https://sakana.ai/ai-scientist/\n\n49. https://menlovc.com/2024-the-state-of-generative-ai-in-the-enterprise/\n\n50. https://mitsloan.mit.edu/ideas-made-to-matter/tapping-power-unstructured-data\n\n51. https://www.braintrust.dev/blog/getting-started-evals\n\n52. https://openai.com/index/introducing-simpleqa/\n\n53. https://kpmg.com/us/en/media/news/gen-ai-survey-august-2024.html\n\n54. https://www.microsoft.com/en-us/investor/events/fy-2024/earnings-fy-2024-q4\n\n55. https://www.theverge.com/2024/10/28/24282017/meta-ai-powered-search-engine-report\n\n56. https://www.statista.com/statistics/266249/advertising-revenue-of-google/\n\n57. https://www.perplexity.ai/hub/blog/why-we-re-experimenting-with-advertising\n\n58.\n\n59. https://investor.salesforce.com/press-releases/press-release-details/2024/Salesforce-Unveils-AgentforceWhat-AI-Was-Meant-to-Be/default.aspx\n\n60. https://www.intercom.com/help/en/articles/8205718-fin-ai-agent-resolutions\n\n61.https://downloads.ctfassets.net/4pxjo1vaz7xk/161qRVqB2B2N8MYwLl9x5A/abcc53df5acfac3fa4bb36d4621db99c/Klarna_Holding_AB_Interim_Report_2024__ENG_.pdf\n\n62. https://www.forbes.com/lists/global2000/\n\n63. https://electrek.co/2024/01/22/tesla-releases-fsd-v12-last-hope-self-driving/\n\n64. https://developer.nvidia.com/isaac/sim\n\n65. https://deepmind.google/discover/blog/rt-2-new-model-translates-vision-and-language-into-action/\n\n66. https://spectrum.ieee.org/solve-robotics\n\n67. https://foundationcapital.com/ai-service-as-software/\n\n68. https://www.anthropic.com/news/3-5-models-and-computer-use\n\n69. https://arxiv.org/pdf/2411.10323\n\n70. https://www.uipath.com/newsroom/uipath-integrates-anthropic-claude-language-models-for-generative-solutions\n\n71. https://a16z.com/rip-to-rpa-the-rise-of-intelligent-automation/\n\n72. https://www.theverge.com/2024/8/7/24211339/humane-ai-pin-more-daily-returns-than-sales\n\n73. https://www.theverge.com/2024/5/2/24147159/rabbit-r1-review-ai-gadget\n\n74. https://techcrunch.com/2024/10/21/metas-smart-glasses-outsell-traditional-ray-bans-in-some-stores-even-before-ai-features-roll-out/\n\n75. https://www.cnbc.com/2024/11/05/openai-hires-metas-former-orion-head-to-lead-its-robotics-efforts.html\n\n76. https://www.theverge.com/2024/9/21/24250867/jony-ive-confirms-collaboration-openai-hardware\n\n77. https://github.com/google-deepmind/alphafold3\n\n78. https://www.nature.com/articles/d41586-024-03214-7\n\n79. https://www.reuters.com/technology/artificial-intelligence/google-hires-characterai-cofounders-licenses-its-models-information-reports-2024-08-02/\n\n80. https://www.reuters.com/technology/microsoft-agreed-pay-inflection-650-mln-while-hiring-its-staff-information-2024-03-21/\n\n81. https://aventis-advisors.com/ai-valuation-multiples/\n\n82. https://techcrunch.com/2024/11/15/heres-the-full-list-of-44-us-ai-startups-that-have-raised-100m-or-more-in-2024/\n\n83. https://pitchbook.com/news/articles/y-combinator-demo-day-ai-startups-analysis\n\n84. https://www.cnbc.com/2024/11/22/amazon-to-invest-another-4-billion-in-anthropic-openais-biggest-rival.html\n\n85. https://techcrunch.com/2024/06/28/amazon-hires-founders-away-from-ai-startup-adept/\n\n86. https://investors.snowflake.com/news/news-details/2024/Snowflake-Agrees-to-Acquire-Open-Data-Integration-Platform-Datavolo/default.aspx\n\n87. https://www.redhat.com/en/about/press-releases/red-hat-acquire-neural-magic\n\n88. https://www.ai-infra-summit.com/blog/nvidia-acquires-octoai-dominate-enterprise-generative-ai-solutions\n\n89. https://www.calcalistech.com/ctechnews/article/bkj6phggr\n\n90. https://blogs.nvidia.com/blog/runai/\n\n91. https://techcrunch.com/2024/05/06/docusign-acquires-ai-powered-contract-management-firm-lexion/\n\n92. https://blogs.cisco.com/news/fortifying-the-future-of-security-for-ai-cisco-announces-intent-to-acquire-robust-intelligence\n\n93. https://www.canva.com/newsroom/news/leonardo-ai/\n\n94. https://www.latimes.com/archives/la-xpm-1999-dec-02-fi-39632-story.html\n\n95. https://www.reuters.com/technology/china-sets-up-475-bln-state-fund-boost-semiconductor-industry-2024-05-27/\n\n96. https://www.reuters.com/technology/artificial-intelligence/china-leads-world-adoption-generative-ai-survey-shows-2024-07-09/\n\n97. https://www.forbes.com/sites/kateoflahertyuk/2024/06/21/meta-just-delayed-its-eu-ai-roll-out-heres-why-it-matters/\n\n98. https://www.tomsguide.com/ai/apples-refusing-to-launch-apple-intelligence-in-the-eu-heres-why\n\n99. https://www.oracle.com/emea/news/announcement/oracle-to-invest-more-than-eight-billion-in-cloud-computing-and-ai-in-japan-2024-04-17/\n\n100. https://news.microsoft.com/apac/2024/04/10/microsoft-to-invest-us2-9-billion-in-ai-and-cloud-infrastructure-in-japan-while-boosting-the-nations-skills-research-and-cybersecurity/\n\n101. https://sakana.ai/series-a/\n\n102. https://www.engadget.com/apple-nvidia-and-anthropic-reportedly-used-youtube-transcripts-without-permission-to-train-ai-models-170827317.html\n\n103. https://www.theverge.com/2024/6/24/24184710/riaa-ai-lawsuit-suno-udio-copyright-umg-sony-warner\n\n104. https://www.theverge.com/2024/10/21/24275924/news-corp-wall-street-journal-perplexity-lawsuit-copyright-infringement\n\n105. https://venturebeat.com/ai/openai-partners-with-wall-street-journal-publisher-news-corp/\n\nABOUT THE AUTHOR\n\nKelvin is a Principal at Translink Capital focused on artificial intelligence and machine learning. He looks at opportunities across the AI stack, including in infrastructure, foundational models, tooling and applications. Prior to joining Translink, Kelvin held various roles in business operations, strategy consulting, and investment banking in the United States and Canada. Kelvin holds an MBA from the Haas School of Business at UC Berkeley and a BA from the Ivey School of Business at Western University. In his spare time, Kelvin’s an avid reader and aims to read one non-fiction book per week. He’s also an aspiring amateur golfer who is on a quest to become a self-taught scratch golfer. He writes regularly about trends in AI, technology and business.\n\nFor the latest content, please follow him at:\n\nLinkedIn: https://www.linkedin.com/in/kelvinmu/\n\nSubstack:\n\nX: https://x.com/kelvinmu87\n\nABOUT TRANSLINK CAPITAL",
      "# [Top 10 Best LLM Ops APIs in 2024 by www.linkedin.com, eddie-chayes on 2024-04-12](https://konfigthis.com/blog/llmops-apis/)\nWith the emergence of Generative AI, kick-started by OpenAI with the public launch of ChatGPT in 2022, it didn’t take long for Generative AI to penetrate numerous industries across the globe. Generative AI refers to AI algorithms which can generate media such as text, audio, or video content. A Large Language Model, or LLM, is a type of Generative AI which is trained on a massive corpus of data and can interpret and generate text. As LLMs became more widespread and companies scrambled to utilize their potential, the need arose for tools which help develop, deploy, and maintain LLM applications; this is referred to as LLM Ops. Large growth is projected for LLMs in the coming years, meaning LLM Ops will become an important part of the developer ecosystem for companies wanting to incorporate LLMs into their operations. Many LLM Ops companies offer an API to integrate their services into your company’s workflows.\n\nEven before the meteoric rise of LLMs a few years ago, MLOps (Machine Learning Operations) was a familiar term referring to tools which manage the lifecycle of ML applications. LLMOps is the natural successor to MLOps, giving developers instruments to deploy, tune, and monitor LLM applications. LLM Ops APIs give companies a broad suite of tools to evaluate LLM performance, increase observability, orchestrate complex tasks, test accuracy and bias, benchmark, deploy updates, ingest data, and more.\n\nLLM Ops APIs are the essential channel for businesses to tap into the potential of Large Language Models. By abstracting the complexities of model deployment and management, these APIs streamline development workflows, enabling rapid prototyping and deployment of Generative AI models for businesses. They also provide a standard interface for accessing a wide range of monitoring and analytics tools, allowing developers to use LLM technologies with minimal overhead. For companies wanting to integrate LLM capabilities into their enterprise, there is no substitute for the essential mechanisms provided by LLM Ops APIs.\n\nMonster API offers a single cost-efficient API abstraction over numerous Generative AI models, allowing developers to easily build ML applications for their business. Monster also provides no-code LLM fine-tuning, deployment, and scaling tools.\n\nPulze AI boasts a single API that can route between different LLM providers to optimize for maximum quality and minimal cost. The company also offers a suite of management and tuning implements, allowing you to upload data, customize and fine-tune prompts, deploy applications, measure analytics, and benchmark results.\n\nContext AI’s API focuses on the performance monitoring aspect of LLM Ops, giving developers tools to evaluate and test models for accuracy and bias prior to deployment, then monitor deployments with data-driven analytics from real users to understand and improve user experience in production.\n\nCarbon offers an LLM Ops API primarily focused on ingesting external data into LLM ecosystems. They offer ready-made components and data connectors, allowing easy integration of external unstructured data from a variety of sources, such as Google Drive, Dropbox, and Sharepoint.\n\nHumanloop API offers a variety of LLM Ops tools including an evaluation and monitoring suite, fine-tuning and optimization tools, and prompt management. Humanloop also provides a unique collaborative, version-controlled playground to manage and iterate prompts.\n\nLangfuse API offers a variety of LLM Ops tools for managing the entire lifecycle of LLM applications, including prompt management, tracings, evaluation, and metrics. Langfuse is uniquely open source, and can also be self-hosted or even run locally.\n\nSimilar to Carbon, the primary offering of Graphlit’s API is a powerful data ingestion tool, allowing you to easily connect data from different sources into your LLM applications. The data can come from websites, cloud storage, applications such as Jira and Slack, or be unstructured data such as documents, images, or videos.\n\nDataRobot boasts a comprehensive platform for building and operating LLM applications. Their API offers data ingestion tools, business application connectors, and native integrations which lets you choose different Gen AI components that best fit your business’s use case, as well as a model deployment and monitoring suite.\n\nOpenPipe is a YC startup focused on training, deployment, fine-tuning, and evaluation of LLM models. They also boast flexible custom pricing plans that can slash the cost of a typical deployment.\n\nAutoblocks is a platform focused on LLM evaluation: monitoring, testing, debugging, analytics, and other post-deployment tools. Autoblocks has flexible integrations that work with any codebase or tech stack, which avoids locking developers into any particular technology.\n\nIf you’re looking to introduce LLM Ops services into your product, then it’s important to choose a provider that serves your needs. Some important things to consider are:\n\nWhat kind of tools you are looking for: LLM Ops is a broad term, and different companies focus on different aspects, from data ingestion to model deployment to monitoring and analytics, or all of the above.\n\nWhat pricing model fits your use case the best: different companies offer different pricing plans for their services, from per-token plans to completely custom pricing models.\n\nHow rigid is the tech stack: Some services might lock you in to a particular technology or LLM provider, where others might be more flexible.",
      "# [ on 2025-01-10](https://www.bizjournals.com/seattle/news/2025/01/10/startups-to-watch-puget-sound-seattle-inno-2025.html)\n"
    ],
    "# OpenPipe Company and Product Report\n\n## Company Overview\n\nOpenPipe is an innovative company focused on developing advanced AI solutions, particularly in the realm of large language models (LLMs). The company has gained attention for its unique approach to fine-tuning and deploying AI models, which allows businesses to create customized AI solutions that are both cost-effective and high-performing. OpenPipe's platform is designed to enable developers to efficiently fine-tune and deploy AI models, making it an attractive option for software developers, AI researchers, and businesses looking to leverage AI technology without incurring the high costs associated with general-purpose models like GPT-4 [(AI Rabbit, Medium, 2025-02-04)](https://medium.com/codex/benchmarking-llms-against-your-own-data-ee8f495160ff).\n\n### Key Executives\n\nOpenPipe is led by a team of experienced professionals in the AI and technology sectors. The leadership team includes industry veterans who have a strong background in AI development and deployment, which positions the company well for future growth and innovation [(Asif Razzaq, MarkTechPost, 2024-06-24)](https://www.marktechpost.com/2024/06/24/openpipe-introduces-a-new-family-of-mixture-of-agents-moa-models-optimized-for-generating-synthetic-training-data-outperform-gpt-4-at-1-25th-the-cost/).\n\n## Product Overview: OpenPipe\n\nOpenPipe's flagship product is its Mixture of Agents (MoA) model, which is optimized for generating synthetic training data. This model has achieved state-of-the-art (SOTA) results in various benchmarking tests, outperforming established models like GPT-4 in several tasks. The MoA architecture is designed to generate high-quality synthetic data, which can be used to fine-tune smaller models, significantly reducing costs and improving performance [(OpenPipe, 2024-06-24)](https://openpipe.ai/blog/mixture-of-agents).\n\n### Performance Metrics\n\n- **Benchmarking Results**: The MoA model scored 84.8 on LMSYS’s Arena Hard Auto and 68.4 on AlpacaEval 2.0, indicating its superior performance in generating high-quality synthetic data [(Asif Razzaq, MarkTechPost, 2024-06-24)](https://www.marktechpost.com/2024/06/24/openpipe-introduces-a-new-family-of-mixture-of-agents-moa-models-optimized-for-generating-synthetic-training-data-outperform-gpt-4-at-1-25th-the-cost/).\n- **Comparison with GPT-4**: In real-world tasks, completions from the MoA model were preferred over GPT-4 59.5% of the time, showcasing its practical applicability [(OpenPipe, 2024-06-24)](https://openpipe.ai/blog/mixture-of-agents).\n\n### Cost Efficiency\n\nOne of the standout features of the MoA model is its cost efficiency. OpenPipe has demonstrated that fine-tuned smaller models, such as Llama 3 8B, can outperform GPT-4 on multiple tasks while being 25 times cheaper and three times faster to run. This positions OpenPipe as a leader in providing cost-effective AI solutions for businesses [(Asif Razzaq, MarkTechPost, 2024-06-24)](https://www.marktechpost.com/2024/06/24/openpipe-introduces-a-new-family-of-mixture-of-agents-moa-models-optimized-for-generating-synthetic-training-data-outperform-gpt-4-at-1-25th-the-cost/).\n\n### Model Design and Implementation\n\nThe MoA model employs a three-prompt chain to generate completions, ensuring high-quality and diverse responses. This structured approach allows the model to critique and refine its outputs, enhancing overall performance [(OpenPipe, 2024-06-24)](https://openpipe.ai/blog/mixture-of-agents).\n\n### Future Developments\n\nOpenPipe is committed to continuous improvement and plans to release enhanced variants of the MoA model, incorporating new techniques and models. The company aims to make these models accessible through its platform, allowing users to leverage the advancements in synthetic data generation [(Asif Razzaq, MarkTechPost, 2024-06-24)](https://www.marktechpost.com/2024/06/24/openpipe-introduces-a-new-family-of-mixture-of-agents-moa-models-optimized-for-generating-synthetic-training-data-outperform-gpt-4-at-1-25th-the-cost/).\n\n## Market Position and Competitive Landscape\n\nOpenPipe operates in a rapidly evolving AI landscape, where competition is intensifying. The company has positioned itself as a key player in the AI infrastructure space, focusing on fine-tuning and model deployment. As the demand for AI solutions continues to grow, OpenPipe's innovative approach to model customization and cost reduction will likely attract interest from investors and prospective candidates looking to join a forward-thinking organization [(Kelvin Mu, AI Timeline, 2024-12-03)](https://kelvinmu.substack.com/p/2024-backward-pass-the-definitive).\n\n### Partnerships and Collaborations\n\nOpenPipe has established partnerships with various organizations, including integration with AI agent frameworks like AutoGen from Microsoft Research and others. This collaboration enhances the company's offerings and expands its reach in the AI market [(Agency AI, FinSMEs, 2024-08-28)](https://www.finsmes.com/2024/08/agency-ai-raises-2-6m-in-pre-seed-funding.html).\n\n## Conclusion\n\nOpenPipe is at the forefront of AI innovation, particularly in the development of cost-effective and high-performing models for synthetic data generation. With its MoA architecture, the company has demonstrated significant advancements in AI capabilities while maintaining a focus on affordability and accessibility. As OpenPipe continues to evolve and expand its offerings, it presents a compelling opportunity for prospective candidates and investors looking to engage with a leader in the AI space. \n\nFor more information, you can visit OpenPipe's official website or follow their updates on various platforms."
  ],
  "lineage": {
    "run_at": "2025-03-28T22:26:55.492435",
    "git_sha": "9e00c41"
  }
}