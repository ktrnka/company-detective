{
  "summary_markdown": "# About Lamini\n\nLamini is a Palo Alto-based startup founded on August 31, 2022, by Sharon Zhou and Greg Diamos. The company specializes in developing a platform that enables enterprises to deploy generative AI applications effectively. Lamini has raised a total of $25 million in funding, with notable investors including Andrew Ng, Andrej Karpathy, and executives from Dropbox and Figma [(Wiggers, TechCrunch, 2024-05-02)](https://techcrunch.com/2024/05/02/dropbox-figma-ceos-back-lamini-a-startup-building-a-generative-ai-platform-for-enterprises/). The company's mission is to provide tailored solutions for enterprises, addressing the challenges they face in adopting generative AI technology [(Anees, Workhub, 2024-05-10)](https://workhub.ai/lamini-a-startup-transforming-enterprises-with-its-generative-ai-platform/).\n\nLamini offers an Enterprise LLM Platform that provides tools for building and deploying mini-agents with high accuracy. Key features include Memory Tuning, Memory RAG, and a Classifier Agent Toolkit (CAT). The platform supports various deployment options, including on-demand, reserved, and self-managed models. Lamini caters to a diverse clientele, including developers, startups, and enterprise teams, and emphasizes user-friendly integration with existing tech stacks.\n\nThe company has a rich history in the field of large language models (LLMs), with a team that has been involved in training, fine-tuning, and preference-tuning LLMs for over two decades. They have contributed significantly to core LLM research and have deployed LLMs in production to over 1 billion users. Lamini's team has also educated nearly a quarter million students on fine-tuning LLMs and mentored tech leads who developed major foundation models like OpenAI’s GPT-3 and GPT-4, Anthropic’s Claude, Meta’s Llama 3.1, Google’s PaLM, and NVIDIA’s Megatron.\n\n# Key Personnel\n\n- **Sharon Zhou**: Co-founder and CEO of Lamini. She was previously a faculty member at Stanford University and a machine learning product manager at Google Cloud. Zhou holds a Ph.D. in generative AI under Andrew Ng [(Chowdhury, Business Insider, 2024-04-01)](https://www.businessinsider.com/nvidia-chips-lamini-ai-amd-jensen-huang-sharon-zhou-2024-4).\n\n- **Greg Diamos**: Co-founder and CTO of Lamini. He is known for his work in AI research at Baidu and as a co-founder of MLCommons, which develops benchmarks for AI models [(Deutscher et al., SiliconANGLE, 2024-05-03)](https://siliconangle.com/2024/05/03/lamini-raises-25m-ai-development-inference-platform/).\n\n# News\n\n## Funding and Financials\n\nLamini has successfully raised $25 million across seed and Series A funding rounds, with the latest round led by Amplify Partners [(Deutscher et al., SiliconANGLE, 2024-05-03)](https://siliconangle.com/2024/05/03/lamini-raises-25m-ai-development-inference-platform/). The funds are intended to expand the company's team and enhance its AI infrastructure, particularly focusing on deeper technical optimizations [(Wiggers, TechCrunch, 2024-05-02)](https://techcrunch.com/2024/05/02/dropbox-figma-ceos-back-lamini-a-startup-building-a-generative-ai-platform-for-enterprises/).\n\n## Product Developments\n\nLamini's flagship product, **Lamini Memory Tuning**, is designed to enhance the factual accuracy of large language models (LLMs) while significantly reducing hallucinations. This method has reportedly achieved 95% accuracy in factual recall, compared to 50% with traditional approaches [(Lamini, 2024-05-02)](https://www.lamini.ai/blog/lamini-memory-tuning). Key features include the Mixture of Memory Experts (MoME) architecture, which allows the model to retrieve relevant facts dynamically during inference.\n\n## Partnerships and Collaborations\n\nLamini has partnered with Meta to enhance the performance of Llama 3, focusing on improving SQL query generation and reducing hallucinations in enterprise applications [(Eva, Analytics India Magazine, 2024-06-26)](https://analyticsindiamag.com/ai-news-updates/lamini-ai-partners-with-meta-to-enhance-llamas-sql-performance/). This collaboration aims to provide enterprises with robust tools for building LLM applications.\n\n## Market Position and Competitive Landscape\n\nLamini positions itself uniquely in the generative AI market by focusing on enterprise-specific needs, contrasting with many general-purpose AI platforms. The company faces competition from established players like Google, AWS, and Microsoft, which are also targeting enterprise solutions [(Anees, Workhub, 2024-05-10)](https://workhub.ai/lamini-a-startup-transforming-enterprises-with-its-generative-ai-platform/). Lamini's competitive advantages include tailored solutions for enterprise workloads and scalability, allowing for flexible deployment in various environments [(Wiggers, TechCrunch, 2024-05-02)](https://techcrunch.com/2024/05/02/dropbox-figma-ceos-back-lamini-a-startup-building-a-generative-ai-platform-for-enterprises/).\n\n## Executive Insights\n\nSharon Zhou emphasizes the importance of maximizing ROI for enterprises adopting generative AI, stating, “The top priority of nearly every CEO, CIO, and CTO is to take advantage of generative AI within their organization with maximal ROI” [(Anees, Workhub, 2024-05-10)](https://workhub.ai/lamini-a-startup-transforming-enterprises-with-its-generative-ai-platform/). This focus on enterprise needs is a core part of Lamini's strategy.\n\nIn conclusion, Lamini is positioned as a promising player in the generative AI landscape, with its innovative Memory Tuning technology setting new standards for accuracy and reliability in LLM applications. With strong backing from notable investors and a clear focus on enterprise needs, Lamini is well-equipped to navigate the competitive landscape and drive significant advancements in AI deployment for businesses.",
  "target": [
    "Lamini",
    "Lamini",
    "lamini.ai",
    null,
    false,
    false,
    null,
    [
      false,
      false
    ]
  ],
  "webpage_result": {
    "summary_markdown": "# Lamini Company Overview\n\n## Company History\nLamini has a rich history in the field of large language models (LLMs), with a team that has been training, fine-tuning, and preference-tuning LLMs for over two decades. They have contributed significantly to core LLM research, including LLM scaling laws, and have deployed LLMs in production to over 1 billion users. The team has also educated nearly a quarter million students on fine-tuning LLMs and mentored tech leads who went on to develop major foundation models like OpenAI’s GPT-3 and GPT-4, Anthropic’s Claude, Meta’s Llama 3.1, Google’s PaLM, and NVIDIA’s Megatron.\n\n## Products and Services\nLamini offers an **Enterprise LLM Platform** that provides tools for building and deploying mini-agents with high accuracy. Key features include:\n\n- **Memory Tuning**: Achieve 90%+ accuracy by injecting precise facts into models, eliminating hallucinations, and allowing for efficient scaling from 10 to 100,000 examples.\n  \n- **Memory RAG**: A simplified approach that boosts LLM accuracy from ~50% to 90-95% without complex setups, using contextual embeddings to capture meaning and relationships.\n\n- **Classifier Agent Toolkit (CAT)**: Enables the creation of classifiers that can categorize large volumes of text inputs with >99% accuracy, processing up to 100k tokens/second.\n\n- **Deployment Options**: Lamini offers three deployment models:\n  - **On-Demand**: Fully-managed training and inference.\n  - **Reserved**: Dedicated GPUs hosted on Lamini's infrastructure.\n  - **Self-Managed**: Run Lamini on your own infrastructure, including air-gapped deployments.\n\n## Customers\nLamini caters to a diverse clientele, including developers, startups, and enterprise teams. Their platform is designed to be user-friendly, with clear documentation and examples, making it easy for users to integrate Lamini into their existing tech stacks.\n\n## Leadership Team\nThe leadership team at Lamini consists of experts in the field of AI and LLMs, with extensive experience in both academia and industry. They have been instrumental in shaping the direction of the company and its innovative offerings.\n\n## Company Culture\nLamini promotes a culture of innovation and collaboration, encouraging users to experiment with their tools and contribute to the community. They emphasize the importance of data privacy and security, allowing users to deploy their models in a secure environment.\n\n## Conclusion\nLamini stands out in the AI landscape by providing powerful tools for LLM tuning and inference, enabling users to build accurate and efficient models tailored to their specific needs. With a commitment to user support and a focus on data privacy, Lamini is well-positioned to help organizations leverage the power of AI effectively.\n\nFor more information, visit their website: [Lamini](https://www.lamini.ai/)",
    "page_markdowns": [
      "# [Enterprise LLM Platform](https://www.lamini.ai/)\n",
      "# [ai/lamini by lamini-ai](https://www.lamini.ai/examples)\nBefore you start, please get your <YOUR-LAMINI-API-KEY> at https://app.lamini.ai/account. Create ~/.lamini/configure.yaml and write the file as follows, this will persist your Lamini credentials:\n\nInstall python requirements\n\npip3 install -r ./lamini-examples/requirements.txt\n\nIn this repo, we include tutorials for achieving high-quality results with Language Models (LLMs), like Llama3, using Lamini. With Lamini, you own the LLM you create -- you can deploy it or release it open source.\n\nThese examples show effective tools for building LLMs. We strongly encourage following the examples in order as the concepts build on each other and are sorted by difficulty.\n\nLlama3 - Generate text with Llama3, a powerful LLM.\n\nEvaluation - Evaluate the quality of your LLM.\n\nPrompt Tuning - Improve the quality of your LLM by tuning the prompts you use.\n\nRAG Tuning - Retrieval Augmented Generation (RAG), improve the quality of your LLM by tuning the retrieval component.\n\nData Pipeline - Prepare your data for training an LLM.\n\nMemory Tuning - Memory tune an LLM on your data.\n\nExtra Examples:\n\nMeta Llama Memory Tuning 🔗 - Embed facts into an LLM and improve factual accuracy on your data.\n\nJSON Output - Extract structured output from an LLM, following a guaranteed JSON schema.\n\nSlackbot - Learn how to create a Slack bot that calls an LLM.\n\nThe goal of this repo is to teach and provide examples of important tools for building LLMs; the examples emphasize simplicitly and readibility, not heavy optimization.\n\nOnce you have mastered a module from this repo, consider forking it and adapting it to your own application.\n\nAll of the code in this repository is licensed Apache 2. You are free to use it for any purpose including commercial applications.\n\nThe source code for this repo can be found on GitHub at lamini-ai/lamini-examples. Feel free to explore and contribute!",
      "# [Enterprise LLM Platform](https://www.lamini.ai/blog)\n",
      "# [Enterprise LLM Platform](https://www.lamini.ai/pricing)\n",
      "# [Lamini - Enterprise LLM Platform](https://www.lamini.ai/product)\nAchieve 90%+ accuracy\n\nRecall facts, i.e., verifiable data points, APIs, product IDs, specific terms, and more with high precision.\n\nBuild your own mini-agents\n\nLeverage smaller models (SLMs) to build highly accurate mini-agents specialized on your proprietary data.\n\nIntuitive interface\n\nBuilt for developers to make the grueling work of MLOps more intuitive, automated, and scalable using familiar development patterns.\n\nPartner with AI experts\n\nOur AI experts work with you to identify the right use case, build your data and evaluation pipelines, and measure success.\n\nEnsure data privacy\n\nDeploy anywhere securely—Lamini's hosted GPUs, on-premise (even air-gapped), or VPC—so you have ultimate control over your data.",
      "# [Enterprise LLM Platform](https://www.lamini.ai/contact)\n",
      "# [Introduction](https://lamini-ai.github.io/)\nWelcome to Lamini 🦙\n\nBuild mini-agents with 90%+ accuracy, whether you're a solo developer or an enterprise team. Get started with $300 in free credits.\n\nQuick Navigation\n\nGoal Description Link 🚀 Get Started Boost your mini LLMs from 50% to 90%+ accuracy Quick Start 💡 Try It Out Test your PDF knowledge base Playground (with Memory RAG) 🎯 Memory Tuning Build accurate, efficient models Memory Tuning 🤖 RAG Tools Create reliable mini-agents Memory RAG 🎯 Classification Deploy scalable classifiers Classifier Agent 🔒 Self-Hosted Install Lamini on your own GPUs Kubernetes Install\n\nQuestions? Contact us. We read every message — or at one of our mini-agents does :)\n\nCore Products\n\nMemory Tuning [paper] [class with Andrew Ng & Meta] [about]\n\nBuild the most accurate and efficient fine-tuned models:\n\nInject precise facts to eliminate hallucinations\n\nStart with only 10 facts & examples, scale to 100,000+\n\nReliably get 95%+ accuracy (removes accuracy ceilings on many tasks)\n\nKeep latency and costs low, by getting away with memory-tuned smaller LMs and mini-agents\n\nOne API, any open model\n\nMemory RAG [paper] [about]\n\nSkip the complex RAG setup. Easier than Memory Tuning. Get 90%+ accuracy out of the box:\n\nBoost accuracy from 50% to 90-95% compared to GPT4, after just a few iterations on your data and telling the model how to improve\n\nSmart embedding that expands your data representation to capture true meaning and relationships\n\nBuild reliable, specializedmini-agents that work together\n\nSimple API, powerful results\n\nClassifier Agent Toolkit [demo] [about]\n\nBuild accurate classifiers in minutes, not months:\n\nHandle any number of categories, from 2 to 1000+\n\nProcess unstructured data at scale with 400K tokens/second\n\nRoute requests automatically, with 99.9% accuracy\n\nTriage code and content efficiently\n\nPerfect For\n\nDevelopers & Startups\n\nSimple SDK and API\n\nStart free, scale as you grow\n\nClear documentation and examples\n\nFast integration into your stack, OpenAI API compatible\n\nEnterprise Teams\n\nProduction-ready security\n\nAir-gapped deployment option\n\nScale across departments\n\nCustom deployment support\n\nReduce production risks with 99.9% accuracy\n\nReal-World Applications\n\nBuild what matters to you:\n\nSQL Generator: Convert natural language to database queries\n\nCustomer Support Agent: Scale customer service intelligently\n\nData Classifier: Automate manual sorting and labeling\n\nCode Helper: Build assistants for any programming language\n\nMini-Agent: Automate planning and execution of specialized tasks\n\nGetting Started Is Easy\n\nStart with $300 in free credits\n\nChoose your deployment (cloud or self-hosted)\n\nUse our SDKs or API\n\nMonitor through our dashboard\n\nWho are we?\n\nLamini's team has been training, fine-tuning, and preference-tuning LLMs over the past two decades. We invented core LLM research like LLM scaling laws, shipped LLMs in production to over 1 billion users, taught nearly a quarter million students about Finetuning LLMs, and mentored the tech leads that went on to build the major foundation models: OpenAI’s GPT-3 and GPT-4, Anthropic’s Claude, Meta’s Llama 3.1, Google’s PaLM, and NVIDIA’s Megatron.\n\nWhat's new?",
      "# [Demo Request](https://www.lamini.ai/demo-request)\n",
      "# [Lamini](https://app.lamini.ai/)\n",
      "# [Lamini Docs](https://lamini-ai.github.io/faq/)\nFAQ\n\nCore Development Questions\n\nHow do I set up authentication?\n\nSee the Authentication guide for getting and configuring your Lamini API key.\n\nHow does model loading work?\n\nModel weights are loaded to GPU memory once and persist between requests\n\nLoading only happens on initial startup or after unexpected events\n\nLoading time scales with model size\n\nWhat systems can I develop with Lamini on?\n\nRecommended: Ubuntu 22.04+ with Python 3.10-3.12\n\nNot officially supported on Windows (use Docker with Linux container instead)\n\nTraining & Tuning\n\nWhat models can I use?\n\nCheck the Models page for the full list of supported models.\n\nHow long can training jobs run?\n\nDefault timeout: 4 hours\n\nJobs automatically checkpoint and resume if timeout occurs\n\nFor longer runs:\n\nRequest more GPUs via gpu_config\n\nContact us for dedicated instances\n\nCan I disable memory tuning (MoME)?\n\nYes, use these settings for cases like summarization where qualitative output is preferred:\n\nfinetune_args={ \"batch_size\": 1, \"index_ivf_nlist\": 1, \"index_method\": \"IndexFlatL2\", \"index_max_size\": 1, }\n\nHow does Lamini optimize model training?\n\nUses LoRAs (low-rank adapters) automatically\n\n266x fewer parameters than full model finetuning\n\n1.09B times faster model switching\n\nNo manual configuration needed\n\nInfrastructure\n\nWhy might my job be queued?\n\nThe On-Demand plan uses shared resources. For dedicated compute: - Consider Lamini Reserved plans - Contact us about running on your own infrastructure\n\nWhat GPU can Lamini run on?\n\nLamini can run on AMD and NVIDIA GPUs\n\nHow do I get started with Lamini private servers or enterprise plans?\n\nContact us to learn more about our reserved plans\n\nRun your own jobs on dedicated compute",
      "# [Lamini Docs](https://lamini-ai.github.io/api/)\n",
      "# [Supported Models](https://lamini-ai.github.io/models/)\nSupported Models\n\nLamini On-Demand\n\nLamini On-Demand supports a variety of the most popular open source LLMs, including Llama 3.1, Mistral 3, Phi-3, Qwen 2, and many more.\n\nModels available on Lamini On-Demand for inference and tuning:\n\nEleutherAI/pythia-410m\n\nEleutherAI/pythia-70m\n\nhf-internal-testing/tiny-random-gpt2\n\nmeta-llama/Llama-2-13b-chat-hf\n\nmeta-llama/Llama-2-7b-chat-hf\n\nmeta-llama/Llama-2-7b-hf\n\nmeta-llama/Meta-Llama-3-8B-Instruct\n\nmeta-llama/Llama-3.1-8B-Instruct\n\nmicrosoft/phi-2\n\nmicrosoft/Phi-3-mini-4k-instruct\n\nmistralai/Mistral-7B-Instruct-v0.1\n\nmistralai/Mistral-7B-Instruct-v0.2\n\nmistralai/Mistral-7B-Instruct-v0.3\n\nQwen/Qwen2-7B-Instruct\n\nLamini Reserved and Self-Managed\n\nLamini Reserved and Self-Managed support all CausalLM models from Hugging Face (excluding those requiring Flash Attention 2 or 3). Roughly 95% of all models on HF are supported. If you're interested in using models that aren't available in Lamini On-Demand, please contact us.\n\nModel size and performance\n\nWith Memory Tuning you can achieve very high factual accuracy with 8B models, without giving up fluent generalization. Using smaller models lowers operating costs and improves latency.\n\nSome factors to consider when thinking about model size:\n\nThe more active parameters a model has, the more GPU memory is required to use the model.\n\nIf a model is larger than a single GPU's memory, it needs to run across multiple GPUs. This means exchanging more data across the network, and both inference and tuning will take longer.\n\nTuning requires significantly more GPU memory than inference.\n\nModel loading\n\nLamini On-Demand only allows use of the models listed above.\n\nIf you're using Lamini Reserved or Self-Managed, you can configure your cluster to use any supported Hugging Face model.\n\nThe batch_model_list in llama_config.yaml lets you specify which models to preload onto your allocated inference GPUs. Inference requests for all other models will be handled by your allocated catchall GPUs, and those models will be loaded from Hugging Face when requested.\n\nBecause models are large (usually tens of GBs), downloading them from Hugging Face and then loading them into GPU memory takes time. Please allow 20-30 minutes for non-preloaded models to load. Requests for models that have not yet loaded will return an error.\n\nWe recommend focusing development on one model or a small set of models, and preloading them. We've seen the highest accuracy and performance gains come from improving data quality and tuning recipes, rather than testing many models hoping to find one that works significantly better out of the box.",
      "# [Quick Start](https://lamini-ai.github.io/cat/)\nClassifier Agent Toolkit\n\nThe Lamini Classifier Agent Toolkit (CAT) allows you to create and refine a key building block for agentic workflows: classifiers that can quickly categorize a large number of text inputs across any number of pre-defined categories.\n\nWhat sets CAT apart from other LLMs and classification tools:\n\nAccuracy for many classes: >99% accuracy on evals even with >500 classes\n\nHigh throughput: process 100k tokens/s\n\nConsistent latency: sub-2s latency, with 1000s of inputs and 100s of classes\n\nConfidence scores: for more accurate workflows\n\nBuilt for iteration: compare models with metrics to measure progress\n\nYou can use CAT via Lamini's REST API, Python SDK, or web interface. Or step through an example notebook.\n\nStep Action Best Practices 1 Create project Set up new classifier 2 Add examples ~3 diverse examples per class, balanced number of examples per class 3 Train & predict Get predictions with confidence scores (~1 min/class) 4 Evaluate Validate metrics and test performance 5 Iterate Add examples and retrain to improve accuracy\n\nQuick Start with Python\n\nFirst, make sure your API key is set (get yours at app.lamini.ai):\n\nCreate a new classifier project:\n\nOnce the project is created, we define the classes. The more detailed the description, the higher your accuracy will be.\n\nAdding example inputs is optional, but will also help with accuracy. You can always do this later - we'll show you how later in this notebook.\n\nNow we initialize the project. This can take about a minute per class, so we'll put in a simple timer to keep us updated on status.\n\nCool, we have our first model version! Let's try it out with a quick test.\n\nHere's the expected output:\n\nNow we can see how useful the classifier output is. We get a list of all the categories we defined in our project, plus a confidence score for each.\n\nWe can go even further to easily quantify the accuracy of our classifier. Let's run an evaluation!\n\nWhat an evaluation means for a classifier: when you provide a set of inputs and the expected output, we can test the accuracy of the model on those inputs, and give you back both overall metrics as well as per-input assessment.\n\nExpected output:\n\nThat first run was ok, but we can do better. Let's add some more examples and retrain to improve accuracy. You control when to add data and when to train.\n\nGreat, now we have a second model version in our project! Let's run an eval and compare it to the first version.\n\nExpected output:\n\nThe eval output makes it easy to compare model versions overall, and to see exactly where the differences are, so you know exactly where to focus to improve your workflow.\n\nHappy classifying!",
      "# [Sync - Webflow HTML Website Template](https://www.lamini.ai/reference/style-guide)\nText Size Large\n\nText Size Medium\n\nText Size Regular\n\nSample text is being used as a placeholder for real text that is normally present. Sample text helps you understand how real text may look on your website. Sample text is being used as a placeholder for real text.\n\nText Size Small\n\nSample text is being used as a placeholder for real text that is normally present. Sample text helps you understand how real text may look on your website. Sample text is being used as a placeholder for real text.\n\nText Size Tiny\n\nSample text is being used as a placeholder for real text that is normally present. Sample text helps you understand how real text may look on your website. Sample text is being used as a placeholder for real text.\n\nText Style Strikethrough\n\nText Style Italic\n\nText Style Muted\n\nText Style Allcaps\n\nText Style Nowrap\n\nText Style Link\n\nHeading 1\n\nHeading 2\n\nHeading 3\n\nHeading 4\n\nHeading 5\n\nHeading 6\n\nSample text is being used as a placeholder for real text that is normally present. Sample text helps you understand how real text may look on your website. Sample text is being used as a placeholder for real text.\n\nThe rich text element allows you to create and format headings,\n\nThe rich text element allows you to create and format headings,\n\nThe rich text element allows you to create and format headings,\n\nThis is a link\n\nSample text is being used as a placeholder for real text that is normally present. Sample text helps you understand how real text may look on your website. Sample text is being used as a placeholder for real text.",
      "# [Quick Start](https://docs.lamini.ai/memory_rag/)\nMemory RAG\n\nMemory RAG is a simple approach that boosts LLM accuracy from ~50% to 90-95% compared to GPT-4. It creates contextual embeddings that capture meaning and relationships, allowing smaller models to achieve high accuracy without complex RAG setups or fine-tuning overhead.\n\nQuick Start with Python or REST API\n\nFirst, make sure your API key is set (get yours at app.lamini.ai):\n\nTo use Memory RAG, build an index by uploading documents and selecting a base open-source LLM. This is the model you'll use to query over your documents.\n\nNext, Memory embedding and indexing will complete. Poll for status:\n\nFinally, run the Memory RAG model:\n\nIteration\n\nYou are now ready to run evaluation of this model. To do so, build out an evaluation set that consists of Question/Answer pairs for the expected answers you have for the provided question. The more representative your questions and answer pairs are to your production use case, the better the model is evaluated. Rate the models performance in reference to this evaluation set. If the model performs poorly, try iterating on the Memory Rag job with additional data. If the model performs well enough, then you can consider it ready for production!",
      "# [Enterprise LLM Platform](https://www.lamini.ai/blog?5b6c907e_page=3)\n",
      "# [Enterprise LLM Platform](https://www.lamini.ai/blog?5b6c907e_page=2)\n",
      "# [Quick Start](https://docs.lamini.ai/quick_start/)\nLet's get you started on Lamini, so you can be on your way to boosting your mini LLMs from 50% to 90%+ accuracy — and building your own mini-agents!\n\nSetup\n\nInstall the Python SDK\n\nGet your Lamini API key (with $300 in free credits) at https://app.lamini.ai/account.\n\nExport your key as an environment variable so it's easy to use later.\n\nRun inference to check it's running\n\nExpected Output\n\nYikes, that's wordy! Plus, we all know llamas are the smart ones. Let's make it more accurate.\n\nMemory Tune a model\n\nStart the tuning job\n\nCheck the status of your tuning job\n\nTry the tuned model\n\nAfter the status is \"COMPLETED\", you can run inference with the tuned model to see how it performs.\n\nExpected Output\n\nThat's much better!\n\nLearn more about tuning\n\nNext steps",
      "# [API Authentication](https://docs.lamini.ai/authenticate/)\n1. Get your Lamini API key 🔑\n\nYour API key is at https://app.lamini.ai/account. If it's your first time, create a free account by logging in.\n\nIf you're self-managing Lamini Platform on your own GPUs, check out the OIDC authentication docs for setting up user auth.\n\n2. Authenticate\n\nAdvanced Python setup: VPC or on premise\n\nIf you are running Lamini in your VPC or on prem, you can change the URL from Lamini's hosted service to your own server URL:\n\nGoogle Colab",
      "# [Lamini Docs](https://lamini-ai.github.io/about/)\nAbout\n\nWhat is Lamini?\n\nLamini provides the best LLM inference and tuning for the enterprise. Factual LLMs. Up in 10min. Deployed anywhere.\n\nLamini Platform\n\nLamini Platform orchestrates GPUs to deliver exceptional LLM tuning and inference capabilities, which easily integrate into enterprise applications via the Lamini Python client,REST API, and web UI.\n\nSee for yourself: take a quick tour (with free API access!) to see how Lamini works, or contact us to run in your own environment.\n\nDeployment Models\n\nLamini Platform is available in three different deployment models:\n\nOn-Demand: fully-managed training and inference at https://app.lamini.ai, with pay-as-you-go pricing.\n\nReserved: dedicated GPUs for your organization, hosted on Lamini's infrastructure, with per-GPU pricing.\n\nSelf-Managed: run Lamini Platform in your environment on your GPUs (on premise, in your VPC, even air-gapped deployments), with per-GPU pricing.\n\nWhat's unique about Lamini?\n\nArea Problem Lamini's solution Tuning Hallucinations 95% accuracy on factual tasks: memory tuning Tuning High infrastructure costs 32x model compression: Memory Tuning with efficient LoRAs Inference Unreliable app integrations 100% accurate JSON schema output: structured output\n\nWho are we?\n\nLamini's team has been finetuning LLMs over the past two decades: we invented core LLM research like LLM scaling laws, shipped LLMs in production to over 1 billion users, taught nearly a quarter million students about Finetuning LLMs, and mentored the tech leads that went on to build the major foundation models: OpenAI’s GPT-3 and GPT-4, Anthropic’s Claude, Meta’s Llama 3.1, Google’s PaLM, and NVIDIA’s Megatron.\n\nWhat's new?",
      "# [Hyperparameters](https://docs.lamini.ai/tuning/hyperparameters/)\nLamini tuning supports most hyperparameters in HuggingFace's training arguments, as well as some Lamini-specific options.\n\nThese can be set in the tune method:\n\n# code/hyperparameters.py fromlaminiimport Lamini llm = Lamini(model_name=\"meta-llama/Llama-3.1-8B-Instruct\") data = [ { \"input\": \"What is Lamini? Is it like a robot or a computer program?\", \"output\": \"Lamini is a program for the execution of LLMs called a large language model engine. It is not a robot, but rather a tool for building and executing LLMs.\", } ] results = llm.tune(data_or_dataset_id=data, finetune_args={\"learning_rate\": 1.0e-4})\n\nSee Memory Tuning for use-case specific suggestions.\n\nfinetune_args\n\nmax_finetuning_examples (int, optional)\n\nDefault: size of the dataset\n\nSets the maximum number of data points for fine-tuning. If not set, the model is fine-tuned on the entire dataset.\n\nmax_steps (int, optional)\n\nDefault: 100\n\nSpecifies the total number of training steps to perform.\n\nThis parameter is passed to HuggingFace's Transformers TrainingArguments.\n\ngradient_accumulation_steps (int, optional)\n\nDefault: 2\n\nNumber of update steps to accumulate the gradients for, before performing a backward/update pass.\n\nUsage note: a higher setting can improve memory efficiency and thus reduce training time, often with a neutral effect on model accuracy.\n\nThis parameter is passed to HuggingFace's Transformers TrainingArguments.\n\nlearning_rate (float, optional)\n\nDefault: 9.0e-4\n\nThe initial learning rate for the fine-tuning.\n\nThis parameter is passed to HuggingFace's Transformers TrainingArguments.\n\nUsage note: see the Memory Tuning section for tips on setting learning rate.\n\nsave_steps (int or float, optional)\n\nDefault: 60\n\nNumber of update steps between two checkpoint saves.\n\nThis parameter is passed to HuggingFace's Transformers TrainingArguments.\n\nmax_length (int, optional)\n\nDefault: 2048\n\nSpecifies the maximum sequence length for the forward pass, acting as the block size for the model.\n\nShould be a power of 2, no larger than 8192.\n\nUsage note: max_length should be at least as large as the size of your datapoints. If training with large datapoints is not converging, increasing this value may help. However, larger values of max_length increase training time, and very large values will exhaust GPU memory. There's often room to reduce the size of your datapoints so a smaller max_length can be used.\n\noptim (str, optional)\n\nDefault: \"adafactor\"\n\nThe optimizer to use: adamw_hf, adamw_torch, adamw_torch_fused, adamw_apex_fused, adamw_anyprecision or adafactor.\n\nThis parameter is passed to HuggingFace's Transformers TrainingArguments.\n\nr_value (int, optional)\n\nDefault: 64\n\nSpecifies the size of the LoRA (Low-Rank Adaptation) component.\n\nindex_method (str, optional)\n\nDefault: \"IndexIVFPQ\"\n\nThe index method used for approximate nearest neighbor search of high-dimensional vectors: IndexIVFPQ, IndexHNSWPQ,IndexHNSWFlat, IndexFlatL2, IndexPQ\n\nindex_k (int, optional)\n\nDefault: 2\n\nDetermines the number of nearest neighbors to consider.\n\nindex_max_size (int, optional)\n\nDefault: 65536\n\nMaximum size of the index.\n\nindex_pq_m (int, optional)\n\nDefault: 8\n\nNumber of factors of product quantization.\n\nOnly used when index_method is IndexIVFPQ or IndexPQ. Ignored otherwise.\n\nindex_pq_nbits (int, optional)\n\nDefault: 8\n\nNumber of bits in which each low-dimensional vector is stored. Range: [1, 16]\n\nOnly used when index_method is IndexIVFPQ or IndexPQ. Ignored otherwise.\n\nindex_ivf_nlist (int, optional)\n\nDefault: 2048\n\nNumber of buckets during clustering for IVFLAT.\n\nOnly used when index_method is IndexIVFPQ. Ignored otherwise.\n\nindex_ivf_nprobe (int, optional)\n\nDefault: 48\n\nNumber of buckets to search during the first step of IVFLAT.\n\nOnly used when index_method is IndexIVFPQ. Ignored otherwise.\n\nindex_hnsw_m (int, optional)\n\nDefault: 32\n\nRange: [4, 64]. Used in HNSW (Hierarchical Navigable Small World Graph) algorithm.\n\nOnly used when index_method is IndexHNSWPQ or HNSWFlat. Ignored otherwise.\n\nindex_hnsw_efConstruction (int, optional)\n\nDefault: 16\n\nExpansion factor at construction time for HNSW. Range: [8, 512]\n\nOnly used when index_method is IndexHNSWPQ or HNSWFlat. Ignored otherwise.\n\nindex_hnsw_efSearch (int, optional)\n\nDefault: 8\n\nExpansion factor at search time for HNSW.\n\nOnly used when index_method is IndexHNSWPQ or HNSWFlat. Ignored otherwise.\n\nImmutables\n\nThe following configs are only supported at their default values:\n\nbatch_size: 1\n\nearly_stopping: false\n\nnum_train_epochs: Will be overriden by max_steps\n\ntemperature: 0\n\nlora_alpha: 1\n\ngpu_config\n\ngpus: (int, optional)\n\nDefault: 1\n\nNumber of GPUs per node to use for the tuning job.\n\nnodes: (int, optional)\n\nDefault: 1\n\nNumber of nodes (machines containing multiple GPUs) to use for the tuning job.\n\nThe Lamini On-Demand allows a maximum of GPUs and nodes based on our server availability. If you are on Lamini Reserved or Self-managed, you can specify any number of GPUs and nodes within your provisioned cluster size. Your job will be queued until the requested number of GPUs and nodes is available.\n\nIf the required GPUs and nodes are not available, the configuration defaults to the system limit, and the job is queued until the resources become available. When using multiple nodes, specify the number of GPUs required per node.\n\nExamples:\n\ngpu_config = {\"gpus\": 8, \"nodes\": 1} # total 8 GPUs gpu_config = {\"gpus\": 8, \"nodes\": 2} # total 16 GPUs gpu_config = {\"gpus\": 4, \"nodes\": 2} # total 8 GPUs gpu_config = {\"gpus\": 9, \"nodes\": 1} # error, assuming max GPUs per node is 8\n\ndata_or_dataset_id\n\ndata_or_dataset_id (JSONL or CSV file or dataset ID of an already uploaded dataset, required)\n\nDefault: no default\n\nSpecifies the dataset to use for the tuning job.",
      "# [Memory RAG](https://www.lamini.ai/memory-rag)\n100%\n\nAccuracy for content classification\n\n1200+h\n\nOf manual work saved annually\n\n\"Lamini's classifier SDK is easy to use... Once [the tuned LLM] was ready, we tested it, and it was so easy to deploy to production. It allowed us to move really rapidly.”\n\nChris Lu\n\nCTO"
    ],
    "search_results": [
      {
        "title": "Lamini - Enterprise LLM Platform",
        "link": "https://www.lamini.ai/",
        "snippet": "Build highly accurate mini-agents, reduce LLM hallucinations by 95%. Get started on the Lamini platform with $300 free credits.",
        "formattedUrl": "https://www.lamini.ai/"
      },
      {
        "title": "lamini-ai/lamini-examples - GitHub",
        "link": "https://www.lamini.ai/examples",
        "snippet": "Lamini is the LLM platform for developers to specialize LLMs on their own data and infrastructure: easier, faster, and better than any LLM for their use case.",
        "formattedUrl": "https://www.lamini.ai/examples"
      },
      {
        "title": "Lamini - Enterprise LLM Platform",
        "link": "https://www.lamini.ai/blog",
        "snippet": "Lamini is the enterprise LLM platform for existing software teams to quickly develop and control their own LLMs. Lamini has built-in best practices for ...",
        "formattedUrl": "https://www.lamini.ai/blog"
      },
      {
        "title": "Pricing",
        "link": "https://www.lamini.ai/pricing",
        "snippet": "Special pricing available for startups · Get started with $300 in free credit · Partner with our team of AI experts to build your LLM application · Get access ...",
        "formattedUrl": "https://www.lamini.ai/pricing"
      },
      {
        "title": "Product | Lamini - Enterprise LLM Platform",
        "link": "https://www.lamini.ai/product",
        "snippet": "The developer platform for teaching LLMs to think. Deploy highly accurate mini-agents to power your agentic workflows.",
        "formattedUrl": "https://www.lamini.ai/product"
      },
      {
        "title": "Enterprise LLM Platform - Lamini",
        "link": "https://www.lamini.ai/contact",
        "snippet": "Lamini helps enterprises reduce hallucinations by 95%, enabling them to build smaller, faster LLMs and agents based on their proprietary data. Lamini can be ...",
        "formattedUrl": "https://www.lamini.ai/contact"
      },
      {
        "title": "Lamini Docs: Introduction",
        "link": "https://lamini-ai.github.io/",
        "snippet": "Build mini-agents with 90%+ accuracy, whether you're a solo developer or an enterprise team. Get started with $300 in free credits.",
        "formattedUrl": "https://lamini-ai.github.io/"
      },
      {
        "title": "Demo Request",
        "link": "https://www.lamini.ai/demo-request",
        "snippet": "The Lamini platform includes a suite of products and tools to help you build high-accuracy LLM applications and agents. We have helped enterprises achieve ...",
        "formattedUrl": "https://www.lamini.ai/demo-request"
      },
      {
        "title": "Lamini",
        "link": "https://app.lamini.ai/",
        "snippet": "Welcome! Sign in. By signing in, you agree to our Privacy Policy and Terms of Service.",
        "formattedUrl": "https://app.lamini.ai/"
      },
      {
        "title": "FAQ - Lamini Docs",
        "link": "https://lamini-ai.github.io/faq/",
        "snippet": "Core Development Questions: How do I set up authentication? See the Authentication guide for getting and configuring your Lamini API key.",
        "formattedUrl": "https://lamini-ai.github.io/faq/"
      },
      {
        "title": "REST API - Lamini Docs",
        "link": "https://lamini-ai.github.io/api/",
        "snippet": "Create and manage model training jobs. POST /v1/train Create a tuned model by kicking off a fine-tuning job on a base model.",
        "formattedUrl": "https://lamini-ai.github.io/api/"
      },
      {
        "title": "Supported Models - Lamini Docs",
        "link": "https://lamini-ai.github.io/models/",
        "snippet": "Model loading. Lamini On-Demand only allows use of the models listed above. If you're using Lamini Reserved or Self-Managed, you can configure your cluster to ...",
        "formattedUrl": "https://lamini-ai.github.io/models/"
      },
      {
        "title": "Quick Start - Lamini Docs",
        "link": "https://lamini-ai.github.io/cat/",
        "snippet": "The Lamini Classifier Agent Toolkit (CAT) allows you to create and refine a key building block for agentic workflows: classifiers that can quickly categorize a ...",
        "formattedUrl": "https://lamini-ai.github.io/cat/"
      },
      {
        "title": "Style Guide | Sync - Webflow HTML Website Template",
        "link": "https://www.lamini.ai/reference/style-guide",
        "snippet": "H2 Sample text is being used as a placeholder as real text. · Heading Xlarge · Heading Large · Heading Medium · Heading Small · Heading Xsmall.",
        "formattedUrl": "https://www.lamini.ai/reference/style-guide"
      },
      {
        "title": "Quick Start - Lamini Docs",
        "link": "https://docs.lamini.ai/memory_rag/",
        "snippet": "Memory RAG is a simple approach that boosts LLM accuracy from ~50% to 90-95% compared to GPT-4. It creates contextual embeddings that capture meaning and ...",
        "formattedUrl": "https://docs.lamini.ai/memory_rag/"
      },
      {
        "title": "Lamini - Enterprise LLM Platform",
        "link": "https://www.lamini.ai/blog?5b6c907e_page=3",
        "snippet": "Lamini is the enterprise LLM platform for existing software teams to quickly develop and control their own LLMs. Lamini has built-in best practices for ...",
        "formattedUrl": "https://www.lamini.ai/blog?5b6c907e_page=3"
      },
      {
        "title": "Lamini - Enterprise LLM Platform",
        "link": "https://www.lamini.ai/blog?5b6c907e_page=2",
        "snippet": "Lamini is the enterprise LLM platform for existing software teams to quickly develop and control their own LLMs. Lamini has built-in best practices for ...",
        "formattedUrl": "https://www.lamini.ai/blog?5b6c907e_page=2"
      },
      {
        "title": "Quick Start - Lamini Docs",
        "link": "https://docs.lamini.ai/quick_start/",
        "snippet": "Let's get you started on Lamini, so you can be on your way to boosting your mini LLMs from 50% to 90%+ accuracy — and building your own mini-agents ...",
        "formattedUrl": "https://docs.lamini.ai/quick_start/"
      },
      {
        "title": "API Authentication - Lamini Docs",
        "link": "https://docs.lamini.ai/authenticate/",
        "snippet": "1. Get your Lamini API key. Your API key is at https://app.lamini.ai/account. If it's your first time, create a free account by logging in.",
        "formattedUrl": "https://docs.lamini.ai/authenticate/"
      },
      {
        "title": "About - Lamini Docs",
        "link": "https://lamini-ai.github.io/about/",
        "snippet": "Lamini Platform orchestrates GPUs to deliver exceptional LLM tuning and inference capabilities, which easily integrate into enterprise applications via the ...",
        "formattedUrl": "https://lamini-ai.github.io/about/"
      },
      {
        "title": "AWS EKS Setup - Lamini Docs",
        "link": "https://lamini-ai.github.io/self_managed/aws_eks_setup/",
        "snippet": "Set Up NFS. Create AWS S3 File Gateway. Type Storage Gateway in the search bar of AWS Console. ... Click on Create gateway. ... Enter gateway name and timezone.",
        "formattedUrl": "https://lamini-ai.github.io/self_managed/aws_eks_setup/"
      },
      {
        "title": "Hyperparameters - Lamini Docs",
        "link": "https://docs.lamini.ai/tuning/hyperparameters/",
        "snippet": "Lamini tuning supports most hyperparameters in HuggingFace's training arguments, as well as some Lamini-specific options. These can be set in the tune method.",
        "formattedUrl": "https://docs.lamini.ai/tuning/hyperparameters/"
      },
      {
        "title": "Prompt Templates - Lamini Docs",
        "link": "https://lamini-ai.github.io/inference/prompt_templates/",
        "snippet": "Different models have different system prompt templates. Using the correct template when prompt tuning can have a large effect on model performance.",
        "formattedUrl": "https://lamini-ai.github.io/inference/prompt_templates/"
      },
      {
        "title": "Quick Start - Lamini Docs",
        "link": "https://lamini-ai.github.io/inference/quick_start/",
        "snippet": "Their advanced cognitive abilities, social complexity, and adaptability make them a strong candidate for exceptional memory. However, it's essential to remember ...",
        "formattedUrl": "https://lamini-ai.github.io/inference/quick_start/"
      },
      {
        "title": "Memory Tuning",
        "link": "https://lamini-ai.github.io/tuning/memory_tuning/",
        "snippet": "Principles for Memory Tuning · Become one with the data. Deeply understand your dataset and your eval, and refine them to high quality · Set up the end-to-end ...",
        "formattedUrl": "https://lamini-ai.github.io/tuning/memory_tuning/"
      },
      {
        "title": "Performance - Lamini Docs",
        "link": "https://lamini-ai.github.io/inference/performance/",
        "snippet": "Inference responses can be truncated (cut off, or appear incomplete) because the request could not be completed in the time allotted (timeout), or because the ...",
        "formattedUrl": "https://lamini-ai.github.io/inference/performance/"
      },
      {
        "title": "JSON Output - Lamini Docs",
        "link": "https://lamini-ai.github.io/inference/json_output/",
        "snippet": "Multiple outputs in JSON schema. You can also add multiple output types in one call. The output is a JSON schema that is also strictly enforced. ... Great! You've ...",
        "formattedUrl": "https://lamini-ai.github.io/inference/json_output/"
      },
      {
        "title": "Batching - Lamini Docs",
        "link": "https://lamini-ai.github.io/inference/batching/",
        "snippet": "A better way to batch · The entire batch blocks on the request that takes the longest to process. · A second batch cannot be processed until the first batch is ...",
        "formattedUrl": "https://lamini-ai.github.io/inference/batching/"
      },
      {
        "title": "Memory RAG",
        "link": "https://www.lamini.ai/memory-rag",
        "snippet": "High-accuracy RAG without the complexity. Memory RAG makes it simple to build highly accurate mini-agents by leveraging embed-time compute to create intelligent ...",
        "formattedUrl": "https://www.lamini.ai/memory-rag"
      },
      {
        "title": "lamini - Lamini Docs",
        "link": "https://lamini-ai.github.io/lamini_python_class/lamini/",
        "snippet": "Get training logs for a memory RAG job. Args: job_id: The ID of the memory RAG job Returns: List of log lines",
        "formattedUrl": "https://lamini-ai.github.io/lamini_python_class/lamini/"
      }
    ]
  },
  "general_search_markdown": "# Official social media\n- [Lamini on LinkedIn](https://linkedin.com/company/lamini) (May 1, 2024)\n- [Lamini on X (formerly Twitter)](https://x.com/LaminiAI) (Date not specified)\n\n# Job boards\n- [Gen AI Product Engineer at Lamini Inc. | Rise Open Jobs](https://app.joinrise.co/jobs/lamini-inc-gen-ai-product-engineer-c443) (Jan 22, 2025)\n- [Lamini AI Product Marketing Lead/Specialist | Welcome to the ...](https://app.otta.com/jobs/-9GZ2_qy) (Date not specified)\n- [Machine Learning Engineer - Lamini | Built In](https://builtin.com/job/machine-learning-engineer/4503239) (4 days ago)\n- [Lamini Inc. - Machine Learning Engineer](https://jobs.lever.co/laminiai/98ad68fc-fa8e-438c-a0dc-054ea713592c) (Date not specified)\n- [Machine Learning Engineering Intern - Lamini](https://talents.vaia.com/companies/lamini/machine-learning-engineering-intern-1267801/) (Date not specified)\n\n# App stores\n- No relevant app store links found.\n\n# Product reviews\n- No detailed product reviews found.\n\n# News articles (most recent first, grouped by event)\n### Lamini Memory Tuning\n- [Lamini AI's Memory Tuning Achieves 95% Accuracy and Reduces ...](https://marktechpost.com/2024/06/17/lamini-ais-memory-tuning-achieves-95-accuracy-and-reduces/) (Jun 17, 2024)\n- [Introducing Lamini Memory Tuning: 95% LLM Accuracy, 10x Fewer ...](https://lamini.ai/59) (Jun 13, 2024)\n- [Banishing LLM Hallucinations Requires Rethinking Generalization](https://arxiv.org/5) (Jun 25, 2024)\n\n### Funding and Partnerships\n- [Lamini raises $25M for its AI development and inference platform ...](https://siliconangle.com/2024/05/03/lamini-raises-25m-for-its-ai-development-and-inference-platform/) (May 3, 2024)\n- [Dropbox, Figma CEOs back Lamini, a startup building a generative ...](https://techcrunch.com/2024/05/02/dropbox-figma-ceos-back-lamini-a-startup-building-a-generative/) (May 2, 2024)\n- [Lamini Raises $25M in Funding](https://finsmes.com/2024/05/03/lamini-raises-25m-in-funding.html) (May 3, 2024)\n\n### Company Overview\n- [Meet Sharon Zhou, the AI Founder Doing Just Fine Without Nvidia ...](https://businessinsider.com/2024/04/01/meet-sharon-zhou-the-ai-founder-doing-just-fine-without-nvidia) (Apr 1, 2024)\n- [Lamini, A Startup Transforming Enterprises With Its Generative AI ...](https://workhub.ai/2024/05/09/lamini-a-startup-transforming-enterprises-with-its-generative-ai/) (May 9, 2024)\n\n# Key employees (grouped by employee)\n### Sharon Zhou\n- [Sharon Zhou, PhD - Lamini | LinkedIn](https://linkedin.com/in/sharon-zhou) (Date not specified)\n- [Finetuning Large Language Models - DeepLearning.AI](https://deeplearning.ai/52) (Date not specified)\n\n# Other pages on the company website\n- [About - Lamini Docs](https://lamini-ai.github.io/21) (Date not specified)\n- [Lamini - Enterprise LLM Platform](https://lamini.ai/56) (Date not specified)\n- [Lamini - Enterprise LLM Platform](https://lamini.ai/63) (Date not specified)\n\n# Other\n- [Lamini: Customizable Enterprise LLM Platform | Deepgram](https://deepgram.com/ai-apps/lamini) (Jan 27, 2025)\n- [Lamini And 11 Other AI Alternatives For Large Language Models](https://theresanaiforthat.com/ai/lamini/) (Mar 7, 2025)\n- [Lamini - Company Profile - Tracxn](https://tracxn.com/d/companies/lamini/__0zhK4zeC_TjBLGUmmJa0wkXDZnw1J5zLfilfESMhY4c) (Feb 1, 2025)",
  "crunchbase_markdown": "# Lamini, founded 2022-08-31 [(Crunchbase, 2025)](https://www.crunchbase.com/organization/powerml)\nLamini is the Enterprise AI platform for Expert AI, enabling every enterprise to safely, quickly, and cost-effectively build their own Expert AI. The company has optimized LLM workloads for Expert AI on any infrastructure, from developer interfaces to GPU hardware optimizations with partners.\n\n- [Website](https://www.lamini.ai)\n- [LinkedIn](https://www.linkedin.com/company/lamini-ai)\n- [Twitter](https://twitter.com/LaminiAI)\n\n## Funding (25M USD total)\n\n- 25M USD on 2024-04-01\n\n## News\n\n- Eye On AI: AMD’s Big Deal Is Just Their Latest In AI ([Chris Metinko, 2024-08-22](https://news.crunchbase.com/ai/amd-big-tech-defense-investment-msft-goog-nvda/))\n- Lamini raises $25M for its AI development and inference platform ([SiliconANGLE, 2024-05-03](https://siliconangle.com/2024/05/03/lamini-raises-25m-ai-development-inference-platform/))\n- Lamini Raises $25M in Funding ([FinSMEs, 2024-05-03](https://www.finsmes.com/2024/05/lamini-raises-25m-in-funding.html))\n- Enterprise Generative AI Platform Lamini Raises $25M ([Voicebot.ai, 2024-05-02](https://voicebot.ai/2024/05/02/enterprise-generative-ai-platform-lamini-raises-25m/))\n- Dropbox, Figma CEOs back Lamini, a startup building a generative AI platform for enterprises ([TechCrunch, 2024-05-02](https://techcrunch.com/2024/05/02/dropbox-figma-ceos-back-lamini-a-startup-building-a-generative-ai-platform-for-enterprises/))\n\n",
  "customer_experience_result": {
    "output_text": "# Company: Lamini\n\n## Overview and Background\n- \"Lamini is a startup with 10 employees at most, existing for 21 months at most, and has 100 or so AMD AI gpus.\" [(bl0797, Reddit, 2023-09-28)](https://www.reddit.com/r/AMD_Stock/comments/16ssmqw/ai_startup_lamini_bets_future_on_amds_instinct/k2n3rsq/)\n- \"Lamini, maybe the highest-profile AMD-backed startup, has raised a total of $25 million across seed and Series A rounds.\" [(bl0797, Reddit, 2024-05-02)](https://www.reddit.com/r/NVDA_Stock/comments/1cion3q/another_reason_not_to_worry_about_amd_coreweave/)\n- \"Over 5000 companies have joined Lamini’s waitlist since we launched several months ago.\" [(mark_mt, Reddit, 2023-09-26)](https://www.reddit.com/r/AMD_Stock/comments/16ssmqw/ai_startup_lamini_bets_future_on_amds_instinct/)\n\n## Achievements and Recognition\n- \"Using Lamini software, ROCm has achieved software parity with CUDA for LLMs.\" [(Greg Diamos, Reddit, 2023-09-26)](https://www.reddit.com/r/AMD_Stock/comments/16ssmqw/ai_startup_lamini_bets_future_on_amds_instinct/k2b3apc/)\n- \"Lamini announced it had been 'secretly running on more than one hundred' AMD Instinct MI200 series GPUs and said the ROCm software platform 'has achieved software parity' with Nvidia’s dominant CUDA platform for such models.\" [(bl0797, Reddit, 2024-05-02)](https://www.reddit.com/r/NVDA_Stock/comments/1cion3q/another_reason_not_to_worry_about_amd_coreweave/)\n- \"On the 2023 Q3 earnings call, Lisa Su said 'AI start-up, Lamini, announced they achieved software parity with CUDA for LLMs running on Instinct MI250 GPUs, enabling enterprise customers to easily deploy production-ready LLMs.'\" [(bl0797, Reddit, 2025-02-19)](https://www.reddit.com/r/AMD_Stock/comments/1it7mkl/greg_diamos_cofounder_and_cto_of_lamini_has_quit/)\n\n## Market Position\n- \"This is a game changer for fine tuning and inference market coupling with the trending LLM open source.\" [(iCoinnn, Reddit, 2023-09-27)](https://www.reddit.com/r/AMD_Stock/comments/16ssmqw/ai_startup_lamini_bets_future_on_amds_instinct/k2ddnrc/)\n- \"Just place an LLM Superstation order to run your own Llama 2-70B out of the box—available now and with an attractive price tag (10x less than AWS).\" [(whatevermanbs, Reddit, 2023-09-26)](https://www.reddit.com/r/AMD_Stock/comments/16ssmqw/ai_startup_lamini_bets_future_on_amds_instinct/k2bmnhq/)\n- \"Lamini LLM superstations have zero lead time and no hardware shortage.\" [(whatevermanbs, Reddit, 2023-09-26)](https://www.reddit.com/r/AMD_Stock/comments/16ssmqw/ai_startup_lamini_bets_future_on_amds_instinct/k2bmnhq/)\n\n# Product: Lamini\n\n## Innovations and Features\n- \"Lamini Memory Tuning is a new way to embed facts into LLMs that improves factual accuracy and reduces hallucinations to previously unachievable levels — for one Fortune 500 customer, Lamini Memory Tuning led to 95% accuracy compared to 50% with other approaches.\" [(we_are_mammals, Reddit, 2024-06-14)](https://www.reddit.com/r/MachineLearning/comments/1dffyfs/r_laminiai_introduces_memory_tuning_95_llm/)\n- \"The method entails tuning millions of expert adapters (e.g. LoRAs) with precise facts on top of any open-source LLM, like Llama 3 or Mistral 3.\" [(we_are_mammals, Reddit, 2024-06-14)](https://www.reddit.com/r/MachineLearning/comments/1dffyfs/r_laminiai_introduces_memory_tuning_95_llm/)\n- \"High accuracy, high speed, low cost: with Lamini Memory Tuning, you don’t have to choose.\" [(we_are_mammals, Reddit, 2024-06-14)](https://www.reddit.com/r/MachineLearning/comments/1dffyfs/r_laminiai_introduces_memory_tuning_95_llm/)\n\n## Comparisons and Challenges\n- \"One of the big questions is: how does AMD compare with NVIDIA?\" [(Greg Diamos, Reddit, 2023-09-26)](https://www.reddit.com/r/AMD_Stock/comments/16ssmqw/ai_startup_lamini_bets_future_on_amds_instinct/k2b3apc/)\n- \"Lamini claimed they did it 16 months ago. How did that work out?\" [(bl0797, Reddit, 2025-02-20)](https://www.reddit.com/r/AMD_Stock/comments/1it7mkl/greg_diamos_cofounder_and_cto_of_lamini_has_quit/mdsjbnd/)\n\n## User Experience and Feedback\n- \"Some people saying just use RAG, but you could layer that too, if you want specific document references or something.\" [(PaleAleAndCookies, Reddit, 2024-06-14)](https://www.reddit.com/r/MachineLearning/comments/1dffyfs/r_laminiai_introduces_memory_tuning_95_llm/l8js0uz/)\n- \"This kind of stuff - retrieval of actual weights (instead of text) according to recent context - is a big part of the future.\" [(blimpyway, Reddit, 2024-06-14)](https://www.reddit.com/r/MachineLearning/comments/1dffyfs/r_laminiai_introduces_memory_tuning_95_llm/l8kbzrf/)",
    "intermediate_steps": [
      "- \"Using Lamini software, ROCm has achieved software parity with CUDA for LLMs.\" [(Greg Diamos, Reddit, 2023-09-26)](cache://reddit/10)\n- \"This is a game changer for fine tuning and inference market coupling with the trending LLM open source.\" [(iCoinnn, Reddit, 2023-09-27)](cache://reddit/32)\n- \"Lamini is a startup with 10 employees at most, existing for 21 months at most, and has 100 or so AMD AI gpus.\" [(bl0797, Reddit, 2023-09-28)](cache://reddit/36)\n- \"One of the big questions is: how does AMD compare with NVIDIA?\" [(Greg Diamos, Reddit, 2023-09-26)](cache://reddit/10)\n- \"Lamini LLM superstations have zero lead time and no hardware shortage.\" [(whatevermanbs, Reddit, 2023-09-26)](cache://reddit/11)\n- \"Over 5000 companies have joined Lamini’s waitlist since we launched several months ago.\" [(mark_mt, Reddit, 2023-09-26)](cache://reddit/1)\n- \"Just place an LLM Superstation order to run your own Llama 2-70B out of the box—available now and with an attractive price tag (10x less than AWS).\" [(whatevermanbs, Reddit, 2023-09-26)](cache://reddit/11)",
      "- \"Lamini, maybe the highest-profile AMD-backed startup, has raised a total of $25 million across seed and Series A rounds.\" [(bl0797, Reddit, 2024-05-02)](cache://reddit/54)\n- \"Lamini announced it had been 'secretly running on more than one hundred' AMD Instinct MI200 series GPUs and said the ROCm software platform 'has achieved software parity' with Nvidia’s dominant CUDA platform for such models.\" [(bl0797, Reddit, 2024-05-02)](cache://reddit/54)\n- \"On the 2023 Q3 earnings call, Lisa Su said 'AI start-up, Lamini, announced they achieved software parity with CUDA for LLMs running on Instinct MI250 GPUs, enabling enterprise customers to easily deploy production-ready LLMs.'\" [(bl0797, Reddit, 2025-02-19)](cache://reddit/76)\n- \"Lamini claimed they did it 16 months ago. How did that work out?\" [(bl0797, Reddit, 2025-02-20)](cache://reddit/88)",
      "",
      "- \"Flan-t5 will take quite a lot of your time and effort to be anywhere closer to usable even on personal projects.\" [(dark_surfer, Reddit, 2023-12-27)](cache://reddit/186)\n- \"The problem with Flan-t5 in general is it gives very short responses.\" [(dark_surfer, Reddit, 2023-12-27)](cache://reddit/186)\n- \"The tokenizer for flan t5 models was also very bad.\" [(santaSJ, Reddit, 2023-12-27)](cache://reddit/187)\n- \"Even when fine-tuned it produces very short outputs which is great for some research tasks but not for what people popularly want out of a model.\" [(advo_k_at, Reddit, 2023-12-28)](cache://reddit/221)",
      "- \"Lamini Memory Tuning is a new way to embed facts into LLMs that improves factual accuracy and reduces hallucinations to previously unachievable levels — for one Fortune 500 customer, Lamini Memory Tuning led to 95% accuracy compared to 50% with other approaches.\" [(we_are_mammals, Reddit, 2024-06-14)](cache://reddit/266)\n- \"The method entails tuning millions of expert adapters (e.g. LoRAs) with precise facts on top of any open-source LLM, like Llama 3 or Mistral 3.\" [(we_are_mammals, Reddit, 2024-06-14)](cache://reddit/266)\n- \"High accuracy, high speed, low cost: with Lamini Memory Tuning, you don’t have to choose.\" [(we_are_mammals, Reddit, 2024-06-14)](cache://reddit/266)\n- \"Some people saying just use RAG, but you could layer that too, if you want specific document references or something.\" [(PaleAleAndCookies, Reddit, 2024-06-14)](cache://reddit/281)\n- \"This kind of stuff - retrieval of actual weights (instead of text) according to recent context - is a big part of the future.\" [(blimpyway, Reddit, 2024-06-14)](cache://reddit/288)"
    ],
    "url_to_review": {},
    "review_markdowns": [
      "# Post ID 1dffyfs: [R] Lamini.AI introduces Memory Tuning: 95% LLM Accuracy, 10x Fewer Hallucinations with +116 score by [(we_are_mammals, Reddit, 2024-06-14)](https://www.reddit.com/r/MachineLearning/comments/1dffyfs/r_laminiai_introduces_memory_tuning_95_llm/)\nhttps://www.lamini.ai/blog/lamini-memory-tuning\n\n* Lamini Memory Tuning is a new way to embed facts into LLMs that improves factual accuracy and reduces hallucinations to previously unachievable levels — for one Fortune 500 customer, Lamini Memory Tuning led to 95% accuracy compared to 50% with other approaches. Hallucinations were reduced from 50% to 5%.\n* Lamini Memory Tuning is a research breakthrough that overcomes a seeming paradox in the AI world: achieving precise factual accuracy (i.e. no hallucinations) while upholding the generalization capabilities that make LLMs valuable in the first place.\n* The method entails tuning millions of expert adapters (e.g. LoRAs) with precise facts on top of any open-source LLM, like Llama 3 or Mistral 3. If the goal is to get Roman Empire facts exactly right, Lamini Memory Tuning would create experts on Caesar, aqueducts, legions, and any other facts you provide. Inspired by information retrieval, the model retrieves only the most relevant experts from an index at inference time — not all the model weights — so latency and cost are dramatically lower. High accuracy, high speed, low cost: with Lamini Memory Tuning, you don’t have to choose.\n\nResearch paper: https://github.com/lamini-ai/Lamini-Memory-Tuning/blob/main/research-paper.pdf\n\n## Comment ID l8j4118 with +54 score by [(WildPersianAppears, Reddit, 2024-06-14)](https://www.reddit.com/r/MachineLearning/comments/1dffyfs/r_laminiai_introduces_memory_tuning_95_llm/l8j4118/) (in reply to ID 1dffyfs):\n\"Mixture of LoRA's\"\n\nOr like, \"We made a Vector Database of LoRA's\".\n\nCurious.\n\n### Comment ID l8jsviu with +26 score by [(keepthepace, Reddit, 2024-06-14)](https://www.reddit.com/r/MachineLearning/comments/1dffyfs/r_laminiai_introduces_memory_tuning_95_llm/l8jsviu/) (in reply to ID l8j4118):\nIt does not look like they use a vector database. They seem to have trained a cross-attention layer for that (plus maybe another layer, their explanation is unclear to me) making it more similar to a MoE.\n\nAns also, that's not your typical LoRA: when they train on facts, they make sure that the loss on the crucial token (like the date for a specific event) is trained until the loss is zero.\n\n#### Comment ID l8lzxil with +10 score by [(light24bulbs, Reddit, 2024-06-14)](https://www.reddit.com/r/MachineLearning/comments/1dffyfs/r_laminiai_introduces_memory_tuning_95_llm/l8lzxil/) (in reply to ID l8jsviu):\nThat is SO cool. God this seems like a great approach. \n\nIt's like doing flash cards until you pass the test.\n\n## Comment ID l8j4xmn with +30 score by [(ElectronicCress3132, Reddit, 2024-06-14)](https://www.reddit.com/r/MachineLearning/comments/1dffyfs/r_laminiai_introduces_memory_tuning_95_llm/l8j4xmn/) (in reply to ID 1dffyfs):\nFeel some deja vu from this idea: having multiple LoRAs and swapping them out per prompt, but don't quite recall where I saw it. Anyone feel the same?\n\n### Comment ID l8j5wsz with +31 score by [(Monkeylashes, Reddit, 2024-06-14)](https://www.reddit.com/r/MachineLearning/comments/1dffyfs/r_laminiai_introduces_memory_tuning_95_llm/l8j5wsz/) (in reply to ID l8j4xmn):\nDidn't apple just announce similar thing on their keynote? Or are you being facetious :p\n\n#### Comment ID l8o4qwa with +5 score by [(None, Reddit, 2024-06-15)](https://www.reddit.com/r/MachineLearning/comments/1dffyfs/r_laminiai_introduces_memory_tuning_95_llm/l8o4qwa/) (in reply to ID l8j5wsz):\nYes, they also tried to make this idea look like their innovation, although they didn't claim they came up with it.\n\nEdit: but this specific paper looks very interesting.\n\n### Comment ID l8oqubj with +4 score by [(Emotional_Egg_251, Reddit, 2024-06-15)](https://www.reddit.com/r/MachineLearning/comments/1dffyfs/r_laminiai_introduces_memory_tuning_95_llm/l8oqubj/) (in reply to ID l8j4xmn):\nThere's [Lorax ](https://github.com/predibase/lorax)and [Multi-LoRAs](https://github.com/uukuguy/multi_loras).\n\nLorax:\n\n>Dynamic Adapter Loading: include any fine-tuned LoRA adapter from HuggingFace, Predibase, or any filesystem in your request, it will be loaded just-in-time without blocking concurrent requests. Merge adapters per request to instantly create powerful ensembles.\n\nMulti-Lora:\n\n>Load multiple LoRA modules simultaneously and automatically switch the appropriate combination of LoRA modules to generate the best answer based on user queries.\n\nBoth repos have been around for about 8+ months, but not sure exactly how close they are to this.\n\n### Comment ID l8k008j with +2 score by [(LouisAckerman, Reddit, 2024-06-14)](https://www.reddit.com/r/MachineLearning/comments/1dffyfs/r_laminiai_introduces_memory_tuning_95_llm/l8k008j/) (in reply to ID l8j4xmn):\nClosest one to me is Prompt-based continual learning (e.g., L2P) where they select an instance-wise prompt from a prompt pool for task-agnostic inference.\n\n## Comment ID l8kzwre with +10 score by [(marr75, Reddit, 2024-06-14)](https://www.reddit.com/r/MachineLearning/comments/1dffyfs/r_laminiai_introduces_memory_tuning_95_llm/l8kzwre/) (in reply to ID 1dffyfs):\nSeems interesting. The performance numbers don't make a lot of sense to me, though. 50% accuracy to 95% accuracy seems... Very low on both sides?\n\nI have evaluation suites for my agentic AI features. They are faithful to the data they retrieve well over 99% of the time. The failures are almost always from a) difficulty mapping the domain they are retrieving from to a tool and a rendering format that will help them succeed or b) some hallucination or reasoning failure unrelated to the data retrieved.\n\nSo:\n\n- 50% seems like \"sub-RAG\" accuracy\n- 95% seems like \"sub-RAG\" accuracy\n- I'd still have to be able to render my domain to the model to fine-tune it, so I'm not saving any design/knowledge work, just moving compute to training time (with positives and negatives)\n- Lamini doesn't appear to solve my highest-priority issues\n\nI'll keep an eye on the project and look forward to seeing use cases I'm not thinking of, but it looks like too much squeeze for not enough juice right now.\n\n## Comment ID l8iu8o1 with +22 score by [(West-Code4642, Reddit, 2024-06-14)](https://www.reddit.com/r/MachineLearning/comments/1dffyfs/r_laminiai_introduces_memory_tuning_95_llm/l8iu8o1/) (in reply to ID 1dffyfs):\nI like the name 'Massive Array of Mixture of Memory Experts' on page 7 (MAMME)\n\n## Comment ID l8js0uz with +19 score by [(PaleAleAndCookies, Reddit, 2024-06-14)](https://www.reddit.com/r/MachineLearning/comments/1dffyfs/r_laminiai_introduces_memory_tuning_95_llm/l8js0uz/) (in reply to ID 1dffyfs):\nSome people saying just use RAG, but you could layer that too, if you want specific document references or something. But LoRA, especially with this approach, will bring in a whole bunch of related knowledge, embedded in the weights rather using up context. If I gave you a chemistry textbook and started quizzing you on it, you'll do much better if you've previously done a science degree, and internalized related topics, than otherwise.\n\n### Comment ID l8m049e with +5 score by [(light24bulbs, Reddit, 2024-06-14)](https://www.reddit.com/r/MachineLearning/comments/1dffyfs/r_laminiai_introduces_memory_tuning_95_llm/l8m049e/) (in reply to ID l8js0uz):\nThis has the potential to be many times better than rag, absolutely. It's going to be really cool to see this applied to domain specific knowledge bases\n\n### Comment ID l8leygk with +4 score by [(xt-89, Reddit, 2024-06-14)](https://www.reddit.com/r/MachineLearning/comments/1dffyfs/r_laminiai_introduces_memory_tuning_95_llm/l8leygk/) (in reply to ID l8js0uz):\nParametrized memory has a higher potential than context memory. Recent research on grokking circuits show that. So if they get to the point of swapping in domain specific grokked circuits then this could be very powerful.\n\n#### Comment ID l8lsnab with +2 score by [(alnrott, Reddit, 2024-06-14)](https://www.reddit.com/r/MachineLearning/comments/1dffyfs/r_laminiai_introduces_memory_tuning_95_llm/l8lsnab/) (in reply to ID l8leygk):\nDo you have any research on grokking circuits show that ? It's interesting see the expert mix as it is currently being used.\n\n## Comment ID l8koecw with +5 score by [(Dr_Love2-14, Reddit, 2024-06-14)](https://www.reddit.com/r/MachineLearning/comments/1dffyfs/r_laminiai_introduces_memory_tuning_95_llm/l8koecw/) (in reply to ID 1dffyfs):\nThe article sounds like it can only handle a customized list of hallucinations, and their workflow diagram shows that the final generated answer doesn't even use the trained non-hallucinating model. Does this mean the application is limited to client searches of a small database and cannot be scaled to a large flagship model to limit general hallucinations?\n\n### Comment ID l8m0byi with +2 score by [(light24bulbs, Reddit, 2024-06-14)](https://www.reddit.com/r/MachineLearning/comments/1dffyfs/r_laminiai_introduces_memory_tuning_95_llm/l8m0byi/) (in reply to ID l8koecw):\nI'd be interested to know also. Even if that's the case, there's still a massively large set of applications where that's useful. Maybe actually most of them. Building expert systems to go along with books, company knowledge bases, etc. Extremely useful and profitable.\n\nBasically anytime people were using rag, which always struck me as very sub-par in results.\n\n## Comment ID l8joaey with +10 score by [(rwl4z, Reddit, 2024-06-14)](https://www.reddit.com/r/MachineLearning/comments/1dffyfs/r_laminiai_introduces_memory_tuning_95_llm/l8joaey/) (in reply to ID 1dffyfs):\nSo, basically, they overfit a LoRA adapter, then index the dataset they trained that adapter with in a vector database, then when a user interacts with the app, it first searches the vector database using the user's prompt and then loads the corresponding adapter?\n\nI guess overfitting FTW?\n\n## Comment ID l8kbzrf with +3 score by [(blimpyway, Reddit, 2024-06-14)](https://www.reddit.com/r/MachineLearning/comments/1dffyfs/r_laminiai_introduces_memory_tuning_95_llm/l8kbzrf/) (in reply to ID 1dffyfs):\nThis kind of stuff - retrieval of actual weights (instead of text) according to recent context - is a big part of the future. \n\nThere are two reasons both with a huge impact in lowering compute costs : \n\n- it allows a huge number of trainable parameters with a relatively small number of active ones during training and inference\n\n- unlike RAG, it doesn't need excessively large token window to fit all potentially relevant documents \n\n  \nI wonder how useful would be applying the same method to continuously fine tune on conversation history itself. Like some kind of agent remembering all its conversation history.\n\n### Comment ID l8kutn4 with +2 score by [(visarga, Reddit, 2024-06-14)](https://www.reddit.com/r/MachineLearning/comments/1dffyfs/r_laminiai_introduces_memory_tuning_95_llm/l8kutn4/) (in reply to ID l8kbzrf):\nyes, when will we have \"context to LoRA\"\n\n## Comment ID l8k545p with +1 score by [(Best-Association2369, Reddit, 2024-06-14)](https://www.reddit.com/r/MachineLearning/comments/1dffyfs/r_laminiai_introduces_memory_tuning_95_llm/l8k545p/) (in reply to ID 1dffyfs):\nYeah but then what about your Lora adapter....\n\n## Comment ID l8k8exz with +1 score by [(ID4gotten, Reddit, 2024-06-14)](https://www.reddit.com/r/MachineLearning/comments/1dffyfs/r_laminiai_introduces_memory_tuning_95_llm/l8k8exz/) (in reply to ID 1dffyfs):\nI wonder if this is going to create a combinatorial number of new ways to jailbreak the model that will make it that much harder to identify and protect against\n\n### Comment ID l8kv7jt with +3 score by [(marr75, Reddit, 2024-06-14)](https://www.reddit.com/r/MachineLearning/comments/1dffyfs/r_laminiai_introduces_memory_tuning_95_llm/l8kv7jt/) (in reply to ID l8k8exz):\nMaybe? This is probably generally true of Mixture of Experts. The near future of guardrails, safety, and alignment is probably activation engineering, though. That might even be easier with Mixture of Experts because you can train an auto-encoder on the activations of each expert instead of a larger network.\n\n## Comment ID l8mqlld with +1 score by [(gBoostedMachinations, Reddit, 2024-06-14)](https://www.reddit.com/r/MachineLearning/comments/1dffyfs/r_laminiai_introduces_memory_tuning_95_llm/l8mqlld/) (in reply to ID 1dffyfs):\n“10x fewer” it’s always amusing when people choose the silliest way possible to communicate an effect size.\n\n## Comment ID l90xzjd with +1 score by [(Naive_Expression_972, Reddit, 2024-06-17)](https://www.reddit.com/r/MachineLearning/comments/1dffyfs/r_laminiai_introduces_memory_tuning_95_llm/l90xzjd/) (in reply to ID 1dffyfs):\nTo me it seems like an overfitted LoRA adapter for each training data (or group of data) being served through LORAX or SLoLRA (run time adapter swapping) and picking the adapter during inference using vector search.\nA RAG with lots of bells and whistles and makes a great client demo.\n\n### Comment ID lgxyltz with +1 score by [(30299578815310, Reddit, 2024-08-07)](https://www.reddit.com/r/MachineLearning/comments/1dffyfs/r_laminiai_introduces_memory_tuning_95_llm/lgxyltz/) (in reply to ID l90xzjd):\nLora is weights though, RAG is context.",
      "# Post ID 1320hyh: [P] Lamini rapidly achieves ChatGPT performance with an LLM Engine with +0 score by [(gdiamos, Reddit, 2023-04-28)](https://www.reddit.com/r/MachineLearning/comments/1320hyh/p_lamini_rapidly_achieves_chatgpt_performance/)\nAccording to the authors, Lamini AI has invented an LLM Engine for rapidly customizing models.  \n\nRead the blog post, github, and huggingface for details.  \n\n* Blog [https://lamini.ai/blog/introducing-lamini](https://lamini.ai/blog/introducing-lamini) \n* Code \n   * Chat data ([https://github.com/lamini-ai/lamini/](https://github.com/lamini-ai/lamini/)) \n   * SQL data ([https://github.com/lamini-ai/lamini-sql/](https://github.com/lamini-ai/lamini-sql/))\n* LLM Type System Playground: [https://app.lamini.ai](https://app.lamini.ai/)\n* Open-source fine-tuned LLMs that follow instructions: \n   * [weights](https://huggingface.co/lamini/instruct-tuned-2.8b) \n   * [playground](https://huggingface.co/spaces/lamini/instruct-playground)\n\n## Comment ID ji2rea0 with +134 score by [(ZestyData, Reddit, 2023-04-28)](https://www.reddit.com/r/MachineLearning/comments/1320hyh/p_lamini_rapidly_achieves_chatgpt_performance/ji2rea0/) (in reply to ID 1320hyh):\nBold and unsubstantiated claims from a person with financial stakes 👍\n\n## Comment ID ji2wkl4 with +45 score by [(ForceBru, Reddit, 2023-04-28)](https://www.reddit.com/r/MachineLearning/comments/1320hyh/p_lamini_rapidly_achieves_chatgpt_performance/ji2wkl4/) (in reply to ID 1320hyh):\nInput to Lamini:\n\n> Given a dog and a cat, describe their relationship.\n\nOutput:\n\n```\nThe dog and the cat are friends. They like each other and have similar personalities. They are both independent and like to be in control. They are both loyal and will protect their friends.\n\n!\n\n!\n\n!\nThe dog and the cat are best friends. They like each other and have similar personalities. They are both independent and like to be in control. They are both loyal and will protect their friends.\n\nagain!\n\n!\n\n!\n\nagain!\n\n!\nThe dog is a dog and the cat is a cat. They are best friends. The like each other and are\n```\n\nApparently, dogs and cats are best friends? Again! ! Again!\n\n### Comment ID ji3o8un with +6 score by [(gdiamos, Reddit, 2023-04-28)](https://www.reddit.com/r/MachineLearning/comments/1320hyh/p_lamini_rapidly_achieves_chatgpt_performance/ji3o8un/) (in reply to ID ji2wkl4):\nThanks for the feedback.\n\nThat hugging face playground didn't have stop tokens enabled.  Try it now:\n\n>Given a dog and a cat, describe their relationship.\n\n\"The dog and the cat are friends. They like each other and have similar personalities. They are both independent and like to be in control. They are both loyal and will protect their friends.\"\n\n## Comment ID ji2qkeo with +55 score by [(Dapper_Cherry1025, Reddit, 2023-04-28)](https://www.reddit.com/r/MachineLearning/comments/1320hyh/p_lamini_rapidly_achieves_chatgpt_performance/ji2qkeo/) (in reply to ID 1320hyh):\n[GPT-3.5 response vs Lamini](https://imgur.com/a/Rf4i2V8)\n\nI really wish the blog told us in what ways it achieves chatGPT performance, but it really tells us nothing about what they're basing that off of. They also don't make a distinction between gpt-3.5 and gpt-4.\n\n### Comment ID ji2rrc4 with +2 score by [(Carrasco_Santo, Reddit, 2023-04-28)](https://www.reddit.com/r/MachineLearning/comments/1320hyh/p_lamini_rapidly_achieves_chatgpt_performance/ji2rrc4/) (in reply to ID ji2qkeo):\nTrue, it would be nice for them to specify which ChatGPT version they are referring to. I think that any open version that has the performance very close or equal to version 3.5 is already great. 4.0 is just a distant dream at the moment. :P\n\n#### Comment ID ji49uj5 with +2 score by [(None, Reddit, 2023-04-28)](https://www.reddit.com/r/MachineLearning/comments/1320hyh/p_lamini_rapidly_achieves_chatgpt_performance/ji49uj5/) (in reply to ID ji2rrc4):\nIt's not close to 3.5 in instruction following capabilities. I tested it out in their playground.\n\n## Comment ID ji340nm with +5 score by [(TheCastleReddit, Reddit, 2023-04-28)](https://www.reddit.com/r/MachineLearning/comments/1320hyh/p_lamini_rapidly_achieves_chatgpt_performance/ji340nm/) (in reply to ID 1320hyh):\nIt's open source in what sense exactly if I cannot run locally the training module?\n\n### Comment ID ji34x61 with +3 score by [(gdiamos, Reddit, 2023-04-28)](https://www.reddit.com/r/MachineLearning/comments/1320hyh/p_lamini_rapidly_achieves_chatgpt_performance/ji34x61/) (in reply to ID ji340nm):\nThe data pipeline is open source.  [https://github.com/lamini-ai/lamini](https://github.com/lamini-ai/lamini)\n\nThere is a high level python library that simplifies training (it runs in the cloud because those accelerator machines are expensive and hard to configure): https://lamini-ai.github.io/\n\nIf you do want to fine-tune on the generated data yourself, and are willing to bring your own GPU, etc.  Consider one of many good open fine-tuning frameworks, e.g. Alpaca-Lora: https://github.com/tloen/alpaca-lora\n\n#### Comment ID ji3k16d with +3 score by [(TheCastleReddit, Reddit, 2023-04-28)](https://www.reddit.com/r/MachineLearning/comments/1320hyh/p_lamini_rapidly_achieves_chatgpt_performance/ji3k16d/) (in reply to ID ji34x61):\nThanks. The alpaca lora repo is great. Chers!\n\n## Comment ID ji2u481 with +4 score by [(None, Reddit, 2023-04-28)](https://www.reddit.com/r/MachineLearning/comments/1320hyh/p_lamini_rapidly_achieves_chatgpt_performance/ji2u481/) (in reply to ID 1320hyh):\n[deleted]\n\n### Comment ID ji2w3cw with +3 score by [(gdiamos, Reddit, 2023-04-28)](https://www.reddit.com/r/MachineLearning/comments/1320hyh/p_lamini_rapidly_achieves_chatgpt_performance/ji2w3cw/) (in reply to ID ji2u481):\nThe first step is to hook up a baseline model.  You can use the python API here.\n\n[https://lamini-ai.github.io/python\\_library](https://lamini-ai.github.io/python_library)\n\nAfter that, you should look at the data generator library: [https://github.com/lamini-ai/lamini](https://github.com/lamini-ai/lamini)\n\nIf you provide about 100 seed examples, you can synthesize a training dataset, e.g. if you had REPL interactions like this, you could use them as seed examples:\n\n`>>> globals()`\n\n`{'__name__': '__main__',...'read_data': <function read_data at 0x104dd4860>,'sample': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],'mean': <function mean at 0x104fe3ec0>,'average': 5.5}`\n\n`>>> mean([2, 3, 3, 2])`\n\n`2.5`\n\n#### Comment ID ji2wrj9 with +5 score by [(None, Reddit, 2023-04-28)](https://www.reddit.com/r/MachineLearning/comments/1320hyh/p_lamini_rapidly_achieves_chatgpt_performance/ji2wrj9/) (in reply to ID ji2w3cw):\n[deleted]\n\n## Comment ID ji3bvkn with +4 score by [(loopy_fun, Reddit, 2023-04-28)](https://www.reddit.com/r/MachineLearning/comments/1320hyh/p_lamini_rapidly_achieves_chatgpt_performance/ji3bvkn/) (in reply to ID 1320hyh):\ni asked this question in the playground it did not understand? see example\r  \n\r  \nwhich of these are ingredients to make a pie sugar,flower.apple filling,paper,pencil,carrot or gasoline\r  \n\r  \n\r  \napple,paper,pencil,carrots\r  \n\r  \n\r  \napple,paper,pencil\r  \n\r  \n\r  \napple,paper,pencils\r  \n\r  \n\r  \napple,paper,pens\r  \n\r  \n\r  \napple,paper,penny\r  \n\r  \n\r  \napple,paper,pennies\r  \n\r  \n\r  \napple,paper,pence\r  \n\r  \n\r  \napple,paper,pences\r  \n\r  \n\r  \napple,paper,pays\r  \n\r  \n\r  \napple,paper,prices\r  \n\r  \n\r  \napple,paper,price\r  \n\r  \n\r  \napple,paper,price\r  \n\r  \n\r  \n\r  \n\r  \napple,paper,price\r  \n\r  \n also\r  \napple,paper,price\n\n### Comment ID ji3pvma with +3 score by [(gdiamos, Reddit, 2023-04-28)](https://www.reddit.com/r/MachineLearning/comments/1320hyh/p_lamini_rapidly_achieves_chatgpt_performance/ji3pvma/) (in reply to ID ji3bvkn):\nThis playground is a bare wrapper around the model.  Things like this don't get handled \"pie sugar,flower.apple filling\" very well and a production model would likely include more post/pre-processing.  Here's a similar question that is better for the tokenizer.\n\n>Which of these are ingredients to make a pie: sugar, flour, apple filling, baking paper, pencil, carrot or gasoline?\n\n\"The ingredients to make a pie are sugar, flour, apple filling and baking paper. Pencil and gasoline are not ingredients to make a pie.\"  \n\n\nThis playground is intended to be an example of how to train a model using the data pipeline.  It is not meant for production use.\n\n#### Comment ID ji4p9jl with +3 score by [(loopy_fun, Reddit, 2023-04-29)](https://www.reddit.com/r/MachineLearning/comments/1320hyh/p_lamini_rapidly_achieves_chatgpt_performance/ji4p9jl/) (in reply to ID ji3pvma):\noh !\n\n## Comment ID ji3h9o6 with +5 score by [(Imnimo, Reddit, 2023-04-28)](https://www.reddit.com/r/MachineLearning/comments/1320hyh/p_lamini_rapidly_achieves_chatgpt_performance/ji3h9o6/) (in reply to ID 1320hyh):\n>Some of the generated data is good, some not. Before fine-tuning, the next step is to filter the generated data to mostly high-quality data (just run this simple script in the same repo)\n\nThe \"simple script\" is literally just de-duplication.  Is this a joke?\n\nhttps://github.com/lamini-ai/lamini/blob/main/remove_duplicates.py\n\n## Comment ID ji44uuk with +3 score by [(FutureIsMine, Reddit, 2023-04-28)](https://www.reddit.com/r/MachineLearning/comments/1320hyh/p_lamini_rapidly_achieves_chatgpt_performance/ji44uuk/) (in reply to ID 1320hyh):\nIs this even legal? Could you fine-tune on OpenAI outputs if a user gets completions for them? Can alpaca be used for commercial purposes?\n\n### Comment ID ji464ov with +7 score by [(gdiamos, Reddit, 2023-04-28)](https://www.reddit.com/r/MachineLearning/comments/1320hyh/p_lamini_rapidly_achieves_chatgpt_performance/ji464ov/) (in reply to ID ji44uuk):\nWe don’t fine tune on OpenAI outputs.  The foundation model to generate questions is fined tuned Pythia.\n\nThe input dataset is from instruct-gpt which has an Apache 2 license.\n\nIf you want higher quality you can switch to GPT4 with one line of code, but as you say, it would violate the OpenAI terms of service.\n\n## Comment ID ji5jp07 with +5 score by [(Motor_Storm_3853, Reddit, 2023-04-29)](https://www.reddit.com/r/MachineLearning/comments/1320hyh/p_lamini_rapidly_achieves_chatgpt_performance/ji5jp07/) (in reply to ID 1320hyh):\nThis is *extremely* misleading.\n\nHow are you going to achieve GPT-3.5 results with an inferior base model, a much smaller dataset, just SFT, and on AI-generated data?\n\n## Comment ID ji2q7be with +8 score by [(None, Reddit, 2023-04-28)](https://www.reddit.com/r/MachineLearning/comments/1320hyh/p_lamini_rapidly_achieves_chatgpt_performance/ji2q7be/) (in reply to ID 1320hyh):\n[deleted]\n\n## Comment ID ji77kpb with +3 score by [(dreamingleo12, Reddit, 2023-04-29)](https://www.reddit.com/r/MachineLearning/comments/1320hyh/p_lamini_rapidly_achieves_chatgpt_performance/ji77kpb/) (in reply to ID 1320hyh):\n“Lamini is the world's most powerful LLM engine, unlocking the power of generative AI for every company by putting their data to work.”\n\nHmm I’m questioning their qualification as researchers.\n\n## Comment ID ji49pxe with +2 score by [(None, Reddit, 2023-04-28)](https://www.reddit.com/r/MachineLearning/comments/1320hyh/p_lamini_rapidly_achieves_chatgpt_performance/ji49pxe/) (in reply to ID 1320hyh):\nThey didn't. I tested it out with the question \"If Luke's brother was twice his age when he was 3 and Luke is 40 now. What's Luke's brother's age?\"\n\n## Comment ID ji4nc1j with +2 score by [(TotesMessenger, Reddit, 2023-04-29)](https://www.reddit.com/r/MachineLearning/comments/1320hyh/p_lamini_rapidly_achieves_chatgpt_performance/ji4nc1j/) (in reply to ID 1320hyh):\nI'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit:\n\n- [/r/datascienceproject] [Lamini rapidly achieves ChatGPT performance with an LLM Engine (r\\/MachineLearning)](https://www.reddit.com/r/datascienceproject/comments/132f5q3/lamini_rapidly_achieves_chatgpt_performance_with/)\n\n&nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*\n\n## Comment ID ji5at2v with +2 score by [(LanchestersLaw, Reddit, 2023-04-29)](https://www.reddit.com/r/MachineLearning/comments/1320hyh/p_lamini_rapidly_achieves_chatgpt_performance/ji5at2v/) (in reply to ID 1320hyh):\nSome cool work with a click-bait title\n\n## Comment ID ji5mcdx with +2 score by [(MadEyeXZ, Reddit, 2023-04-29)](https://www.reddit.com/r/MachineLearning/comments/1320hyh/p_lamini_rapidly_achieves_chatgpt_performance/ji5mcdx/) (in reply to ID 1320hyh):\nhow is lamini different from \\[LangChain\\](https://github.com/hwchase17/langchain)\n\n### Comment ID ji5ncxv with +1 score by [(gdiamos, Reddit, 2023-04-29)](https://www.reddit.com/r/MachineLearning/comments/1320hyh/p_lamini_rapidly_achieves_chatgpt_performance/ji5ncxv/) (in reply to ID ji5mcdx):\nGood question.  The LLM ecosystem is just getting started and it is exciting to see new tools.  Another good framework to check out is AutoGPT [https://github.com/Significant-Gravitas/Auto-GPT](https://github.com/Significant-Gravitas/Auto-GPT) , which can manage memory, and connect to popular websites and platforms.\n\nUse lamini to create, improve, and deploy an LLM, e.g. by connecting your data pipeline.\n\nUse langchain and AutoGPT for composing multiple LLMs together, creating agents, or connecting an LLM to an index.",
      "# Post ID 1dfec4y: Introducing Lamini Memory Tuning: 95% LLM Accuracy, 10x Fewer Hallucinations with +74 score by [(mihemihe, Reddit, 2024-06-14)](https://www.reddit.com/r/singularity/comments/1dfec4y/introducing_lamini_memory_tuning_95_llm_accuracy/)\n\n\n## Comment ID l8isij7 with +11 score by [(Empty-Tower-2654, Reddit, 2024-06-14)](https://www.reddit.com/r/singularity/comments/1dfec4y/introducing_lamini_memory_tuning_95_llm_accuracy/l8isij7/) (in reply to ID 1dfec4y):\nDont know what to feel about today.\n\nWe need a AI Explained video asap\n\n### Comment ID l8jcyxs with +7 score by [(Creative-robot, Reddit, 2024-06-14)](https://www.reddit.com/r/singularity/comments/1dfec4y/introducing_lamini_memory_tuning_95_llm_accuracy/l8jcyxs/) (in reply to ID l8isij7):\nSAAAAMMMMEEEEE! I feel so lost and confused and i want him to tell me that everything’s okay in his sultry accent.😫\n\n## Comment ID l8ihipc with +15 score by [(Creative-robot, Reddit, 2024-06-14)](https://www.reddit.com/r/singularity/comments/1dfec4y/introducing_lamini_memory_tuning_95_llm_accuracy/l8ihipc/) (in reply to ID 1dfec4y):\nIs this credible? Big if true.\n\n### Comment ID l8iibyg with +11 score by [(mihemihe, Reddit, 2024-06-14)](https://www.reddit.com/r/singularity/comments/1dfec4y/introducing_lamini_memory_tuning_95_llm_accuracy/l8iibyg/) (in reply to ID l8ihipc):\nI have doubts about how this could hinder the generalization capabilities of LLM. They mention that it does not get affected, but it is probably difficult to measure.\n\nThey have one example implemented for a Fortune 200 customer for some text-to-SQL queries.\n\nIt seems useful for memory-based tasks at least, but I do not think it pushes forward the reasoning capabilities we look for on Singularity.\n\n#### Comment ID l8ilmsd with +12 score by [(SgathTriallair, Reddit, 2024-06-14)](https://www.reddit.com/r/singularity/comments/1dfec4y/introducing_lamini_memory_tuning_95_llm_accuracy/l8ilmsd/) (in reply to ID l8iibyg):\nEven if it limits the amount of creativity, you could have a master model that determines whether it is a creative or truth based task and then shunt it to the right model.\n\n#### Comment ID l8jd3l7 with +1 score by [(SynthAcolyte, Reddit, 2024-06-14)](https://www.reddit.com/r/singularity/comments/1dfec4y/introducing_lamini_memory_tuning_95_llm_accuracy/l8jd3l7/) (in reply to ID l8iibyg):\nMaybe indirectly—if LLM's are really going to help, they will need to be more accurate.\n\n### Comment ID l8iqhn3 with +2 score by [(RemarkableGuidance44, Reddit, 2024-06-14)](https://www.reddit.com/r/singularity/comments/1dfec4y/introducing_lamini_memory_tuning_95_llm_accuracy/l8iqhn3/) (in reply to ID l8ihipc):\nThey dont have a price and I would say because it looks like just another wrapper around what we are currently doing today with RAG and Verification around results. \n\nAlso 50% RAG + Prompting that's bullshit, I can get 90% if not more with just a RAG setup.\n\n## Comment ID l8jg3tr with +2 score by [(Warm_Iron_273, Reddit, 2024-06-14)](https://www.reddit.com/r/singularity/comments/1dfec4y/introducing_lamini_memory_tuning_95_llm_accuracy/l8jg3tr/) (in reply to ID 1dfec4y):\nWho creates and verifies these \"facts\"? Sounds like it would lead to a new type of hallucination: Unfounded confidence in a \"fact\" that is not actually a fact.\n\n### Comment ID l8k7il2 with +3 score by [(Empty-Tower-2654, Reddit, 2024-06-14)](https://www.reddit.com/r/singularity/comments/1dfec4y/introducing_lamini_memory_tuning_95_llm_accuracy/l8k7il2/) (in reply to ID l8jg3tr):\nWe're doing this since we learn how to talk \n\nLiterally\n\n#### Comment ID l8kb7qn with +1 score by [(Warm_Iron_273, Reddit, 2024-06-14)](https://www.reddit.com/r/singularity/comments/1dfec4y/introducing_lamini_memory_tuning_95_llm_accuracy/l8kb7qn/) (in reply to ID l8k7il2):\nHuh?\n\n## Comment ID l8l0wk0 with +2 score by [(ebolathrowawayy, Reddit, 2024-06-14)](https://www.reddit.com/r/singularity/comments/1dfec4y/introducing_lamini_memory_tuning_95_llm_accuracy/l8l0wk0/) (in reply to ID 1dfec4y):\n> The method entails tuning millions of expert adapters (e.g. LoRAs) with precise facts on top of any open-source LLM, like Llama 3 or Mistral 3.\n\nThat's a really good idea.\n\n## Comment ID l8j37rh with +1 score by [(None, Reddit, 2024-06-14)](https://www.reddit.com/r/singularity/comments/1dfec4y/introducing_lamini_memory_tuning_95_llm_accuracy/l8j37rh/) (in reply to ID 1dfec4y):\nI tried to understand what they were talking about on the website but this is above me right now.\n\n## Comment ID l8mfbni with +1 score by [(Akimbo333, Reddit, 2024-06-14)](https://www.reddit.com/r/singularity/comments/1dfec4y/introducing_lamini_memory_tuning_95_llm_accuracy/l8mfbni/) (in reply to ID 1dfec4y):\nHow though?\n\n## Comment ID l8s6u3e with +1 score by [(Gratitude15, Reddit, 2024-06-15)](https://www.reddit.com/r/singularity/comments/1dfec4y/introducing_lamini_memory_tuning_95_llm_accuracy/l8s6u3e/) (in reply to ID 1dfec4y):\nhttps://youtu.be/Bs36gxpKcqk\n\nWhat this tells me is a big deal.\n\n1-gpt4 class intelligence is probably enough to be an agent and a robot\n\n2-the world is about to change a lot\n\nGetting things wrong is a huge hindrance. This level of change means that you don't need to just add intelligence to get rid of it.\n\nI'm looking at where we are, and imagining adding them to this-\n\n-AI that is creative, smart, and makes few mistakes (although not world class or AGI level)\n\n-agentic capacity\n\n-multimodal capacity\n\n-very low voice latency\n\n-real-time low latency on screen photorealistic avatars\n\n-massive context window between 2m tokens and infinite\n\n-physics related virtual environments for mass training\n\n-robot manufacturing capabilities of various level humanoid for roughly $20k range\n\nI mean, this is stunning for 1 year. This is all current capacity, just not integrated. You can see what's going to happen. Things still missing but I'll imagine are soon-\n\n-a drag and drop virtual environment to connect any type of robot with gpt4 level intelligence (such that you can now say 'stand up' and it can follow the command)\n\n-next level intelligence (gpt5) that at least integrates the findings to date and adds agentic capacity, if not is actually smarter. \n\nI just don't see how we aren't with agents and robots by 2027, AGI or not - I'm talking a billion agents and a hundred million robots. That means remote work and in person labor starting to really be impacted.",
      "# Post ID 16ssmqw: AI startup Lamini bets future on AMD's Instinct GPUs with +90 score by [(mark_mt, Reddit, 2023-09-26)](https://www.reddit.com/r/AMD_Stock/comments/16ssmqw/ai_startup_lamini_bets_future_on_amds_instinct/)\n[https://www.theregister.com/2023/09/26/amd\\_instinct\\_ai\\_lamini/](https://www.theregister.com/2023/09/26/amd_instinct_ai_lamini/)\n\n  Lamini claims its platform, which has attracted interest from Amazon, Walmart, eBay, GitLab, and Adobe, to name a few, has been running on \"more than a 100 AMD GPUs in production all year\" and could be scaled up to \"thousands of MI GPUs.\" \n\nThe partnership with Lamini marks AMD's latest ecosystem play to make developing for its Instinct accelerators and ROCm runtime more accessible. The startup claims that using its software, AMD's ROCm runtime achieves software parity with Nvidia's CUDA, at least for large language models. \n\n&#x200B;\n\n[https://www.lamini.ai/blog/lamini-amd-paving-the-road-to-gpu-rich-enterprise-llms](https://www.lamini.ai/blog/lamini-amd-paving-the-road-to-gpu-rich-enterprise-llms)\n\n&#x200B;\n\n* **We’re unveiling a big secret: Lamini has been running LLMs on AMD InstinctTM** **GPUs over the past year—in production. Enterprise customers appreciate the top-notch performance.‍**\n* **Lamini is an exclusive way for enterprises to easily run production-ready LLMs on AMD Instinct GPUs—with only 3 lines of code today.‍**\n* **Join Fortune 500 enterprises and** [**buy your own LLM Superstation**](https://9gc3kt44b8q.typeform.com/to/mgh0j6Y5) **from Lamini today to run and finetune LLMs in your VPC or on-premise.**\n\n&#x200B;\n\nDemand for enterprise LLMs is exploding. **Over 5000 companies have joined Lamini’s waitlist since we launched several months ago.** In a [recent survey from AMD](https://www.amd.com/content/dam/amd/en/documents/solutions/ai/ai-outlook-are-it-teams-prepared.pdf) \\[1\\] of technology decision-makers, over 75% reported increasing AI investment, with 90% already having significant returns.\n\n&#x200B;\n\n&#x200B;\n\n&#x200B;\n\n## Comment ID k2b0osz with +30 score by [(whatevermanbs, Reddit, 2023-09-26)](https://www.reddit.com/r/AMD_Stock/comments/16ssmqw/ai_startup_lamini_bets_future_on_amds_instinct/k2b0osz/) (in reply to ID 16ssmqw):\nhttps://www.lamini.ai/blog/lamini-amd-paving-the-road-to-gpu-rich-enterprise-llms\n\nThe order button ain't working.\nEDIT: working now\n\n\"Now, we’re excited to open up LLM-ready GPUs to more folks. Our LLM Superstation is available both in the cloud and on-premise. It combines Lamini's easy-to-use enterprise LLM infrastructure with AMD Instinct™ MI210 and MI250 accelerators.\"\n\n\"Just place an LLM Superstation order to run your own Llama 2-70B out of the box—available now and with an attractive price tag (10x less than AWS).\"\n\n\"One of the big questions is: how does AMD compare with NVIDIA? As a former early architect on CUDA at NVIDIA, the cofounder of MLPerf, and the CTO at Lamini, Greg Diamos, says:\"\n\n\n\"Using Lamini software, ROCm has achieved software parity with CUDA for LLMs. We chose the Instinct MI250 as the foundation for Lamini because it runs the biggest models that our customers demand and integrates finetuning optimizations. We use the large HBM capacity (128GB) on MI250 to run bigger models with lower software complexity than clusters of A100s.\"—Greg Diamos, CTO at Lamini\n\n### Comment ID k2b6obd with +23 score by [(holojon, Reddit, 2023-09-26)](https://www.reddit.com/r/AMD_Stock/comments/16ssmqw/ai_startup_lamini_bets_future_on_amds_instinct/k2b6obd/) (in reply to ID k2b0osz):\nGreat news! Software parity with CUDA!!!\n\nLisa’s tweet: https://x.com/lisasu/status/1706707561809105331?s=46&t=zKqpkLhvoPYKzPPd2zKsIw\n\n### Comment ID k2b3apc with +13 score by [(whatevermanbs, Reddit, 2023-09-26)](https://www.reddit.com/r/AMD_Stock/comments/16ssmqw/ai_startup_lamini_bets_future_on_amds_instinct/k2b3apc/) (in reply to ID k2b0osz):\n\"To enable finetuning on clusters with hundreds of AMD Instinct GPUs, Lamini leverages several layers of specialized software. First, Lamini includes a high-performance inference server that allows running large models with low latency and high throughput by leveraging model caching and dynamic batching. Linked with this server is the PEFT capability to finetune tens of thousands of finetuned LLMs efficiently. \n\nOne common LLM pattern is retrieval augmented generation, which is optimized on Lamini by pushing an embedding cache directly into GPU HBM memory colocated with the LLM. Finally, Lamini is able to horizontally scale LLMs across large clusters of thousands of MI GPUs using an inference load balancer and containerized auto-scaling SLURM. Lamini LLM superstations have zero lead time and no hardware shortage.  They can be networked together to create powerful finetuning and inference systems.\n\"\n\n#### Comment ID k2bmnhq with +2 score by [(whatevermanbs, Reddit, 2023-09-26)](https://www.reddit.com/r/AMD_Stock/comments/16ssmqw/ai_startup_lamini_bets_future_on_amds_instinct/k2bmnhq/) (in reply to ID k2b3apc):\nThis whole thing sounds like a software layers workarround to not having something like nvswitch+nvlink.\n\nNext level tuning is targeting common llm patterns as described in second para. Hopefully the platform cost vs competitor is worth all the effort. Cannot complain if this is a \"what can be done now\" approach untill we have proper hardware that lets us scale uniformly (xswitch rumour).\n\n### Comment ID k2bpv6b with +2 score by [(GanacheNegative1988, Reddit, 2023-09-26)](https://www.reddit.com/r/AMD_Stock/comments/16ssmqw/ai_startup_lamini_bets_future_on_amds_instinct/k2bpv6b/) (in reply to ID k2b0osz):\nButton worked fine for me on an Android.\n\n### Comment ID k2by7ti with +1 score by [(mark_mt, Reddit, 2023-09-26)](https://www.reddit.com/r/AMD_Stock/comments/16ssmqw/ai_startup_lamini_bets_future_on_amds_instinct/k2by7ti/) (in reply to ID k2b0osz):\n\"The order button ain't working.\"\n\nCan't handle more than 5,000 customers ... come back in 2025 LOL\n\n#### Comment ID k2dpr2q with +2 score by [(whatevermanbs, Reddit, 2023-09-27)](https://www.reddit.com/r/AMD_Stock/comments/16ssmqw/ai_startup_lamini_bets_future_on_amds_instinct/k2dpr2q/) (in reply to ID k2by7ti):\nWorking now\n\n## Comment ID k2azydk with +17 score by [(makmanred, Reddit, 2023-09-26)](https://www.reddit.com/r/AMD_Stock/comments/16ssmqw/ai_startup_lamini_bets_future_on_amds_instinct/k2azydk/) (in reply to ID 16ssmqw):\nMore from Lamini's CEO Sharon Zhou: \n\nhttps://twitter.com/realSharonZhou/status/1706701693684154766\n\n### Comment ID k2b1kkb with +29 score by [(makmanred, Reddit, 2023-09-26)](https://www.reddit.com/r/AMD_Stock/comments/16ssmqw/ai_startup_lamini_bets_future_on_amds_instinct/k2b1kkb/) (in reply to ID k2azydk):\nLOL [https://twitter.com/realSharonZhou/status/1706708589837254946](https://twitter.com/realSharonZhou/status/1706708589837254946)\n\n#### Comment ID k2b2fyk with +16 score by [(whatevermanbs, Reddit, 2023-09-26)](https://www.reddit.com/r/AMD_Stock/comments/16ssmqw/ai_startup_lamini_bets_future_on_amds_instinct/k2b2fyk/) (in reply to ID k2b1kkb):\nHahah..she's got sass lol\n\n### Comment ID k2b0cth with +16 score by [(None, Reddit, 2023-09-26)](https://www.reddit.com/r/AMD_Stock/comments/16ssmqw/ai_startup_lamini_bets_future_on_amds_instinct/k2b0cth/) (in reply to ID k2azydk):\n[deleted]\n\n#### Comment ID k2b551z with +5 score by [(whatevermanbs, Reddit, 2023-09-26)](https://www.reddit.com/r/AMD_Stock/comments/16ssmqw/ai_startup_lamini_bets_future_on_amds_instinct/k2b551z/) (in reply to ID k2b0cth):\nYes imagine imagine. \n\nBack of the head: hope with this imagination stock goes up. \nThe stock: :|\n\nOP. stay put..you did a good job not giving a spin to the post in the post text. Keep it nuetral. Folks here want to imagine things done here affect the stock.. no.\n\n## Comment ID k2c3mjk with +12 score by [(bl0797, Reddit, 2023-09-26)](https://www.reddit.com/r/AMD_Stock/comments/16ssmqw/ai_startup_lamini_bets_future_on_amds_instinct/k2c3mjk/) (in reply to ID 16ssmqw):\nThis is incredible!  I want to hear more details.  A secret company started in 2022 with 6 employees has already accomplished all this, all with only 100 or so AMD gpus.  All those companies who bought tens of thousands of Nvidia gpus must be feeling pretty stupid right about now.\n\n\"Lamini is built by a team finetuning LLMs over the past two decades: we invented core LLM research like LLM scaling laws, shipped LLMs in production to over 1 billion users, taught nearly a quarter million students online (Finetuning LLMs), mentored the tech leads that went on to build the major foundation models: OpenAI’s GPT-3 and GPT-4, Anthropic’s Claude, Meta’s Llama 2, Google’s PaLM.\"\n\nhttps://pitchbook.com/profiles/company/525601-27#overview\n\n### Comment ID k2dkzld with +7 score by [(solodav, Reddit, 2023-09-27)](https://www.reddit.com/r/AMD_Stock/comments/16ssmqw/ai_startup_lamini_bets_future_on_amds_instinct/k2dkzld/) (in reply to ID k2c3mjk):\nCan AMD buy this company or hire Sharon Zhou?\n\n## Comment ID k2b2sl2 with +9 score by [(whatevermanbs, Reddit, 2023-09-26)](https://www.reddit.com/r/AMD_Stock/comments/16ssmqw/ai_startup_lamini_bets_future_on_amds_instinct/k2b2sl2/) (in reply to ID 16ssmqw):\nOP. May be you can edit. And paste section on perf and how lamini helps scale across clusters..\n\nThis is the first content coming our way about how customers can use instint and scale + some perf info\n\n## Comment ID k2be9ul with +12 score by [(ElementII5, Reddit, 2023-09-26)](https://www.reddit.com/r/AMD_Stock/comments/16ssmqw/ai_startup_lamini_bets_future_on_amds_instinct/k2be9ul/) (in reply to ID 16ssmqw):\nThis was bound to happen. If you (Jensen) make it too expensive and take too much margin people are going to look elsewhere. AMD got their foot in with hard work, the right kind of products and with Jensens greed.\n\n\n\n### Comment ID k2dl8hv with +1 score by [(solodav, Reddit, 2023-09-27)](https://www.reddit.com/r/AMD_Stock/comments/16ssmqw/ai_startup_lamini_bets_future_on_amds_instinct/k2dl8hv/) (in reply to ID k2be9ul):\nWhat does this entire post by OP all mean?  What exactly was built, how is AMD implicated, and what does this mean going forward?\n\n## Comment ID k2c880k with +3 score by [(erichang, Reddit, 2023-09-26)](https://www.reddit.com/r/AMD_Stock/comments/16ssmqw/ai_startup_lamini_bets_future_on_amds_instinct/k2c880k/) (in reply to ID 16ssmqw):\nSo, this thing has been running for a year with some real customers....\n\nSo, why didn't this news come out 3 months ago ?!\n\nWTH ?\n\n### Comment ID k2cae36 with +2 score by [(mark_mt, Reddit, 2023-09-26)](https://www.reddit.com/r/AMD_Stock/comments/16ssmqw/ai_startup_lamini_bets_future_on_amds_instinct/k2cae36/) (in reply to ID k2c880k):\nAMD is afraid customers, employees and investors might know about it. They could have said a lot without saying who the customer is but could not figure out how! They were also afraid if Nvidia hears about it - Nvidia might accelerate their AI efforts and be further ahead of them and own 99% of the market instead of 98% it is now.\n\n#### Comment ID k2cghwf with +3 score by [(erichang, Reddit, 2023-09-26)](https://www.reddit.com/r/AMD_Stock/comments/16ssmqw/ai_startup_lamini_bets_future_on_amds_instinct/k2cghwf/) (in reply to ID k2cae36):\nthat does not make any sense to me.....\n\n#### Comment ID k2fa3e4 with +1 score by [(robmafia, Reddit, 2023-09-27)](https://www.reddit.com/r/AMD_Stock/comments/16ssmqw/ai_startup_lamini_bets_future_on_amds_instinct/k2fa3e4/) (in reply to ID k2cae36):\n> Nvidia might accelerate their AI efforts and be further ahead of them and own 99% of the market instead of 98% it is now.\n\niirc, nvda is releasing the successor to the h100 in q4 2024, so the mi300 may already have to compete with next gen, anyway.\n\nwhich isn't toooo surprising, given that the h100 came out awhile ago... but ffs.  amd and timing, name a worse combination.\n\n## Comment ID k2ddnrc with +5 score by [(iCoinnn, Reddit, 2023-09-27)](https://www.reddit.com/r/AMD_Stock/comments/16ssmqw/ai_startup_lamini_bets_future_on_amds_instinct/k2ddnrc/) (in reply to ID 16ssmqw):\nThis is a game changer for fine tuning and inference market coupling with the trending LLM open source. Enterprise/startup don’t need a cluster of H100s to train a new model, buy Lamini rack at a cheaper price to fine tune open sourced LLM with your dataset and use it for inference. \n\nNVDA stills owns the training market but this is a good start for AMD\n\n### Comment ID k2hvcfn with +1 score by [(iCoinnn, Reddit, 2023-09-27)](https://www.reddit.com/r/AMD_Stock/comments/16ssmqw/ai_startup_lamini_bets_future_on_amds_instinct/k2hvcfn/) (in reply to ID k2ddnrc):\nTho I just heard about Lamini. How reliable is their claim and is there any benchmark to validate the claim?\n\n## Comment ID k2dywhq with +3 score by [(jobu999, Reddit, 2023-09-27)](https://www.reddit.com/r/AMD_Stock/comments/16ssmqw/ai_startup_lamini_bets_future_on_amds_instinct/k2dywhq/) (in reply to ID 16ssmqw):\nOn the Q2 conference call Lisa Su said there would be an uptick in MI250 accelerator sales in Q3.  I suspect this and the other startup that announced linking MI210s via PCIE4 to create clusters both are the foundation of that claim on her part.\n\nAMD just needs the confidence to increase production in order to take AI share.  Now they need to find volume packaging production to move forward.  Hopefully between Samsung and TSMC they will have enough to start growing their production\n\n## Comment ID k2dwobw with +2 score by [(_not_so_cool_, Reddit, 2023-09-27)](https://www.reddit.com/r/AMD_Stock/comments/16ssmqw/ai_startup_lamini_bets_future_on_amds_instinct/k2dwobw/) (in reply to ID 16ssmqw):\nThis is very interesting news. I love the timing too!\n\n## Comment ID k2n3rsq with +2 score by [(bl0797, Reddit, 2023-09-28)](https://www.reddit.com/r/AMD_Stock/comments/16ssmqw/ai_startup_lamini_bets_future_on_amds_instinct/k2n3rsq/) (in reply to ID 16ssmqw):\nTell me if I've got this right:   \n\nLamini is a startup with 10 employees at most, existing for 21 months at most, and has 100 or so AMD AI gpus. It claims to have a 5000 company waiting list for an LLM hosting service that was a secret until 2 days ago.  This will compete directly with all the large cloud service providers. \n \nSimultaneously, it is selling the LLM Superstation, a white box server that will compete directly with products from large system integrators like Supermicro, Dell, HPE, etc.\n\nIs anyone else even slightly skeptical about this?\n\n### Comment ID k2n6bd5 with +1 score by [(mark_mt, Reddit, 2023-09-28)](https://www.reddit.com/r/AMD_Stock/comments/16ssmqw/ai_startup_lamini_bets_future_on_amds_instinct/k2n6bd5/) (in reply to ID k2n3rsq):\nI love it - all that Nvidia moat talk up in smoke! The difference with Lamini and the Dells etc is - Lamini Software/IP is buried in the \"white boxes\".\n\n## Comment ID k2b00n5 with +5 score by [(None, Reddit, 2023-09-26)](https://www.reddit.com/r/AMD_Stock/comments/16ssmqw/ai_startup_lamini_bets_future_on_amds_instinct/k2b00n5/) (in reply to ID 16ssmqw):\n[deleted]\n\n### Comment ID k2bsxcl with +3 score by [(GanacheNegative1988, Reddit, 2023-09-26)](https://www.reddit.com/r/AMD_Stock/comments/16ssmqw/ai_startup_lamini_bets_future_on_amds_instinct/k2bsxcl/) (in reply to ID k2b00n5):\nKinda what you've been asking for... See, customers and sales....\n\n## Comment ID k2bt45h with +4 score by [(norcalnatv, Reddit, 2023-09-26)](https://www.reddit.com/r/AMD_Stock/comments/16ssmqw/ai_startup_lamini_bets_future_on_amds_instinct/k2bt45h/) (in reply to ID 16ssmqw):\n\"Greg Diamos  \nCo-Founder  \nGreg's mission is to build complex systems that the world can use.  \nHe had previously co-founded MLPerf & MLCommons, which have now set the industry standards for high performance machine learning systems. \"\n\nSO, where are the benchmarks?\n\n### Comment ID k2ditcv with +13 score by [(gdiamos, Reddit, 2023-09-27)](https://www.reddit.com/r/AMD_Stock/comments/16ssmqw/ai_startup_lamini_bets_future_on_amds_instinct/k2ditcv/) (in reply to ID k2bt45h):\nSee the blog for kernel benchmarks.\n\nThis is probably controversial, but I don’t think any full application benchmarks adequately cover LLM finetuning.  \n\nIt’s not the same as foundation model training, which MLPerf does cover.  https://mlcommons.org/en/training-normal-30/\n\nThe ML space moves rapidly and benchmarks are often leading edge but not bleeding edge.\n\n#### Comment ID k2fapto with +4 score by [(robmafia, Reddit, 2023-09-27)](https://www.reddit.com/r/AMD_Stock/comments/16ssmqw/ai_startup_lamini_bets_future_on_amds_instinct/k2fapto/) (in reply to ID k2ditcv):\nyou seriously just did more to promote the instinct series than amd ever did.  kudos.\n\n#### Comment ID k2dqfan with +1 score by [(whatevermanbs, Reddit, 2023-09-27)](https://www.reddit.com/r/AMD_Stock/comments/16ssmqw/ai_startup_lamini_bets_future_on_amds_instinct/k2dqfan/) (in reply to ID k2ditcv):\nThanks for the benchmarks.\nWhat level of fine tuning is required for nvidia clusters? I am trying to get a picture of how nvidia solutions scale.\n\nIs it uniform without needing any tuning?\n\n\n\n## Comment ID k2cu4bd with +1 score by [(bobthafarmer, Reddit, 2023-09-26)](https://www.reddit.com/r/AMD_Stock/comments/16ssmqw/ai_startup_lamini_bets_future_on_amds_instinct/k2cu4bd/) (in reply to ID 16ssmqw):\nOne of the co-founder has worked for both nvidia and amd before\n\n## Comment ID k2dhtd3 with +1 score by [(solodav, Reddit, 2023-09-27)](https://www.reddit.com/r/AMD_Stock/comments/16ssmqw/ai_startup_lamini_bets_future_on_amds_instinct/k2dhtd3/) (in reply to ID 16ssmqw):\nCan someone summarize the implications of this for AMD/$AMD (business and stock price) in the near and long-term?  3-5 sentences or less for us non-tech literature investors.  We would be tremendously grateful.  Thank you all very much!\n\n### Comment ID k2dqszs with +6 score by [(whatevermanbs, Reddit, 2023-09-27)](https://www.reddit.com/r/AMD_Stock/comments/16ssmqw/ai_startup_lamini_bets_future_on_amds_instinct/k2dqszs/) (in reply to ID k2dhtd3):\n$AMD is anybody's goes..\n\nAMD - These are early green shoots we want to see. We were all waiting for q4 news.. but things appear to be front loading in AMD's favour.  You can call this a toe in the door.\n\n## Comment ID k2h4yu2 with +1 score by [(Kimura1986, Reddit, 2023-09-27)](https://www.reddit.com/r/AMD_Stock/comments/16ssmqw/ai_startup_lamini_bets_future_on_amds_instinct/k2h4yu2/) (in reply to ID 16ssmqw):\nIsn't this like, massive news? Or no?\n\n### Comment ID k2iwfxt with +1 score by [(mark_mt, Reddit, 2023-09-28)](https://www.reddit.com/r/AMD_Stock/comments/16ssmqw/ai_startup_lamini_bets_future_on_amds_instinct/k2iwfxt/) (in reply to ID k2h4yu2):\nIt's a great start, would have been even better if AMD had hinted about this effort in the pr war with Nvidia much earlier.\n\n## Comment ID k2j00eb with +1 score by [(Illustrious-Room4444, Reddit, 2023-09-28)](https://www.reddit.com/r/AMD_Stock/comments/16ssmqw/ai_startup_lamini_bets_future_on_amds_instinct/k2j00eb/) (in reply to ID 16ssmqw):\nQuote \"ROCm, AMD’s software stack for coding software on GPUs, for having “achieved software parity” with Nvidia’s CUDA platform for LLMS.\"\n\nThis is a big thing to say. Is there any published research across the industry to support this statement?\n\n### Comment ID k2j977a with +1 score by [(mark_mt, Reddit, 2023-09-28)](https://www.reddit.com/r/AMD_Stock/comments/16ssmqw/ai_startup_lamini_bets_future_on_amds_instinct/k2j977a/) (in reply to ID k2j00eb):\n5000 customers in Queue\n\n#### Comment ID k2lii5h with +1 score by [(bl0797, Reddit, 2023-09-28)](https://www.reddit.com/r/AMD_Stock/comments/16ssmqw/ai_startup_lamini_bets_future_on_amds_instinct/k2lii5h/) (in reply to ID k2j977a):\nBeing on waiting list does not equal paying customer.\n\n## Comment ID k2k1zab with +1 score by [(60I08, Reddit, 2023-09-28)](https://www.reddit.com/r/AMD_Stock/comments/16ssmqw/ai_startup_lamini_bets_future_on_amds_instinct/k2k1zab/) (in reply to ID 16ssmqw):\nBullish news",
      "# Post ID 1it7mkl: Greg Diamos, co-founder and CTO of Lamini, has quit and joined Tensorwave with +13 score by [(bl0797, Reddit, 2025-02-19)](https://www.reddit.com/r/AMD_Stock/comments/1it7mkl/greg_diamos_cofounder_and_cto_of_lamini_has_quit/)\nOn the 2023 Q3 earnings call (10/31/2023), Lisa Su said \"AI start-up, Lamini, announced they achieved software parity with CUDA for LLMs running on Instinct MI250 GPUs, enabling enterprise customers to easily deploy production-ready LLMs.\"\n\nSounds like another over-promise and under-deliver.  The Lamini CTO has bailed.\n\n[https://www.linkedin.com/in/gregory-diamos-1a8b9083/](https://www.linkedin.com/in/gregory-diamos-1a8b9083/)\n\n[https://www.lamini.ai/blog/lamini-amd-paving-the-road-to-gpu-rich-enterprise-llms](https://www.lamini.ai/blog/lamini-amd-paving-the-road-to-gpu-rich-enterprise-llms)\n\n\n\n## Comment ID mdn9mbh with +11 score by [(GanacheNegative1988, Reddit, 2025-02-19)](https://www.reddit.com/r/AMD_Stock/comments/1it7mkl/greg_diamos_cofounder_and_cto_of_lamini_has_quit/mdn9mbh/) (in reply to ID 1it7mkl):\nThat's a desperately wrong take.\n\n### Comment ID mezofx1 with +2 score by [(HotAisleInc, Reddit, 2025-02-27)](https://www.reddit.com/r/AMD_Stock/comments/1it7mkl/greg_diamos_cofounder_and_cto_of_lamini_has_quit/mezofx1/) (in reply to ID mdn9mbh):\nHe didn't quit.\n\n## Comment ID mdpgyb0 with +8 score by [(BetweenThePosts, Reddit, 2025-02-19)](https://www.reddit.com/r/AMD_Stock/comments/1it7mkl/greg_diamos_cofounder_and_cto_of_lamini_has_quit/mdpgyb0/) (in reply to ID 1it7mkl):\nTensor wave is also amd backed though\n\n## Comment ID mdmmq5p with +6 score by [(robmafia, Reddit, 2025-02-19)](https://www.reddit.com/r/AMD_Stock/comments/1it7mkl/greg_diamos_cofounder_and_cto_of_lamini_has_quit/mdmmq5p/) (in reply to ID 1it7mkl):\ni remember when their ceo (or whatever she was) made those videos and was everywhere for about a week, before then getting some weird amd deal... and then lamini seemed to promptly vanish.\n\n## Comment ID mdmuiz7 with +2 score by [(EntertainmentKnown14, Reddit, 2025-02-19)](https://www.reddit.com/r/AMD_Stock/comments/1it7mkl/greg_diamos_cofounder_and_cto_of_lamini_has_quit/mdmuiz7/) (in reply to ID 1it7mkl):\nGreg is a techie, he can contribute more in tensorwave by enhancing the ROCM ecosystem. He has already built up the infra for Lamini and the work he's building here in tensorwave can help Lamni also. Lamni needs compute backend and its focus should be on the enterprise AI application side of things.\n\n## Comment ID mdnrpjc with +1 score by [(johnnytshi, Reddit, 2025-02-19)](https://www.reddit.com/r/AMD_Stock/comments/1it7mkl/greg_diamos_cofounder_and_cto_of_lamini_has_quit/mdnrpjc/) (in reply to ID 1it7mkl):\nProbably did not hit the growth target. Besides, AMD seems to have sorted out the software side. Beside, I am sure some Chinese team will come up with something\n\n## Comment ID mdnx7b6 with +1 score by [(lawyoung, Reddit, 2025-02-19)](https://www.reddit.com/r/AMD_Stock/comments/1it7mkl/greg_diamos_cofounder_and_cto_of_lamini_has_quit/mdnx7b6/) (in reply to ID 1it7mkl):\nThe CEO was on media and summit quite a lot, Greg is at back doing a lot of plumbing work.\n\n## Comment ID mdob0zq with +1 score by [(bl0797, Reddit, 2025-02-19)](https://www.reddit.com/r/AMD_Stock/comments/1it7mkl/greg_diamos_cofounder_and_cto_of_lamini_has_quit/mdob0zq/) (in reply to ID 1it7mkl):\nTensorwave hired his wife, too? Looks like Greg and Sudnya both went to Georgia Institute of Technology around 2010, worked at Nvidia  just after that.\n\nhttps://www.linkedin.com/in/sudnya?trk=fullfeedcard_main-feed-card-text\n\n### Comment ID mdqv5o9 with +1 score by [(EntertainmentKnown14, Reddit, 2025-02-20)](https://www.reddit.com/r/AMD_Stock/comments/1it7mkl/greg_diamos_cofounder_and_cto_of_lamini_has_quit/mdqv5o9/) (in reply to ID mdob0zq):\nTensorwave is pretty serious of breaking CUDA with ROCM. This is bullish as fu$k\n\n#### Comment ID mdsjbnd with +1 score by [(bl0797, Reddit, 2025-02-20)](https://www.reddit.com/r/AMD_Stock/comments/1it7mkl/greg_diamos_cofounder_and_cto_of_lamini_has_quit/mdsjbnd/) (in reply to ID mdqv5o9):\nLamini claimed they did it 16 months ago.  How did that work out?",
      "# Post ID 178yxhq: Code gen equivalent of MBZUAI/LaMini-Flan-T5-248M with +2 score by [(IamFuckinTomato, Reddit, 2023-10-16)](https://www.reddit.com/r/LocalLLaMA/comments/178yxhq/code_gen_equivalent_of_mbzuailaminiflant5248m/)\nSo as the title says, I am looking for a code gen model, that is equivalent to the MBZUAI/LaMini-Flan-T5-248M model for chat. I used the Flan-T5 model for a RAG application and it worked pretty well.\n\n\n\nNow I am looking for a similar model(small and fast) for simple code kind of response generations:\n\nThe model will be given a json object with details like the function name and their attributes with their descriptions, it needs to return an object in the below given format based on the user's query.\n\n    {\n\n    \"function\": \"function\\_name\",\n\n    \"arguments\": {\n\n    \"argument1\": \"argument\\_value\",\n\n    \"argument2\": \"argument\\_value\"\n\n    ....\n\n    }\n\n## Comment ID k54v3dc with +1 score by [(tylerjdunn, Reddit, 2023-10-16)](https://www.reddit.com/r/LocalLLaMA/comments/178yxhq/code_gen_equivalent_of_mbzuailaminiflant5248m/k54v3dc/) (in reply to ID 178yxhq):\nThe most popular open-source LLMs right now are [Code Llama, WizardCoder, Phind-CodeLlama, Mistral, StarCoder, and Llama 2](https://github.com/continuedev/what-llm-to-use). Many of them should be able to used for the task you are describing\n\n### Comment ID k54vap5 with +1 score by [(IamFuckinTomato, Reddit, 2023-10-16)](https://www.reddit.com/r/LocalLLaMA/comments/178yxhq/code_gen_equivalent_of_mbzuailaminiflant5248m/k54vap5/) (in reply to ID k54v3dc):\nYeah, I want them to be able to run on CPU quickly like the Flan model I mentioned.\n\n#### Comment ID k552utt with +1 score by [(tylerjdunn, Reddit, 2023-10-16)](https://www.reddit.com/r/LocalLLaMA/comments/178yxhq/code_gen_equivalent_of_mbzuailaminiflant5248m/k552utt/) (in reply to ID k54vap5):\nGot it. I misunderstood how small you want. I'm not sure I've seen anything comparable for code generation that I think can complete that task. Maybe [CodeGen (350M)](https://huggingface.co/Salesforce/codegen-2B-multi)?",
      "# Post ID 1cion3q: Another reason not to worry about AMD - Coreweave vs. Lamini with +16 score by [(bl0797, Reddit, 2024-05-02)](https://www.reddit.com/r/NVDA_Stock/comments/1cion3q/another_reason_not_to_worry_about_amd_coreweave/)\nTwo stories in my newsfeed today:\n\n**CoreWeave, an Nvidia-backed cloud infrastructure startup, has raised another $1.1 billion** in funding, raising its valuation to $19 billion, up from $7 billion just 5 months ago from its last funding round.  In the past year, Coreweave has expanded from 3 data center locations to 14 and has **quadrupled its employee count to more than 550**.  The company plans to have 28 data centers by the end of 2024.\n\n[https://www.cnbc.com/2024/05/01/nvidia-backed-gpu-cloud-provider-coreweave-is-worth-19-billion.html](https://www.cnbc.com/2024/05/01/nvidia-backed-gpu-cloud-provider-coreweave-is-worth-19-billion.html)\n\n=================================\n\n**Lamini, maybe the higest-profile AMD-backed startup, \"has raised a total of $25 million** across seed and Series A rounds  (Amplify led the Series A). CEO Zhou says the **money is being put toward tripling the company’s 10-person team**, expanding its compute  infrastructure, and kicking off development into “deeper technical optimizations.\"\n\n[https://techcrunch.com/2024/05/02/dropbox-figma-ceos-back-lamini-a-startup-building-a-generative-ai-platform-for-enterprises/](https://techcrunch.com/2024/05/02/dropbox-figma-ceos-back-lamini-a-startup-building-a-generative-ai-platform-for-enterprises/)\n\nIn September 2023, Lamini emerged from \"stealth mode\", making bold claims that it had a 5000 customer waiting list (built while in stealth mode) to use its new LLM cloud services.  Lamini  announced it had been “secretly running on more than one hundred” AMD Instinct MI200 series GPUs  and said the ROCm software platform “has achieved  software parity” with Nvidia’s dominant CUDA platform for such models.  Lamini also annouced its LLM Superstation is available both in the cloud and on-premise. It combines Lamini's easy-to-use enterprise LLM infrastructure with AMD  Instinct MI210 and MI250 accelerators. It is optimized for private enterprise LLMs, built to be heavily differentiated with proprietary data.   There's no public evidence that Superstation sales are greater than zero.\n\n[https://www.crn.com/news/components-peripherals/llm-startup-embraces-amd-gpus-says-rocm-has-parity-with-nvidia-s-cuda-platform](https://www.crn.com/news/components-peripherals/llm-startup-embraces-amd-gpus-says-rocm-has-parity-with-nvidia-s-cuda-platform)\n\nLamini's CEO Sharon Zhou joined Lisa Su on stage at the MI300X launch event on 12/6/2023 to discuss how they are \"leveraging AMD Instinct MI300X accelerators and the open ROCm 6 software stack to deliver differentiated AI solutions for enterprise customers\". \n\n&#x200B;\n\n## Comment ID l2bu3ju with +3 score by [(Charuru, Reddit, 2024-05-03)](https://www.reddit.com/r/NVDA_Stock/comments/1cion3q/another_reason_not_to_worry_about_amd_coreweave/l2bu3ju/) (in reply to ID 1cion3q):\nNGL lamini seems pretty cool, the tech chops of that company is solid, I'd want nvidia to buy them. Coreweave are crypto bros, but they sure know business, amazing rollout in just 1 year probably some of the greatest wealth creation ever happened for the people involved there.\n\n### Comment ID l30osq6 with +1 score by [(malinefficient, Reddit, 2024-05-07)](https://www.reddit.com/r/NVDA_Stock/comments/1cion3q/another_reason_not_to_worry_about_amd_coreweave/l30osq6/) (in reply to ID l2bu3ju):\nHaven't taken them seriously since an established 10Y+ GPU server vendor contacted them about building AMD servers for them having already prototyped them internally and they were shot down immediately and informed that a company that old doesn't understand the unique requirements of generative AI.\n\n#### Comment ID l30w2li with +1 score by [(Charuru, Reddit, 2024-05-07)](https://www.reddit.com/r/NVDA_Stock/comments/1cion3q/another_reason_not_to_worry_about_amd_coreweave/l30w2li/) (in reply to ID l30osq6):\nWhere can I hear more about this story? Would Lamini in this story be the supplier or the customer?\n\n#### Comment ID lc15w2e with +1 score by [(nodeocracy, Reddit, 2024-07-07)](https://www.reddit.com/r/NVDA_Stock/comments/1cion3q/another_reason_not_to_worry_about_amd_coreweave/lc15w2e/) (in reply to ID l30osq6):\nWas your comment referring to lamini or CoreWeave? Sorry was ambiguous\n\n## Comment ID l2b72vt with +2 score by [(saveamerica1, Reddit, 2024-05-02)](https://www.reddit.com/r/NVDA_Stock/comments/1cion3q/another_reason_not_to_worry_about_amd_coreweave/l2b72vt/) (in reply to ID 1cion3q):\nFunny nobody has heard of Lamini and everyone has heard of coreweave. Don’t trust Su or anyone around her after falsifying benchmarks on mi3000s. You could write a book and I wouldn’t believe one word. Nvidia and coreweave years ahead!\n\n### Comment ID l2fodkq with +2 score by [(Yafka, Reddit, 2024-05-03)](https://www.reddit.com/r/NVDA_Stock/comments/1cion3q/another_reason_not_to_worry_about_amd_coreweave/l2fodkq/) (in reply to ID l2b72vt):\nOh, interesting.. Where can I find out more about this mi3000s benchmark story?\n\n#### Comment ID l2xwe4v with +3 score by [(HotAisleInc, Reddit, 2024-05-07)](https://www.reddit.com/r/NVDA_Stock/comments/1cion3q/another_reason_not_to_worry_about_amd_coreweave/l2xwe4v/) (in reply to ID l2fodkq):\nWe are working on it. I've got 18 people lined up to do testing and publish benchmarks. Not started quite yet, but going as quickly as we can to make it happen.\n\nUpdate: 18 now.\n\n#### Comment ID l2xix3k with +2 score by [(HippoLover85, Reddit, 2024-05-07)](https://www.reddit.com/r/NVDA_Stock/comments/1cion3q/another_reason_not_to_worry_about_amd_coreweave/l2xix3k/) (in reply to ID l2fodkq):\nwccftech.com/wp-content/uploads/2023/12/NVIDIA-Hopper-H100-vs-AMD-Instinct-MI300X-AI-GPU-Performance-Main.jpg\n\nAmd then released a blog post showing optimized software on the mi300x, which once again showed it beating the h100 (using the nivida optimized software this time) again by about 50%ish.\n\nThere were also people saying mi300 couldnt support larger batch sizes so nvidia is better.  But that is just nonsense.\n\nEdit: i tried, mobile on reddit is hard.\n\n#### Comment ID l2fozpk with +1 score by [(saveamerica1, Reddit, 2024-05-03)](https://www.reddit.com/r/NVDA_Stock/comments/1cion3q/another_reason_not_to_worry_about_amd_coreweave/l2fozpk/) (in reply to ID l2fodkq):\nLook up testing comparison done by AMD H100 vs mi3000 they used a third party software instead of Cuda to perform speed test against mi3000. Then said mi3000 faster.\n\n## Comment ID l2xj3fr with +2 score by [(HippoLover85, Reddit, 2024-05-07)](https://www.reddit.com/r/NVDA_Stock/comments/1cion3q/another_reason_not_to_worry_about_amd_coreweave/l2xj3fr/) (in reply to ID 1cion3q):\nThis subs infatuation with amd is bizarre to me.\n\nEven if amd takes 20% share, hyperscalers and ai growth will far outweigh any amd market share take.\n\n### Comment ID l2z6yji with +2 score by [(instars3, Reddit, 2024-05-07)](https://www.reddit.com/r/NVDA_Stock/comments/1cion3q/another_reason_not_to_worry_about_amd_coreweave/l2z6yji/) (in reply to ID l2xj3fr):\nRight? There won’t be losers between NVDA and AMD\n\n## Comment ID l2bcpu8 with +1 score by [(upvotemeok, Reddit, 2024-05-02)](https://www.reddit.com/r/NVDA_Stock/comments/1cion3q/another_reason_not_to_worry_about_amd_coreweave/l2bcpu8/) (in reply to ID 1cion3q):\nlame ini",
      "# Post ID 1acviv0: Just WTH is the 2024 AMD AI revenue REALITY? Let's read AMD CEO's facial xpressions & body language :-O with +63 score by [(ed2727, Reddit, 2024-01-28)](https://www.reddit.com/r/AMD_Stock/comments/1acviv0/just_wth_is_the_2024_amd_ai_revenue_reality_lets/)\n\n\n## Comment ID kjx13a8 with +50 score by [(Random_Forestry, Reddit, 2024-01-28)](https://www.reddit.com/r/AMD_Stock/comments/1acviv0/just_wth_is_the_2024_amd_ai_revenue_reality_lets/kjx13a8/) (in reply to ID 1acviv0):\nIt’s funny you posted this. I just thought to myself today, why not go back and rewatch to see if I could catch any clues before ER. Always need to keep expectations tempered though, it’s a lot of fun to speculate and get carried away. I think the consensus at this point is we’re looking at anywhere from $4 - $6B, with a *decent* probability we’ll hit the coveted $8B target. Either way, it’s early innings and a lot of the real action will start in the back half of the year when ramping of MI300x truly starts to take off. One thing I may be reading into too much is just after she says “We’re excited to see how the next year will play out,” and immediately smirks when the camera pans out, right around 1:33. It looks like she can’t contain her excitement and knows something we don’t, so I’m hopeful Tuesday will meet or exceed our expectations for guidance. Fingers crossed!\n\n### Comment ID kjx7l3i with +8 score by [(ed2727, Reddit, 2024-01-28)](https://www.reddit.com/r/AMD_Stock/comments/1acviv0/just_wth_is_the_2024_amd_ai_revenue_reality_lets/kjx7l3i/) (in reply to ID kjx13a8):\nGood read! \n\nExperts say they need a baseline for their subjects, so I don’t have one for her, but the way her neck elongated after reporter mentioned “software”, it seemed it was a concern of hers. She really didn’t answer his question directly either\n\n#### Comment ID kjyu4gi with +24 score by [(noiserr, Reddit, 2024-01-28)](https://www.reddit.com/r/AMD_Stock/comments/1acviv0/just_wth_is_the_2024_amd_ai_revenue_reality_lets/kjyu4gi/) (in reply to ID kjx7l3i):\nShe actually did answer it.\n\nThe tick you describe is a tick every engineer has when they are asked a question that they need to boil down for average folk audience. \n\nIf you read between the lines, her answer was. We are so competitive on performance that the companies are willing to work with us on software. And there is a shortage of GPU so customers don't really have a choice.\n\nStuff that's actually going through her head at the time is much more complex than that. It's things like:\n\n- Open source ecosystem advantage\n\n- If Meta spends $10B on H100s and can buy the same amount of compute by  spending $7B on AMD, you really think that for $3B they won't write their software (which they already custom write for Nvidia) for AMD too?\n\n- The fact that even Nvidia needs optimization in the new emerging workloads. So there is literally no difference for the bleeding edge hyper scale solutions.\n\n- And the fact that yes Nvidia does have a big advantage when it comes to just the sheer amount of tools supported. Which matters less for the big singular deployments.\n\n- The investment AMD has made in this area. Xilinx merger, and the unified front towards AI software.\n\n- Customers actually don't want a single winner. They do want to buy from companies other than Nvidia.\n\n- Whatever else she has in the pipeline.\n\nSo all that stuff has to be boiled down on the spot, without sounding disparaging towards competitors, without sounding like AMD is engaging in a price war, packaged so that the layman can understand it.\n\nHer answer is perfect.\n\n#### Comment ID kjxe2dq with +6 score by [(trembeczking, Reddit, 2024-01-28)](https://www.reddit.com/r/AMD_Stock/comments/1acviv0/just_wth_is_the_2024_amd_ai_revenue_reality_lets/kjxe2dq/) (in reply to ID kjx7l3i):\nThe body language experts who are always saying this baseline stuff are bogus snake-oil salesman, basically the same people\n\n### Comment ID kjzvn1a with +1 score by [(ACiD_80, Reddit, 2024-01-28)](https://www.reddit.com/r/AMD_Stock/comments/1acviv0/just_wth_is_the_2024_amd_ai_revenue_reality_lets/kjzvn1a/) (in reply to ID kjx13a8):\nIt all looks a bit too fake, theatrical though.. im not buying it.\n\n## Comment ID kjxa9bs with +19 score by [(HippoLover85, Reddit, 2024-01-28)](https://www.reddit.com/r/AMD_Stock/comments/1acviv0/just_wth_is_the_2024_amd_ai_revenue_reality_lets/kjxa9bs/) (in reply to ID 1acviv0):\none of the indicators that speaks most clearly to me is lisa constantly says AI is AMD's number one focus. Meaning . . . She sees it being the largest revenue and profit generator for AMD. So she sees more customer engagements looking to buy more MI300 than they are buying EPYC. So sometime in the near future lisa sees a revenue in excess of 3b quarterly revenue (as epyc will likely be 3b quarterly in the next few quarters). Does that happen in late 2024? dies that happen only after MI400 launches? who knows.  \n\n\nI am very confident AMD will sell between 4b and 12b in MI300x and MI200 series cards next year. In fact, by my math AMD achieving 10b yearly AI revenue would put them at a COWOS production capacity of half that of nvidia . . . which is what rumors were talking saying earlier.  \n\n\nI dunno. This feels all like im falling for a lot of hopium.\n\n### Comment ID kjxgzxu with +23 score by [(Beazly79, Reddit, 2024-01-28)](https://www.reddit.com/r/AMD_Stock/comments/1acviv0/just_wth_is_the_2024_amd_ai_revenue_reality_lets/kjxgzxu/) (in reply to ID kjxa9bs):\nAMD is the gateway for startups on the AI software side!  There is massive investment on the software side.  \n\nAMD, INTEL, NVIDIA, AMAZON. and a few others all use the same chip maker and material supplier!  This is the bottleneck.  You can't just start making these chip right away, it takes years to get fully debugged manufacturing process worked out when dealing with these chips, hints Intel recent slide.  A Dutch company can make the tooling and owns the processes technology to make these types of wafers.\n\nAmazing how two little companies are the sole source of AI chips for the entire world, AND it is going to remain that way for at least another year, i bet.\n\nThis will give a year for all the little software startups time to get funding and creating AI for anything that will make someone's life easier!   \n\nThat is the KEY!   People buy when it makes life easier.  Netflix, movies streaming directly into house.\nDoor dash\nUber\n\nAI purpose is to make things easier for people.  Investing in AI isn't a bet, its a gift.  The trick is which softwares are going to hit a home run and make everyone's life easier....\n\n\nI have AMD and Nvidia and holding for at least 10yrs.\n\nI am not making the same mistakes I made with Tesla and Netflix...     had I held, I wouldn't be working anymore.\n\n#### Comment ID kjy0682 with +9 score by [(Charming_Squirrel_13, Reddit, 2024-01-28)](https://www.reddit.com/r/AMD_Stock/comments/1acviv0/just_wth_is_the_2024_amd_ai_revenue_reality_lets/kjy0682/) (in reply to ID kjxgzxu):\nThat last sentence hits home, curse missed opportunities\n\n#### Comment ID kk01lmq with +3 score by [(None, Reddit, 2024-01-28)](https://www.reddit.com/r/AMD_Stock/comments/1acviv0/just_wth_is_the_2024_amd_ai_revenue_reality_lets/kk01lmq/) (in reply to ID kjxgzxu):\n>AMD is the gateway for startups on the AI software side!  There is massive investment on the software side.\n\nNo, NVDA invests lot more in startups than AMD and also provides credits to access their hardware through their \"AI Datacenter\" startups and through other cloud providers. Nvidia has a better and bigger mature ecosystem than AMD.\n\nI agree to rest of the post, but AMD has to improve with their availability through AWS, GCP and have more \"AI Datacenters\".\n\n### Comment ID kjyvnac with +4 score by [(Gahvynn, Reddit, 2024-01-28)](https://www.reddit.com/r/AMD_Stock/comments/1acviv0/just_wth_is_the_2024_amd_ai_revenue_reality_lets/kjyvnac/) (in reply to ID kjxa9bs):\nThe fact we’re now trying to analyze the vibe in a video is just beyond me.  Things like this convince me more than ever AMD has run up too far and too fast and I’m likely to close most of my options positions (LEAPS, profitable thankfully) before the market closes on the 30th.\n\n#### Comment ID kk0vmpv with +6 score by [(HippoLover85, Reddit, 2024-01-28)](https://www.reddit.com/r/AMD_Stock/comments/1acviv0/just_wth_is_the_2024_amd_ai_revenue_reality_lets/kk0vmpv/) (in reply to ID kjyvnac):\nI mean this sub has always had people stretching too far to try and extrapolate way past any reliable indicators.  \n\n\nI see a lot of fear, and a lot of hype here. People like yourself afraid of hype and the runup. a lot of technical people putting selling pressure on. A lot of people trying to catch AMD's ai hype train.  \n\n\nEither way, i think selling options going into this ER is a decent play given the IV. But when i break it down to a big picture type thing . . .  \n\n\n1. AI is currently one of the biggest cycles in Computing history, and AMD has literally the best hardware to sell into it.\n2. All of AMD's other businesses should be doing very well to and have competitive advantages (datacenter and PC), or are in the process of recovering from inventory glut (embeded/xilinx)\n3. Assuming AMD can carry forward all of their MI300a capacity from Q4 2023 into 2024 . . . Just given the ASP difference between MI300a for el Capitan and mi300x for ai customers, AMD could start out with 800m Ai revenue in Q1 2024 with no capacity increase. To get to 10b ai revenue in 2024 AMD just needs to finish the year with 5x capacity as they do in Q1. This is a big ramp. But let's put it into perspective. This means AMD needs to ramp to sell about 200k MI300x per quarter during Q4. This is what that takes:\n   1. **MI300 Demand.** Great question. Most people are assuming demand is insatiable. for 2024 i tend to agree i think? I don't think Demand is a limiting factor\n   2. **200k unit assembly**. Boards, Heat sinks, VRM, etc etc all take capacity. 200k ramp is a lot. But AMD sells \\~3million consumer GPUs quarterly. Given Mi300x takes a lot more than a consumer card for a board, vrm, heatsink etc . . . It is still pretty comparable to a high end card. The capacity to do this should be quite easy to attain. Should not be a limiting factor\n   3. **38 million GB of HBM3/HBM3E**. This is a little more difficult. Nvidia only uses around 37million GB per quarter for H100 (probably less than that). But for comparison AMD already uses 40 million GB per quarter of GDDR6 for their consumer cards. Nvidia probably uses 4-5x that. In addition 38 million GB of HBM3 in terms of cost is only about 400m revenue for someone like samsung or SKhynix. This is going to be a key limiting factor and dependent upon partners ability to ramp. this is a 50/50.\n   4. **5nm and 6nm silicon.** Shouldn't be an issue, no problem\n   5. **COWOS supply.**  Same as HBM . . who knows? 50/50\n\nWhen i run my estimates through some calces. i get a \\~75% certainty that AMD can hit 4.5 billion Revenue for 2024. This is where i think lisa will guide. When i look at the above limitations i get a 50% chance of AMD being able to do it when they hit about 6.5 billion in revenue. This is where i think AMD will land for FY 2024.  \n\n\nWhen i plug 6.5 billion revenue into my model. I get an EPS that supports a share price of 160-195 with a strong trajectory to \\~$250 by EOY 2024. Wouldn't be surprised if we go right to 250 and trade flat for the year. Wouldnt be surprised if we dip hard to 150 and then spike mid 2024. This is a good time to be versatile with AMD stock. It's not quite dumping time yet.\n\n## Comment ID kjxraco with +20 score by [(ElementII5, Reddit, 2024-01-28)](https://www.reddit.com/r/AMD_Stock/comments/1acviv0/just_wth_is_the_2024_amd_ai_revenue_reality_lets/kjxraco/) (in reply to ID 1acviv0):\nSo the market is supply constraint by what AMD and Nvidia can deliver. Nvidia forecast is $70B. I don't doubt they will sell what they can. \n\nI do wonder though if AMDs supply of $10B or 15% will dampen that $70B somewhat? Or asked differently is Nvidia really going to get away with their ridiculous asking prices and their $70B forecast if AMD has something competing for those who are not willing to pay the Nvidia tax?\n\nWhat if Nvidia makes $50B of AI GPU sales and AMD $10B? I don't think Nvidias evaluation will hold if that happens.\n\n### Comment ID kjynzxq with +5 score by [(RetdThx2AMD, Reddit, 2024-01-28)](https://www.reddit.com/r/AMD_Stock/comments/1acviv0/just_wth_is_the_2024_amd_ai_revenue_reality_lets/kjynzxq/) (in reply to ID kjxraco):\n>Nvidia forecast is $70B\n\nDid nVidia make this forecast?  I have not listened to every second of their earnings calls but I have yet to actually observe any statement coming from nVidia itself containing a forecast any further out than one quarter.  It seems we are expecting AMD to do something that not even nVidia will do.  Am I wrong?\n\n#### Comment ID kk00jzh with +2 score by [(None, Reddit, 2024-01-28)](https://www.reddit.com/r/AMD_Stock/comments/1acviv0/just_wth_is_the_2024_amd_ai_revenue_reality_lets/kk00jzh/) (in reply to ID kjynzxq):\nNvidia has given a big forecast, but I do not remember an exact number for the entire year. A while ago Nvidia declined to give entire year projection and focus on each Q. 70B for 2024 is the number I have heard from ANALYSTS, not Nvidia but maybe they are quoting something that was said in some news channel.\n\n## Comment ID kjycmws with +13 score by [(Coyote_Tex, Reddit, 2024-01-28)](https://www.reddit.com/r/AMD_Stock/comments/1acviv0/just_wth_is_the_2024_amd_ai_revenue_reality_lets/kjycmws/) (in reply to ID 1acviv0):\nYes, AMD is behind on the software front, but taking action to begin closing the gap.  Keep in mind Lisa has a hardware mindset and has a killer strategy in place on the front.  So, she recently acquired a software company with likely several goals in mind.  O e to directly assist companies to rapidly develop solutions on AMD hardware.  Regardless what the company chooses, AMD or Nvidia, the software applications specific to the companies needs, must be developed.  While one might suggest experienced resources might be available today with experience on Nvidia solutions, they are in short supply as the number of AI projects explode thus these resources cannot possibly meet the need and are being bid up to astronomical rates.  Thus, it makes sense for companies to invest in developing internal resources to deliver AI projects on AMD hardware. \n\nNext, this core team is looking to quickly ramp and develop resources for software consulting teams to facilitate this explosive growth in demand for talent.  \n\nThe desire for many in the industry to see this AI revolution as a long-term investment and the desire to not be locked into a single source provider, they are very inclined to make investments in developing internal resources and expertise. \n\nWith the generally accepted view of Nvidia having a massive lead, one must consider if AMD can capture 10 ro 20 percent of this AI TAM over the next few years or just how much by what time.  Virtually all of the major players in AI see this as a massive technological shift and  easily see multiple levels of continuing refinement and improvement in hardware over the next 10 years and realistically much longer, so making the shift and investment now is potentially crucial to the organization even perhaps to survivability in the future.\n\n### Comment ID kk9r87r with +1 score by [(Live_Market9747, Reddit, 2024-01-30)](https://www.reddit.com/r/AMD_Stock/comments/1acviv0/just_wth_is_the_2024_amd_ai_revenue_reality_lets/kk9r87r/) (in reply to ID kjycmws):\nThe question is, what is the AI TAM?\n\nNvidia's AI TAM is much higher than AMD's because Nvidia is competing in markets where AMD simply doesn't exist. \n\nHere some examples:\n\n\\- $4500 license fee per GPU for Nvidia AI including NeMo and other stuff\n\n\\- DGX cloud and on-prem systems -> unlike AMD, Nvidia enforces CSP to use DGX platform because end user demand wants DGX\n\n\\- buildout of entire data centers, not only CPU/GPU -> AMD needs Cray and others for this\n\n\\- Omniverse for GenAI accelerated real world simulation\n\n\\- DriveSim for automotive\n\nand many more like in medical and robotics solutions.\n\n&#x200B;\n\nAMD's TAM is only in HW AI chips but Nvidia has way more TAM in application frameworks and AI solutions.\n\n## Comment ID kk12a3c with +7 score by [(coffeewithalex, Reddit, 2024-01-29)](https://www.reddit.com/r/AMD_Stock/comments/1acviv0/just_wth_is_the_2024_amd_ai_revenue_reality_lets/kk12a3c/) (in reply to ID 1acviv0):\nSo, with 400B market by 2027 (estimated), and 215B market in 2022 for servers and PC semiconductors, that will grow to let's say 400B in 2027 as well (let's say).\n\nHow much of that will be AMD?\n\nLet's say 50% AI market, 20% server + PC market (AMD doesn't make other chips on the motherboard, doesn't make RAM, SSDs, etc). That's a total of 280B revenue for AMD, estimated, by 2027. That, I think, would be on the optimistic side.\n\nGiven an average profit margin of 10%, that means a profit of 28B per year. At a P/E ratio of 30, that would mean that by 2027, the market cap would be in the ballpark of $900B. Unbelievable, but I did use pretty optimistic numbers in each part. That means roughly 3x the current price.\n\nNow I could be wrong (probably am), and AMD could take only 1/3 of the market that I thought it would. Or the market could be less than predicted.\n\nIDK, I'm just not comfortable selling at this price yet. I'll hold on for now. It looks like there is room to grow.\n\n### Comment ID kk12gst with +1 score by [(ed2727, Reddit, 2024-01-29)](https://www.reddit.com/r/AMD_Stock/comments/1acviv0/just_wth_is_the_2024_amd_ai_revenue_reality_lets/kk12gst/) (in reply to ID kk12a3c):\nThat’s a lot of Hopium!\n\n### Comment ID kk7rpdn with +1 score by [(Charming_Squirrel_13, Reddit, 2024-01-30)](https://www.reddit.com/r/AMD_Stock/comments/1acviv0/just_wth_is_the_2024_amd_ai_revenue_reality_lets/kk7rpdn/) (in reply to ID kk12a3c):\nI love this company but I highly highly doubt 50% of the ai market by 2027. Possible but quite the long shot\n\n## Comment ID kjymaam with +8 score by [(None, Reddit, 2024-01-28)](https://www.reddit.com/r/AMD_Stock/comments/1acviv0/just_wth_is_the_2024_amd_ai_revenue_reality_lets/kjymaam/) (in reply to ID 1acviv0):\nAMD could deliver the best possible news on ER and stock could still tank, lets wait and see\n\n### Comment ID kjyv8ny with +4 score by [(Gahvynn, Reddit, 2024-01-28)](https://www.reddit.com/r/AMD_Stock/comments/1acviv0/just_wth_is_the_2024_amd_ai_revenue_reality_lets/kjyv8ny/) (in reply to ID kjymaam):\nPeople acting like they know for sure AMD is going to go up, watching theory videos (I had to check which sub I was on), and openly saying things like they’re hoping it goes up should be concerning.\n\nNobody knows which way this thing is going this week. Lisa could say “$5bn in AI revenue for 2024 is now the baseline” and AMD could tank.  AMD could go up 5% and then when the Fed doesn’t cut rates the next day AMD could fall 10%.\n\n### Comment ID kjynx0q with +1 score by [(UpNDownCan, Reddit, 2024-01-28)](https://www.reddit.com/r/AMD_Stock/comments/1acviv0/just_wth_is_the_2024_amd_ai_revenue_reality_lets/kjynx0q/) (in reply to ID kjymaam):\nI fully expect a dip in the After Market/Pre Market and early on Wednesday, because people still don't know enough about AMD operations and AMD financials to make an early call on whether the results are good or not.  Expect a lot of hand-wringing about GAAP results.\n\nBut, many analysts have upgraded just in the past two weeks.  AMD will have to \\*prove them wrong\\* before they will change their new price targets.  So the analysts are on our side for once.  That means that soothing words for them should lead us to higher ATHs over the two weeks following ER, probably starting around mid-day Wednesday.  And, of course, we're all waiting for Hans to lead the charge!\n\n## Comment ID kjwzh5w with +19 score by [(ed2727, Reddit, 2024-01-28)](https://www.reddit.com/r/AMD_Stock/comments/1acviv0/just_wth_is_the_2024_amd_ai_revenue_reality_lets/kjwzh5w/) (in reply to ID 1acviv0):\nNOTE: I did major in psychology (but the degree has no bearance in Real Life!)\n\nI've constantly seen the 2024 revenue estimates on the AMD hype machine for the last 2-3 weeks go from 50mph to 150mph, because the STOCK HAS DOUBLED since end of Oct. 2023.\n\n$4B, 8b, 10+ billion. Everybody says Lisa Su is ultra-conservative, but by re-watching this Dec. 23 video, it's easy to see:\n\n1) She HAS SHOWN US HER CARDS: Total market was $150B... now she says $400B in 2027. What does this imply? Her 2024 revenue should be multiplied by the same multiple... 2.67X\n\n\\- so $2B x 2.67 = $5.2B??\n\n2) When the CNBC reporter asked her about lineage of AMD revenue in regards to software infrastructure (ie CUDA), Lisa Su tensed up, neck out! They don't have a similar product yet, so she's receiving a lot of pushback from clients.\n\n### Comment ID kjx559z with +14 score by [(GanacheNegative1988, Reddit, 2024-01-28)](https://www.reddit.com/r/AMD_Stock/comments/1acviv0/just_wth_is_the_2024_amd_ai_revenue_reality_lets/kjx559z/) (in reply to ID kjwzh5w):\nI basically argree with your first point but not with the second. John asked about 'linearity of demand'. Not sure if you just typoed or really mean lineage, but the context you talk about make me think you ment that. His question was basically about how demand will ramp. She didn't neck out that I perceived, but she obfuscated the way she often does when she's not willing to let the cat out of the bag. She brought up the customer list out on stage with them, all AI OEM heavys, and the growth change that speeks to. We all can workout a certain amount of that ramp based on there capex and projectios. She says AI demand is nothing like her or even the industry has seen before then ends on that they are absolutely focused on ramping as fast as possible! She in no way said or implied with body language that they are getting pushback or any sort.\n\n#### Comment ID kjx7u6u with +1 score by [(ed2727, Reddit, 2024-01-28)](https://www.reddit.com/r/AMD_Stock/comments/1acviv0/just_wth_is_the_2024_amd_ai_revenue_reality_lets/kjx7u6u/) (in reply to ID kjx559z):\nRight, he asked about lineage of demand in regards to software and AMD’s lack of CUDA equivalent.\n\nI read that he was just tactfully asking, “what gives you the confidence of revenue projection if you only have good hardware, but not a strong software (like CUDA) component that complements it like your #1 competitor does?”\n\n### Comment ID kjydsf4 with +3 score by [(HMI115_GIGACHAD, Reddit, 2024-01-28)](https://www.reddit.com/r/AMD_Stock/comments/1acviv0/just_wth_is_the_2024_amd_ai_revenue_reality_lets/kjydsf4/) (in reply to ID kjwzh5w):\nwhat does your major have anything to do with it\n\n#### Comment ID kk056bi with +2 score by [(eric-janaika, Reddit, 2024-01-28)](https://www.reddit.com/r/AMD_Stock/comments/1acviv0/just_wth_is_the_2024_amd_ai_revenue_reality_lets/kk056bi/) (in reply to ID kjydsf4):\nHe thinks he can read people's minds based on their body language.\n\n## Comment ID kjzk0ci with +3 score by [(idwtlotplanetanymore, Reddit, 2024-01-28)](https://www.reddit.com/r/AMD_Stock/comments/1acviv0/just_wth_is_the_2024_amd_ai_revenue_reality_lets/kjzk0ci/) (in reply to ID 1acviv0):\nRewatching part of this.\n\nThe last question about software. Her answer was basically we have demand(or strong engagement) and are are focused on ramping as fast as possible. Her answer was not we have worked on software a lot and continue to do so....it was we are focused on ramping.\n\nDo you think that was dodging the question, or answering with 'our software is already good enough to solicit ample demand'. Or another way to state it 'our hardware+pricing is so good, they want it despite the software gaps, ie they are willing to fill in the software gaps'.\n\nI do not read it as dodging, just wondering if others think it was dodging.\n\n### Comment ID kk9u6b7 with +1 score by [(Live_Market9747, Reddit, 2024-01-30)](https://www.reddit.com/r/AMD_Stock/comments/1acviv0/just_wth_is_the_2024_amd_ai_revenue_reality_lets/kk9u6b7/) (in reply to ID kjzk0ci):\nThe software part is very complex and it has different facets which many seem to ignore.\n\nFor companies like MS and Meta the SW part is less of a problem due to their own strength in customizing SW internally.\n\nBut what do you expect companies in Fortune 500 which aren't non-Tech to do? Do you think they will start hiring 1000s of SW engineers to do stuff like Meta? That's a bit far fetched. Technically any company could built their own ERP or cloud system but why bother if you can find good solutions on the market?\n\nAnd the same will be the case for AI solutions. And that is the key difference. CUDA vs. RoCm isn't AI solutions, it's API. PyTorch and many other things are frameworks, programming languages and interfaces but no solutions. They are tools.\n\nBut NeMo for example from Nvidia, is a solutions toolbox. It includes foundation models of LLMs and everything needed to get started in aggregation and training. AND the key aspect is that Nvidia is in the consulting business as well and will send their AI engineers over to assist Fortune 500 companies so that the primary task is data aggregation and training, no need to understand how a LLM works. And all of this is enterprise level graded by Nvidia just like an ERP or cloud system. So Nvidia guarantees for security and regular patching.\n\nAnd that's why there is demand and there is demand. AMD will have good demand among Tech companies with SW engineers for SW customizing and who are interested in DIY. But enterprises which are non-Tech will have close to zero demand for AMD since they will want off the shelf solutions with enterprise grade.\n\nThe indicator of this is strong in DGX cloud. DGX cloud is a defined environment of Nvidia where no customization is allowed and it runs only Nvidia SW solutions on Nvidia HW. And what we see is a strong adaptation of DGX cloud by all major CSPs. The CSPs themselves probably hate DGX cloud but CSP's customers want it as it enables Nvidia Enterprise AI. Even Amazon AWS which is known for customizing their cloud solutions has began to offer DGX cloud. You can see there the market force and the demand dynamic of customers who enforce CSPs to use Nvidia environment because they want Nvidia AI SW solutions.\n\nNvidia has been in the field of AI research, AI deployment, AI consultation and AI solutions driver for a decade. To assume that AMD with some chips can now easily keep up, is an illusion. I would even dare to say that Nvidia themselves is among the largest net user of AI models in the world.\n\n## Comment ID kk0pziz with +3 score by [(Asleep_Salad_3275, Reddit, 2024-01-28)](https://www.reddit.com/r/AMD_Stock/comments/1acviv0/just_wth_is_the_2024_amd_ai_revenue_reality_lets/kk0pziz/) (in reply to ID 1acviv0):\nI don’t think we need to push very far to read this interview. I think it’s as simple as this:\n-Clear line of sight = 2b preordered and sold beforehand.\n-Customers want more and very high demand = alot of negotiations going on and they clearly gonna sell everything they can supply this year.\n-We have significant supply for next year = This is the 1000$ question, significant could be 2,3,4,5 x 2b.\n-We are excited to see how the next year will play out =\n🚀\n\n## Comment ID kjz9fmq with +2 score by [(wprodrig, Reddit, 2024-01-28)](https://www.reddit.com/r/AMD_Stock/comments/1acviv0/just_wth_is_the_2024_amd_ai_revenue_reality_lets/kjz9fmq/) (in reply to ID 1acviv0):\nLisa is pretty awesome, happy to have her as a boss man. She wants to destroy her cousin over at Nvidia in AI, trying to make all of the right decisions to support that cause. Sounds good to me :)\n\n## Comment ID kjzu0b9 with +2 score by [(johnny2much, Reddit, 2024-01-28)](https://www.reddit.com/r/AMD_Stock/comments/1acviv0/just_wth_is_the_2024_amd_ai_revenue_reality_lets/kjzu0b9/) (in reply to ID 1acviv0):\nAll this info is great if your buying or have long term shares. Not day trading or call options which profited in the recent run up. I still have my calls but think about selling . There has to be pause or profit taking soon\n\n### Comment ID kk0tpbo with +1 score by [(ed2727, Reddit, 2024-01-28)](https://www.reddit.com/r/AMD_Stock/comments/1acviv0/just_wth_is_the_2024_amd_ai_revenue_reality_lets/kk0tpbo/) (in reply to ID kjzu0b9):\nAgree. Up 100% since 10/29 based on 0 earnings report should have anyone profusely sweating\n\n## Comment ID kjydkoh with +1 score by [(HMI115_GIGACHAD, Reddit, 2024-01-28)](https://www.reddit.com/r/AMD_Stock/comments/1acviv0/just_wth_is_the_2024_amd_ai_revenue_reality_lets/kjydkoh/) (in reply to ID 1acviv0):\nh100 killer\n\n## Comment ID kjz77i3 with +1 score by [(happy30thbirthday, Reddit, 2024-01-28)](https://www.reddit.com/r/AMD_Stock/comments/1acviv0/just_wth_is_the_2024_amd_ai_revenue_reality_lets/kjz77i3/) (in reply to ID 1acviv0):\nThis is some serious straw-grasping, guys. Please do not make your decision based on facial expressions and body language, good grief.\n\n## Comment ID kjyi4ug with +1 score by [(ProfessionalRow9300, Reddit, 2024-01-28)](https://www.reddit.com/r/AMD_Stock/comments/1acviv0/just_wth_is_the_2024_amd_ai_revenue_reality_lets/kjyi4ug/) (in reply to ID 1acviv0):\nRemember buy calls so it falls\n\n## Comment ID kjywk6n with +1 score by [(CROSSTHEM0UT, Reddit, 2024-01-28)](https://www.reddit.com/r/AMD_Stock/comments/1acviv0/just_wth_is_the_2024_amd_ai_revenue_reality_lets/kjywk6n/) (in reply to ID 1acviv0):\nMy favorite part is this comment here: https://youtu.be/8Bdg0J7-7uI?si=FT9-CWDsvYjqdjfz&t=1m26s",
      "# Post ID 18rryf1: Why is no-one fine-tuning something like t5? with +93 score by [(No_Baseball_7130, Reddit, 2023-12-27)](https://www.reddit.com/r/LocalLLaMA/comments/18rryf1/why_is_noone_finetuning_something_like_t5/)\nI know this isn't about LLaMA, but flan T5 3B regularly outperforms other 3b models like mini orca 3b and lamini flan t5 783m (fine-tuned flan-t5-small) outperforms tinyllama-1.1B. So that begs the question: Why aren't many people fine-tuning flan t5 / t5?\n\n## Comment ID kf45j0x with +66 score by [(anommm, Reddit, 2023-12-27)](https://www.reddit.com/r/LocalLLaMA/comments/18rryf1/why_is_noone_finetuning_something_like_t5/kf45j0x/) (in reply to ID 18rryf1):\nIn the research community, FlanT5 is widely used. The FlanT5 3B model is currently the best for \"classical\" tasks such as information extraction, QA, and translation, and can be fine-tuned using a single A100 GPU (without LoRA). The Flan dataset comprises a variety of classic NLP tasks, explaining FlanT5's proficiency in them. Many companies are also utilizing FlanT5.\n\nOutside the research community, however, FlanT5 is not as popular. One reason is that the base model is almost useless on its own. FlanT5 is great when fine-tuned for specific tasks, but the base model does almost nothing. The encoder-decoder architecture is not compatible with most apps that efficiently run large language models. Additionally, the FlanT5 tokenizer is terrible, it cannot tokenize languages other than English and is not suited for tasks like coding, often replacing coding symbols with the 'UKN' token. The encoder-decoder architecture is also suboptimal for multi-turn chat, which is what most people in r/LocalLLaMA cares about. \n\n&#x200B;\n\nIf you have a specific tasks that you want to solve, and a large training dataset. FlanT5 is probably the best choice. But it is not a good model to use without finetuning, or a good model for multi-turn chat.\n\n### Comment ID kf5vemg with +4 score by [(vannaplayagamma, Reddit, 2023-12-27)](https://www.reddit.com/r/LocalLLaMA/comments/18rryf1/why_is_noone_finetuning_something_like_t5/kf5vemg/) (in reply to ID kf45j0x):\n> FlanT5 is great when fine-tuned for specific tasks, but the base model does almost nothing.\n\nI think this is the biggest reason. Most people aren't fine tuning and merging models, they just rely on the published models. So fine tuners will try to produce models that work for as many people as possible (e.g Dolphin), and people just try to improve their prompting around those models\n\n### Comment ID kf4z1ke with +5 score by [(Careless-Age-4290, Reddit, 2023-12-27)](https://www.reddit.com/r/LocalLLaMA/comments/18rryf1/why_is_noone_finetuning_something_like_t5/kf4z1ke/) (in reply to ID kf45j0x):\nAs I've struggled between LLM's and other ML approaches, one dividing line seems to come up: do I have good training data to use? If I don't, LLM's are either the way to go or the way to make the training data. If I do have quality training data, there might be other AI methods that would be far faster at scale, even if fine-tuning the LLM might be the most accurate or powerful way to do it. Especially for simple tasks like labeling where the labels are known.\n\n### Comment ID lexbbwv with +1 score by [(Training-Adeptness57, Reddit, 2024-07-25)](https://www.reddit.com/r/LocalLLaMA/comments/18rryf1/why_is_noone_finetuning_something_like_t5/lexbbwv/) (in reply to ID kf45j0x):\nAny paper that showed that FlanT5 is better at classical tasks ?\n\n### Comment ID kf7uaqm with +1 score by [(No_Baseball_7130, Reddit, 2023-12-28)](https://www.reddit.com/r/LocalLLaMA/comments/18rryf1/why_is_noone_finetuning_something_like_t5/kf7uaqm/) (in reply to ID kf45j0x):\niirc mT5 can generate other languages and even code.\n\n#### Comment ID kf90mkn with +1 score by [(anommm, Reddit, 2023-12-28)](https://www.reddit.com/r/LocalLLaMA/comments/18rryf1/why_is_noone_finetuning_something_like_t5/kf90mkn/) (in reply to ID kf7uaqm):\nYes, mT5 is probably the best multilingual model available. And the tokenizer is much better than the one in FlanT5. But it has only been pretrained with a mask language objective, so it cannot generate text unless you finetune it. mT0 was finetuned with instructions, but similar to FlanT5 it can only answer to a few specific prompts. If you have a multilingual task you want to solve and the hardware to finetune mT5-xl on it, mT5/mT0 is the way to go. But mT5 without finetuning cannot do any tasks. That is why, again, mT5 is popular in the research community, a lot of papers use it, but not between hobbyists that want a pretrained model that can solve tasks without finetuning.\n\n## Comment ID kf38jjg with +60 score by [(unculturedperl, Reddit, 2023-12-27)](https://www.reddit.com/r/LocalLLaMA/comments/18rryf1/why_is_noone_finetuning_something_like_t5/kf38jjg/) (in reply to ID 18rryf1):\nT5 models : LLMs :: Old and busted* : new hotness\n\nThere was a recent paper where some team fine tuned a t5, RoBERTa, and Llama 2 7b  for a specific task and found that RoBERTA and t5 were both better after fine tuning.  \n\nfor folks who want to complain they didn't fine tune 70b or something else, feel free to re-run the comparison for your specific needs and report back.\n\n* if you're not aware of the Men in Black Old and Busted meme, it's from a movie, T5 is not busted.\n\n### Comment ID kf38oko with +14 score by [(No_Baseball_7130, Reddit, 2023-12-27)](https://www.reddit.com/r/LocalLLaMA/comments/18rryf1/why_is_noone_finetuning_something_like_t5/kf38oko/) (in reply to ID kf38jjg):\nt5 is suprizingly good for it's size (except hallucinations, but i bet that can be fixed by lowering temprature)\n\n#### Comment ID kf39kva with +4 score by [(unculturedperl, Reddit, 2023-12-27)](https://www.reddit.com/r/LocalLLaMA/comments/18rryf1/why_is_noone_finetuning_something_like_t5/kf39kva/) (in reply to ID kf38oko):\nThere's also tooling and community to take into consideration.  And as always, your results may vary.  \n\nAt this moment in time more people are probably better off doing smaller model work than want to.  In a couple of years, things will also be in a very different place.  If this is a corporate effort, how long will they want to support it?  Personal stuff is more a matter of what effort you're willing to invest.\n\n#### Comment ID kf3e7m7 with +1 score by [(jetaudio, Reddit, 2023-12-27)](https://www.reddit.com/r/LocalLLaMA/comments/18rryf1/why_is_noone_finetuning_something_like_t5/kf3e7m7/) (in reply to ID kf38oko):\nLower temperature and you will be fine. I find tuning top_p give me better results.\n\n### Comment ID kf3b0e5 with +7 score by [(wind_dude, Reddit, 2023-12-27)](https://www.reddit.com/r/LocalLLaMA/comments/18rryf1/why_is_noone_finetuning_something_like_t5/kf3b0e5/) (in reply to ID kf38jjg):\nI wouldn’t say t5 is old and busted. But I think there’s a lot to new and hotness and chasing the new thing. OpenAI is a marketing machine, and everyone seems to be chasing recreation it to get a little piece of the shiny.\n\n### Comment ID kf3j9mb with +4 score by [(m98789, Reddit, 2023-12-27)](https://www.reddit.com/r/LocalLLaMA/comments/18rryf1/why_is_noone_finetuning_something_like_t5/kf3j9mb/) (in reply to ID kf38jjg):\nThis one?\n\nhttps://arxiv.org/pdf/2302.08091.pdf\n\n#### Comment ID kf6s7re with +1 score by [(unculturedperl, Reddit, 2023-12-27)](https://www.reddit.com/r/LocalLLaMA/comments/18rryf1/why_is_noone_finetuning_something_like_t5/kf6s7re/) (in reply to ID kf3j9mb):\nNo, but that one's interesting as well.  It does appear that targeted fine tuning is a really unexplored area that we need to do more with.\n\n### Comment ID kw5u5w5 with +1 score by [(eslXist, Reddit, 2024-03-23)](https://www.reddit.com/r/LocalLLaMA/comments/18rryf1/why_is_noone_finetuning_something_like_t5/kw5u5w5/) (in reply to ID kf38jjg):\nLink to the paper please?\n\n### Comment ID la71gam with +1 score by [(Affectionate-Cap-600, Reddit, 2024-06-25)](https://www.reddit.com/r/LocalLLaMA/comments/18rryf1/why_is_noone_finetuning_something_like_t5/la71gam/) (in reply to ID kf38jjg):\nDeBERTa v2 xxl (1.5B) is one of the best model i have ever tried, and if I'm not wrong it was the first model that \"beat\" human avg score in glue benchmark\n\nAlso DeBERTa v3 is amazing (it use the training strategy of ELECTRA), but unfortunately the biggest size is the large version, with \"only\" something like 200M parameters\n\n## Comment ID kf3ecw8 with +32 score by [(nuvalab, Reddit, 2023-12-27)](https://www.reddit.com/r/LocalLLaMA/comments/18rryf1/why_is_noone_finetuning_something_like_t5/kf3ecw8/) (in reply to ID 18rryf1):\nFrom MLSys perspective, T5 is encoder-decoder architecture that makes it significantly harder to scale compare to the decoder-only models of ChatGPT / LLaMA. A concrete example is, T5 has the last encoder output fed into all decoders as context, as a result, it's much harder to find a balanced graph cut that scales well for both training and inference, where decoder only LLMs are much more homogenous in structure.\n\nI personally think T5 architecture has a lot of potential in its design, but due to the size limitation, I doubt if the industry had an easy time to push it beyond 11b size vertical to demonstrate its upper limit.\n\n### Comment ID kf3zcy8 with +6 score by [(archiesteviegordie, Reddit, 2023-12-27)](https://www.reddit.com/r/LocalLLaMA/comments/18rryf1/why_is_noone_finetuning_something_like_t5/kf3zcy8/) (in reply to ID kf3ecw8):\nHey I'm a noob in the LLM world. I didn't get the part where you said why T5 is hard to scale. If you don't mind, can you please explain it to me as to why it is hard to scale?\n\n#### Comment ID kf5hepr with +6 score by [(nuvalab, Reddit, 2023-12-27)](https://www.reddit.com/r/LocalLLaMA/comments/18rryf1/why_is_noone_finetuning_something_like_t5/kf5hepr/) (in reply to ID kf3zcy8):\nConsider some institution that needs to train 175B LLM to impress people with its quality in a GPU cluster. This means we need to split transformer blocks of a single forward + backward pass into multiple machines, mostly along two dimensions: tensor (split a matrix multiplication across multiple devices) and pipeline (partition blocks across multiple devices and streamline the compute between them).\n\nThe easier it is to make a balanced partitioning, the more performant your training & inference is across multiple devices. The best partitioning strategy needs to consider compute / memory / network cost across all partitions.\n\nFor decoder only LLMs, it's much simpler. Each block feeds into the next one, and you can often get away with just splitting along the head dimension for each block. Compute / memory / network cost even and simple.\n\nFor T5, think of it as a similar sequence of blocks but more edges representing more complicated data dependencies during compute. Compute / memory cost between encoder and decoder is different and uneven. Communication edge between encoder to decoder makes networking more involved too, and many \"simple\" scaling strategy that works well for decoder only LLM breaks down.\n\n### Comment ID kf3eoes with +3 score by [(No_Baseball_7130, Reddit, 2023-12-27)](https://www.reddit.com/r/LocalLLaMA/comments/18rryf1/why_is_noone_finetuning_something_like_t5/kf3eoes/) (in reply to ID kf3ecw8):\nthere has been flan-ul2 (20b), but overall i find this kinda true\n\n## Comment ID kf3e0o2 with +8 score by [(jetaudio, Reddit, 2023-12-27)](https://www.reddit.com/r/LocalLLaMA/comments/18rryf1/why_is_noone_finetuning_something_like_t5/kf3e0o2/) (in reply to ID 18rryf1):\nI'm creating a Chinese - Vietnamese translation model right now and T5 variant is definitely the one I chose. It's way better than decoder only transformers models.\n\n### Comment ID kf3emxi with +4 score by [(No_Baseball_7130, Reddit, 2023-12-27)](https://www.reddit.com/r/LocalLLaMA/comments/18rryf1/why_is_noone_finetuning_something_like_t5/kf3emxi/) (in reply to ID kf3e0o2):\nyea, t5 is really good for its size. tinyllama also works but is larger (benefit is that it's more supported) and performs slightly worse.\n\n#### Comment ID kf3nlae with +3 score by [(jetaudio, Reddit, 2023-12-27)](https://www.reddit.com/r/LocalLLaMA/comments/18rryf1/why_is_noone_finetuning_something_like_t5/kf3nlae/) (in reply to ID kf3emxi):\nI've read somewhere that the encoder can capture the semantic meaning of the input text. 🤔 I think it's task-specific: when you only need the model to complete the text for you, decoder-only is the way to go; when you need the model to do something with your data (translation, summarization, etc.), encoder-decoder gives better results.\n\n### Comment ID kf3jk5s with +1 score by [(vTuanpham, Reddit, 2023-12-27)](https://www.reddit.com/r/LocalLLaMA/comments/18rryf1/why_is_noone_finetuning_something_like_t5/kf3jk5s/) (in reply to ID kf3e0o2):\nIsn't the T5 tokenizer incompatible with vietnamese ?\n\n#### Comment ID kf3my15 with +3 score by [(jetaudio, Reddit, 2023-12-27)](https://www.reddit.com/r/LocalLLaMA/comments/18rryf1/why_is_noone_finetuning_something_like_t5/kf3my15/) (in reply to ID kf3jk5s):\nI'm using t5 variant, not original one. mT5 and umT5 both support vietnamese\n\n### Comment ID kf3oar7 with +1 score by [(Significant-Cap6692, Reddit, 2023-12-27)](https://www.reddit.com/r/LocalLLaMA/comments/18rryf1/why_is_noone_finetuning_something_like_t5/kf3oar7/) (in reply to ID kf3e0o2):\ndo you any experiment result of this kind of models?\n\n#### Comment ID kf3patv with +2 score by [(jetaudio, Reddit, 2023-12-27)](https://www.reddit.com/r/LocalLLaMA/comments/18rryf1/why_is_noone_finetuning_something_like_t5/kf3patv/) (in reply to ID kf3oar7):\nI’m fine-tuning a T5 model to translate Chinese web novels into Vietnamese right now, and although the BLEU score is not very high, it produces quite good results. The Vietnamese version is better than what I can translate myself. 😂\n\n## Comment ID kf5mpgu with +7 score by [(IndianaCahones, Reddit, 2023-12-27)](https://www.reddit.com/r/LocalLLaMA/comments/18rryf1/why_is_noone_finetuning_something_like_t5/kf5mpgu/) (in reply to ID 18rryf1):\nMany of the new participants in the LLM world don’t realize they are doing NLP. A number of the use cases they are trying to solve are over-engineered since the BERT family of transformers and even TF-IDF are viable, production-ready solutions. Some of us started with NLTK and Python 2.x so the lower barrier of entry today is a blessing and a curse. Part of why we have so many unintentionally overfit LLM fine tunes on huggingface as the old train_test_split concept is foreign to them. The basics of ML, like CRISP-DM, are simply not taught in cut/paste tutorials.\n\n### Comment ID ks739dv with +3 score by [(ducknificient, Reddit, 2024-02-26)](https://www.reddit.com/r/LocalLLaMA/comments/18rryf1/why_is_noone_finetuning_something_like_t5/ks739dv/) (in reply to ID kf5mpgu):\nI'm interested by the over-engineered meaning. Currently we're making a chatbot for enterprise and domain specific knowledge (maritime logistics). The source knowledge is from a pdf and a free-text corpus. Our first approach was using distilBERT architecture with context from the pdf file. Its work for factoid question. However, When faced with open question or information not available in pdf file, its pretty bad. The example context is like this\n\nquestion = \"Who is the head of operation divisions ?\"\ncontext = \"No one sit in the director chair. General manager of operations is Johnny. General manager of operations is Farrel. Vice Manager of operations is Andy. Senior manager of operations is Bobby\"\n\nThe model cannot predict which one is higher between director and manager.\n\nSo, what's over-engineered and do you have some ideas about our case ? thanks\n\n## Comment ID kf31j3y with +12 score by [(AutomataManifold, Reddit, 2023-12-27)](https://www.reddit.com/r/LocalLLaMA/comments/18rryf1/why_is_noone_finetuning_something_like_t5/kf31j3y/) (in reply to ID 18rryf1):\nMostly because people have been focused on text generation models. Arguably, it would be equally valuable to train other types of models (like sequence to sequence ones). Bit the tools to train them aren't as accessible.\n\n### Comment ID kf357r1 with +5 score by [(m98789, Reddit, 2023-12-27)](https://www.reddit.com/r/LocalLLaMA/comments/18rryf1/why_is_noone_finetuning_something_like_t5/kf357r1/) (in reply to ID kf31j3y):\nT5 does text generation too.\n\n#### Comment ID kf3c6p0 with +5 score by [(AutomataManifold, Reddit, 2023-12-27)](https://www.reddit.com/r/LocalLLaMA/comments/18rryf1/why_is_noone_finetuning_something_like_t5/kf3c6p0/) (in reply to ID kf357r1):\nYes, but it's text-to-text, which is slightly different from the GPT token-completion approach. Though it's maybe more natural for instructions to be a text2text thing? But you're right, it'd be misleading to imply that it doesn't generate text.\n\n### Comment ID kf38lid with +1 score by [(No_Baseball_7130, Reddit, 2023-12-27)](https://www.reddit.com/r/LocalLLaMA/comments/18rryf1/why_is_noone_finetuning_something_like_t5/kf38lid/) (in reply to ID kf31j3y):\nHuggingface trainer works well\n\n## Comment ID kf86mty with +5 score by [(the__storm, Reddit, 2023-12-28)](https://www.reddit.com/r/LocalLLaMA/comments/18rryf1/why_is_noone_finetuning_something_like_t5/kf86mty/) (in reply to ID 18rryf1):\nWe use fine-tunes of flan-t5-xl (3B) in production exclusively, around 10 million inferences/day.  They're not flashy but really solid - the big LLMs can handle more complex prompts and larger contexts but are harder to corner into doing exactly what you want.  Every once in a while we go looking for something better but are yet to find anything we're interested in switching to.\n\n### Comment ID kf86ysh with +2 score by [(No_Baseball_7130, Reddit, 2023-12-28)](https://www.reddit.com/r/LocalLLaMA/comments/18rryf1/why_is_noone_finetuning_something_like_t5/kf86ysh/) (in reply to ID kf86mty):\nWhat is your use case? Also, some 7B models are pretty good (like Chupacabra-7B) or \\~11B (like SauerkrautLM-UNA-SOLAR-Instruct) and follow instructions better. The benefit of this is you can quantize them using awq / gguf which is better than gptq (only supported t5 quant method other than bitsandbytes)\n\n### Comment ID lxg8xdh with +1 score by [(cozycookie55, Reddit, 2024-11-16)](https://www.reddit.com/r/LocalLLaMA/comments/18rryf1/why_is_noone_finetuning_something_like_t5/lxg8xdh/) (in reply to ID kf86mty):\nI see that there are not a lot of inference optimizations for flan-t5, what tools do you use to deploy it?\n\n## Comment ID kf3uzm5 with +3 score by [(dark_surfer, Reddit, 2023-12-27)](https://www.reddit.com/r/LocalLLaMA/comments/18rryf1/why_is_noone_finetuning_something_like_t5/kf3uzm5/) (in reply to ID 18rryf1):\nVery informative thread. I'd like to learn more about flan-t5.\n\n1) Does AutoAWQ support Flan-t5 lineup?\n2) Has anyone tried to LORA or QLORA with Flan-t5?\n3) How to do RAG wit it?\n4) Can we start Small Language Model sub reddit, where we share our experiences with SLMs and learn more about them?\n\nI am interested in models like Facebook/OPT, Phi-2, gpt-neo, pythia, Mamba, etc. All these are sub 3B models and are important for GPU poor people like me to learn various techniques like fine-tuning, RAG, LORA, QUANTIZATION etc.\n\n### Comment ID kf3y1m0 with +4 score by [(No_Baseball_7130, Reddit, 2023-12-27)](https://www.reddit.com/r/LocalLLaMA/comments/18rryf1/why_is_noone_finetuning_something_like_t5/kf3y1m0/) (in reply to ID kf3uzm5):\nsadly no for #1 but GPTQ supports with [https://github.com/qwopqwop200/GPTQ-for-LLaMa/tree/t5](https://github.com/qwopqwop200/GPTQ-for-LLaMa/tree/t5)\n\n2: Yes\n\n3: I guess you could feed it with google results, i'll provide some code below\n\n4: ig?\n\ncode for #3\n\n    #!/usr/bin/python3\n    # Run \"pip install transformers googlesearch-python requests BeautifulSoup4\" before use\n    from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n    from googlesearch import search\n    import requests\n    from bs4 import BeautifulSoup\n    tokenizer = AutoTokenizer.from_pretrained(\"declare-lab/flan-alpaca-large\")\n    model = AutoModelForSeq2SeqLM.from_pretrained(\"declare-lab/flan-alpaca-large\").to(\"cpu\")\n    \n    def fetch_from_google(query, num_results=5):\n        urls = search(query, num_results=num_results)\n        texts = []\n        for url in urls:\n            try:\n                response = requests.get(url)\n                soup = BeautifulSoup(response.content, 'html.parser')\n                texts.append(soup.get_text())\n            except Exception as e:\n                print(f\"Error fetching {url}: {e}\")\n        return texts\n    \n    def rag(query):\n        search_results = fetch_from_google(query)\n        combined_input = query + \" \" + \" \".join(search_results)\n        inputs = tokenizer(combined_input, return_tensors=\"pt\", truncation=True, max_length=512)\n        output = model.generate(**inputs)\n        response = tokenizer.decode(output[0], skip_special_tokens=True)\n        return response\n    \n    while True:\n        print(\"Output: \", rag(input(\"Input: \")))\n\n### Comment ID kf45vv3 with +2 score by [(Mkboii, Reddit, 2023-12-27)](https://www.reddit.com/r/LocalLLaMA/comments/18rryf1/why_is_noone_finetuning_something_like_t5/kf45vv3/) (in reply to ID kf3uzm5):\nYes it can be used for RAG i used the xl for this purpose in February. A bottleneck was that it has a context length of 512.\n\n#### Comment ID kf4zdft with +2 score by [(Careless-Age-4290, Reddit, 2023-12-27)](https://www.reddit.com/r/LocalLLaMA/comments/18rryf1/why_is_noone_finetuning_something_like_t5/kf4zdft/) (in reply to ID kf45vv3):\nWhat was speed like compared to an LLM ran on the same hardware?\n\n#### Comment ID kf5pyb6 with +2 score by [(dark_surfer, Reddit, 2023-12-27)](https://www.reddit.com/r/LocalLLaMA/comments/18rryf1/why_is_noone_finetuning_something_like_t5/kf5pyb6/) (in reply to ID kf45vv3):\nThe problem with Flan-t5 in general is it gives very short responses. If you ask it it to summarize a 200 words paragraph it will answer in mere 10 words. \n\nIt gives factually incorrect responses. When I asked 'Tell me boiling point of the water?' it replies: 212 C. That's it, answers in Fahrenheit and puts C at the end. That's the answer .\n\nFlan-t5 will take quite a lot of your time and effort to be anywhere closer to usable even on personal projects. I don't even know where to begin.\n\n## Comment ID kf41lrl with +4 score by [(santaSJ, Reddit, 2023-12-27)](https://www.reddit.com/r/LocalLLaMA/comments/18rryf1/why_is_noone_finetuning_something_like_t5/kf41lrl/) (in reply to ID 18rryf1):\nIsn't the context length a major limitation of t5?\n\nThe tokenizer for flan t5 models was also very bad.\n\n### Comment ID koropw8 with +1 score by [(m98789, Reddit, 2024-02-03)](https://www.reddit.com/r/LocalLLaMA/comments/18rryf1/why_is_noone_finetuning_something_like_t5/koropw8/) (in reply to ID kf41lrl):\nIt’s relative positioning based so theoretically no limit, just the usual quadratic scaling challenge, but if you have the memory, you can go well beyond 512.\n\n## Comment ID kf3whsq with +3 score by [(ramprasad27, Reddit, 2023-12-27)](https://www.reddit.com/r/LocalLLaMA/comments/18rryf1/why_is_noone_finetuning_something_like_t5/kf3whsq/) (in reply to ID 18rryf1):\nI did a long time ago https://huggingface.co/0-hero\n\n### Comment ID kf3wquz with +1 score by [(No_Baseball_7130, Reddit, 2023-12-27)](https://www.reddit.com/r/LocalLLaMA/comments/18rryf1/why_is_noone_finetuning_something_like_t5/kf3wquz/) (in reply to ID kf3whsq):\nWoah, these are nice! im gonna check them out soon. BTW can you GPTQ quantize them with  qwopqwop200's GPTQ-for-Llama repo on the t5 branch? [https://github.com/qwopqwop200/GPTQ-for-LLaMa/tree/t5](https://github.com/qwopqwop200/GPTQ-for-LLaMa/tree/t5)\n\nRemindMe! 1 hour\n\n#### Comment ID kf3wt2t with +1 score by [(RemindMeBot, Reddit, 2023-12-27)](https://www.reddit.com/r/LocalLLaMA/comments/18rryf1/why_is_noone_finetuning_something_like_t5/kf3wt2t/) (in reply to ID kf3wquz):\nI will be messaging you in 1 hour on [**2023-12-27 11:50:40 UTC**](http://www.wolframalpha.com/input/?i=2023-12-27%2011:50:40%20UTC%20To%20Local%20Time) to remind you of [**this link**](https://www.reddit.com/r/LocalLLaMA/comments/18rryf1/why_is_noone_finetuning_something_like_t5/kf3wquz/?context=3)\n\n[**CLICK THIS LINK**](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5Bhttps%3A%2F%2Fwww.reddit.com%2Fr%2FLocalLLaMA%2Fcomments%2F18rryf1%2Fwhy_is_noone_finetuning_something_like_t5%2Fkf3wquz%2F%5D%0A%0ARemindMe%21%202023-12-27%2011%3A50%3A40%20UTC) to send a PM to also be reminded and to reduce spam.\n\n^(Parent commenter can ) [^(delete this message to hide from others.)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Delete%20Comment&message=Delete%21%2018rryf1)\n\n*****\n\n|[^(Info)](https://www.reddit.com/r/RemindMeBot/comments/e1bko7/remindmebot_info_v21/)|[^(Custom)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5BLink%20or%20message%20inside%20square%20brackets%5D%0A%0ARemindMe%21%20Time%20period%20here)|[^(Your Reminders)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=List%20Of%20Reminders&message=MyReminders%21)|[^(Feedback)](https://www.reddit.com/message/compose/?to=Watchful1&subject=RemindMeBot%20Feedback)|\n|-|-|-|-|\n\n## Comment ID kf42xit with +2 score by [(ramprasad27, Reddit, 2023-12-27)](https://www.reddit.com/r/LocalLLaMA/comments/18rryf1/why_is_noone_finetuning_something_like_t5/kf42xit/) (in reply to ID 18rryf1):\nIf anyone wants to fine tune. You can use this repo https://github.com/declare-lab/flan-alpaca it’s quite easy to use\n\n## Comment ID kf4fqvm with +2 score by [(MLTyrunt, Reddit, 2023-12-27)](https://www.reddit.com/r/LocalLLaMA/comments/18rryf1/why_is_noone_finetuning_something_like_t5/kf4fqvm/) (in reply to ID 18rryf1):\nI've thought about this, too, also fine tuning it. I think ul2 might still have some applications in tasks like information extraction and summarization.\n\nI use autotrainer advanced and other scripts to fine tune LLMs. Neither autotrain nor Axolotl seems to support t5/ul2.\n\nIt's not hard to code the fine tuning code, but:\n\n\\- I have not found a great, easy code example of qlora fine tuning t5, where you can just throw in a csv\n\n\\- It's not sota anymore and the apache license is no longer it's sole usp\n\nAt this point, it seems like we got the flan fine tunes and that's it. There is very few fine tunes of ul2 on hf and it has been around since quite a while.\n\nI've also not heard of efforts to quantize or speed up inference of the model.\n\nI don't feel encoder-decoder is a dead architecture per se, there is just little interest and decoder only models seem to work well, too.\n\nThere is an alpaca, dolly, samsum-flan-ul2 and flan-OIG-ul2 fine tune adapter on hf for flan ul2.\n\nflan ul2 has also been 8 bit quantized for faster inference with ctranslate2.\n\nThat's pretty much all mentionworthy community engagement on ul2, the successor of t5.\n\n## Comment ID kf4vy2n with +2 score by [(dahara111, Reddit, 2023-12-27)](https://www.reddit.com/r/LocalLLaMA/comments/18rryf1/why_is_noone_finetuning_something_like_t5/kf4vy2n/) (in reply to ID 18rryf1):\nI assume you are talking about parameter size when you say T5 is smaller, but if the same parameters are used, wouldn't the file size be smaller in llama2?\n\n### Comment ID kf53joc with +2 score by [(jmickeyd, Reddit, 2023-12-27)](https://www.reddit.com/r/LocalLLaMA/comments/18rryf1/why_is_noone_finetuning_something_like_t5/kf53joc/) (in reply to ID kf4vy2n):\nThe idea is that due to the bidirectional attention of the encoder side of T5, it can theoretically achieve similar performance with fewer parameters than a decode-only model like llama or gpt.\n\n#### Comment ID kf7e9lh with +2 score by [(dahara111, Reddit, 2023-12-28)](https://www.reddit.com/r/LocalLLaMA/comments/18rryf1/why_is_noone_finetuning_something_like_t5/kf7e9lh/) (in reply to ID kf53joc):\nThanks for the reply.\n\nJust so there is no misunderstanding, I think the T5 is great and I was impressed when it appeared.\n\nBut the following model, for example, is based on the T5, right? I think the size is much larger and harder to execute than a typical llama2 7b based models.\n\n[https://huggingface.co/google/madlad400-7b-mt](https://huggingface.co/google/madlad400-7b-mt)\n\n## Comment ID kf3e3gw with +3 score by [(CasulaScience, Reddit, 2023-12-27)](https://www.reddit.com/r/LocalLLaMA/comments/18rryf1/why_is_noone_finetuning_something_like_t5/kf3e3gw/) (in reply to ID 18rryf1):\n1. t5 has been around for quite a while and was the standard for experimenting prior to llama's release.\n2. I never saw a t5-based chatbot that was anywhere near as good as llama 7B variants. \n\n  2a. The presumption is that llama7B would beat flan when fine tuned on a task.\n\n3. llama and t5 are basically the same, llama is just trained on much more data, which again suggests it has an advantage.\n4. IDK about llama variants smaller than 7B, are you sure those are official llama models trained by meta... or are they just random projects that other people trained and took the name?\n\n### Comment ID kf3kx8k with +8 score by [(Distinct-Target7503, Reddit, 2023-12-27)](https://www.reddit.com/r/LocalLLaMA/comments/18rryf1/why_is_noone_finetuning_something_like_t5/kf3kx8k/) (in reply to ID kf3e3gw):\n>llama and t5 are basically the same, llama is just trained on much more data, which again suggests it has an advantage.\n\nUhmm... I disagree with that.\n\nEncoder-Decoder models are different from decoder-only... As text2text is different from text competition.\n\n#### Comment ID kf5lepd with +1 score by [(CasulaScience, Reddit, 2023-12-27)](https://www.reddit.com/r/LocalLLaMA/comments/18rryf1/why_is_noone_finetuning_something_like_t5/kf5lepd/) (in reply to ID kf3kx8k):\nNobody actually thinks encoder-decoder vs decoder only matters. Its basically just about the training data and size. And the objective is the same, text2text vs text completion is just an artifact of having an encoder\n\n### Comment ID kf3eh1n with +1 score by [(No_Baseball_7130, Reddit, 2023-12-27)](https://www.reddit.com/r/LocalLLaMA/comments/18rryf1/why_is_noone_finetuning_something_like_t5/kf3eh1n/) (in reply to ID kf3e3gw):\nall of these make no sense. where did i mention llama 7b? there are no 7b (or similar) models in flan-t5 lineup. i only mentioned mini orca 3b and tinyllama 1.1b (both unofficial). but we can compare flan t5 11b to llama 2 13b as they are similar sizes, and both perform similarly imo.\n\n## Comment ID lf3dl4a with +1 score by [(Ok_Issue_9284, Reddit, 2024-07-26)](https://www.reddit.com/r/LocalLLaMA/comments/18rryf1/why_is_noone_finetuning_something_like_t5/lf3dl4a/) (in reply to ID 18rryf1):\nThe main reason I'm sticking to Autoregressive models is the amount of optimization and tools that were made just for them.\n\nwith something like VLLM + GPTQ I can use a model 5 times bigger than the smallest ByT5 ( a variant of T5 I tend to use for my specific task) and get twice average throughput speed. for me, this a deal-breaker, there's just not a whole lot of reasons to switch gears.\n\n## Comment ID kf3bb5z with +1 score by [(wind_dude, Reddit, 2023-12-27)](https://www.reddit.com/r/LocalLLaMA/comments/18rryf1/why_is_noone_finetuning_something_like_t5/kf3bb5z/) (in reply to ID 18rryf1):\nPossibly part of the reason flan outperforms orca minis is the CoT data was recreated from flan, but someone didn’t keep the original data and source answers before piping it OpenAI so there was no easy way to remove hallucinations.\n\n## Comment ID kf3qktm with +1 score by [(AnomalyNexus, Reddit, 2023-12-27)](https://www.reddit.com/r/LocalLLaMA/comments/18rryf1/why_is_noone_finetuning_something_like_t5/kf3qktm/) (in reply to ID 18rryf1):\nI'll probably give it a try for [this task](/r/LocalLLaMA/comments/185e84u/are_there_any_data_cleaning_focused_llms_also_rant/). Might do well for that sort of task & given small size I can probably fine tune on my 3090\n\n### Comment ID kf3qpqg with +1 score by [(No_Baseball_7130, Reddit, 2023-12-27)](https://www.reddit.com/r/LocalLLaMA/comments/18rryf1/why_is_noone_finetuning_something_like_t5/kf3qpqg/) (in reply to ID kf3qktm):\nyou should fine tune lmsys/fastchat-t5-3b-v1.0 on smth like openorca\n\n#### Comment ID kf3rda7 with +2 score by [(AnomalyNexus, Reddit, 2023-12-27)](https://www.reddit.com/r/LocalLLaMA/comments/18rryf1/why_is_noone_finetuning_something_like_t5/kf3rda7/) (in reply to ID kf3qpqg):\nLeaning more towards base because I specifically don't want it to be chatbot like. I want to give it a piece of text and get back clean text.\n\nBut at 3B I can definitely try a few approaches. Collection a custom dataset is what is going to take time\n\n## Comment ID kf6h86x with +1 score by [(yoomiii, Reddit, 2023-12-27)](https://www.reddit.com/r/LocalLLaMA/comments/18rryf1/why_is_noone_finetuning_something_like_t5/kf6h86x/) (in reply to ID 18rryf1):\n[This image generator (pix-art alpha)](https://github.com/PixArt-alpha/PixArt-alpha) is using T5 as its language encoder. How easy would it be to quantize the model so less VRAM is needed?\n\n## Comment ID kf6rlpr with +1 score by [(1EvilSexyGenius, Reddit, 2023-12-27)](https://www.reddit.com/r/LocalLLaMA/comments/18rryf1/why_is_noone_finetuning_something_like_t5/kf6rlpr/) (in reply to ID 18rryf1):\nAnd Godel by Microsoft also previously known as DialoGPT is really good at using external knowledge to answer questions. So why is no one using this one?\n\n### Comment ID kf81797 with +1 score by [(No_Baseball_7130, Reddit, 2023-12-28)](https://www.reddit.com/r/LocalLLaMA/comments/18rryf1/why_is_noone_finetuning_something_like_t5/kf81797/) (in reply to ID kf6rlpr):\ndialogpt hallucinates a lot\n\n## Comment ID kfbwril with +1 score by [(advo_k_at, Reddit, 2023-12-28)](https://www.reddit.com/r/LocalLLaMA/comments/18rryf1/why_is_noone_finetuning_something_like_t5/kfbwril/) (in reply to ID 18rryf1):\nEven when fine-tuned it produces very short outputs which is great for some research tasks but not for what people popularly want out of a model is usually."
    ],
    "sources": {
      "steam_url": null,
      "steam_reviews": null,
      "google_play_url": null,
      "google_play_reviews": null,
      "apple_store_url": null,
      "apple_reviews": null,
      "reddit_urls": [
        "https://www.reddit.com/r/MachineLearning/comments/1dffyfs/r_laminiai_introduces_memory_tuning_95_llm/",
        "https://www.reddit.com/r/MachineLearning/comments/1320hyh/p_lamini_rapidly_achieves_chatgpt_performance/",
        "https://www.reddit.com/r/singularity/comments/1dfec4y/introducing_lamini_memory_tuning_95_llm_accuracy/",
        "https://www.reddit.com/r/AMD_Stock/comments/16ssmqw/ai_startup_lamini_bets_future_on_amds_instinct/",
        "https://www.reddit.com/r/Python/comments/1322c2p/we_are_building_lamini_can_we_rapidly_achieve/",
        "https://www.reddit.com/r/AMD_Stock/comments/1it7mkl/greg_diamos_cofounder_and_cto_of_lamini_has_quit/",
        "https://www.reddit.com/r/LocalLLaMA/comments/178yxhq/code_gen_equivalent_of_mbzuailaminiflant5248m/",
        "https://www.reddit.com/r/NVDA_Stock/comments/1cion3q/another_reason_not_to_worry_about_amd_coreweave/",
        "https://www.reddit.com/r/AMD_Stock/comments/1acviv0/just_wth_is_the_2024_amd_ai_revenue_reality_lets/",
        "https://www.reddit.com/r/LocalLLaMA/comments/18rryf1/why_is_noone_finetuning_something_like_t5/"
      ],
      "reddit_search_url": "https://www.google.com/search?q=site%3Areddit.com+%22Lamini%22+related%3Alamini.ai+"
    }
  },
  "glassdoor_result": null,
  "news_result": [
    [
      "Lamini",
      "Lamini",
      "lamini.ai",
      null,
      false,
      false,
      null,
      [
        false,
        false
      ]
    ],
    [
      {
        "title": "Introducing Lamini Memory Tuning: 95% LLM Accuracy, 10x Fewer ...",
        "link": "https://www.lamini.ai/blog/lamini-memory-tuning",
        "snippet": "Jun 13, 2024 ... Hallucinations were reduced from 50% to 5%. Lamini Memory Tuning is a research breakthrough that overcomes a seeming paradox in the AI world: achieving precise ...",
        "formattedUrl": "https://www.lamini.ai/blog/lamini-memory-tuning"
      },
      {
        "title": "Dropbox, Figma CEOs back Lamini, a startup building a generative ...",
        "link": "https://techcrunch.com/2024/05/02/dropbox-figma-ceos-back-lamini-a-startup-building-a-generative-ai-platform-for-enterprises/",
        "snippet": "May 2, 2024 ... Lamini, a new startup with funding from Andrew Ng, has emerged from stealth with a generative AI platform aimed at enterprises.",
        "formattedUrl": "https://techcrunch.com/.../dropbox-figma-ceos-back-lamini-a-startup-buildi..."
      },
      {
        "title": "Guest Post: How I reached 95.8% accuracy from factual data with ...",
        "link": "https://www.lamini.ai/blog/llm-accuracy-from-factual-data",
        "snippet": "Sep 5, 2024 ... In this post, I share my journey from AI novice to achieving impressive accuracy with a fine-tuned LLM in a short period of time. Using Lamini and Llama 3.1, I ...",
        "formattedUrl": "https://www.lamini.ai/blog/llm-accuracy-from-factual-data"
      },
      {
        "title": "Lamini raises $25M for its AI development and inference platform ...",
        "link": "https://siliconangle.com/2024/05/03/lamini-raises-25m-ai-development-inference-platform/",
        "snippet": "May 3, 2024 ... The group develops benchmarks that are used to compare the performance of neural networks, graphics cards and related technologies. Lamini, officially PowerML ...",
        "formattedUrl": "https://siliconangle.com/.../lamini-raises-25m-ai-development-inference-plat..."
      },
      {
        "title": "Meta x Lamini: Tune Llama 3 to query enterprise data safely and ...",
        "link": "https://www.lamini.ai/blog/meta-lamini-llama3-sql",
        "snippet": "Jun 24, 2024 ... This tutorial demonstrates how to tune Llama 3 with Lamini Memory Tuning for a SQL LLM to remove hallucinations, lift accuracy to 95%, and continually improve ...",
        "formattedUrl": "https://www.lamini.ai/blog/meta-lamini-llama3-sql"
      },
      {
        "title": "Lamini AI Partners with Meta to Enhance LLaMA's SQL Performance ...",
        "link": "https://analyticsindiamag.com/ai-news-updates/lamini-ai-partners-with-meta-to-enhance-llamas-sql-performance/",
        "snippet": "Jun 26, 2024 ... Lamini AI announced Lamini Memory Tuning on June 13, wherein the tool has ... Latest AI News. ChatGPT Use Linked to Increased Loneliness, Finds OpenAI ...",
        "formattedUrl": "https://analyticsindiamag.com/...news.../lamini-ai-partners-with-meta-to-enh..."
      },
      {
        "title": "Lamini AI's Memory Tuning Achieves 95% Accuracy and Reduces ...",
        "link": "https://www.marktechpost.com/2024/06/17/lamini-ais-memory-tuning-achieves-95-accuracy-and-reduces-hallucinations-by-90-in-large-language-models/",
        "snippet": "Jun 17, 2024 ... Lamini AI has introduced a groundbreaking advancement in large language models (LLMs) with the release of Lamini Memory Tuning.",
        "formattedUrl": "https://www.marktechpost.com/.../lamini-ais-memory-tuning-achieves-95-a..."
      },
      {
        "title": "Meet Sharon Zhou, the AI Founder Doing Just Fine Without Nvidia ...",
        "link": "https://www.businessinsider.com/nvidia-chips-lamini-ai-amd-jensen-huang-sharon-zhou-2024-4",
        "snippet": "Apr 1, 2024 ... Lamini AI CEO and cofounder Sharon Zhou shows it's possible to build an AI startup without Nvidia's GPUs by using chips from AMD instead.",
        "formattedUrl": "https://www.businessinsider.com/nvidia-chips-lamini-ai-amd-jensen-huang-..."
      },
      {
        "title": "lamini-ai/llm-classifier: Classify data instantly using an LLM - GitHub",
        "link": "https://github.com/lamini-ai/llm-classifier",
        "snippet": "Oct 28, 2024 ... Classify data instantly using an LLM. Contribute to lamini-ai/llm-classifier development by creating an account on GitHub.",
        "formattedUrl": "https://github.com/lamini-ai/llm-classifier"
      },
      {
        "title": "Lamini, A Startup Transforming Enterprises With Its Generative AI ...",
        "link": "https://workhub.ai/lamini-a-startup-transforming-enterprises-with-its-generative-ai-platform/",
        "snippet": "May 9, 2024 ... Lamini, a Palo Alto-based startup, is reshaping the landscape of enterprise AI with a transformative $25 million investment led by prominent investors.",
        "formattedUrl": "https://workhub.ai/lamini-a-startup-transforming-enterprises-with-its-genera..."
      },
      {
        "title": "Lamini AI raises tens of millions for its LLM design platform - Silicon ...",
        "link": "https://www.bizjournals.com/sanjose/news/2024/04/03/lamini-powerml-rasies-24m.html",
        "snippet": "Apr 3, 2024 ... According to a regulatory filing, PowerML Inc. (publicly known as Lamini AI) has big plans ... Yahoo acquires Bay Area AI news startup. Yahoo acquires Bay ...",
        "formattedUrl": "https://www.bizjournals.com/.../news/2024/.../lamini-powerml-rasies-24m.h..."
      },
      {
        "title": "Lamini Raises $25M in Funding",
        "link": "https://www.finsmes.com/2024/05/lamini-raises-25m-in-funding.html",
        "snippet": "May 3, 2024 ... Lamini, a Palo Alto, CA-based startup that is building a platform to help enterprises deploy generative AI applications, raised $25m in seed and Series A ...",
        "formattedUrl": "https://www.finsmes.com/2024/05/lamini-raises-25m-in-funding.html"
      },
      {
        "title": "Improving Accuracy of LLM Applications - DeepLearning.AI",
        "link": "https://www.deeplearning.ai/short-courses/improving-accuracy-of-llm-applications/",
        "snippet": "Aug 10, 2024 ... Join our new short course, Improving Accuracy of LLM Applications with Lamini and Meta. Learn from Sharon Zhou, Co-founder & CEO of Lamini, and Amit Sangani, ...",
        "formattedUrl": "https://www.deeplearning.ai/short.../improving-accuracy-of-llm-application..."
      },
      {
        "title": "Lamini: Customizable Enterprise LLM Platform | Deepgram",
        "link": "https://deepgram.com/ai-apps/lamini",
        "snippet": "Jan 27, 2025 ... Lamini is breaking new ground in the realm of artificial intelligence by offering a state-of-the-art AI-powered LLM (Large Language Model) platform geared ...",
        "formattedUrl": "https://deepgram.com/ai-apps/lamini"
      },
      {
        "title": "Mixture of Memory Experts: Lamini Memory Tuning | by Isaac Kargar ...",
        "link": "https://medium.com/pythons-gurus/mixture-of-memory-experts-lamini-memory-tuning-9f81f3f2765a",
        "snippet": "Jun 15, 2024 ... I just came across a blog post by Lamini called “Introducing Lamini Memory Tuning: 95% LLM Accuracy, 10x Fewer Hallucinations ... AI in the direction of better AI ...",
        "formattedUrl": "https://medium.com/.../mixture-of-memory-experts-lamini-memory-tuning-..."
      },
      {
        "title": "Lamini - Company Profile - Tracxn",
        "link": "https://tracxn.com/d/companies/lamini/__0zhK4zeC_TjBLGUmmJa0wkXDZnw1J5zLfilfESMhY4c",
        "snippet": "Feb 1, 2025 ... Lamini - About the company. What does Lamini do? AI-powered LLM platform for enterprise software development. The platform enables developers to build ...",
        "formattedUrl": "https://tracxn.com/.../lamini/__0zhK4zeC_TjBLGUmmJa0wkXDZnw1J5zL..."
      },
      {
        "title": "Machine Learning Engineer - Lamini | Built In",
        "link": "https://builtin.com/job/machine-learning-engineer/4503239",
        "snippet": "4 days ago ... Our customers own their own models, trained on their data. Lamini optimizes for Expert AI workloads with minimal hallucination, enterprise-grade security, and ...",
        "formattedUrl": "https://builtin.com/job/machine-learning-engineer/4503239"
      },
      {
        "title": "Lamini And 11 Other AI Alternatives For Large Language Models",
        "link": "https://theresanaiforthat.com/ai/lamini/",
        "snippet": "Mar 7, 2025 ... Lamini is an enterprise-level LLM platform designed to help software teams swiftly develop and manage their own Language Learning Models (LLMs).",
        "formattedUrl": "https://theresanaiforthat.com/ai/lamini/"
      },
      {
        "title": "Gen AI Product Engineer at Lamini Inc. | Rise Open Jobs",
        "link": "https://app.joinrise.co/jobs/lamini-inc-gen-ai-product-engineer-c443",
        "snippet": "Jan 22, 2025 ... Exposure to AI frameworks and libraries (TensorFlow, PyTorch, or similar). $180,000 - $250,000 a year. At Lamini AI, we are committed to providing an ...",
        "formattedUrl": "https://app.joinrise.co/jobs/lamini-inc-gen-ai-product-engineer-c443"
      },
      {
        "title": "Fine-Tune Open-Source LLMs Using Lamini - Analytics Vidhya",
        "link": "https://www.analyticsvidhya.com/blog/2024/09/fine-tune-open-source-llms-using-lamini/",
        "snippet": "Nov 29, 2024 ... Explore how to fine-tune open-source LLMs using Lamini to improve domain-specific accuracy and enhance AI performance.",
        "formattedUrl": "https://www.analyticsvidhya.com/.../fine-tune-open-source-llms-using-lami..."
      }
    ],
    [
      "# [Introducing Lamini Memory Tuning: 95% LLM Accuracy, 10x Fewer Hallucinations](https://www.lamini.ai/blog/lamini-memory-tuning)\nTLDR:\n\nLamini Memory Tuning is a new way to embed facts into LLMs that improves factual accuracy and reduces hallucinations to previously unachievable levels — for one Fortune 500 customer, Lamini Memory Tuning led to 95% accuracy compared to 50% with other approaches. Hallucinations were reduced from 50% to 5%.\n\nLamini Memory Tuning is a research breakthrough that overcomes a seeming paradox in the AI world: achieving precise factual accuracy (i.e. no hallucinations) while upholding the generalization capabilities that make LLMs valuable in the first place.\n\nThe method entails tuning millions of expert adapters (e.g. LoRAs) with precise facts on top of any open-source LLM, like Llama 3 or Mistral 3. If the goal is to get Roman Empire facts exactly right, Lamini Memory Tuning would create experts on Caesar, aqueducts, legions, and any other facts you provide. Inspired by information retrieval, the model retrieves only the most relevant experts from an index at inference time — not all the model weights — so latency and cost are dramatically lower. High accuracy, high speed, low cost: with Lamini Memory Tuning, you don’t have to choose.\n\nContact us to try Lamini Memory Tuning.\n\n‍\n\n‍Accuracy matters immensely\n\nYet, general-purpose LLMs are designed to hallucinate, because they are trained to reduce the average error across the examples they’ve seen. They’re pretty good at everything, but perfect at nothing. They can produce fluent English prose because they’ve seen so much of it across the internet, but specific facts—like a date, a revenue number, or a variable name—get muddled in probabilities. As a result, companies have not been able to count on LLMs for the most critical and most valuable use cases – until now.\n\nIntroducing Lamini Memory Tuning\n\nLamini Memory Tuning is a completely new way to fine-tune any existing LLM by tuning millions of LoRA adapters and selecting across them in a wide Mixture of Experts at inference time.\n\nInstead of optimizing average error on everything, Lamini Memory Tuning optimizes for zero error on the specific facts you tell it to remember, so it recalls those facts nearly perfectly. That’s not special on its own. This approach is particularly groundbreaking because it preserves the LLM’s ability to generalize with average error on everything else, and thus continue to produce fluent prose around those facts. Lamini Memory Tuning is a systematic tool for eliminating hallucinations on the facts you care about.\n\nFortune 500 customers are already using Lamini Memory Tuning to achieve 95% factual accuracy on critical use cases where previous state-of-the-art approaches peaked at 50%.\n\n‍\n\nTHE PROBLEM\n\nPrompting and RAG: necessary but not sufficient\n\nPrompting and Retrieval Augmented Generation (RAG) are important methods for surfacing relevant information to the model, shifting its probabilities to consider similar information. This is an important step to getting the model to condition on the right concepts and information, because the model has been trained on so many tasks. Good prompt-engineering and RAG pipelines are critical to improve the overall accuracy of the model.\n\nAt times, this is all you need. But other times, you provide the relevant information and the response is still wrong but so close to right — leading to hallucinations.\n\nWhy do hallucinations happen with the right data? In the model’s internal representation, the right answer is likely clustered with similar, but wrong, options. The right context increases the probabilities of the right answer and nearby wrong options. The model doesn’t know that a nearly right answer is still wrong, because general models don’t distinguish between exactly right and nearly right — they never learned to take the loss on those answers to zero. Prompting and RAG don’t change that.\n\nLamini Memory Tuning addresses this directly, by combining methods from information retrieval and AI to teach the model that getting the answer nearly right is the same as getting it totally wrong.\n\nInstruction fine-tuning: the wrong tool for the job\n\nMany teams turn to instruction fine-tuning when other techniques hit a wall on factual accuracy, but instruction fine-tuning, with or without LoRAs, lead to the same issue that pre-training has: it gets to be pretty good at a more narrow dataset, but still perfect at nothing, while being finicky to work with (losing the ability to perform on some general tasks, if you do it wrong).\n\nAs a result, teams struggle with unclear choices, long feedback loops, high compute bills, and ultimately underwhelming performance improvements. While instruction fine-tuning can be really valuable (it’s what turned GPT-3 into ChatGPT), it doesn't make models perfect at the facts that matter. In other words, traditional fine-tuning does not ensure that the model's answers are faithful to facts in its training data.\n\nThis is why we developed Lamini Memory Tuning.\n\n‍\n\nOUR INNOVATION\n\nLamini Memory Tuning: near-perfect fact recall via 1 million-way MoE\n\nLamini Memory Tuning is a fundamentally different fine-tuning approach that effectively teaches any open-source LLM to be near-perfect on facts, while still maintaining its ability to be pretty good at everything else. When the model is supposed to recall a specific fact, Lamini Memory Tuning shifts the entire probability mass to that particular fact (i.e. specific tokens within a particular context), such as the exact SQL schema for your database. This results in output probabilities that are not just closer to the right result, but exactly there.\n\nTo do this, Lamini Memory Tuning tunes a massive mixture of memory experts on any open-source LLM. Each memory expert acts like a LoRA adapter that functionally operates as memory for the model. Together, the memory experts specialize in a million different ways to ensure faithful and factual accuracy to the data that it was tuned on. Inspired by information retrieval, these million memory experts are equivalent to indices from which the model intelligently retrieves and routes. At inference time, the model retrieves the most relevant experts at each layer and merges back into the base model to respond to the user query.\n\nThe result is a sparsely activated model, called a Mixture of Memory Experts (MoME), that can scale to an enormous number of parameters at a fixed computational inference cost. This means MoMEs have extremely high capacity for the number of facts that can be learned, bounded only by the total size of the training data set. Llama 3 was trained on 15 trillion tokens. Realistically, you will run out of system memory before you run out of memory capacity in a MoME.\n\nUltimately, this approach makes what were impossible use cases that critically suffer from hallucinations within reach, and drastically improves LLM time-to-accuracy and thus time-to-market.\n\nRead more details in our research paper.\n\nResults\n\nLamini Memory Tuning has been a game-changing capability with Lamini’s Fortune 500 clients, who are deploying it for the following use cases:\n\nHigh precision text-to-SQL\n\nClient need: Democratize data access by using LLMs to turn natural language questions into database queries.\n\nChallenge: The relevant databases had unique internal names and large, messy schemas.\n\nResult: We achieved 95% accuracy with Lamini Memory Tuning after 50% accuracy with RAG.\n\nHigh precision classification\n\nClient need: Save thousands of hours by automatically labeling data accurately.\n\nChallenge: We had to adhere to an exact taxonomy of 900 categories.\n\nResult: We achieved 100% accuracy across thousands of documents.\n\nHigh precision recommendations\n\nClient need: Increase cart size and revenue with AI-powered product suggestions.\n\nChallenge: Applications break when product IDs are hallucinated.\n\nResult: We achieved 88% accuracy across a 50,000 product database.\n\nA new frontier\n\nLamini Memory Tuning changes several of the fundamental dynamics and tradeoffs governing how we work with LLMs. We’re in the early days of this new paradigm, and we’re still learning alongside our customers what’s possible. Summarizing a few areas we’re most excited about:\n\nHigher accuracy enables full automation as opposed to copiloting.\n\nLower costs let you take your product from internal demos to a wider production audience.\n\nLower latency enables seamless user experiences.\n\nSmaller models mean faster development and improvement cycles.\n\nWhat could you do with models that ran faster, were more accurate, and cost less to develop and run?\n\nStart using Lamini Memory Tuning\n\nBecause Lamini Memory Tuning is a cutting-edge technique that embeds your unique data in a new model architecture, we’re exclusively working with select partners.\n\nContact us to try Lamini Memory Tuning.\n\nWant to learn more?\n\nRead the research paper for details on the underlying approach.",
      "# [Dropbox, Figma CEOs back Lamini, a startup building a generative AI platform for enterprises by Kyle Wiggers on 2024-05-02](https://techcrunch.com/2024/05/02/dropbox-figma-ceos-back-lamini-a-startup-building-a-generative-ai-platform-for-enterprises/)\nLamini, a Palo Alto-based startup building a platform to help enterprises deploy generative AI tech, has raised $25 million from investors, including Stanford computer science professor Andrew Ng.\n\nLamini, co-founded several years ago by Sharon Zhou and Greg Diamos, has an interesting sales pitch.\n\nMany generative AI platforms are far too general purpose, Zhou and Diamos argue, and don’t have solutions and infrastructure geared to meet the needs of corporations. In contrast, Lamini was built from the ground up with enterprises in mind and is focused on delivering high generative AI accuracy and scalability.\n\n“The top priority of nearly every CEO, CIO and CTO is to take advantage of generative AI within their organization with maximal ROI,” Zhou, Lamini’s CEO, told TechCrunch. “But while it’s easy to get a working demo on a laptop for an individual developer, the path to production is strewn with failures left and right.”\n\nTo Zhou’s point, many companies have expressed frustration with the hurdles to meaningfully embracing generative AI across their business functions.\n\nAccording to a March poll from MIT Insights, only 9% of organizations have widely adopted generative AI despite 75% having experimented with it. Top hurdles run the gamut from a lack of IT infrastructure and capabilities to poor governance structures, insufficient skills and high implementation costs. Security is a major factor, too — in a recent survey by Insight Enterprises, 38% of companies said security was impacting their ability to leverage generative AI tech.\n\nSo what’s Lamini’s answer?\n\nZhou says that “every piece” of Lamini’s tech stack has been optimized for enterprise-scale generative AI workloads, from the hardware to the software, including the engines used to support model orchestration, fine-tuning, running and training. “Optimized” is a vague word, granted, but Lamini is pioneering one step that Zhou calls “memory tuning,” which is a technique to train a model on data such that it recalls parts of that data exactly.\n\nMemory tuning can potentially reduce hallucinations, Zhou claims, or instances when a model makes up facts in response to a request.\n\n“Memory tuning is a training paradigm — as efficient as fine-tuning, but goes beyond it — to train a model on proprietary data that includes key facts, numbers and figures so that the model has high precision,” Nina Wei, an AI designer at Lamini, told me via email, “and can memorize and recall the exact match of any key information instead of generalizing or hallucinating.”\n\nI’m not sure I buy that. “Memory tuning” appears to be more a marketing term than an academic one; there aren’t any research papers about it — none that I managed to turn up, at least. I’ll leave Lamini to show evidence that its “memory tuning” is better than the other hallucination-reducing techniques that are being/have been attempted.\n\nFortunately for Lamini, memory tuning isn’t its only differentiator.\n\nZhou says the platform can operate in highly secured environments, including air-gapped ones. Lamini lets companies run, fine-tune, and train models on a range of configurations, from on-premises data centers to public and private clouds. And it scales workloads “elastically,” reaching over 1,000 GPUs if the application or use case demands it, Zhou says.\n\n“Incentives are currently misaligned in the market with closed source models,” Zhou said. “We aim to put control back into the hands of more people, not just a few, starting with enterprises who care most about control and have the most to lose from their proprietary data owned by someone else.”\n\nFor what it’s worth, Lamini’s co-founders are quite accomplished in the AI space. They’ve also separately brushed shoulders with Ng, which no doubt explains his investment.\n\nZhou was previously faculty at Stanford, where she headed a group that was researching generative AI. Prior to receiving her doctorate in computer science under Ng, she was a machine learning product manager at Google Cloud.\n\nDiamos, for his part, co-founded MLCommons, the engineering consortium dedicated to creating standard benchmarks for AI models and hardware, as well as the MLCommons benchmarking suite, MLPerf. He also led AI research at Baidu, where he worked with Ng while the latter was chief scientist there. Diamos was also a software architect on Nvidia’s CUDA team.\n\nThe co-founders’ industry connections appear to have given Lamini a leg up on the fundraising front. In addition to Ng, Figma CEO Dylan Field, Dropbox CEO Drew Houston, OpenAI co-founder Andrej Karpathy, and — strangely enough — Bernard Arnault, the CEO of luxury goods giant LVMH, have all invested in Lamini.\n\nAMD Ventures is also an investor (a bit ironic considering Diamos’ Nvidia roots), as are First Round Capital and Amplify Partners. AMD got involved early, supplying Lamini with data center hardware, and today, Lamini runs many of its models on AMD Instinct GPUs, bucking the industry trend.\n\nLamini makes the lofty claim that its model training and running performance is on par with Nvidia equivalent GPUs, depending on the workload. Since we’re not equipped to test that claim, we’ll leave it to third parties.\n\nTo date, Lamini has raised $25 million across seed and Series A rounds (Amplify led the Series A). Zhou says the money is being put toward tripling the company’s 10-person team, expanding its compute infrastructure, and kicking off development into “deeper technical optimizations.”\n\nThere are a number of enterprise-oriented, generative AI vendors that could compete with aspects of Lamini’s platform, including tech giants like Google, AWS and Microsoft (via its OpenAI partnership). Google, AWS and OpenAI, in particular, have been aggressively courting the enterprise in recent months, introducing features like streamlined fine-tuning, private fine-tuning on private data, and more.\n\nI asked Zhou about Lamini’s customers, revenue and overall go-to-market momentum. She wasn’t willing to reveal much at this somewhat early juncture but said that AMD (via the AMD Ventures tie-in), AngelList and NordicTrack are among Lamini’s early (paying) users, along with several undisclosed government agencies.\n\n“We’re growing quickly,” she added. “The number one challenge is serving customers. We’ve only handled inbound demand because we’ve been inundated. Given the interest in generative AI, we’re not representative in the overall tech slowdown — unlike our peers in the hyped AI world, we have gross margins and burn that look more like a regular tech company.”\n\nAmplify general partner Mike Dauber said, “We believe there’s a massive opportunity for generative AI in enterprises. While there are a number of AI infrastructure companies, Lamini is the first one I’ve seen that is taking the problems of the enterprise seriously and creating a solution that helps enterprises unlock the tremendous value of their private data while satisfying even the most stringent compliance and security requirements.”",
      "# [Guest Post: How I reached 95.8% accuracy from factual data with Lamini and Llama 3.1](https://www.lamini.ai/blog/llm-accuracy-from-factual-data)\nThis is a guest blog post by Allan Ray Jasa.‍\n\nIn this post, I share my journey from AI novice to achieving impressive accuracy with a fine-tuned LLM in a short period of time. Using Lamini and Llama 3.1, I explored the process of improving an LLM's performance on a book’s first chapter.\n\nWhen Meta released Llama 3.1, I got so excited. Finally, I thought, there’s an open source LLM that matches the capabilities of GPT-4. This was an idea that seemed more and more unachievable when the months turned to more than a year after GPT-4’s initial release. Llama 3.1 is so impressive that even the versions with fewer parameters, such as the Llama-3.1-8B, appear to perform on par with GPT-4o in terms of adherence to tool/function calling.\n\nAfter finishing the DeepLearning.AI course called Improving Accuracy of LLM Applications, where Sharon Zhou of Lamini teaches how to fine-tune a Llama 3.1 model on the Lamini platform, I thought of getting my hands in the clay with this LLM fine-tuning business.\n\nTo be completely honest, I had tried fine-tuning an LLM before and thought it was not for me. I had taken an online course on fine-tuning a FLAN-T5 model and the Jupyter notebook looked like esoteric incantations to a nameless, faceless LLM deity: what do you mean I need to set the target modules as “q” and “v” in the Lora config? Why should I set auto_find_batch_size to true in the training arguments? With so much code and so many parameters that I didn’t know what for, I felt intimidated.\n\nI thought fine-tuning an LLM was a task that was more suited for data scientists with PhDs, or research engineers at OpenAI or Anthropic. As an iOS developer trying to pivot to an AI career, I had resigned myself to the idea that all I could do was funnel data to an LLM, such as a RAG application, and then present the result of inference back to the user. So imagine my surprise when, with the DeepLearning course I mentioned above, Sharon demonstrated that with Lamini, you can fine-tune a state-of-the-art LLM such as Llama 3.1 with just two to three lines of code.\n\nWe will get to the details as we go along, but just to show proof, this is the final code that gave me a fine-tuned Llama 3.1-8B-Instruct that achieved 95.8% accuracy. Three lines:\n\nSo what did I do? I chose a different problem to work on. The DeepLearning.AI course focused on fine-tuning an LLM to give accurate SQL statements from natural language. Since the documentation mentioned that Lamini can handle facts from texts, I thought of giving that a try.\n\nI’ve always admired this particular writer and I have a copy of his biography on my shelf. So I thought, why not train the model on the first chapter of this book and see if it can answer my questions? I extracted the text from the first chapter and fed it directly to the model.\n\nOf course, since I am quite new to this, I initially didn't realize that fine-tuning an LLM using Lamini's technology requires question-and-answer pairs, even when the answers are facts rather than SQL statements. This requirement is common in many current LLM fine-tuning approaches. I appreciated Lamini’s descriptive error message so that I could arrive at that conclusion.\n\nTaking advantage of Anthropic Claude’s very wide context window, I asked it to generate about 100 questions from the book’s first chapter, and then, using the sample guide from the documentation, I trained the model, which was Llama 3.1-8B-Instruct.\n\nI thought the fine-tuned model was giving good, although brief, answers until I built a RAG application using the same text from the book, powered by Llama 3.1 (running as Ollama in a MacBook Air):\n\nI compared the answers and saw what was missing: the RAG application was able to give more details to the answer than the fine-tuned LLM. And it’s not like these details weren’t available in the fine-tuning data. They were all there. It’s just that the model wasn’t able to connect the dots, or generalize and integrate the data it was trained with.\n\nUsing Claude again, I asked it to generate about 49 new Q&A pairs that generalize information from those previously generated, 100 Q&A pairs. I combined them both, and as instructed by Lamini’s documentation, multiplied it by ten before using it as training data.\n\nWith a bit of prompt engineering, I was able to achieve something similar to the RAG application, or at least I thought I did just by asking the same question earlier, and comparing the answer to the RAG application’s answer.\n\nBut of course I needed some quantitative measurement. I needed to know overall how this fine-tuned model performs in comparison to the RAG application. So I asked Claude again to generate 25 questions from the text, but I added that the answers to the questions should not be reducible to a name, place, date, or any simple fact. For example, this pair is not valid:\n\nQ: What is the author’s brother’s name? A: The author’s brother’s name is Allan.\n\nBecause it can be reduced to just Allan. This is a good one:\n\nQ. Describe the historical context surrounding the author’s birth. A: [a sentence or two]\n\nBecause the model will look at the author’s birth date and think what other historical events occurred that time, which was also included in the fine-tuning data.\n\nThis set of questions will serve as a gold standard for evaluation, questions that the model hasn’t seen before. And in order to evaluate, I created a sort of scorer, powered by OpenAI’s GPT-4o, that has this prompt:\n\nAnd then I evaluated by going through all the gold standard questions, asking both the fine-tuned model and the RAG application the same question, having the scorer evaluate each of the answers, and then collecting the results:\n\nFor the second iteration, I got this result:\n\nMy initial thought was, why bother fine-tuning an LLM when it achieves the same accuracy as a RAG application? But then I inspected the evaluation and found out that on only one question did both the fine-tuned model and RAG application fail. In others, it’s either the fine-tuned was correct, or the RAG was correct, which led me to believe that the generalizations in the fine-tuning data were probably not enough because it wasn’t able to infer in those areas.\n\nIn my third iteration, I thought of adding an equal amount of generalization Q&A pairs to the 100 base Q&A pairs. I had assumed Claude had generated a hundred pairs because I kept asking it to continue when it got cut off due to the response limit, but it turned out it only generated 84 new Q&A pairs. Maybe that was the extent to which 100 facts could be generalized.\n\nI fed that again to the model, and this was my final result:\n\nI was so surprised that I was able to increase its performance. Bear in mind that as of this writing, Lamini doesn’t support retraining fine-tuned models. So all I adjusted was the training data, but the starting base model remained the same in all of my iterations.\n\nThis journey, from knowing nothing about fine-tuning to achieving 95.8% accuracy in just two days, has been both exciting and enlightening. I am so impressed with what I have accomplished with Lamini. Reflecting on this experience, I have come to appreciate several valuable lessons:\n\nAccessibility of AI technology: Fine-tuning state-of-the-art language models is no longer limited to PhD-level data scientists. With the right tools, even those new to the field like me can achieve impressive results.\n\nIterative improvement: Investigating what to adjust with each round of fine-tuning, and studying the corresponding evaluation, provided insights that guided the next steps.\n\nQuality of training data: The importance of high-quality, diverse training data cannot be overstated. Generalizing the information and creating thoughtful question-answer pairs significantly boosted the model's performance.\n\nSimplified fine-tuning process: Thanks to Lamini's API simplicity, the complexity of parameter adjustment is greatly reduced. With only 2-3 key parameters to consider (in my case), users can focus primarily on studying evaluation results and creating quality data, making the fine-tuning process more approachable and efficient.\n\nComparison with RAG: While RAG systems are powerful, fine-tuned models can achieve comparable or even superior results in specific domains. The choice between these approaches depends on the use case and available resources.\n\nPotential for specialized models: This experiment demonstrates the potential for creating highly accurate, domain-specific models that can outperform general-purpose LLMs in niche areas.\n\nWhen I look ahead, I see several exciting avenues to explore:\n\nApplying this fine-tuning approach to other domains, models, or larger datasets\n\nInvestigating ways to combine fine-tuned models with RAG systems for optimal performance\n\nAs AI technology continues to evolve, the ability to create custom, highly accurate models will likely become an increasingly valuable skill. This project has shown me that with perseverance, creativity, and the right tools, impressive results are within reach for AI enthusiasts at all levels.\n\n‍",
      "# [Lamini raises $25M for its AI development and inference platform by Maria Deutscher, DUNCAN RILEY, PAUL GILLIN, KYT DOTSON, MIKE WHEATLEY on 2024-05-03](https://siliconangle.com/2024/05/03/lamini-raises-25m-ai-development-inference-platform/)\nLamini, a startup with a platform for building artificial intelligence models and deploying them in production, has received $25 million from a who’s who of tech investors.\n\nThe company announced the investment, which was spread over two funding rounds, on Thursday. Lamini’s institutional backers include Advanced Micro Devices Inc.’s venture capital arm, First Round Capital and Amplify Partners. They were joined by AI pioneer Andrew Ng, OpenAI co-founder Andrej Karpathy and the chief executives of Dropbox Inc., Figma Inc. and Louis Vuitton parent company LVMH.\n\nLamini CEO Sharon Zhou earned her doctorate degree under Ng at Stanford University, where she was a faculty member before launching the startup. Co-founder Greg Diamos, Lamini’s Chief Technology Officer, previously co-founded the MLPerf machine learning consortium. The group develops benchmarks that are used to compare the performance of neural networks, graphics cards and related technologies.\n\nLamini, officially PowerML Inc., provides a software platform that software teams can use to train AI models. It can run neural networks on graphics processing units from AMD or Nvidia Corp. in both cloud and on-premises environments. Companies that go down the on-premises route may deploy Lamini on air-gapped infrastructure, or hardware that is isolated at the network level for cybersecurity reasons.\n\nLamini built its platform with large-scale AI projects in mind. According to the company, customers can distribute workloads across more than 1,000 graphics cards when necessary.\n\nOne of the most challenging tasks involved in training a large language model is configuring its hyperparameters, settings that define details such as how many artificial neurons it includes. Lamini provides a set of default hyperparameters that spare developers the hassle of setting up everything from scratch. At the same time, software teams with more advanced requirements have access to a tool for defining custom LLM settings.\n\nLamini says its platform can also be used to fine-tune AI models that have already been trained. That’s the process of optimizing a neural network in a way that allows it to perform a specific task more effectively. The platform provides several ways of going about the task.\n\nTraditionally, fine-tuning an LLM required modifying a significant number of parameters, configuration settings that influence how an AI processes data. Lamini supports a fine-tuning approach called PEFT that significantly reduces the number of parameter changes involved in the process. The technique can reduce the cost of adapting neural networks to new tasks.\n\nSome AI projects use a different fine-tuning method, dubbed RAG, that makes it possible to teach a model new tasks without code changes. Lamini supports that technique as well. For added measure, it provides a dashboard that enables developers to compare the accuracy of their fine-tuned models with the original version.\n\nBesides streamlining AI development, Lamini also promises to ease the task of deploying newly created LLMs in production. It provides a set of inference management features that allow developers to regulate the style in which a language model generates text, the format of the outputted data and related details. It claims its platform makes it possible to perform inference significantly more cost-efficiently than with proprietary LLMs such as Claude 3.\n\nLamini will use its newly disclosed funding to hire more employees and expand its AI infrastructure. The effort will place a particular emphasis on adding more AMD graphics cards. In conjunction, it plans to develop “deeper technical optimizations” for machine learning workloads.",
      "# [Meta x Lamini: Tune Llama 3 to query enterprise data safely and accurately](https://www.lamini.ai/blog/meta-lamini-llama3-sql)\nMeta x Lamini\n\nLamini and Meta are teaming up to make it easy to build powerful LLM applications with Llama 3.\n\nThis tutorial demonstrates how to tune Llama 3 with Lamini Memory Tuning for a SQL LLM to remove hallucinations, lift accuracy to 95%, and continually improve it. By improving the quality of SQL queries generated, you'll be able to improve upon Llama 3's baseline performance. Try it yourself:\n\nMeta Llama 3 Lamini recipe repo: https://github.com/meta-llama/llama-cookbook/tree/main/3p-integrations/lamini/text2sql_memory_tuning\n\n‍\n\nWhy is this useful?\n\nFaster answers for business users, reducing response time by up to 50%.\n\nLess data team time spent answering simple questions, decreasing their workload by approximately 30%.\n\nMore reliable than business users writing their own queries without the data knowledge that the model gets from tuning, achieving a 95% accuracy rate compared to 75% for untrained users.\n\nFor this tutorial, you’ll go through several iterations of creating data generation LLMs, model evaluation LLMs, and Lamini Memory Tuning of the model:\n\nGenerate SQL queries with Llama 3\n\nDevelop an Evaluation (Eval) that is scalable and robust\n\nAuto-generate data to Memory Tune Llama 3\n\nIterate and Memory Tune Again\n\n‍\n\nStep 1: Create a SQL LLM with Llama 3 and Diagnose Hallucinations\n\nYou can first establish baseline performance with Llama 3 to understand what the behavior of calling an off-the-shelf API can provide. Here’s an example query with Llama 3 using prompt tuning and a system prompt.\n\nYou'll use the nba_roster database, which contains information about NBA players, teams, and games. This database will serve as the foundation for your tuning process.\n\nllm = lamini.Lamini(model_name=\"meta-llama/Meta-Llama-3-8B-Instruct\") defmake_llama_3_prompt(user, system=\"\"): system_prompt = \"\"if system != \"\": system_prompt = ( f\"<|start_header_id|>system<|end_header_id|>\\n\\n{system}<|eot_id|>\" ) returnf\"<|begin_of_text|>{system_prompt}<|start_header_id|>user<|end_header_id|> \\n\\n{user}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\" question = \"\"\"Who is the highest paid NBA player?\"\"\"system = f\"\"\"\\ You are an NBA analyst with 15 years of experience writing complex SQL queries. Consider the nba_roster table with the following schema: 0|Team|TEXT eg. \"Toronto Raptors\" 1|NAME|TEXT eg. \"Otto Porter Jr.\" 2|Jersey|TEXT eg. \"0\" and when null has a value \"NA\" 3|POS|TEXT eg. \"PF\" 4|AGE|INT eg. \"22\" in years 5|HT|TEXT eg. `6' 7\"` or `6' 10\"` 6|WT|TEXT eg. \"232 lbs\" 7|COLLEGE|TEXT eg. \"Michigan\" and when null has a value \"--\" 8|SALARY|TEXT eg. \"$9,945,830\" and when null has a value \"--\" Write a sqlite query to answer the following question. Follow instructions exactly\"\"\" prompt = make_llama_3_prompt(question, system) # Ask the model to generate a sql query to answer the questionprint(llm.generate(prompt, max_new_tokens=200))\n\n‍\n\nLlama 3 will respond with the following:\n\nTo answer this question, we can use the following SQLite query: ```sql SELECT NAME, SALARY FROM nba_roster WHERE SALARY!= '--' ORDER BY CAST(SALARY AS REAL) DESC LIMIT 1; ``` This query first filters out the rows where the salary is '--'(i.e., the players who donot have a salary listed). Then, it orders the remaining rows by the salary in descending order (highest to lowest). Finally, it returns the top row, which corresponds to the highest paid NBA player.\n\n‍\n\nThis is interesting but there are few things wrong with it:\n\nThe response must be parsed to extract the sql query.\n\nThe query is incorrect—it returns Saddiq Bey making $4,556,983 instead of Stephen Curry making $51,915,615!\n\n‍\n\nStep 2: Create an Evaluation Dataset\n\nAn Evaluation Dataset is a representative dataset to measure your model’s performance. An Evaluation Dataset can start with as few as 20-100 data points. The goal is to get started quickly on improving your model, so don’t get bogged down—it’s fine to start small.\n\nHere are a few example data points from a reference dataset. Note the question, answer, sql structure—we need all three to assess the model’s accuracy.\n\n{'answer': '24', 'question': \"What's the average age of the Trail Blazers?\", 'sql': \"select avg(age) from nba_roster where team='Portland Trail \"\"Blazers';\"}, {'answer': '25', 'question': \"What's the median age of the NBA?\", 'sql': 'select CAST(AGE as INTEGER) as percentile from nba_roster order by ''percentile limit 1 offset (select count(*) from nba_roster)/2;'}, {'answer': '26', 'question': \"What's the median age of the Miami Heat?\", 'sql': 'select CAST(AGE as INTEGER) as percentile from nba_roster where '\"team='Miami Heat' order by percentile limit 1 offset (select \"\"count(*) from nba_roster where team='Miami Heat')/2;\"}, {'answer': 'Golden State Warriors, Milwaukee Bucks, Miami Heat, LA Clippers, ''Phoenix Suns', 'question': 'What are the 5 teams with the oldest average age in the NBA', 'sql': 'SELECT team, AVG(AGE) AS average_age FROM nba_roster GROUP BY team ''ORDER BY average_age DESC LIMIT 5;'}, {'answer': '$10948045', 'question': 'What is the average salary of Power Forward players in the NBA', 'sql': \"select avg(CAST(REPLACE(REPLACE(SALARY, '$', ''), ',','') AS \"\"INTEGER)) as average_salary from nba_roster where POS = 'PF';\"}\n\n‍\n\nYou can do it! Writing an initial evaluation dataset can feel tedious, but a minor investment in time can lead to drastic improvement in quality. Some rough time estimates: it took ~20 minutes to write 20 queries, leading to a jump in perf from 25% to 75%. A more intense ~1 hour long data generation and cleaning workflow improved perf from 75% to 95%.\n\n‍\n\nStep 3: Evaluate the SQL LLM with an Eval LLM\n\nTo get evaluation results faster, we’ll use Lamini’s Inference Pipeline SDK. First, define a QueryStage and ScoreStage by extending the GenerationNode class. We can take the following steps during the evaluation:\n\nQuery Stage‍\n\nModel generates SQL query\n\nRun SQL query against SQLite database\n\nRun Evaluation Dataset SQL query against SQLite database\n\nclassQueryStage(GenerationNode):def__init__(self, model_name):super().__init__( model_name=model_name, max_tokens=400, ) defgenerate( self, prompt: Union[Iterator[PromptObject], AsyncIterator[PromptObject]], *args, **kwargs, ): results = super().generate( prompt, output_type={\"sqlite_query\": \"str\"}, *args, **kwargs, ) return results defpostprocess(self, obj: PromptObject):# Run both the generated and reference (Gold Dataset) SQL queries# Assessing whether the SQL queries succeeded in hitting the database (not correctness yet!) query_succeeded = Falsetry: obj.data[\"generated_query\"] = obj.response[\"sqlite_query\"] df = pd.read_sql(obj.response[\"sqlite_query\"], con=engine) obj.data['df'] = df query_succeeded = Trueexcept Exception as e: logger.error( f\"Failed to run SQL query: {obj.response['sqlite_query']}\" ) df = pd.read_sql(obj.data[\"sql\"], con=engine) obj.data['reference_df'] = df obj.data[\"query_succeeded\"] = query_succeeded defpreprocess(self, obj: PromptObject): new_prompt = make_llama_3_prompt(**self.make_prompt(obj.data)) obj.prompt = new_prompt defmake_prompt(self, data: dict): system = \"You are an NBA analyst with 15 years of experience writing complex SQL queries.\\n\" system += \"Consider the nba_roster table with the following schema:\\n\" system += get_schema() + \"\\n\" system += ( \"Write a sqlite SQL query that would help you answer the following question:\\n\" ) user = data[\"question\"] return { \"user\": user, \"system\": system, }\n\n‍\n\nScore Stage‍\n\nCompare model query result and Evaluation Dataset query result\n\nclassScoreStage(GenerationNode):def__init__(self):super().__init__( model_name=\"meta-llama/Meta-Llama-3-8B-Instruct\", max_tokens=400, ) defgenerate( self, prompt: Union[Iterator[PromptObject], AsyncIterator[PromptObject]], *args, **kwargs, ): results = super().generate( prompt, output_type={\"explanation\": \"str\", \"similar\": \"bool\"}, *args, **kwargs, ) return results defpreprocess(self, obj: PromptObject): obj.prompt = make_llama_3_prompt(**self.make_prompt(obj)) defpostprocess(self, obj: PromptObject): obj.data['is_matching'] = self.is_matching(obj.data, obj.response) obj.data['explanation'] = obj.response[\"explanation\"] obj.data['similar'] = obj.response[\"similar\"] defis_matching(self, data, response):return (str(data.get('df',\"None\")).lower() == str(data['reference_df']).lower() or response['similar']) defmake_prompt(self, obj: PromptObject):# Your evaluation model compares SQL output from the # generated and reference SQL queries, using another # LLM in the pipeline system_prompt = \"Compare the following two dataframes. \" system_prompt += \"They are similar if they are almost identical, \" system_prompt += \"or if they convey the same information about the nba_roster dataset\" system_prompt += \"Respond with valid JSON {'explanation' : str, 'similar' : bool}\" user_prompt = ( f\"========== Dataframe 1 =========\\n{str(obj.data.get('df','None')).lower()}\\n\\n\" ) user_prompt += ( f\"========== Dataframe 2 =========\\n{str(obj.data['reference_df']).lower()}\\n\\n\" ) user_prompt += f\"Can you tell me if these dataframes are similar?\"return { \"system\": system_prompt, \"user\": user_prompt }\n\n‍\n\nThen, chain these two LLMs together by defining an evaluation pipeline. In this pipeline, you can indicate that one stage feeds into the next by passing the output of the query stage into the input of the score stage in the forward function.\n\nclassEvaluationPipeline(GenerationPipeline):def__init__(self, args):super().__init__() self.query_stage = QueryStage(args.sql_model_name) self.score_stage = ScoreStage() defforward(self, x): x = self.query_stage(x) x = self.score_stage(x) return x results = EvaluationPipeline(args).call(dataset)\n\n‍\n\nWhen we call this pipeline, we can write the results directly into a file and inspect the outputs as it runs.\n\n‍\n\nStep 4: Generate Tuning Data with Data LLMs\n\nYou might be thinking, \"I'd like to do a little better!\" The next step is Lamini Memory Tuning.\n\nFirst, you need tuning data. Let's use Llama 3 to generate some tuning data! You want question and sql datapoints to help tune the model to generate SQL about the nba_roster dataset. The trick here is to work backwards in a pipeline (generate SQL from the schema, then questions from the generated SQL) and to constrain the prompts, so that the generations are more likely to be correct.\n\nWe can take the following steps during generation:\n\nModel Stage\n\nProvide several randomly selected labeled data points and ask Llama 3 to generate SQL queries.\n\nclassModelStage(GenerationNode):def__init__(self):super().__init__( model_name=\"meta-llama/Meta-Llama-3-8B-Instruct\", max_tokens=400, ) defgenerate( self, prompt: Union[Iterator[PromptObject], AsyncIterator[PromptObject]], *args, **kwargs, ): prompt = self.add_template(prompt) results = super().generate( prompt, output_type={ \"explanation\": \"str\", \"sql_query_1\": \"str\", \"sql_query_2\": \"str\", }, *args, **kwargs, ) return results asyncdefadd_template(self, prompts):asyncfor prompt in prompts: new_prompt = make_llama_3_prompt(**self.make_prompt(prompt.data)) yield PromptObject(prompt=new_prompt, data=prompt.data) asyncdefprocess_results(self, results):asyncfor result in results: if result isNone: continueif result.response isNone: continueif self.check_sql_query(result.response[\"sql_query_1\"]): new_result = PromptObject(prompt=\"\", data=copy.deepcopy(result.data)) new_result.data.generated_sql_query = result.response[\"sql_query_1\"] yield new_result if self.check_sql_query(result.response[\"sql_query_2\"]): new_result = PromptObject(prompt=\"\", data=copy.deepcopy(result.data)) new_result.data.generated_sql_query = result.response[\"sql_query_2\"] yield new_result defmake_prompt(self, data): system = \"You are an NBA analyst with 15 years of experience writing complex SQL queries.\\n\" system += ( \"Consider a table called 'nba_roster' with the following schema (columns)\\n\" ) system += get_schema() system += \"Consider the following questions, and queries used to answer them:\\n\"for example in data.sample: system += \"Question: \" + example[\"question\"] + \"\\n\" system += \"Query: \" + example[\"sql\"] + \"\\n\" # Important: generate relevant queries to your reference data# Ideally, close to those that are failing so you can show the model examples of how to do it right! user = \"Write two queries that are similar but different to those above.\\n\" user += \"Format the queries as a JSON object, i.e.\\n\" user += '{ \"explanation\": str, \"sql_query_1\" : str, \"sql_query_2\": str }.\\n' # Next, use Chain of Thought (CoT) and prompt-engineering to help with generating SQL queries user += \"First write an explanation of why you decided to write these new queries in about \" user += \"3-5 sentences, then write valid sqlite SQL queries for each of the 2 new queries. \" user += \"Make sure each query is complete and ends with a ;\\n\"return {\"system\": system, \"user\": user} defcheck_sql_query(self, query):try: pd.read_sql(query, con=engine) except Exception as e: logger.debug(f\"Error in SQL query: {e}\") returnFalsereturnTrue\n\n‍\n\nQuestion Stage\n\nAsk Llama 3 to generate a question that can be answered by the generated queries.\n\nclassQuestionStage(GenerationNode):def__init__(self):super().__init__( model_name=\"meta-llama/Meta-Llama-3-8B-Instruct\", max_tokens=400, ) defgenerate( self, prompt: Union[Iterator[PromptObject], AsyncIterator[PromptObject]], *args, **kwargs, ): results = super().generate( prompt, output_type={ \"explanation\": \"str\", \"question\": \"str\", }, *args, **kwargs, ) return results defpreprocess(self, obj: PromptObject): new_prompt = make_llama_3_prompt(**self.make_question_prompt(obj.data)) obj.prompt = new_prompt defmake_question_prompt(self, data): system = \"You are an NBA analyst with 15 years of experience writing complex SQL queries.\\n\" system += ( \"Consider a table called 'nba_roster' with the following schema (columns)\\n\" ) system += get_schema() + \"\\n\" system += \"Queries, and questions that they are used to answer:\\n\"for example in data.sample: system += \"Query: \" + example[\"sql\"] + \"\\n\" system += \"Question: \" + example[\"question\"] + \"\\n\" user = \"Now consider the following query.\\n\" user += \"Query: \" + data.generated_sql_query + \"\\n\" user += \"Write a question that this query could be used to answer.\\n\" # Using Chain of Thought (CoT) again# This time you can do it programmatically with function calling, # so you can easily extract a question out of the JSON object user += \"Format your response as a JSON object, i.e.\\n\" user += '{ \"explanation\": str, \"question\": str }.\\n' user += \"First write an explanation in about 3-5 sentences, then write a one sentence question.\\n\" return {\"system\": system, \"user\": user}\n\n‍\n\nLike we did above for evals, you can define a pipeline to generate these queries. This one also has multiple stages, and as mentioned above, the trick is that you are working backwards. The first stage writes SQL pertinent to nba_roster. You're using prompt tuning to get queries that may be inspired by a sample of our gold dataset—that way, you're getting examples that are relevant to the evaluation (ideally, showing correct examples similar to those that were previously incorrect). Then, you use the question stage to inspect those queries and generate a question that can be answered by the generated query.\n\nSince the point is to create a model that can move forwards (generate), working backward like this is just one creative method for data generation that can help constrain the prompts and produce more accurate generated data for tuning.\n\nclassQueryGenPipeline(GenerationPipeline): def__init__(self): super().__init__() self.model_stage= ModelStage() self.question_stage = QuestionStage() def forward(self, x): x = self.model_stage(x) x = self.question_stage(x) return x QueryGenPipeline().call(seed_queries)\n\n‍\n\nFrom this pipeline, you can generate an arbitrarily large amount of data. Filtering this generated data is important to maintain the quality of the data you tune with. Some queries and questions might be duplicated, and some queries may not run.\n\nA few improvements you can easily make programmatically:\n\nRemove duplicates\n\nRemove invalid SQL\n\nRemove queries that filter by \"Null\"\n\nRemove queries that return an empty dataframe\n\nRemove queries that use incorrect query components like \"AVG(HT)\" in the query\n\nAdd missing semicolons to the end\n\n‍\n\nStep 5: Tune Llama 3 with Lamini Memory Tuning\n\nWhen you are satisfied with your generated data, you can launch a training job.\n\nllm = lamini.Lamini(model_name=\"meta-llama/Meta-Llama-3-8B-Instruct\") dataset = get_dataset(args, make_question) finetune_args = get_default_finetune_args() llm.train( data_or_dataset_id=dataset, finetune_args=finetune_args, is_public=True, # For sharing )\n\n‍\n\nAfter you submit a job, you can monitor its status at https://app.lamini.ai/train. There, you'll have access to the interface shown below, which will help you track jobs, view logs, and get the model ID once training is complete.\n\nLet’s ask the newly-tuned model a question!\n\nllm = lamini.Lamini(model_name=\"a5ebf1c4879569101f32444afae5adcafbfce9c5a6ed13035fd892147f7d59bc\") question = \"\"\"Who is the highest paid NBA player?\"\"\"system = f\"\"\"\\ You are an NBA analyst with 15 years of experience writing complex SQL queries. Consider the nba_roster table with the following schema: 0|Team|TEXT eg. \"Toronto Raptors\" 1|NAME|TEXT eg. \"Otto Porter Jr.\" 2|Jersey|TEXT eg. \"0\" and when null has a value \"NA\" 3|POS|TEXT eg. \"PF\" 4|AGE|INT eg. \"22\" in years 5|HT|TEXT eg. `6' 7\"` or `6' 10\"` 6|WT|TEXT eg. \"232 lbs\" 7|COLLEGE|TEXT eg. \"Michigan\" and when null has a value \"--\" 8|SALARY|TEXT eg. \"$9,945,830\" and when null has a value \"--\" Write a sqlite query to answer the following question. Follow instructions exactly\"\"\" prompt = make_llama_3_prompt(question, system) print(llm.generate(prompt, max_new_tokens=200))\n\nselect salary, name from nba_roster where SALARY!='--'ORDERBYCAST(REPLACE(REPLACE(SALARY, '$', ''), ',','') ASINTEGER) DESC LIMIT 1;\n\n‍\n\nMuch better! This query now correctly returns Stephen Curry!\n\n‍\n\nStep 6: Evaluate the Tuned Llama 3\n\nTo compare how results have improved quantitatively, you can rerun the SQL evaluation pipeline with the tuned model and analyze the errors. Here’s an example\n\nError 1: The tuned model does notfilterfor null salaries `\"What is the average salary of Power Forward players in the NBA\"` ```sql SELECT AVG(CAST(REPLACE(REPLACE(SALARY, '$', ''), ',','') AS INTEGER)) as average_salary FROM nba_roster WHERE POS='PF' AND SALARY!= '--'; 12355651.6714286 ``` Reference: ```sql select avg(CAST(REPLACE(REPLACE(SALARY, '$', ''), ',','') AS INTEGER)) as average_salary from nba_roster where POS = 'PF'; 10948045.7848101 ```\n\n‍\n\nStep 7: Improve the Tuned Llama 3\n\nTuning a model takes multiple iterations: you can measure progress in each tuning cycle by re-running the evaluation and then sift through the results to adjust the data generation pipeline to cover what's still missing.\n\nSometimes, those adjustments are surprisingly small—just like in prompt engineering, it's hard to predict what those adjustments might be, so being able to quickly iterate using your evaluation pipeline and inspect the results quickly is absolutely key.\n\nThat's why Lamini's high-performance inference engine is built to optimize processes for evaluation and data generation and then unify them by tuning into a rapid feedback cycle.\n\nJust for a gauge of what's expected: in the creation of this notebook, over 20 models were tuned. So don't get discouraged if it's not top-notch on your first try: the point is actually to build that muscle of iteration, which is the most important piece towards getting the best results.\n\n‍\n\nSummary\n\nHere’s what it took to get 95% accuracy for question answering:\n\nMultiple automated and manual filtering and editing pass over the tuning data.\n\nMultiple iterations on the Gold Dataset by adding data points the model needed to have coverage over.\n\nMany tuning jobs (30+) on different iterations of the tuning data.\n\nMultiple iterations on the evaluation pipeline and prompt.\n\nDeep error analysis: reading the errors and determining if it's an error in our evaluation pipeline or a model error.\n\nAll this to say—tuning is a highly iterative process. Don't be discouraged if it doesn't work the first time! Trust that incremental progress can be made and codified by storing training datasets.\n\nKeep in mind that you can always improve the model—even the archived datasets we hand filtered can be improved for further performance. Time box the process and don't hesitate to move on to the next step!\n\nShipping the model in production can often gather better feedback and data points to incorporate into the next tuning iteration. You can automatically collect data that your users care about that you might never have thought of. Again, start small: \"Shipping in production\" can begin with a limited release to a handful of users.\n\nYou can find the code for this tutorial here: https://github.com/meta-llama/llama-recipes/tree/main/recipes/3p_integrations/lamini/text2sql_memory_tuning",
      "# [Lamini AI Partners with Meta to Enhance LLaMA’s SQL Performance by Donna Eva on 2024-06-26](https://analyticsindiamag.com/ai-news-updates/lamini-ai-partners-with-meta-to-enhance-llamas-sql-performance/)\nLess than two weeks after the launch of Lamini Memory Tuning, Lamini AI has officially partnered with Meta.\n\nLamini AI announced Lamini Memory Tuning on June 13, wherein the tool has showcased the ability to improve factual accuracy while reducing hallucinations by as much as 95%.\n\n“Lamini Memory Tuning is a research breakthrough that overcomes a seeming paradox in the AI world: achieving precise factual accuracy (i.e. no hallucinations) while upholding the generalisation capabilities that make LLMs valuable in the first place,” the startup said.\n\nThe tuning method was used on open-source models like LLaMa 3 and Mistral 3. Now, however, the company has partnered with Meta to improve LLaMa 3’s baseline performance by improving the quality of SQL queries.\n\nAs part of this, Meta published a repository of Llama 3 Lamini recipes to help tune Llama models, specifically for enterprises.\n\n“Lamini Memory Tuning is a new tool you can use to embed facts into LLMs that improve factual accuracy and reduce hallucinations. Inspired by information retrieval, this method has set a new standard of accuracy for LLMs with less developer effort,” the repository stated.",
      "# [Lamini AI’s Memory Tuning Achieves 95% Accuracy and Reduces Hallucinations by 90% in Large Language Models by Asif Razzaq, Asif Razzaq https:, www.marktechpost.com, www.facebook.com on 2024-06-17](https://www.marktechpost.com/2024/06/17/lamini-ais-memory-tuning-achieves-95-accuracy-and-reduces-hallucinations-by-90-in-large-language-models/)\nLamini AI has introduced a groundbreaking advancement in large language models (LLMs) with the release of Lamini Memory Tuning. This innovative technique significantly enhances factual accuracy and reduces hallucinations in LLMs, considerably improving existing methodologies. The method has already demonstrated impressive results, achieving 95% accuracy compared to the 50% typically seen with other approaches and reducing hallucinations from 50% to a mere 5%.\n\nLamini Memory Tuning addresses a fundamental paradox in AI: how to ensure precise factual accuracy while maintaining the generalization capabilities that make LLMs versatile and valuable. This method involves tuning millions of expert adapters (such as Low-Rank Adapters or LoRAs) with precise facts on top of any open-source LLM, like Llama 3 or Mistral 3. The technique embeds facts within the model to retrieve only the most relevant information during inference, dramatically lowering latency and costs while maintaining high accuracy and speed.\n\nThe need for accurate memory tuning arises from the inherent design of general-purpose LLMs, which are trained to reduce average error across a broad range of examples. This design makes them proficient at many tasks but perfect at none, often resulting in muddled specific facts like dates or revenue numbers. Lamini Memory Tuning, however, optimizes for zero error on particular facts provided to it, enabling the model to recall these facts nearly perfectly without compromising its generalization capabilities.\n\nA notable success story involves a Fortune 500 company that utilized Lamini Memory Tuning to achieve 95% accuracy in critical applications, whereas previous state-of-the-art approaches only reached 50%. This level of precision is particularly crucial for applications requiring exact fact recall, such as converting natural language questions into SQL database queries, where accuracy is paramount.\n\nTraditional methods like Prompting and Retrieval-Augmented Generation (RAG) have their place in improving LLM accuracy but often fall short of eliminating hallucinations. These methods enhance the probability of the right answer but still need to eliminate nearly right yet incorrect responses. Lamini Memory Tuning overcomes this by combining information retrieval techniques with AI, teaching the model that an almost correct answer is effectively as wrong as a completely incorrect one.\n\nLamini Memory Tuning’s innovative approach involves creating a massive mixture of memory experts (MoMEs) akin to specialized indices in information retrieval systems. These experts are tuned to recall specific facts with high fidelity and are dynamically selected during inference. This method preserves the model’s ability to generate fluent prose and ensures near-perfect recall of critical facts. The result is a sparsely activated model capable of scaling to many parameters while maintaining low inference costs, thus extending the practical applications of LLMs into areas previously hindered by hallucinations.\n\nIn conclusion, implementing Lamini Memory Tuning represents a new frontier in developing and applying LLMs. It promises higher accuracy, lower costs, and faster development cycles, enabling broader adoption and deployment in various industries. As Lamini AI continues to refine this technology, the potential for fully automated, highly accurate AI-driven solutions becomes increasingly attainable.",
      "# [Meet Sharon Zhou, the AI founder doing just fine without Nvidia's chips by Hasan Chowdhury on 2024-04-01](https://www.businessinsider.com/nvidia-chips-lamini-ai-amd-jensen-huang-sharon-zhou-2024-4)\nTech CEOs with big plans for artificial intelligence spent a bunch of time scrambling around in search of Nvidia chips last year.\n\nThe Santa Clara giant's chips, known as GPUs, became the hottest property of the generative AI boom. Figures as powerful as Mark Zuckerberg and Sam Altman raced to secure supplies of the vital computing resources needed to power apps like ChatGPT.\n\nHowever, there's one AI boss who hasn't put herself at the mercy of Nvidia's billionaire leader Jensen Huang, and his $2.2 trillion GPU empire. Meet Sharon Zhou.\n\nThe 30-year-old has had quite the career.\n\nShe's the first person to major in both classics and computer science at Harvard. She received a Ph.D. in generative AI at Stanford under machine learning pioneer Andrew Ng, became an adjunct professor at the university, and has made time for online teaching and angel investing. If that wasn't enough, she was also asked to be on the early founding team of Anthropic, the OpenAI rival that just raised an extra $2.75 billion from Amazon.\n\nHer ambitions have taken her in a slightly different direction, however, as she's now forging her own path forward by taking charge of an AI startup of her own.\n\nWho needs Nvidia?\n\nIn April last year, Zhou and her cofounder Greg Diamos, based in Palo Alto, brought their new startup, Lamini AI, out of stealth. Its main ambition was to offer a platform that makes it easy for enterprises to train and create customized large language models with \"just a few lines of code.\"\n\nRelated stories\n\nBusiness Insider tells the innovative stories you want to know\n\nBusiness Insider tells the innovative stories you want to know\n\nThat could mean taking a foundation model like GPT from OpenAI and making it easy for an enterprise to fine-tune that model with its own data. \"What we're doing is making it essentially possible for every enterprise to have OpenAI's infrastructure but in-house,\" Zhou said.\n\nAn equally interesting revelation came months later, however.\n\nIn September, Zhou revealed that Lamini's platform had been building customized LLMs with customers over the past year by exclusively using GPUs from Nvidia's main rival, AMD, the chip giant run by Huang's cousin, Lisa Su.\n\nIt was a big deal given that almost everyone seemed to be exclusively obsessed with H100 — GPUs that Nvidia has struggled to meet the demand of amid supply constraints. Lamini's reveal even came with a video of Zhou teasing Nvidia about the shortage.\n\nAs Zhou acknowledges, though, it wasn't an easy decision to look away from the thing everyone in generative AI has been desperate for. \"The decision-making process was a long one,\" she said. \"It was not a trivial, small one.\"\n\nA few things helped the decision. For one, her cofounder Diamos played a key role in helping make the realization that GPUs other than those from Nvidia work perfectly well.\n\nAs a former Nvidia software architect, Diamos understood that while GPU hardware was vital for getting top performance out of AI models — he was, after all, the coauthor of a paper on \"scaling laws\" that showed the importance of computing power — software was important too.\n\nDiamos was witness to that having worked on CUDA, the software first developed by Nvidia in the 2000s. It makes using AI models with GPUs like the H100 and Nvidia's new Blackwell chip, as simple as a plug-and-play system.\n\nSo it became clear that if another company could build a similar software ecosystem around its GPUs, there'd be no reason they couldn't compete with Nvidia. Fortunately for them, after consulting with Diamos, according to Zhou, AMD was on its way to building a rival system that they would eventually test.\n\n\"Greg and I were just jamming on things, so this has been years in the making, and then once the prototypes worked we were just like let's just double down on this,\" Zhou said.\n\nMore broadly, Zhou recognizes that businesses are so \"excited to use LLMs,\" but many may not want to — or simply can't afford to — wait around for Nvidia to shore up enough supply of its GPUs to meet the demand.\n\nIt's another reason AMD has proven so valuable to her ambitions. Thanks to its GPUs being more available, Zhou was confident that Lamini could offer \"infrastructure that makes meeting that skyrocketing demand\" for LLMs possible.\n\n\"This is because Lamini fully utilizes LLM compute at 10x performance and makes it possible to scale quickly without supply constraints, by offering vendor-agnostic compute options, i.e. it's indiscernible to customers to run Lamini on Nvidia and AMD GPUs,\" she explained.\n\nNo wonder the company is ready to double down on AMD. In January, Zhou shared an image to X of the MI300X — AMD's new chip first unveiled in December by CEO Su as the \"highest performing accelerator in the world\" — live in production at Lamini.\n\nNvidia's Huang might be leading one of the most powerful companies in Silicon Valley now, but the competition is coming for him. Or as Zhou said of AMD: \"They have a real horse in this race.\"",
      "# [classifier: Classify data instantly using an LLM by lamini-ai](https://github.com/lamini-ai/llm-classifier)\nTrain a new classifier with just a prompt. No data needed -- but add data to boost, if you have it.\n\nThen, predict!\n\nNote: each prompt class makes 10 LLM inference calls. See advanced section below to change this.\n\nThis can help with improving your classifier. For example, if the LLM is ever wrong:\n\nAnd your LLM classifier will learn it:\n\nIf you include data on classes that aren't in your classes, then the classifier will include them as new classes, and learn to predict them. However, without a prompt, it won't have a description to use to further boost them.\n\nGeneral guideline: if you don't have any or little data on a class, then make sure to include a good prompt for it. Like prompt-engineering any LLM, creating good descriptions---e.g. with details and examples---helps the LLM get the right thing.\n\nFinally, you can save the data.\n\nFormat of what examples.jsonl looks like:\n\nChange the number of LLM examples (and thus inference calls up to this number) per prompt:\n\nNote that we found 10 to be a good proxy for training an effective classifier.\n\nIf you're working with more classes and want to lift performance, a higher augmented_example_count can help.\n\n./train.sh\n\nWe have some default classes. You can specify your own super easily like this:\n\nThe prompts are descriptions of your classes.\n\nYou can get the probabilities for all the classes, in this case dog (62%) and cat (38%). These can help with gauging uncertainty.\n\nHere are our cat/dog prompts.\n\nCat prompt:\n\nDog prompt:\n\nClone this repo, and run the train.sh or classify.sh command line tools.\n\nRequires docker: https://docs.docker.com/get-docker\n\nSetup your lamini keys (free): https://lamini-ai.github.io/\n\ngit clone git@github.com:lamini-ai/llm-classifier.git\n\ncd llm-classifier\n\nTrain a new classifier.\n\nClassify your data.\n\nThese command line scripts just call python inside of docker so you don't have to care about an environment.\n\nIf you hate docker, you can also run this from python easily...\n\nInstall it pip install lamini\n\nInstantiate a classifier\n\nDefine classes using prompts\n\nOr if you have some training examples (optional)\n\nClassify your data\n\nSave your model\n\nLoad your model\n\nThe LLM classifier converts your prompts into a pile of data, using the Llama 2 LLM. It then finetunes another LLM to distinguish between each pile of data.\n\nWe use several specialized LLMs derived from Llama 2 to convert prompts into piles of training examples for each class. The code for this is available in the lamini python package if you want to look at it.\n\nNo, this is a week night hackathon project, give us feedback and we will improve it. Some known issues:\n\nIt doesn't use batching aggressively over classes, so training on many classes could be sped up by more than 100x.\n\nWe are refining the LLM example generators. Send us any issues you find with your prompts and we can improve these models.\n\nYou don't need to label any data using LaminiClassifier. Labeling data sucks.\n\nNo fiddling with hyperparameters. Fiddle with prompts instead. Hopefully English is easier than attention_dropout_pcts.\n\nA classifier always outputs a valid class. An LLM might answer the question \"Is this talking about a cat\" with \"Well... that depends on ....\". Writing a parser sucks.",
      "# [Lamini, A Startup Transforming Enterprises With Its Generative AI Platform by Muhammad Anees on 2024-05-10](https://workhub.ai/lamini-a-startup-transforming-enterprises-with-its-generative-ai-platform/)\nA Palo Alto startup named Lamini has recently secured a notable investment of $25 million from various investors, among them being Andrew Ng, a renowned professor in computer science at Stanford University. Lamini specializes in developing a platform tailored to assist enterprises in effectively implementing generative AI technology.\n\nFounded several years ago by Sharon Zhou and Greg Diamos, Lamini offers a unique proposition in the realm of AI technology. Zhou and Diamos emphasize that many existing generative AI platforms lack specificity, serving as overly generalized solutions that fail to address the particular needs of corporations. In contrast, Lamini stands out by being purposefully designed with enterprises in mind right from its beginning. The company prioritizes delivering exceptional accuracy and scalability in generative AI, aligning closely with the demands of modern businesses.\n\n“The top priority of nearly every CEO, CIO and CTO is to take advantage of generative AI within their organization with maximal ROI, but while it’s easy to get a working demo on a laptop for an individual developer, the path to production is strewn with failures left and right.” said Lamini’s CEO Sharon Zhou.\n\nZhou’s observation resonates with the sentiments of numerous companies facing challenges in effectively integrating generative AI into their business operations.\n\nA March survey conducted by MIT Insights underscores this issue, revealing that although 75% of organizations have experimented with generative AI, only a mere 9% have truly adopted it. The obstacles encountered vary widely, ranging from insufficient IT infrastructure and capabilities to governance frameworks, scarcity of skilled personnel, and the considerable costs associated with implementation. Security concerns loom large as well, as indicated by a recent study from Insight Enterprises, where 38% of surveyed companies cited security issues as a significant barrier to harnessing the potential of generative AI technology.\n\nSo what is Lamini’s answer to all this?\n\nZhou emphasizes that Lamini has meticulously fine-tuned every aspect of its technology stack to cater specifically to the demands of enterprise-scale generative AI workloads. This optimization encompasses not only hardware and software but also extends to the engines utilized for tasks such as model orchestration, fine-tuning, running, and training.\n\nWhile the term “optimized” may seem ambiguous, Lamini is at the forefront of pioneering a groundbreaking technique referred to as “memory tuning,” as highlighted by Zhou. This innovative approach involves training a model on data in a manner that enables it to recall specific details from that dataset accurately.\n\nRead More: Snowflake’s Enterprise-Grade Leap in Generative AI\n\nZhou suggests that memory tuning holds the potential to mitigate occurrences of “hallucinations” within models, where they fabricate information in response to queries. By incorporating this technique, Lamini aims to enhance the reliability and accuracy of its generative AI solutions, setting a new standard for performance and efficacy in the industry.\n\nNina Wei, an AI designer at Lamini, said in an email that “Memory tuning is a training paradigm — as efficient as fine-tuning, but goes beyond it — to train a model on proprietary data that includes key facts, numbers, and figures so that the model has high precision and can memorize and recall the exact match of any key information instead of generalizing or hallucinating.”\n\n“Memory tuning” may lack academic recognition, as it appears to be more of a marketing term without accompanying research papers to support its efficacy. However, Lamini will need to demonstrate evidence of its effectiveness compared to other hallucination-reducing techniques.\n\nThankfully for Lamini, its strengths extend beyond memory tuning. According to Zhou, the platform excels in operating within highly secure environments, including air-gapped ones. Lamini offers companies the flexibility to run, fine-tune, and train models across various configurations, from on-premises data centers to public and private clouds. Additionally, the platform demonstrates impressive scalability, capable of dynamically scaling workloads to over 1,000 GPUs to meet specific application or use case demands, as highlighted by Zhou.\n\nZhou said, “Incentives are currently misaligned in the market with closed source models, we aim to put control back into the hands of more people, not just a few, starting with enterprises who care most about control and have the most to lose from their proprietary data owned by someone else.”\n\nTeam and Investors\n\nLamini’s co-founders boast impressive credentials within the AI sphere, which likely contributed to attracting investments, including from Andrew Ng. Sharon Zhou, formerly a faculty member at Stanford University, led a research group focusing on generative AI. Before earning her doctorate in computer science under Ng’s mentorship, she served as a machine learning product manager at Google Cloud.\n\nGreg Diamos, on the other hand, co-founded MLCommons, an engineering consortium dedicated to establishing standard benchmarks for AI models and hardware. He also played a pivotal role in developing MLPerf, the benchmarking suite. Diamos led AI research at Baidu, collaborating with Ng during Ng’s tenure as chief scientist there. Additionally, Diamos served as a software architect on Nvidia’s CUDA team.\n\nThe extensive industry connections of Lamini’s co-founders have undoubtedly facilitated the company’s fundraising success. Alongside Ng, notable investors include Dylan Field, CEO of Figma; Drew Houston, CEO of Dropbox; Andrej Karpathy, co-founder of OpenAI; and surprisingly, Bernard Arnault, CEO of luxury goods conglomerate LVMH.\n\nIn addition to prominent investors like First Round Capital and Amplify Partners, Lamini has garnered early support from AMD Ventures, a somewhat unexpected move given Greg Diamos’ background with Nvidia. AMD’s involvement commenced with supplying Lamini’s data center hardware, and today, Lamini mainly operates its models on AMD Instinct GPUs, deviating from the prevailing industry trend.\n\nLamini boldly asserts that its model training and performance on AMD Instinct GPUs rival those of Nvidia equivalents, depending on the specific workload. However, as we lack the means to verify this claim independently, we defer to third-party assessments.\n\nThus far, Lamini has amassed $25 million in funding through seed and Series A rounds, with Amplify leading the Series A. Sharon Zhou explains that these funds will primarily fuel the expansion of Lamini’s 10-person team, bolster its compute infrastructure, and initiate efforts toward deeper technical optimizations.\n\nWhile Lamini asserts its unique value proposition in the enterprise-oriented generative AI market, it faces competition from established players such as Google, AWS, and Microsoft (through its partnership with OpenAI). In recent months, these tech giants have intensified their efforts to court enterprises, introducing features like streamlined fine-tuning and private fine-tuning on proprietary data.\n\nZhou remained somewhat tight-lipped when asked about Lamini’s customer base, revenue, and overall market traction, citing the company’s relatively early stage. However, she disclosed that AMD, AngelList, and NordicTrack are among Lamini’s initial paying users. Additionally, some undisclosed government agencies are part of Lamini’s early clientele.\n\nShe added, “We’re growing quickly. The number one challenge is serving customers. We’ve only handled inbound demand because we’ve been inundated. Given the interest in generative AI, we’re not representative in the overall tech slowdown — unlike our peers in the hyped AI world, we have gross margins and burn that look more like a regular tech company.”",
      "# [ on 2024-04-03](https://www.bizjournals.com/sanjose/news/2024/04/03/lamini-powerml-rasies-24m.html)\n",
      "# [Lamini Raises $25M in Funding by FinSMEs on 2024-05-03](https://www.finsmes.com/2024/05/lamini-raises-25m-in-funding.html)\nLamini, a Palo Alto, CA-based startup that is building a platform to help enterprises deploy generative AI applications, raised $25m in seed and Series A funding.\n\nBackers included including Amplify Partners (led Series A), First Round Capital (led Seed), Andrew Ng, Andrej Karpathy, Bernard Arnault, Pierre Lamond, Sarah Guo, Dylan Field, Lip-Bu Tan, Drew Houston, AMD Ventures, and others.\n\nThe new funding will accelerate development into deeper technical optimizations and expand the team to offer enterprises the strategic support they need at scale.\n\nLed by Sharon Zhou, Co-founder and CEO, Lamini ‍provides an Enterprise AI platform designed for enterprises to fully take advantage of their proprietary data, by turning that data into new LLM capabilities, deploying securely to vendor-agnostic compute options, and empowering their in-house software teams to uplevel to OpenAI-level AI teams.\n\nFinSMEs\n\n03/05/2024",
      "# [Improving Accuracy of LLM Applications on 2024-08-08](https://www.deeplearning.ai/short-courses/improving-accuracy-of-llm-applications/)\nJoin our new short course, Improving Accuracy of LLM Applications with Lamini and Meta. Learn from Sharon Zhou, Co-founder & CEO of Lamini, and Amit Sangani, Senior Director of Partner Engineering, Meta.\n\nMany developers have experienced frustration with inconsistent results when working with LLM applications. This course offers a systematic approach to enhance the accuracy and reliability of your LLM applications.\n\nYou will build an SQL agent, add evaluation metrics to measure performance, and use prompt engineering and self-reflection to make the model perform better. Finally, you will fine-tune the model with techniques like LoRA and memory tuning that embeds facts in model weights to reduce hallucinations.\n\nIn this course, you’ll use Llama’s family of open-source models.\n\nWhat you’ll do:\n\nBuild a text to SQL agent and simulate situations where it hallucinates to begin the evaluation process.\n\nBuild an evaluation framework to systematically measure performance, including criteria for good evaluations, best practices, and how to develop an evaluation score.\n\nLearn how instruction fine-tuning enhances pre-trained LLMs to follow instructions, and how memory fine-tuning embeds facts to reduce hallucinations.\n\nBreak fine-tuning myths and see how Performance-Efficient Fine-tuning (PEFT) techniques like Low-Rank Adaptation(LoRA) reduce training time by 100x and Mixture of Memory Experts (MoME) reduces it even further.\n\nGo through an iterative process of generating training data and fine-tuning, learning practical tips such as adding examples, generating variations, and filtering generated data to increase model accuracy.\n\nStart improving the accuracy of LLM applications today!",
      "# [Lamini: Customizable Enterprise LLM Platform](https://deepgram.com/ai-apps/lamini)\n0.5 Stars1 Star1.5 Stars2 Stars2.5 Stars3 Stars3.5 Stars4 Stars4.5 Stars5 StarsEmpty",
      "# [Mixture of Memory Experts: Lamini Memory Tuning by Isaac Kargar, kargarisaac.medium.com on 2024-06-15](https://medium.com/pythons-gurus/mixture-of-memory-experts-lamini-memory-tuning-9f81f3f2765a)\nI just came across a blog post by Lamini called “Introducing Lamini Memory Tuning: 95% LLM Accuracy, 10x Fewer Hallucinations”. They propose the idea of the Mixture of Memory Experts (MoME) which can be another game-changing technique after the mixture of experts and agentic AI in the direction of better AI solutions. The main goal of MoME is to reduce hallucination and they claim it can reduce hallucination from 50% to 5%, which is huge. Hallucination refers to the problem that LLMs generate information that is not presented in their training data.\n\nOther views that are in use these days, like prompting and RAG+prompting, suggest that hallucinations are the result of a trade-off between creativity and factuality, which can be mitigated by connecting LLMs with external knowledge sources. However, this study challenges that view and shows evidence that traditional approaches can not explain why LLMs hallucinate in practice.\n\nProposed Solution\n\nThe paper proposes a new approach called Lamini Memory Tuning and introduces a first-generation model, Lamini-1, which relies on a massive MoME and targets near-zero training loss for key facts that should not be hallucinated. This architecture is designed to store and dynamically retrieve facts using millions of memory experts.\n\nIn this work we build on information retrieval, database system, and LLM training systems to propose Lamini-1, a model architecture eschewing transformers for knowledge retrieval and instead relying entirely on a massive mixture of memory experts (MoME). Previous work has shown how it’s possible to inject memories directly into LLMs Meng et al. (2022) . Lamini-1 allows for significantly more parallelization and can reach a new state-of-the-art in factual recall after 1 hour of training on 8 MI300X GPUs.\n\nThe authors argue that a single epoch is relevant for tasks requiring generalization and creativity, where some random choice between similar tokens is acceptable. However, it is not sufficient to achieve a high level of precision for factual tasks where getting the answer exactly right matters.\n\nIn the paper, they mention that training large language models is extremely computationally intensive. For instance, training Llama 2 with 70 billion parameters required 35 days using 2,000 A100 GPUs for just a single epoch. Achieving zero training loss on key facts would require 100 epochs, increasing the computational demand by 100 times.\n\nTo address these high costs, Lamini-1 uses a transformer backbone (like Llama2) plus a massive amount of MoMEs. They freeze the transformer backbone and then train the memory experts on a dataset to memorize the facts.\n\nThe massive MoME is designed to cut down on the amount of computation required to memorize facts. This is accomplished by the following training algorithm:\n\n1. For a given question, select a subset of experts, e.g. 32 out of the array of one million.\n\n2. Freeze the weights of the backbone network and the cross attention used to select the expert.\n\n3. Take gradient descent steps until the loss is reduced sufficiently to memorize the fact.\n\nThe computation cost of memorizing each fact now scales with the number of training examples, not with the total number of parameters in the network.\n\nThe following plot shows a hypothetical case comparing the training loss of memory-tuned models against an underfit model, an overfit model, and a model that is trained to minimize generalization error.\n\nVery interesting paper in general. Let’s see what other works will come after it.\n\nPython’s Gurus🚀\n\nThank you for being a part of the Python’s Gurus community!\n\nBefore you go:",
      "# [2025 Company Profile, Funding & Competitors by Tracxn on 2023-10-17](https://tracxn.com/d/companies/lamini/__0zhK4zeC_TjBLGUmmJa0wkXDZnw1J5zLfilfESMhY4c)\nUnlock full details of this profile with our free Lite plan!\n\nSignup and get free access\n\nLamini - About the company\n\nLamini is a series A company based in Palo Alto (United States), founded in 2022 by Sharon Zhou and Greg Diamos. It operates as an AI-powered LLM platform for enterprise software development . Lamini has raised $25M in funding from First Round Capital and Amplify Partners. The company has 31 active competitors, including 17 funded and 3 that have exited . Its top competitor s include companies like Gretel, Hazy and Mostly AI.\n\nCompany Details\n\nEmail ID\n\n*****@lamini.ai\n\nKey Metrics\n\nFounded Year\n\n2022\n\nLocation\n\nPalo Alto, United States\n\nStage\n\nSeries A\n\nTotal Funding\n\n$25M in 1 round\n\nLatest Funding Round\n\nInvestors\n\nRanked\n\n10th among 31 active competitors\n\nSimilar Companies\n\nLamini's funding and investors\n\nLamini has raised a total funding of $25M over 1 round . Its latest funding round was a Series A round on May 02, 2024 for $25M . 11 investor s participated in its latest round, which includes In-Q-Tel, First Round Capital, Amplify Partners and AMD.\n\nLamini has 4 institutional investor s including First Round Capital, Amplify Partners and In-Q-Tel. LipBu Tan and 7 other s are Angel Investors in Lamini .\n\nHere is the list of recent funding rounds of Lamini :\n\nDate of funding\n\nFunding Amount\n\nRound Name\n\nPost money valuation\n\nRevenue multiple\n\nInvestors\n\nMay 02, 2024\n\n$25M\n\nSeries A\n\n6852884\n\n2292996\n\nAccess funding benchmarks and valuations. Sign up today!\n\nView details of Lamini 's funding rounds and investors\n\nLamini's founders and board of directors\n\nFounder? Claim Profile\n\nThe founders of Lamini are Sharon Zhou and Greg Diamos. Sharon Zhou is the CEO of Lamini .\n\nLamini's Competitors and alternates\n\nTop competitor s of Lamini include Gretel, Hazy and Mostly AI. Here is the list of Top 10 competitors of Lamini , ranked by Tracxn score :\n\nOverall Rank\n\nCompany Details\n\nShort Description\n\nTotal Funding\n\nInvestors\n\nTracxn Score\n\n1st\n\nGretel\n\n2019 , San Diego (United States) , Series B\n\nProvider of solutions for data anonymization\n\n$65.5M\n\n61/100\n\n2nd\n\nHazy\n\n2017 , London (United Kingdom) , Acquired\n\nAI based platform offering synthetic data generation solutions\n\n$11.3M\n\n57/100\n\n3rd\n\nMostly AI\n\n2017 , Vienna (Austria) , Series B\n\nPlatform offering synthetic data generation for enterprises\n\n$31.1M\n\n51/100\n\n4th\n\nDatagen\n\n2018 , Tel Aviv (Israel) , Series B\n\nProvider of AI-based synthetic data platform for training computer vision models\n\n$70M\n\n50/100\n\n5th\n\nNurdle\n\n2023 , San Francisco (United States) , Seed\n\nAI deployment platform to accelerate AI production with data\n\n-\n\n47/100\n\n6th\n\nRendered.ai\n\n2019 , Seattle (United States) , Seed\n\nProvider of AI and cloud-based tools to produce synthetic datasets for AI training\n\n$6.55M\n\n47/100\n\n7th\n\nSeMI Technologies\n\n2019 , Amsterdam (Netherlands) , Series A\n\nAI and cloud based storage solutions for objects and vectors\n\n$17.6M\n\n45/100\n\n8th\n\nSynthesis\n\n2019 , San Francisco (United States) , Series A\n\nProvider of a data generation platform for computer vision applications\n\n$25.4M\n\n45/100\n\n9th\n\nYData\n\n2019 , Lisbon (Portugal) , Seed\n\nAI based platform for data anonymization\n\n$3.24M\n\n44/100\n\n10th\n\nLamini\n\n2022 , Palo Alto (United States) , Series A\n\nAI-powered LLM platform for enterprise software development\n\n$25M\n\n45/100\n\nGet insights and benchmarks for competitors of 2M+ companies! Sign up today!\n\nLooking for more details on Lamini 's competitors? Click here to see the top ones\n\nLamini's Investments and acquisitions\n\nLamini has made no investments or acquisitions yet.\n\nReports related to Lamini\n\nHere is the latest report on Lamini's sector:\n\nFree\n\nAI Infrastructure - Sector Report\n\nEdition: February, 2025 (109 Pages)\n\nNews related to Lamini\n\nMedia has covered Lamini for a total of 3 events in the last 1 year , 1 of them has been about partnerships .\n\n•\n\nLamini Raises $25 Million to Help Enterprises Deploy Generative AI TechGlobal Village Space•May 02, 2024•Lamini\n\nGet curated news about company updates, funding rounds, M&A deals and others. Sign up today!\n\nFrequently asked questions about Lamini\n\nWhen was Lamini founded?\n\nLamini was founded in 2022.\n\nWhere is Lamini located?\n\nLamini is located in Palo Alto, United States.\n\nIs Lamini a funded company?\n\nLamini is a funded company, its first funding round was on May 02, 2024.\n\nWhen was the latest funding round of Lamini?\n\nLamini's latest funding round was on May 02, 2024.",
      "# [Machine Learning Engineer - Lamini](https://builtin.com/job/machine-learning-engineer/4503239)\nLamini enables every enterprise to safely, quickly, and cost-effectively build their own Expert AI. Our customers own their own models, trained on their data. Lamini optimizes for Expert AI workloads with minimal hallucination, enterprise-grade security, and enterprise flexibility, running on any infrastructure. Our team is made up of highly committed engineers, researchers, and tech industry veterans excited by mission and technology. We’re backed by leading VCs as well as computing and technology companies.\n\nAbout the role:\n\nWe are looking for exceptionally talented Machine Learning Engineers to join our small team. You will be responsible for end-to-end ownership of scalable Machine Learning systems — from data pipelines, to training, to analyzing performance in a production environment. Since you’ll be joining an early-stage startup at the ground level, you’ll need to be able to wear multiple hats and thrive while working in a dynamic environment.\n\nDesign and train new production-ready machine learning models. You will lead the development of new machine learning models and data pipelines. You will apply fundamental machine learning concepts to quickly iterate and debug model related issues and develop new techniques to handle unique cases with each customer.\n\nCollect, process and analyze data. A big part of our machine learning projects is understanding and analyzing the data. You must build pipelines and processes for cleaning and organizing data as well as build tools to help analyze data. You must also understand what types of data models are struggling with and use this analysis to propose solutions.\n\nAnalyze and improve existing models. You will also be responsible for analyzing performance of our existing models and work to improve their accuracy by applying the latest published research, feature engineering and tuning of hyperparameters.\n\nMust to have:\n\nAt least 5+ years of professional experience designing, training, and deploying machine learning models\n\nStrong computer science foundation, including data structures, algorithms, and design patterns\n\nExpertise in Python demonstrated by implementing multiple medium to large-scale projects\n\nProven ability to implement and debug machine learning models\n\nExcellent communication skills and the ability to have in-depth technical discussions with both the engineering team and business people\n\nFamiliarity with machine learning frameworks and libraries (e.g., scikit-learn, Keras, TensorFlow, PyTorch)\n\nIndustry experience with relational databases and SQL-based tools\n\nBSc in Computer Science, Mathematics or similar field; Master’s or Ph.D. degree is a plus\n\nSelf-starter and comfortable working in an early-stage environment\n\nNice to have:\n\nExperience with big data pipeline technologies such as BigQuery, SnowFlake, Spark, Kafka\n\nResearch experience in machine learning or artificial intelligence related field\n\nContributions to open source ML projects\n\nExperience working on logistics or shipping-related products\n\nExperience with Agile development\n\nAt Lamini AI, we are committed to providing an environment of mutual respect where equal employment opportunities are available to all applicants without regard to race, color, religion, sex, pregnancy (including childbirth, lactation and related medical conditions), national origin, age, physical and mental disability, marital status, sexual orientation, gender identity, gender expression, genetic information (including characteristics and testing), military and veteran status, and any other characteristic protected by applicable law. Lamini AI believes that diversity and inclusion among our employees is critical to our success as a company, and we seek to recruit, develop and retain the most talented people from a diverse candidate pool. Selection for employment is decided on the basis of qualifications, merit, and business need.",
      "# [Gen AI Product Engineer at Lamini Inc.](https://app.joinrise.co/jobs/lamini-inc-gen-ai-product-engineer-c443)\nLamini enables every enterprise to safely, quickly, and cost-effectively build their own Expert AI. Our customers own their own models, trained on their data. Lamini optimizes for Expert AI workloads with minimal hallucination, enterprise-grade security, and enterprise flexibility, running on any infrastructure. Our team is made up of highly committed engineers, researchers, and tech industry veterans excited by mission and technology. We’re backed by leading VCs as well as computing and technology companies.\n\nWe are seeking a Senior/Staff AI Product Engineer. This role is ideal for an experienced full-stack engineer, with deep knowledge of scalable AI applications and products. The successful candidate will be responsible for designing and implementing usable, elegant, and robust APIs and interfaces, on top of complex AI infrastructure.\n\nResponsibilities:\n\nHave experience building applications/products that leverage LLMs\n\nWork directly with the CEO, ML and AI infra teams to develop key products and features\n\nLead product engineering development, from idea inception to production deployment\n\nHave developed or created intuitive engaging interfaces, with bonus points for ones that are fun and AI-related\n\nPrototype effectively at the Pareto frontier of feasibility and usability, which is necessary for AI products\n\nHave affinity for directly engaging with users/customers to get direct customer feedback\n\nCollaborate cross-functionally\n\nBonus points for enjoying teaching, or thinking about engaging and intuitive instruction\n\nQualifications:\n\n5+ years of experience in software development, with a strong focus on full-stack engineering.\n\nProficiency in modern front-end frameworks (e.g., React, Angular, or Vue.js).\n\nExpertise in back-end frameworks and languages (e.g., Node.js, Python, or Java).\n\nStrong knowledge of API design principles, including RESTful\n\nFamiliarity with cloud platforms (AWS, GCP, or Azure) and containerization tools (Docker, Kubernetes).\n\nUnderstanding of AI/ML concepts and the ability to work closely with machine learning engineers.\n\nExcellent communication and collaboration skills.\n\nStrong sense of ownership and accountability.\n\nPreferred Qualifications:\n\nExperience with CI/CD pipelines and DevOps practices.\n\nFamiliarity with database technologies (SQL and NoSQL).\n\nExposure to AI frameworks and libraries (TensorFlow, PyTorch, or similar).\n\n$180,000 - $250,000 a year\n\nAt Lamini AI, we are committed to providing an environment of mutual respect where equal employment opportunities are available to all applicants without regard to race, color, religion, sex, pregnancy (including childbirth, lactation and related medical conditions), national origin, age, physical and mental disability, marital status, sexual orientation, gender identity, gender expression, genetic information (including characteristics and testing), military and veteran status, and any other characteristic protected by applicable law. Lamini AI believes that diversity and inclusion among our employees is critical to our success as a company, and we seek to recruit, develop and retain the most talented people from a diverse candidate pool. Selection for employment is decided on the basis of qualifications, merit, and business need.",
      "# [Source LLMs Using Lamini by Avikumar talaviya on 2024-09-15](https://www.analyticsvidhya.com/blog/2024/09/fine-tune-open-source-llms-using-lamini/)\nRecently, with the rise of large language models and AI, we have seen innumerable advancements in natural language processing. Models in domains like text, code, and image/video generation have archived human-like reasoning and performance. These models perform exceptionally well in general knowledge-based questions. Models like GPT-4o, Llama 2, Claude, and Gemini are trained on publicly available datasets. They fail to answer domain or subject-specific questions that may be more useful for various organizational tasks.\n\nFine-tuning helps developers and businesses adapt and train pre-trained models to a domain-specific dataset that archives high accuracy and coherency on domain-related queries. Fine-tuning enhances the model’s performance without requiring extensive computing resources because pre-trained models have already learned the general text from the vast public data.\n\nThis blog will examine why we must fine-tune pre-trained models using the Lamini platform. This allows us to fine-tune and evaluate models without using much computational resources.\n\nSo, let’s get started!\n\nLearning Objectives\n\nTo explore the need to Fine-Tune Open-Source LLMs Using Lamini\n\nTo find out the use of Lamini and under instructions on fine-tuned models\n\nTo get a hands-on understanding of the end-to-end process of fine-tuning models.\n\nThis article was published as a part of the Data Science Blogathon.\n\nWhy Should One Fine-Tune Large Language Models?\n\nPre-trained models are primarily trained on vast general data with a high chance of lack of context or domain-specific knowledge. Pre-trained models can also result in hallucinations and inaccurate and incoherent outputs. Most popular large language models based on chatbots like ChatGPT, Gemini, and BingChat have repeatedly shown that pre-trained models are prone to such inaccuracies. This is where fine-tuning comes to the rescue, which can help to adapt pre-trained LLMs to subject-specific tasks and questions effectively. Other ways to align models to your objectives include prompt engineering and few-shot prompt engineering.\n\nStill, fine-tuning remains an outperformer when it comes to performance metrics. Methods such as Parameter efficient fine-tuning and Low adaptive ranking fine-tuning have further improved the model fine-tuning and helped developers generate better models. Let’s look at how fine-tuning fits in a large language model context.\n\n# Load the fine-tuning dataset filename = \"lamini_docs.json\" instruction_dataset_df = pd.read_json(filename, lines=True) instruction_dataset_df # Load it into a python's dictionary examples = instruction_dataset_df.to_dict() # prepare a samples for a fine-tuning if \"question\" in examples and \"answer\" in examples: text = examples[\"question\"][0] + examples[\"answer\"][0] elif \"instruction\" in examples and \"response\" in examples: text = examples[\"instruction\"][0] + examples[\"response\"][0] elif \"input\" in examples and \"output\" in examples: text = examples[\"input\"][0] + examples[\"output\"][0] else: text = examples[\"text\"][0] # Using a prompt template to create instruct tuned dataset for fine-tuning prompt_template_qa = \"\"\"### Question: {question} ### Answer: {answer}\"\"\"\n\nThe above code shows that instruction tuning uses a prompt template to prepare a dataset for instruction tuning and fine-tune a model for a specific dataset. We can fine-tune the pre-trained model to a specific use case using such a custom dataset.\n\nThe next section will examine how Lamini can help fine-tune large language models (LLMs) for custom datasets.\n\nHow to Fine-Tune Open-Source LLMs Using Lamini?\n\nThe Lamini platform enables users to fine-tune and deploy models seamlessly without much cost and hardware setup requirements. Lamini provides an end-to-end stack to develop, train, tune,e, and deploy models at user convenience and model requirements. Lamini provides its own hosted GPU computing network to train models cost-effectively.\n\nLamini memory tuning tools and compute optimization help train and tune models with high accuracy while controlling costs. Models can be hosted anywhere, on a private cloud or through Lamini’s GPU network. Next, we will see a step-by-step guide to prepare data to fine-tune large language models (LLMs) using the Lamini platform.\n\nData Preparation\n\nGenerally, we need to select a domain-specific dataset for data cleaning, promotion, tokenization, and storage to prepare data for any fine-tuning task. After loading the dataset, we preprocess it to convert it into an instruction-tuned dataset. We format each sample from the dataset into an instruction, question, and answer format to better fine-tune it for our use cases. Check out the source of the dataset using the link given here. Let’s look at the code example instructions on tuning with tokenization for training using the Lamini platform.\n\nimport pandas as pd # load the dataset and store it as an instruction dataset filename = \"lamini_docs.json\" instruction_dataset_df = pd.read_json(filename, lines=True) examples = instruction_dataset_df.to_dict() if \"question\" in examples and \"answer\" in examples: text = examples[\"question\"][0] + examples[\"answer\"][0] elif \"instruction\" in examples and \"response\" in examples: text = examples[\"instruction\"][0] + examples[\"response\"][0] elif \"input\" in examples and \"output\" in examples: text = examples[\"input\"][0] + examples[\"output\"][0] else: text = examples[\"text\"][0] prompt_template = \"\"\"### Question: {question} ### Answer:\"\"\" # Store fine-tuning examples as an instruction format num_examples = len(examples[\"question\"]) finetuning_dataset = [] for i in range(num_examples): question = examples[\"question\"][i] answer = examples[\"answer\"][i] text_with_prompt_template = prompt_template.format(question=question) finetuning_dataset.append({\"question\": text_with_prompt_template, \"answer\": answer})\n\nIn the above example, we have formatted “questions” and “answers” in a prompt template and stored them in a separate file for tokenization and padding before training the LLM.\n\nTokenize the Dataset\n\n# Tokenization of the dataset with padding and truncation def tokenize_function(examples): if \"question\" in examples and \"answer\" in examples: text = examples[\"question\"][0] + examples[\"answer\"][0] elif \"input\" in examples and \"output\" in examples: text = examples[\"input\"][0] + examples[\"output\"][0] else: text = examples[\"text\"][0] # padding tokenizer.pad_token = tokenizer.eos_token tokenized_inputs = tokenizer( text, return_tensors=\"np\", padding=True, ) max_length = min( tokenized_inputs[\"input_ids\"].shape[1], 2048 ) # truncation of the text tokenizer.truncation_side = \"left\" tokenized_inputs = tokenizer( text, return_tensors=\"np\", truncation=True, max_length=max_length ) return tokenized_inputs\n\nThe above code takes the dataset samples as input for padding and truncation with tokenization to generate preprocessed tokenized dataset samples, which can be used for fine-tuning pre-trained models. Now that the dataset is ready, we will look into the training and evaluation of models using the Lamini platform.\n\nFine-Tuning Process\n\nNow that we have a dataset prepared in an instruction-tuning format, we will load the dataset into the environment and fine-tune the pre-trained LLM model using Lamini’s easy-to-use training techniques.\n\nSetting up an Environment\n\nTo begin the fine-tuning open-source LLMs Using Lamini, we must first ensure that our code environment has suitable resources and libraries installed. We must ensure you have a suitable machine with sufficient GPU resources and install necessary libraries such as transformers, datasets, torches, and pandas. You must securely load environment variables like api_url and api_key, typically from environment files. You can use packages like dotenv to load these variables. After preparing the environment, load the dataset and models for training.\n\nimport os from lamini import Lamini lamini.api_url = os.getenv(\"POWERML__PRODUCTION__URL\") lamini.api_key = os.getenv(\"POWERML__PRODUCTION__KEY\") # import necessary library and load the environment files import datasets import tempfile import logging import random import config import os import yaml import time import torch import transformers import pandas as pd import jsonlines # Loading transformer architecture and [[ from utilities import * from transformers import AutoTokenizer from transformers import AutoModelForCausalLM from transformers import TrainingArguments from transformers import AutoModelForCausalLM from llama import BasicModelRunner logger = logging.getLogger(__name__) global_config = None\n\nLoad Dataset\n\nAfter setting up logging for monitoring and debugging, prepare your dataset using datasets or other data handling libraries like jsonlines and pandas. After loading the dataset, we will set up a tokenizer and model with training configurations for the training process.\n\n# load the dataset from you local system or HF cloud dataset_name = \"lamini_docs.jsonl\" dataset_path = f\"/content/{dataset_name}\" use_hf = False # dataset path dataset_path = \"lamini/lamini_docs\"\n\nSet up model, training config, and tokenizer\n\nNext, we select the model for fine-tuning open-source LLMs Using Lamini, “EleutherAI/pythia-70m,” and define its configuration under training_config, specifying the pre-trained model name and dataset path. We initialize the AutoTokenizer with the model’s tokenizer and set padding to the end-of-sequence token. Then, we tokenize the data and split it into training and testing datasets using a custom function, tokenize_and_split_data. Finally, we instantiate the base model using AutoModelForCausalLM, enabling it to perform causal language modeling tasks. Also, the below code sets up compute requirements for our model fine-tuning process.\n\n# model name model_name = \"EleutherAI/pythia-70m\" # training config training_config = { \"model\": { \"pretrained_name\": model_name, \"max_length\" : 2048 }, \"datasets\": { \"use_hf\": use_hf, \"path\": dataset_path }, \"verbose\": True } # setting up auto tokenizer tokenizer = AutoTokenizer.from_pretrained(model_name) tokenizer.pad_token = tokenizer.eos_token train_dataset, test_dataset = tokenize_and_split_data(training_config, tokenizer) # set up a baseline model from lamini base_model = Lamini(model_name) # gpu parallization device_count = torch.cuda.device_count() if device_count > 0: logger.debug(\"Select GPU device\") device = torch.device(\"cuda\") else: logger.debug(\"Select CPU device\") device = torch.device(\"cpu\")\n\nSetup Training to Fine-Tune, the Model\n\nFinally, we set up training argument parameters with hyperparameters. It includes learning rate, epochs, batch size, output directory, eval steps, sav, warmup steps, evaluation and logging strategy, etc., to fine-tune the custom training dataset.\n\nmax_steps = 3 # trained model name trained_model_name = f\"lamini_docs_{max_steps}_steps\" output_dir = trained_model_name training_args = TrainingArguments( # Learning rate learning_rate=1.0e-5, # Number of training epochs num_train_epochs=1, # Max steps to train for (each step is a batch of data) # Overrides num_train_epochs, if not -1 max_steps=max_steps, # Batch size for training per_device_train_batch_size=1, # Directory to save model checkpoints output_dir=output_dir, # Other arguments overwrite_output_dir=False, # Overwrite the content of the output directory disable_tqdm=False, # Disable progress bars eval_steps=120, # Number of update steps between two evaluations save_steps=120, # After # steps model is saved warmup_steps=1, # Number of warmup steps for learning rate scheduler per_device_eval_batch_size=1, # Batch size for evaluation evaluation_strategy=\"steps\", logging_strategy=\"steps\", logging_steps=1, optim=\"adafactor\", gradient_accumulation_steps = 4, gradient_checkpointing=False, # Parameters for early stopping load_best_model_at_end=True, save_total_limit=1, metric_for_best_model=\"eval_loss\", greater_is_better=False )\n\nAfter setting the training arguments, the system calculates the model’s floating-point operations per second (FLOPs) based on the input size and gradient accumulation steps. Thus giving insight into the computational load. It also assesses memory usage, estimating the model’s footprint in gigabytes. Once these calculations are complete, a Trainer initializes the base model, FLOPs, total training steps, and the prepared datasets for training and evaluation. This setup optimizes the training process and enables resource utilization monitoring, critical for efficiently handling large-scale model fine-tuning. At the end of training, the fine-tuned model is ready for deployment on the cloud to serve users as an API.\n\n# model parameters model_flops = ( base_model.floating_point_ops( { \"input_ids\": torch.zeros( (1, training_config[\"model\"][\"max_length\"]) ) } ) * training_args.gradient_accumulation_steps ) print(base_model) print(\"Memory footprint\", base_model.get_memory_footprint() / 1e9, \"GB\") print(\"Flops\", model_flops / 1e9, \"GFLOPs\") # Set up a trainer trainer = Trainer( model=base_model, model_flops=model_flops, total_steps=max_steps, args=training_args, train_dataset=train_dataset, eval_dataset=test_dataset, )\n\nConclusion\n\nIn conclusion, this article provides an in-depth guide to understanding the need to fine-tune LLMs using the Lamini platform. It gives a comprehensive overview of why we must fine-tune the model for custom datasets and business use cases and the benefits of using Lamini tools. We also saw a step-by-step guide to fine-tuning the model using a custom dataset and LLM with tools from Lamini. Let’s summarise critical takeaways from the blog.\n\nKey Takeaways\n\nLearning is needed for fine-tuning models against prompt engineering and retrieval augmented generation methods.\n\nUUtilizationof platforms like Lamini for easy-to-use hardware setup and deployment techniques for fine-tuned models to serve the user requirements\n\nWe are preparing data for the fine-tuning task and setting up a pipeline to train a base model using a wide range of hyperparameters.\n\nExplore the code behind this article on GitHub.\n\nThe media shown in this article are not owned by Analytics Vidhya and is used at the Author’s discretion.\n\nFrequently Asked Questions\n\nI specialize in data science and machine learning with hands-on experience in working on various end-to-end data science projects. I am the chapter co-lead of the Mumbai local chapter of Omdena. I am also a kaggle master and educator ambassador at streamlit with volunteers around the world."
    ],
    "# Comprehensive Analyst Report on Lamini\n\n## Company Overview\n\nLamini, founded in 2022 by Sharon Zhou and Greg Diamos, is a Palo Alto-based startup focused on developing a platform that enables enterprises to deploy generative AI applications effectively. The company has raised a total of $25 million in funding, with notable investors including Andrew Ng, Andrej Karpathy, and executives from Dropbox and Figma [(Wiggers, TechCrunch, 2024-05-02)](https://techcrunch.com/2024/05/02/dropbox-figma-ceos-back-lamini-a-startup-building-a-generative-ai-platform-for-enterprises/). Lamini's mission is to provide tailored solutions for enterprises, addressing the challenges they face in adopting generative AI technology [(Anees, Workhub, 2024-05-10)](https://workhub.ai/lamini-a-startup-transforming-enterprises-with-its-generative-ai-platform/).\n\n### Founders and Leadership\n\n- **Sharon Zhou**: Co-founder and CEO, previously a faculty member at Stanford University and a machine learning product manager at Google Cloud. Zhou holds a Ph.D. in generative AI under Andrew Ng [(Chowdhury, Business Insider, 2024-04-01)](https://www.businessinsider.com/nvidia-chips-lamini-ai-amd-jensen-huang-sharon-zhou-2024-4).\n  \n- **Greg Diamos**: Co-founder and CTO, known for his work in AI research at Baidu and as a co-founder of MLCommons, which develops benchmarks for AI models [(Deutscher et al., SiliconANGLE, 2024-05-03)](https://siliconangle.com/2024/05/03/lamini-raises-25m-ai-development-inference-platform/).\n\n## Funding and Financials\n\nLamini has successfully raised $25 million across seed and Series A funding rounds, with the latest round led by Amplify Partners [(Deutscher et al., SiliconANGLE, 2024-05-03)](https://siliconangle.com/2024/05/03/lamini-raises-25m-ai-development-inference-platform/). The funds are intended to expand the company's team and enhance its AI infrastructure, particularly focusing on deeper technical optimizations [(Wiggers, TechCrunch, 2024-05-02)](https://techcrunch.com/2024/05/02/dropbox-figma-ceos-back-lamini-a-startup-building-a-generative-ai-platform-for-enterprises/).\n\n## Product Overview: Lamini Memory Tuning\n\nLamini's flagship product, **Lamini Memory Tuning**, is a groundbreaking technique designed to enhance the factual accuracy of large language models (LLMs) while significantly reducing hallucinations. This method has reportedly achieved 95% accuracy in factual recall, compared to 50% with traditional approaches [(Lamini, 2024-05-02)](https://www.lamini.ai/blog/lamini-memory-tuning). \n\n### Key Features of Lamini Memory Tuning\n\n- **Mixture of Memory Experts (MoME)**: This innovative architecture allows the model to retrieve relevant facts dynamically during inference, optimizing for zero error on specific facts while maintaining generalization capabilities [(Razzaq, MarkTechPost, 2024-06-17)](https://www.marktechpost.com/2024/06/17/lamini-ais-memory-tuning-achieves-95-accuracy-and-reduces-hallucinations-by-90-in-large-language-models/).\n\n- **Performance Improvements**: Lamini Memory Tuning has demonstrated significant improvements in various applications, such as:\n  - **Text-to-SQL**: Achieving 95% accuracy in converting natural language questions into SQL queries.\n  - **Classification**: Attaining 100% accuracy in labeling data across thousands of documents.\n  - **Recommendations**: Reaching 88% accuracy in AI-powered product suggestions [(Lamini, 2024-05-02)](https://www.lamini.ai/blog/lamini-memory-tuning).\n\n### Use Cases and Applications\n\nLamini's technology is particularly beneficial for enterprises looking to automate processes that require high precision, such as:\n- **Data Access**: Democratizing data access through accurate SQL generation.\n- **Data Labeling**: Automating the labeling of large datasets to save time and resources.\n- **Product Recommendations**: Enhancing e-commerce strategies through accurate product suggestions [(Lamini, 2024-05-02)](https://www.lamini.ai/blog/lamini-memory-tuning).\n\n## Partnerships and Collaborations\n\nLamini has recently partnered with Meta to enhance the performance of Llama 3, focusing on improving SQL query generation and reducing hallucinations in enterprise applications [(Eva, Analytics India Magazine, 2024-06-26)](https://analyticsindiamag.com/ai-news-updates/lamini-ai-partners-with-meta-to-enhance-llamas-sql-performance/). This collaboration aims to provide enterprises with robust tools for building LLM applications.\n\n## Market Position and Competitive Landscape\n\nLamini positions itself uniquely in the generative AI market by focusing on enterprise-specific needs, contrasting with many general-purpose AI platforms. The company faces competition from established players like Google, AWS, and Microsoft, which are also targeting enterprise solutions [(Anees, Workhub, 2024-05-10)](https://workhub.ai/lamini-a-startup-transforming-enterprises-with-its-generative-ai-platform/).\n\n### Competitive Advantages\n- **Tailored Solutions**: Lamini's platform is designed specifically for enterprise workloads, addressing common challenges such as security, compliance, and integration with existing IT infrastructure [(Wiggers, TechCrunch, 2024-05-02)](https://techcrunch.com/2024/05/02/dropbox-figma-ceos-back-lamini-a-startup-building-a-generative-ai-platform-for-enterprises/).\n- **Scalability**: The platform can scale to utilize over 1,000 GPUs, allowing for flexible deployment in various environments, including on-premises and cloud setups [(Deutscher et al., SiliconANGLE, 2024-05-03)](https://siliconangle.com/2024/05/03/lamini-raises-25m-ai-development-inference-platform/).\n\n## Executive Insights\n\nSharon Zhou emphasizes the importance of maximizing ROI for enterprises adopting generative AI, stating, “The top priority of nearly every CEO, CIO, and CTO is to take advantage of generative AI within their organization with maximal ROI” [(Anees, Workhub, 2024-05-10)](https://workhub.ai/lamini-a-startup-transforming-enterprises-with-its-generative-ai-platform/). This focus on enterprise needs is a core part of Lamini's strategy.\n\n## Conclusion\n\nLamini is positioned as a promising player in the generative AI landscape, with its innovative Memory Tuning technology setting new standards for accuracy and reliability in LLM applications. With strong backing from notable investors and a clear focus on enterprise needs, Lamini is well-equipped to navigate the competitive landscape and drive significant advancements in AI deployment for businesses.\n\n---\n\nThis report provides a comprehensive overview of Lamini, its product offerings, market position, and future potential, making it a valuable resource for prospective candidates and investors."
  ],
  "lineage": {
    "run_at": "2025-03-28T22:02:27.976130",
    "git_sha": "9e00c41"
  }
}