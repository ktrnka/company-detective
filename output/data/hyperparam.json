{
  "summary_markdown": "# About hyperparam\n\nHyperparam is a company dedicated to improving the data science process by providing tools for the exploration and curation of large machine learning (ML) datasets. The company emphasizes the importance of data quality in developing advanced AI models and aims to empower teams to create optimal training sets. Hyperparam's primary product is a browser-based platform that allows users to interactively explore and curate large datasets, particularly those in the Apache Parquet format. This tool is designed to facilitate fast, free-form data exploration and interactive analysis without the limitations of traditional data tools.\n\nKey features of Hyperparam's platform include:\n\n- **Highly Scalable UI**: A user interface optimized for speed and efficiency, enabling the exploration of datasets with billions of rows directly in the browser.\n- **Model-Assisted Data Curation**: The use of machine learning models to help users identify high-quality data, streamlining the data curation process.\n- **Local-First Application**: The application operates entirely in the browser, allowing users to work with data without relying on complex services or data pipelines.\n- **Open-Source Commitment**: Hyperparam is committed to open-source principles, with its code available on GitHub to promote collaboration and transparency.\n\nHyperparam targets a diverse range of users, from large AI companies to small enterprise teams, all of whom require high-quality datasets for their machine learning models. The platform is particularly beneficial for data engineers and data scientists who need to understand and curate their training data effectively.\n\nThe company promotes a culture of innovation and collaboration, emphasizing the importance of open-source software and community engagement. Hyperparam is dedicated to making data exploration and curation more intuitive and effective, reflecting a commitment to improving the data science workflow.\n\n# Key personnel\n\nWhile specific details about the leadership team are not provided in the available content, the company's mission suggests a team with expertise in data science, machine learning, and software development. The focus on data quality and user empowerment indicates a leadership team that values innovation and collaboration.\n\n# News\n\nCurrently, there are no specific news articles provided about Hyperparam. For more information, you can visit their [website](https://hyperparam.app/) and [blog](https://blog.hyperparam.app/) for updates and insights into their developments and initiatives.",
  "target": [
    "hyperparam",
    "hyperparam",
    "hyperparam.app",
    [
      "dataset"
    ],
    true,
    true,
    null,
    [
      true,
      true
    ]
  ],
  "webpage_result": {
    "summary_markdown": "# Hyperparam Company Overview\n\n## Company History\nHyperparam is focused on enhancing the data science process by providing tools that allow for the exploration and curation of massive machine learning (ML) datasets. The company recognizes the critical role that data quality plays in building advanced AI models and aims to empower teams to create the best training sets for their models.\n\n## Services\nHyperparam offers a browser-based platform that enables users to interactively explore and curate large datasets. Key features include:\n\n- **Highly Scalable UI**: A user interface designed for speed and efficiency, allowing for the exploration of datasets with billions of rows directly in the browser.\n  \n- **Model-Assisted Data Curation**: Utilizes machine learning models to help users identify high-quality data, making the data curation process more efficient.\n\n- **Local-First Application**: The application runs entirely in the browser, allowing users to work with data without relying on complex services or data pipelines.\n\n- **Open-Source Commitment**: Hyperparam is open-source first, with its code available on GitHub, promoting collaboration and transparency.\n\n## Products\nHyperparam's primary product is its browser-based data exploration tool, which supports the Apache Parquet format, a column-oriented data structure that allows for efficient querying of large datasets. The tool is designed to facilitate:\n\n- Fast free-form data exploration.\n- Interactive analysis of large datasets without the limitations of traditional data tools.\n- Integration of machine learning models to assist in evaluating data quality.\n\n## Customers\nHyperparam targets a wide range of users, from mega-scale AI companies to small enterprise teams, all of whom require high-quality datasets for their machine learning models. The platform is particularly beneficial for data engineers and data scientists who need to understand and curate their training data effectively.\n\n## Leadership Team\nWhile specific details about the leadership team are not provided in the available content, the company's mission emphasizes a strong focus on data quality and user empowerment, suggesting a team with expertise in data science, machine learning, and software development.\n\n## Culture\nHyperparam promotes a culture of innovation and collaboration, emphasizing the importance of open-source software and community engagement. The company is dedicated to making data exploration and curation more intuitive and effective, reflecting a commitment to improving the data science workflow.\n\n## Conclusion\nHyperparam is positioned to reshape how data scientists and engineers interact with their datasets, leveraging modern web technologies and machine learning to enhance data quality and streamline the model-building process. The company's focus on user experience and data curation sets it apart in the rapidly evolving field of AI and machine learning.\n\nFor more information, visit [Hyperparam](https://hyperparam.app/) and check out their [blog](https://blog.hyperparam.app/).",
    "page_markdowns": [
      "# [Look At Your Data 👀](https://hyperparam.app/)\nAll of Wikipedia in the Hyperparam Viewer Demo:\n\nThis is a demo of what you can do with parquet files being read directly in the browser. A parquet file with all of the English Wikipedia articles is stored in S3 and rows are retrieved on demand using hyparquet.\n\nHighly scalable dataset tool\n\nThe first step in data science is to be deeply familiar with your training data.\n\nBut where do you even start? Most data tools cannot handle the scale of modern data interactively. Using modern data formats like parquet, Hyperparam can load and explore datasets with billions of rows directly in the browser.\n\nModel assisted data curation\n\nWelcome to the era of model-assisted data exploration and curation.\n\nUsing models to reflect back on their own training data can help you find the best quality data, in order to build the best quality models.\n\nLocal-first\n\nA new type of app that moves everything to the browser.\n\nHyperparam is a local-first app that can run entirely in the browser.\n\nDrop a parquet file on this page, or install the Hyperparam CLI tool:\n\nnpx hyperparam\n\nOpen-source\n\nEveryone benefits from open source software and open data standards.\n\nHyperparam is open-source first. Code available on GitHub.",
      "# [Hyperparam Blog by Hyperparam Blog](https://blog.hyperparam.app/)\nHyperparam: How Browser-Based Tools Will Re-Shape AI\n\nWhat is the key to building the most advanced AI models? Data quality.\n\nEveryone wants better AI models: smarter, cheaper, and with style. How does one achieve that? Whether you’re a mega-scale AI company, or a small enterprise team, the only real lever for making better models is to construct a better training set.\n\nHow do you build a better training set? This is a question that has always been one of the most challenging, and labor-intensive parts of the data science process.\n\nWhy is data cleaning and data understanding so time-consuming? Because current tools often miss three key capabilities: 1) should enable very fast free-form data exploration by the user, which is key to finding insights in your data, 2) use AI models to assist looking at huge volumes of data that would be impractical for a person, and 3) should be simple to run locally in the browser and not depend on complex services and data pipelines. Instead, most tools are built around Python, arguably the worst language for creating modern, compelling UIs and tools. This might seem controversial, but think about what is the most common interface for python? Jupyter Notebooks. Notebooks are great for iteration and experimentation, but they are extremely weak when it comes to interactive data exploration. If you’ve ever tried to open a parquet file (the most common format for modern ML datasets) in a notebook it looks like this:\n\nThis table is practically useless. You can’t paginate to the next set of rows. You can’t even see the entire data in a cell (which in this case is an entire github source file). So how are you supposed to get an intuitive sense of your data if you can’t even see it?\n\nCan we do better? If you want to build a highly performant user interface, there is only one choice: JavaScript. The browser is the only place for building modern UIs.\n\nThe problem is that ML datasets are massive (often multiple gigabytes of compressed text data), so it’s not obvious if it’s even possible to work with large scale datasets in the browser. However, by using modern data formats like Apache Parquet, and clever frontend engineering, it is in fact possible to work with massive datasets directly in the browser.\n\nAside: Apache Parquet files are a column-oriented data structure that contains a built-in index. This allows tools like hadoop and duckdb to efficiently query parquet datasets without having to retrieve all the data. Furthermore it allows doing these queries without a server, simply by putting the parquet files in a storage service like S3. What if you could do this same trick in the browser, and pull in just the data needed to render the current view. Hello Hyparquet.\n\nHyparquet is a new JavaScript parquet parser which can efficiently query against parquet files stored in the cloud. This enables the creation of a new type of client-side only parquet data viewer which is significantly faster than anything that could be done with a server.\n\nThe goal here is to get data engineers to look at their data 👀 Anyone who has worked with data for a model before knows that looking at your data is the key to understanding the domain you’re trying to model, and it is virtually impossible to do good data science without looking at your data. Looking at your data is the easiest way to find data and model issues, and is a constant source of ideas of how to improve them.\n\nThis is one of the core workflows in data science: build a model, see what data was correctly or incorrectly modeled, fix the data and/or the model, and repeat. This is a repeatable, teachable process! And if it can be taught to a human data scientist, why can’t it be taught to a model to assist?\n\nCan you use a model to assist with dataset curation? The challenges are two-fold: 1) How do you leverage human expertise to express what you want from the model? 2) These datasets are huge, so the cost of running a model across all the data is expensive.\n\nYou need the human in the loop to express their intent for the data. There is not just one definition of “good” versus “bad” data. What matters is the question “is this data useful for the model I’m trying to build?” This is where the UI comes in as a way to allow the user to look at the data, and use the data to express their intent.\n\nAs for the cost, we are entering a new era of LLMs where for the first time it is affordable to do dataset-scale inference in which you run an entire dataset through a model to help filter and label data. In 2023 it cost $5,000,000 USD to process 1 trillion input tokens with a sota model (gpt-4-turbo). In 2024 it cost $75,000 USD to process 1 trillion input tokens with a similar model (gpt-4o-mini). This trend will continue to make dataset-scale inference accessible to model builders. Model-based quality filtering has already been used by Meta to filter the training set for llama3 using labels generated by llama2 [1].\n\nWe’re entering a new era in which dataset-scale inference and interactive, browser-based data exploration will define how AI models are built and refined. By combining efficient data formats, high-performance JavaScript interfaces, and affordable AI-based annotations, teams can finally put data quality front and center without prohibitively high costs or clunky workflows.\n\nThe future belongs to those who seamlessly blend human expertise with AI-assisted insights—an approach that makes data cleaning faster, more intuitive, and ultimately, far more effective in powering the next generation of advanced AI models.",
      "# [Hello Hyperparam by Hyperparam Blog on 2024-01-27](https://blog.hyperparam.app/2024/01/27/hello-hyperparam)\n“Model behavior is not determined by architecture, hyperparameters, or optimizer choices. It’s determined by your dataset, nothing else. When you refer to “Lambda”, “ChatGPT”, “Bard”, or “Claude”, it’s not the model weights that you are referring to. It’s the dataset.” – jbetker @ openai\n\nMachine learning models are only as good as the data they’re trained on. Our mission is simple: create the best training sets to build the world’s best models.\n\nEveryone agrees that data quality is critical for building state-of-the-art models. But how do you build a great training dataset?\n\nAt Hyperparam we believe that it is impossible to do good data science without being intimately familiar with your training data. But where do you even start? Modern LLM depend on terabytes of unstructured text data. Most data tools cannot handle this scale of data interactively, or require sampling to show only a tiny slice of your data.\n\nIf you want to build a highly interactive tool for working with data, the browser is the only tool for building modern UIs. The question is: can the browser handle massive text datasets interactively? Yes. By leveraging modern web APIs, and with an obsessive focus on speed and architecture, we are building the world’s most scalable UI for data.\n\nBuilding a UI for machine learning data is a necessary first step, but does not solve the problem of finding good vs bad quality data within massive datasets. To find the “needle in a haystack” we use machine learning models to reflect back on their own training set. Everyone evaluates models – we evaluate data.\n\nCombine this new scalable UI with methods for evaluating ML data, and you have a powerful engine for iteratively developing the world’s best quality models.",
      "# [Hyperparam: How Browser-Based Tools Will Re-Shape AI by Hyperparam Blog on 2025-01-21](https://blog.hyperparam.app/2025/01/21/browser-based-tools-will-reshape-ai)\nWhat is the key to building the most advanced AI models? Data quality.\n\nEveryone wants better AI models: smarter, cheaper, and with style. How does one achieve that? Whether you’re a mega-scale AI company, or a small enterprise team, the only real lever for making better models is to construct a better training set.\n\nHow do you build a better training set? This is a question that has always been one of the most challenging, and labor-intensive parts of the data science process.\n\nWhy is data cleaning and data understanding so time-consuming? Because current tools often miss three key capabilities: 1) should enable very fast free-form data exploration by the user, which is key to finding insights in your data, 2) use AI models to assist looking at huge volumes of data that would be impractical for a person, and 3) should be simple to run locally in the browser and not depend on complex services and data pipelines. Instead, most tools are built around Python, arguably the worst language for creating modern, compelling UIs and tools. This might seem controversial, but think about what is the most common interface for python? Jupyter Notebooks. Notebooks are great for iteration and experimentation, but they are extremely weak when it comes to interactive data exploration. If you’ve ever tried to open a parquet file (the most common format for modern ML datasets) in a notebook it looks like this:\n\nThis table is practically useless. You can’t paginate to the next set of rows. You can’t even see the entire data in a cell (which in this case is an entire github source file). So how are you supposed to get an intuitive sense of your data if you can’t even see it?\n\nCan we do better? If you want to build a highly performant user interface, there is only one choice: JavaScript. The browser is the only place for building modern UIs.\n\nThe problem is that ML datasets are massive (often multiple gigabytes of compressed text data), so it’s not obvious if it’s even possible to work with large scale datasets in the browser. However, by using modern data formats like Apache Parquet, and clever frontend engineering, it is in fact possible to work with massive datasets directly in the browser.\n\nAside: Apache Parquet files are a column-oriented data structure that contains a built-in index. This allows tools like hadoop and duckdb to efficiently query parquet datasets without having to retrieve all the data. Furthermore it allows doing these queries without a server, simply by putting the parquet files in a storage service like S3. What if you could do this same trick in the browser, and pull in just the data needed to render the current view. Hello Hyparquet.\n\nHyparquet is a new JavaScript parquet parser which can efficiently query against parquet files stored in the cloud. This enables the creation of a new type of client-side only parquet data viewer which is significantly faster than anything that could be done with a server.\n\nThe goal here is to get data engineers to look at their data 👀 Anyone who has worked with data for a model before knows that looking at your data is the key to understanding the domain you’re trying to model, and it is virtually impossible to do good data science without looking at your data. Looking at your data is the easiest way to find data and model issues, and is a constant source of ideas of how to improve them.\n\nThis is one of the core workflows in data science: build a model, see what data was correctly or incorrectly modeled, fix the data and/or the model, and repeat. This is a repeatable, teachable process! And if it can be taught to a human data scientist, why can’t it be taught to a model to assist?\n\nCan you use a model to assist with dataset curation? The challenges are two-fold: 1) How do you leverage human expertise to express what you want from the model? 2) These datasets are huge, so the cost of running a model across all the data is expensive.\n\nYou need the human in the loop to express their intent for the data. There is not just one definition of “good” versus “bad” data. What matters is the question “is this data useful for the model I’m trying to build?” This is where the UI comes in as a way to allow the user to look at the data, and use the data to express their intent.\n\nAs for the cost, we are entering a new era of LLMs where for the first time it is affordable to do dataset-scale inference in which you run an entire dataset through a model to help filter and label data. In 2023 it cost $5,000,000 USD to process 1 trillion input tokens with a sota model (gpt-4-turbo). In 2024 it cost $75,000 USD to process 1 trillion input tokens with a similar model (gpt-4o-mini). This trend will continue to make dataset-scale inference accessible to model builders. Model-based quality filtering has already been used by Meta to filter the training set for llama3 using labels generated by llama2 [1].\n\nWe’re entering a new era in which dataset-scale inference and interactive, browser-based data exploration will define how AI models are built and refined. By combining efficient data formats, high-performance JavaScript interfaces, and affordable AI-based annotations, teams can finally put data quality front and center without prohibitively high costs or clunky workflows.\n\nThe future belongs to those who seamlessly blend human expertise with AI-assisted insights—an approach that makes data cleaning faster, more intuitive, and ultimately, far more effective in powering the next generation of advanced AI models.",
      "# [About Hyperparam](https://hyperparam.app/about)\nWhat is Hyperparam?\n\nHyperparam enables exploration and curation of massive ML datasets. By combining 1) a highly scalable browser-based UI with 2) innovative machine learning techniques for dataset evaluation, Hyperparam empowers teams to engineer the highest quality datasets.\n\nModern LLMs depend on gigabytes of carefully curated, high-quality data. To curate data effectively, engineers need to deeply understand their data. Ideally they should be looking at their data. 👀 But there are no tools that allow this kind of interactive analysis needed for modern ML.\n\nHyperparam’s mission is to use the browser – the only tool for building modern UIs – to enable engineers to curate massive datasets interactively. By leveraging modern web APIs, and with an obsessive focus on speed and architecture, Hyperparam is building the world’s most scalable UI for dataset curation.\n\nThe UI puts the expert in the driver seat, but models give the efficiency gains needed to curate entire datasets single-handedly. We use machine learning models to reflect back on their own training set and assist in finding “good” vs “bad” data. Everyone evaluates models – we evaluate data.\n\nCombine this new scalable UI with methods for evaluating ML data, and you have a powerful engine for iteratively developing the world’s best quality models.",
      "# [hyperparam on 2023-10-31](https://hyperparam.app/privacy)\nPrivacy Policy\n\nBy using this application, you agree to the collection and use of your personal information as described in this policy.\n\nData collection\n\nUsers may choose to create accounts and authenticate using the Google Sign-In service. As part of this process we collect personal information including: name and email address.\n\nAuthenticated users may choose to upload data for processing.\n\nData use\n\nWe access your personal information as necessary to provide the service, maintenance, technical support, and as required by law. We do not share your personal information with a 3rd party without your permission, unless required by law.\n\nData retention\n\nWe retain your data as long as your account is active or as necessary for our services. You can delete your data at any time.\n\nCookies\n\nThe website uses cookies to identify users. Because that's how the internet works.",
      "# [hyperparam on 2023-10-31](https://hyperparam.app/terms)\nTerms of Service\n\nBy using our service, you agree to comply with these Terms.\n\nServices\n\nWe reserve the right to modify or discontinue our services at any time.\n\nUser Conduct\n\nYou are responsible for any code or data you upload. Illegal or harmful content is prohibited.\n\nLiability\n\nWe are not liable for any data loss or other damages arising from the use of our service.\n\nTermination\n\nWe reserve the right to terminate your account for violating these Terms.\n\nChanges\n\nWe may update these Terms at any time. Continued use signifies acceptance."
    ],
    "search_results": [
      {
        "title": "Hyperparam - Look At Your Data",
        "link": "https://hyperparam.app/",
        "snippet": "A new type of app that moves everything to the browser. Hyperparam is a local-first app that can run entirely in the browser. Drop a parquet ...",
        "formattedUrl": "https://hyperparam.app/"
      },
      {
        "title": "Hyperparam Blog | The Missing UI for AI Data",
        "link": "https://blog.hyperparam.app/",
        "snippet": "Jan 21, 2025 ... Current tools often miss three key capabilities: 1) should enable very fast free-form data exploration by the user, which is key to finding insights in your ...",
        "formattedUrl": "https://blog.hyperparam.app/"
      }
    ]
  },
  "general_search_markdown": "# Official social media\n- [Hyperparam LinkedIn](https://hyperparam.app/.)  \n- [Kenny Daniel - Hyperparam LinkedIn](https://www.linkedin.com/in/kennydaniel)  \n\n# Job boards\n- [Senior Javascript Engineer at Hyperparam • Seattle | Wellfound](https://wellfound.com/jobs/3185363-senior-javascript-engineer)  \n\n# App stores\n- [Hyperparam - Look At Your Data](https://hyperparam.app/)  \n\n# Product reviews\n- No relevant product reviews found.\n\n# News articles (most recent first, grouped by event)\n- No significant news articles found.\n\n# Key employees (grouped by employee)\n- **Kenny Daniel**  \n  [Kenny Daniel - Hyperparam | LinkedIn](https://www.linkedin.com/in/kennydaniel)  \n\n# Other pages on the company website\n- [Hyperparam Blog | The Missing UI for AI Data](https://blog.hyperparam.app/) (Jan 21, 2025)  \n- [Hyperparameter tuning on Google Cloud Platform is now faster and ...](https://cloud.google.com/blog/products/gcp/hyperparameter-tuning-on-google-cloud-platform-is-now-faster-and-smarter) (Mar 14, 2018)  \n- [Vertex AI: Hyperparameter Tuning](https://codelabs.developers.google.com/vertex_hyperparameter_tuning) (May 23, 2022)  \n- [Hyperparam | Official Website](https://hyperparam.app/)  \n\n# Other\n- [Learning Diffusion using Hyperparameters](http://proceedings.mlr.press/v80/kalimeris18a/kalimeris18a.pdf)  \n- [Grouped Sequential Optimization Strategy--the Application of ...](https://arxiv.org/pdf/2503.05106) (Mar 7, 2025)  \n- [Choosing Prior Hyperparameters: With Applications To Time ...](https://cm1518.github.io/files/HP.pdf) (Mar 15, 2018)  \n- [Kubernetes For AI Hyperparameter Search Experiments | NVIDIA ...](https://developer.nvidia.com/blog/kubernetes-ai-hyperparameter-search-experiments/) (Dec 14, 2018)  \n- [Hyperparameter tuning using Bayesian optimization - PyTorch Forums](https://discuss.pytorch.org/t/hyperparameter-tuning-using-bayesian-optimization/36145) (Feb 1, 2019)  \n- [Hyperparameter tuning | Databricks Documentation](https://docs.databricks.com/aws/en/machine-learning/automl-hyperparam-tuning) (Sep 12, 2024)  \n- [Hyperparameter tuning optimization](https://docs.mlrun.org/en/latest/hyper-params.html)  \n- [Hyperparameter optimization with Dask — Dask Examples ...](https://examples.dask.org/machine-learning/hyperparam-opt.html)  \n- [An Efficient Approach for Assessing Hyperparameter Importance](https://ml.informatik.uni-freiburg.de/wp-content/uploads/papers/14-ICML-HyperparameterAssessment-longversion.pdf)  \n- [TRAINING OVER A DISTRIBUTION OF HYPERPARAM- ETERS ...](https://openreview.net/pdf?id=UZQl0rbxj6)  \n- [Amazon SageMaker Model Building Pipeline — sagemaker 2.242.0 ...](https://sagemaker.readthedocs.io/en/stable/amazon_sagemaker_model_building_pipeline.html)  \n- [python - Ray Tune scheduler hyperparam_mutations vs ...](https://stackoverflow.com/questions/74408892/ray-tune-scheduler-hyperparam-mutations-vs-param-space)  \n- [Cross validation : hyper-parameter tuning ? or model validation ...](https://stats.stackexchange.com/questions/377236/cross-validation-hyper-parameter-tuning-or-model-validation)  \n- [Bayesian Hyperparameter Optimization: Basics & Quick Tutorial](https://www.run.ai/guides/hyperparameter-tuning/bayesian-hyperparameter-optimization)  \n- [Convolutional Neural Networks Hyperparameter Tunning for ...](https://www.tandfonline.com/doi/full/10.1080/08839514.2022.2058165)  \n- [HotOS July 13 2020](https://www.usenix.org/system/files/hotcloud20_hotstorage20_slides_stoica.pdf)  \n- [Hyperparam tuning (Grid Search/Randomized) - Zindi](https://zindi.africa/discussions/24076)",
  "crunchbase_markdown": null,
  "customer_experience_result": null,
  "glassdoor_result": null,
  "news_result": null,
  "lineage": {
    "run_at": "2025-03-28T21:47:36.337640",
    "git_sha": "9e00c41"
  }
}