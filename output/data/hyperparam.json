{
  "summary_markdown": "# About hyperparam\n\nHyperparam is a company focused on providing tools for the exploration and curation of large machine learning (ML) datasets. The company offers a platform that combines a scalable browser-based user interface (UI) with advanced machine learning techniques to evaluate datasets. This platform is designed to help teams engineer high-quality datasets, which are essential for developing modern large language models (LLMs).\n\n## Company History and Mission\n\nHyperparam's mission is to enhance the understanding of training data, which is crucial for building effective AI models. The company emphasizes the importance of data quality in the development of advanced AI models and aims to provide tools that allow engineers to interactively analyze and curate their datasets. By leveraging modern web technologies, Hyperparam is building what it claims to be the world's most scalable UI for dataset curation, with a focus on speed and architecture.\n\n## Services and Products\n\n- **Browser-Based Dataset Exploration**: Hyperparam enables users to load and explore datasets with billions of rows directly in the browser, using modern data formats like Apache Parquet.\n  \n- **Model-Assisted Data Curation**: The platform uses machine learning models to help users identify high-quality data, thereby improving the model training process.\n\n- **Hyparquet**: This is a JavaScript parquet parser that allows efficient querying of parquet files stored in the cloud, providing a client-side data viewer that is faster than traditional server-based solutions.\n\n## Key Features\n\n- **Local-First Application**: The platform operates entirely in the browser, removing the need for complex services and data pipelines.\n  \n- **Interactive Data Analysis**: The UI is designed for fast, free-form data exploration, enabling users to effectively gain insights from their data.\n\n- **Cost-Effective Dataset-Scale Inference**: Hyperparam aims to make dataset-scale inference accessible to model builders by leveraging the decreasing costs of running models on large datasets.\n\n## Customers\n\nHyperparam targets a diverse range of users, from large AI companies to small enterprise teams, all of whom require high-quality training datasets to build better AI models.\n\n## Leadership Team and Culture\n\nWhile specific details about the leadership team are not provided, the culture at Hyperparam emphasizes innovation, efficiency, and a user-centric approach to data science. The company is committed to integrating human expertise with AI-assisted insights to streamline the data cleaning process.\n\n# Key Personnel\n\nThe available information does not provide specific names or roles of key personnel at Hyperparam. However, the company culture is described as innovative and user-centric, focusing on blending human expertise with AI-assisted insights.\n\n# News\n\nThere are no specific news articles provided in the summary. For more information, you can visit [Hyperparam](https://hyperparam.app/) and check out their blog for insights on data quality and AI model development [(Hyperparam Blog)](https://blog.hyperparam.app/).\n\nIn conclusion, Hyperparam is positioned as a leader in reshaping how data scientists interact with their datasets, focusing on data quality and user experience to support the development of advanced AI models.",
  "target": [
    "hyperparam",
    "hyperparam",
    "hyperparam.app",
    [
      "dataset"
    ],
    true,
    true
  ],
  "webpage_result": {
    "summary_markdown": "# Hyperparam Overview\n\nHyperparam is a cutting-edge platform designed to enable the exploration and curation of massive machine learning (ML) datasets. It combines a highly scalable browser-based user interface (UI) with innovative machine learning techniques for dataset evaluation, empowering teams to engineer high-quality datasets essential for modern large language models (LLMs).\n\n## Company History and Mission\n\nHyperparam's mission is to facilitate a deep understanding of training data, which is crucial for building effective AI models. The company recognizes that data quality is the key to developing advanced AI models and aims to provide tools that allow engineers to interactively analyze and curate their datasets. By leveraging modern web technologies, Hyperparam is building the world's most scalable UI for dataset curation, focusing on speed and architecture.\n\n## Services and Products\n\n- **Browser-Based Dataset Exploration**: Hyperparam allows users to load and explore datasets with billions of rows directly in the browser, utilizing modern data formats like Apache Parquet.\n  \n- **Model-Assisted Data Curation**: The platform employs machine learning models to assist users in identifying high-quality data, thereby enhancing the model training process.\n\n- **Hyparquet**: A JavaScript parquet parser that enables efficient querying of parquet files stored in the cloud, allowing for a client-side only data viewer that is significantly faster than server-based solutions.\n\n## Key Features\n\n- **Local-First Application**: Hyperparam operates entirely in the browser, eliminating the need for complex services and data pipelines.\n  \n- **Interactive Data Analysis**: The UI is designed to facilitate fast, free-form data exploration, enabling users to gain insights from their data effectively.\n\n- **Cost-Effective Dataset-Scale Inference**: With the decreasing costs of running models on large datasets, Hyperparam aims to make dataset-scale inference accessible to model builders.\n\n## Customers\n\nHyperparam targets a wide range of users, from mega-scale AI companies to small enterprise teams, all of whom require high-quality training datasets to build better AI models.\n\n## Leadership Team and Culture\n\nWhile specific details about the leadership team are not provided, the culture at Hyperparam emphasizes innovation, efficiency, and a user-centric approach to data science. The company is committed to blending human expertise with AI-assisted insights to streamline the data cleaning process.\n\n## Conclusion\n\nHyperparam is at the forefront of reshaping how data scientists interact with their datasets, making it easier to build high-quality training sets for advanced AI models. By focusing on data quality and user experience, Hyperparam is poised to play a significant role in the future of AI development.\n\nFor more information, visit [Hyperparam](https://hyperparam.app/) and check out their blog for insights on data quality and AI model development [(Hyperparam Blog)](https://blog.hyperparam.app/).",
    "page_markdowns": [
      "# [Look At Your Data üëÄ](https://hyperparam.app/)\nHighly scalable dataset tool\n\nThe first step in data science is to be deeply familiar with your training data.\n\nBut where do you even start? Most data tools cannot handle the scale of modern data interactively. Using modern data formats like parquet, Hyperparam can load and explore datasets with billions of rows directly in the browser.\n\nModel assisted data curation\n\nWelcome to the era of model-assisted data exploration and curation.\n\nUsing models to reflect back on their own training data can help you find the best quality data, in order to build the best quality models.\n\nLocal-first\n\nA new type of app that moves everything to the browser.\n\nHyperparam is a local-first app that can run entirely in the browser.\n\nDrop a parquet file on this page, or install the Hyperparam CLI tool:\n\nnpx hyperparam",
      "# [Hyperparam Blog by Hyperparam Blog](https://blog.hyperparam.app/)\nHyperparam: How Browser-Based Tools Will Re-Shape AI\n\nWhat is the key to building the most advanced AI models? Data quality.\n\nEveryone wants better AI models: smarter, cheaper, and with style. How does one achieve that? Whether you‚Äôre a mega-scale AI company, or a small enterprise team, the only real lever for making better models is to construct a better training set.\n\nHow do you build a better training set? This is a question that has always been one of the most challenging, and labor-intensive parts of the data science process.\n\nWhy is data cleaning and data understanding so time-consuming? Because current tools often miss three key capabilities: 1) should enable very fast free-form data exploration by the user, which is key to finding insights in your data, 2) use AI models to assist looking at huge volumes of data that would be impractical for a person, and 3) should be simple to run locally in the browser and not depend on complex services and data pipelines. Instead, most tools are built around Python, arguably the worst language for creating modern, compelling UIs and tools. This might seem controversial, but think about what is the most common interface for python? Jupyter Notebooks. Notebooks are great for iteration and experimentation, but they are extremely weak when it comes to interactive data exploration. If you‚Äôve ever tried to open a parquet file (the most common format for modern ML datasets) in a notebook it looks like this:\n\nThis table is practically useless. You can‚Äôt paginate to the next set of rows. You can‚Äôt even see the entire data in a cell (which in this case is an entire github source file). So how are you supposed to get an intuitive sense of your data if you can‚Äôt even see it?\n\nCan we do better? If you want to build a highly performant user interface, there is only one choice: JavaScript. The browser is the only place for building modern UIs.\n\nThe problem is that ML datasets are massive (often multiple gigabytes of compressed text data), so it‚Äôs not obvious if it‚Äôs even possible to work with large scale datasets in the browser. However, by using modern data formats like Apache Parquet, and clever frontend engineering, it is in fact possible to work with massive datasets directly in the browser.\n\nAside: Apache Parquet files are a column-oriented data structure that contains a built-in index. This allows tools like hadoop and duckdb to efficiently query parquet datasets without having to retrieve all the data. Furthermore it allows doing these queries without a server, simply by putting the parquet files in a storage service like S3. What if you could do this same trick in the browser, and pull in just the data needed to render the current view. Hello Hyparquet.\n\nHyparquet is a new JavaScript parquet parser which can efficiently query against parquet files stored in the cloud. This enables the creation of a new type of client-side only parquet data viewer which is significantly faster than anything that could be done with a server.\n\nThe goal here is to get data engineers to look at their data üëÄ Anyone who has worked with data for a model before knows that looking at your data is the key to understanding the domain you‚Äôre trying to model, and it is virtually impossible to do good data science without looking at your data. Looking at your data is the easiest way to find data and model issues, and is a constant source of ideas of how to improve them.\n\nThis is one of the core workflows in data science: build a model, see what data was correctly or incorrectly modeled, fix the data and/or the model, and repeat. This is a repeatable, teachable process! And if it can be taught to a human data scientist, why can‚Äôt it be taught to a model to assist?\n\nCan you use a model to assist with dataset curation? The challenges are two-fold: 1) How do you leverage human expertise to express what you want from the model? 2) These datasets are huge, so the cost of running a model across all the data is expensive.\n\nYou need the human in the loop to express their intent for the data. There is not just one definition of ‚Äúgood‚Äù versus ‚Äúbad‚Äù data. What matters is the question ‚Äúis this data useful for the model I‚Äôm trying to build?‚Äù This is where the UI comes in as a way to allow the user to look at the data, and use the data to express their intent.\n\nAs for the cost, we are entering a new era of LLMs where for the first time it is affordable to do dataset-scale inference in which you run an entire dataset through a model to help filter and label data. In 2023 it cost $5,000,000 USD to process 1 trillion input tokens with a sota model (gpt-4-turbo). In 2024 it cost $75,000 USD to process 1 trillion input tokens with a similar model (gpt-4o-mini). This trend will continue to make dataset-scale inference accessible to model builders. Model-based quality filtering has already been used by Meta to filter the training set for llama3 using labels generated by llama2 [1].\n\nWe‚Äôre entering a new era in which dataset-scale inference and interactive, browser-based data exploration will define how AI models are built and refined. By combining efficient data formats, high-performance JavaScript interfaces, and affordable AI-based annotations, teams can finally put data quality front and center without prohibitively high costs or clunky workflows.\n\nThe future belongs to those who seamlessly blend human expertise with AI-assisted insights‚Äîan approach that makes data cleaning faster, more intuitive, and ultimately, far more effective in powering the next generation of advanced AI models.",
      "# [Hyperparam Blog by Hyperparam Blog](https://blog.hyperparam.app)\nHyperparam: How Browser-Based Tools Will Re-Shape AI\n\nWhat is the key to building the most advanced AI models? Data quality.\n\nEveryone wants better AI models: smarter, cheaper, and with style. How does one achieve that? Whether you‚Äôre a mega-scale AI company, or a small enterprise team, the only real lever for making better models is to construct a better training set.\n\nHow do you build a better training set? This is a question that has always been one of the most challenging, and labor-intensive parts of the data science process.\n\nWhy is data cleaning and data understanding so time-consuming? Because current tools often miss three key capabilities: 1) should enable very fast free-form data exploration by the user, which is key to finding insights in your data, 2) use AI models to assist looking at huge volumes of data that would be impractical for a person, and 3) should be simple to run locally in the browser and not depend on complex services and data pipelines. Instead, most tools are built around Python, arguably the worst language for creating modern, compelling UIs and tools. This might seem controversial, but think about what is the most common interface for python? Jupyter Notebooks. Notebooks are great for iteration and experimentation, but they are extremely weak when it comes to interactive data exploration. If you‚Äôve ever tried to open a parquet file (the most common format for modern ML datasets) in a notebook it looks like this:\n\nThis table is practically useless. You can‚Äôt paginate to the next set of rows. You can‚Äôt even see the entire data in a cell (which in this case is an entire github source file). So how are you supposed to get an intuitive sense of your data if you can‚Äôt even see it?\n\nCan we do better? If you want to build a highly performant user interface, there is only one choice: JavaScript. The browser is the only place for building modern UIs.\n\nThe problem is that ML datasets are massive (often multiple gigabytes of compressed text data), so it‚Äôs not obvious if it‚Äôs even possible to work with large scale datasets in the browser. However, by using modern data formats like Apache Parquet, and clever frontend engineering, it is in fact possible to work with massive datasets directly in the browser.\n\nAside: Apache Parquet files are a column-oriented data structure that contains a built-in index. This allows tools like hadoop and duckdb to efficiently query parquet datasets without having to retrieve all the data. Furthermore it allows doing these queries without a server, simply by putting the parquet files in a storage service like S3. What if you could do this same trick in the browser, and pull in just the data needed to render the current view. Hello Hyparquet.\n\nHyparquet is a new JavaScript parquet parser which can efficiently query against parquet files stored in the cloud. This enables the creation of a new type of client-side only parquet data viewer which is significantly faster than anything that could be done with a server.\n\nThe goal here is to get data engineers to look at their data üëÄ Anyone who has worked with data for a model before knows that looking at your data is the key to understanding the domain you‚Äôre trying to model, and it is virtually impossible to do good data science without looking at your data. Looking at your data is the easiest way to find data and model issues, and is a constant source of ideas of how to improve them.\n\nThis is one of the core workflows in data science: build a model, see what data was correctly or incorrectly modeled, fix the data and/or the model, and repeat. This is a repeatable, teachable process! And if it can be taught to a human data scientist, why can‚Äôt it be taught to a model to assist?\n\nCan you use a model to assist with dataset curation? The challenges are two-fold: 1) How do you leverage human expertise to express what you want from the model? 2) These datasets are huge, so the cost of running a model across all the data is expensive.\n\nYou need the human in the loop to express their intent for the data. There is not just one definition of ‚Äúgood‚Äù versus ‚Äúbad‚Äù data. What matters is the question ‚Äúis this data useful for the model I‚Äôm trying to build?‚Äù This is where the UI comes in as a way to allow the user to look at the data, and use the data to express their intent.\n\nAs for the cost, we are entering a new era of LLMs where for the first time it is affordable to do dataset-scale inference in which you run an entire dataset through a model to help filter and label data. In 2023 it cost $5,000,000 USD to process 1 trillion input tokens with a sota model (gpt-4-turbo). In 2024 it cost $75,000 USD to process 1 trillion input tokens with a similar model (gpt-4o-mini). This trend will continue to make dataset-scale inference accessible to model builders. Model-based quality filtering has already been used by Meta to filter the training set for llama3 using labels generated by llama2 [1].\n\nWe‚Äôre entering a new era in which dataset-scale inference and interactive, browser-based data exploration will define how AI models are built and refined. By combining efficient data formats, high-performance JavaScript interfaces, and affordable AI-based annotations, teams can finally put data quality front and center without prohibitively high costs or clunky workflows.\n\nThe future belongs to those who seamlessly blend human expertise with AI-assisted insights‚Äîan approach that makes data cleaning faster, more intuitive, and ultimately, far more effective in powering the next generation of advanced AI models.",
      "# [Hello Hyperparam by Hyperparam Blog on 2024-01-27](https://blog.hyperparam.app/2024/01/27/hello-hyperparam/)\n‚ÄúModel behavior is not determined by architecture, hyperparameters, or optimizer choices. It‚Äôs determined by your dataset, nothing else. When you refer to ‚ÄúLambda‚Äù, ‚ÄúChatGPT‚Äù, ‚ÄúBard‚Äù, or ‚ÄúClaude‚Äù, it‚Äôs not the model weights that you are referring to. It‚Äôs the dataset.‚Äù ‚Äì jbetker @ openai\n\nMachine learning models are only as good as the data they‚Äôre trained on. Our mission is simple: create the best training sets to build the world‚Äôs best models.\n\nEveryone agrees that data quality is critical for building state-of-the-art models. But how do you build a great training dataset?\n\nAt Hyperparam we believe that it is impossible to do good data science without being intimately familiar with your training data. But where do you even start? Modern LLM depend on terabytes of unstructured text data. Most data tools cannot handle this scale of data interactively, or require sampling to show only a tiny slice of your data.\n\nIf you want to build a highly interactive tool for working with data, the browser is the only tool for building modern UIs. The question is: can the browser handle massive text datasets interactively? Yes. By leveraging modern web APIs, and with an obsessive focus on speed and architecture, we are building the world‚Äôs most scalable UI for data.\n\nBuilding a UI for machine learning data is a necessary first step, but does not solve the problem of finding good vs bad quality data within massive datasets. To find the ‚Äúneedle in a haystack‚Äù we use machine learning models to reflect back on their own training set. Everyone evaluates models ‚Äì we evaluate data.\n\nCombine this new scalable UI with methods for evaluating ML data, and you have a powerful engine for iteratively developing the world‚Äôs best quality models.",
      "# [Hyperparam: How Browser-Based Tools Will Re-Shape AI by Hyperparam Blog on 2025-01-21](https://blog.hyperparam.app/2025/01/21/browser-based-tools-will-reshape-ai/)\nWhat is the key to building the most advanced AI models? Data quality.\n\nEveryone wants better AI models: smarter, cheaper, and with style. How does one achieve that? Whether you‚Äôre a mega-scale AI company, or a small enterprise team, the only real lever for making better models is to construct a better training set.\n\nHow do you build a better training set? This is a question that has always been one of the most challenging, and labor-intensive parts of the data science process.\n\nWhy is data cleaning and data understanding so time-consuming? Because current tools often miss three key capabilities: 1) should enable very fast free-form data exploration by the user, which is key to finding insights in your data, 2) use AI models to assist looking at huge volumes of data that would be impractical for a person, and 3) should be simple to run locally in the browser and not depend on complex services and data pipelines. Instead, most tools are built around Python, arguably the worst language for creating modern, compelling UIs and tools. This might seem controversial, but think about what is the most common interface for python? Jupyter Notebooks. Notebooks are great for iteration and experimentation, but they are extremely weak when it comes to interactive data exploration. If you‚Äôve ever tried to open a parquet file (the most common format for modern ML datasets) in a notebook it looks like this:\n\nThis table is practically useless. You can‚Äôt paginate to the next set of rows. You can‚Äôt even see the entire data in a cell (which in this case is an entire github source file). So how are you supposed to get an intuitive sense of your data if you can‚Äôt even see it?\n\nCan we do better? If you want to build a highly performant user interface, there is only one choice: JavaScript. The browser is the only place for building modern UIs.\n\nThe problem is that ML datasets are massive (often multiple gigabytes of compressed text data), so it‚Äôs not obvious if it‚Äôs even possible to work with large scale datasets in the browser. However, by using modern data formats like Apache Parquet, and clever frontend engineering, it is in fact possible to work with massive datasets directly in the browser.\n\nAside: Apache Parquet files are a column-oriented data structure that contains a built-in index. This allows tools like hadoop and duckdb to efficiently query parquet datasets without having to retrieve all the data. Furthermore it allows doing these queries without a server, simply by putting the parquet files in a storage service like S3. What if you could do this same trick in the browser, and pull in just the data needed to render the current view. Hello Hyparquet.\n\nHyparquet is a new JavaScript parquet parser which can efficiently query against parquet files stored in the cloud. This enables the creation of a new type of client-side only parquet data viewer which is significantly faster than anything that could be done with a server.\n\nThe goal here is to get data engineers to look at their data üëÄ Anyone who has worked with data for a model before knows that looking at your data is the key to understanding the domain you‚Äôre trying to model, and it is virtually impossible to do good data science without looking at your data. Looking at your data is the easiest way to find data and model issues, and is a constant source of ideas of how to improve them.\n\nThis is one of the core workflows in data science: build a model, see what data was correctly or incorrectly modeled, fix the data and/or the model, and repeat. This is a repeatable, teachable process! And if it can be taught to a human data scientist, why can‚Äôt it be taught to a model to assist?\n\nCan you use a model to assist with dataset curation? The challenges are two-fold: 1) How do you leverage human expertise to express what you want from the model? 2) These datasets are huge, so the cost of running a model across all the data is expensive.\n\nYou need the human in the loop to express their intent for the data. There is not just one definition of ‚Äúgood‚Äù versus ‚Äúbad‚Äù data. What matters is the question ‚Äúis this data useful for the model I‚Äôm trying to build?‚Äù This is where the UI comes in as a way to allow the user to look at the data, and use the data to express their intent.\n\nAs for the cost, we are entering a new era of LLMs where for the first time it is affordable to do dataset-scale inference in which you run an entire dataset through a model to help filter and label data. In 2023 it cost $5,000,000 USD to process 1 trillion input tokens with a sota model (gpt-4-turbo). In 2024 it cost $75,000 USD to process 1 trillion input tokens with a similar model (gpt-4o-mini). This trend will continue to make dataset-scale inference accessible to model builders. Model-based quality filtering has already been used by Meta to filter the training set for llama3 using labels generated by llama2 [1].\n\nWe‚Äôre entering a new era in which dataset-scale inference and interactive, browser-based data exploration will define how AI models are built and refined. By combining efficient data formats, high-performance JavaScript interfaces, and affordable AI-based annotations, teams can finally put data quality front and center without prohibitively high costs or clunky workflows.\n\nThe future belongs to those who seamlessly blend human expertise with AI-assisted insights‚Äîan approach that makes data cleaning faster, more intuitive, and ultimately, far more effective in powering the next generation of advanced AI models.",
      "# [About Hyperparam](https://hyperparam.app/about)\nWhat is Hyperparam?\n\nHyperparam enables exploration and curation of massive ML datasets. By combining 1) a highly scalable browser-based UI with 2) innovative machine learning techniques for dataset evaluation, Hyperparam empowers teams to engineer the highest quality datasets.\n\nModern LLMs depend on gigabytes of carefully curated, high-quality data. To curate data effectively, engineers need to deeply understand their data. Ideally they should be looking at their data. üëÄ But there are no tools that allow this kind of interactive analysis needed for modern ML.\n\nHyperparam‚Äôs mission is to use the browser ‚Äì the only tool for building modern UIs ‚Äì to enable engineers to curate massive datasets interactively. By leveraging modern web APIs, and with an obsessive focus on speed and architecture, Hyperparam is building the world‚Äôs most scalable UI for dataset curation.\n\nThe UI puts the expert in the driver seat, but models give the efficiency gains needed to curate entire datasets single-handedly. We use machine learning models to reflect back on their own training set and assist in finding ‚Äúgood‚Äù vs ‚Äúbad‚Äù data. Everyone evaluates models ‚Äì we evaluate data.\n\nCombine this new scalable UI with methods for evaluating ML data, and you have a powerful engine for iteratively developing the world‚Äôs best quality models.",
      "# [hyperparam on 2023-10-31](https://hyperparam.app/privacy)\nPrivacy Policy\n\nBy using this application, you agree to the collection and use of your personal information as described in this policy.\n\nData collection\n\nUsers may choose to create accounts and authenticate using the Google Sign-In service. As part of this process we collect personal information including: name and email address.\n\nAuthenticated users may choose to upload data for processing.\n\nData use\n\nWe access your personal information as necessary to provide the service, maintenance, technical support, and as required by law. We do not share your personal information with a 3rd party without your permission, unless required by law.\n\nData retention\n\nWe retain your data as long as your account is active or as necessary for our services. You can delete your data at any time.\n\nCookies\n\nThe website uses cookies to identify users. Because that's how the internet works.",
      "# [hyperparam on 2023-10-31](https://hyperparam.app/terms)\nTerms of Service\n\nBy using our service, you agree to comply with these Terms.\n\nServices\n\nWe reserve the right to modify or discontinue our services at any time.\n\nUser Conduct\n\nYou are responsible for any code or data you upload. Illegal or harmful content is prohibited.\n\nLiability\n\nWe are not liable for any data loss or other damages arising from the use of our service.\n\nTermination\n\nWe reserve the right to terminate your account for violating these Terms.\n\nChanges\n\nWe may update these Terms at any time. Continued use signifies acceptance."
    ],
    "search_results": [
      {
        "title": "Hyperparam - Look At Your Data",
        "link": "https://hyperparam.app/",
        "snippet": "hyperparam is the missing UI for machine learning.",
        "formattedUrl": "https://hyperparam.app/"
      },
      {
        "title": "Hyperparam Blog | The Missing UI for AI Data",
        "link": "https://blog.hyperparam.app/",
        "snippet": "7 days ago ... What is the key to building the most advanced AI models? Data quality. Everyone wants better AI models: smarter, cheaper, and with style. How¬†...",
        "formattedUrl": "https://blog.hyperparam.app/"
      }
    ]
  },
  "general_search_markdown": "# Official social media\n- [Hyperparam on LinkedIn](https://www.linkedin.com/company/hyperparam)  \n- [Hyperparam Blog](https://blog.hyperparam.app)  \n\n# Job boards\n- [Senior Javascript Engineer at Hyperparam ‚Ä¢ Seattle | Wellfound](https://wellfound.com/jobs/3185363-senior-javascript-engineer)  \n\n# App stores\n- [Hyperparam - Look At Your Data](https://hyperparam.app/)  \n\n# Product reviews\n- No relevant product reviews found.\n\n# News articles (most recent first, grouped by event)\n- No significant news articles found.\n\n# Key employees (grouped by employee)\n- **Kenny Daniel**  \n  [Kenny Daniel - Hyperparam | LinkedIn](https://www.linkedin.com/in/kennydaniel)  \n\n# Other pages on the company website\n- [Hyperparam Blog | The Missing UI for AI Data](https://blog.hyperparam.app/)  \n\n# Other\n- [Learning Diffusion using Hyperparameters](http://proceedings.mlr.press/v80/kalimeris18a/kalimeris18a.pdf)  \n- [A Hyperparameter Optimization Toolkit for Neural Machine ...](https://aclanthology.org/2023.acl-demo.15.pdf)  \n- [Improving Unsupervised Video Object Segmentation with Motion ...](https://arxiv.org/abs/2212.08816)  \n- [Hyperparameter tuning on Google Cloud Platform is now faster and ...](https://cloud.google.com/blog/products/gcp/hyperparameter-tuning-on-google-cloud-platform-is-now-faster-and-smarter)  \n- [Vertex AI: Hyperparameter Tuning](https://codelabs.developers.google.com/vertex_hyperparameter_tuning)  \n- [Hyperparameter tuning using Bayesian optimization - PyTorch Forums](https://discuss.pytorch.org/t/hyperparameter-tuning-using-bayesian-optimization/36145)  \n- [Hyperopt best practices and troubleshooting | Databricks on AWS](https://docs.databricks.com/en/machine-learning/automl-hyperparam-tuning/hyperopt-best-practices.html)  \n- [Hyperparameter optimization with Dask ‚Äî Dask Examples ...](https://examples.dask.org/machine-learning/hyperparam-opt.html)  \n- [KerasTuner](https://keras.io/keras_tuner/)  \n- [Hyperparameter Tuning for Machine and Deep Learning with R](https://library.oapen.org/bitstream/id/240dc394-c1a4-414e-8465-d9dc8824c7e5/978-981-19-5170-1.pdf)  \n- [An Efficient Approach for Assessing Hyperparameter Importance](https://ml.informatik.uni-freiburg.de/wp-content/uploads/papers/14-ICML-HyperparameterAssessment-longversion.pdf)  \n- [Meta Hyperparameter Optimization with Adversarial Proxy Subsets ...](https://mn.cs.tsinghua.edu.cn/xinwang/PDF/papers/2021_Meta%20Hyperparameter%20Optimization%20with%20Adversarial%20Proxy%20Subsets%20Sampling.pdf)  \n- [Privacy Policy ‚Äì PostgresML](https://postgresml.org/privacy)  \n- [MLflow Hyperparameter Tuning Guide ‚Äî Restack](https://www.restack.io/docs/mlflow-knowledge-mlflow-hyperparameter-tuning)  \n- [Bayesian Hyperparameter Optimization: Basics & Quick Tutorial](https://www.run.ai/guides/hyperparameter-tuning/bayesian-hyperparameter-optimization)  \n- [Convolutional Neural Networks Hyperparameter Tunning for ...](https://www.tandfonline.com/doi/full/10.1080/08839514.2022.2058165)",
  "crunchbase_markdown": null,
  "customer_experience_result": null,
  "glassdoor_result": null,
  "news_result": null,
  "lineage": {
    "run_at": "2025-01-27T20:37:32.331813",
    "git_sha": "48bf84b"
  }
}