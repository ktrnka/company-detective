{
  "summary_markdown": "# About Pinecone\n\nPinecone is a company that specializes in providing vector databases, which are essential for efficient similarity searches in AI applications. The company was founded in 2019 by Edo Liberty, who previously held research director positions at AWS and Yahoo! [(Crunchbase, 2025)](https://www.crunchbase.com/organization/pinecone). Pinecone's primary product is a vector database designed to store and query high-dimensional vectors, facilitating semantic search and retrieval. This product is particularly useful for applications such as recommendation systems, spam detection, and anomaly detection [(Pinecone, 2024)](https://www.withorb.com/blog/pinecone-pricing).\n\nPinecone operates on a B2B model, serving a diverse range of customers across various industries. Notable clients include 1up, Hyperleap, DISCO, Chipper Cash, and InpharmD, who have leveraged Pinecone's technology to enhance their operations in areas like response generation, click-through rates, legal technology, fraud detection, and healthcare [(Pinecone, 2024)](https://www.pinecone.io/).\n\nThe company generates revenue by offering its vector database as a cloud-native solution, eliminating the need for users to manage infrastructure. This approach is advantageous for businesses seeking to integrate AI capabilities without the overhead of traditional database management [(Pinecone, 2024)](https://www.withorb.com/blog/pinecone-pricing). Pinecone has raised a total of $138 million in funding, with significant investments in 2021, 2022, and 2023 [(Crunchbase, 2025)](https://www.crunchbase.com/organization/pinecone).\n\nPinecone's products are distributed through cloud platforms, allowing for seamless integration with existing AI applications. The company has recently introduced Pinecone Local, an in-memory database emulator, and enhanced security features such as customer-managed encryption keys and private endpoints [(Pinecone, 2024)](https://docs.pinecone.io/release-notes/2024).\n\n# Key Personnel\n\nEdo Liberty is the founder and leader of Pinecone. His background includes significant roles in research at AWS and Yahoo!, which have informed his vision of democratizing access to AI technology. The leadership team at Pinecone is composed of experienced professionals from various tech backgrounds, focusing on innovation and customer success [(Pinecone, 2024)](https://www.pinecone.io/).\n\n# News\n\n## Recent Developments\n\n### Product Launches and Features\n\n- **Pinecone Local**: Launched as an in-memory database emulator available as a Docker image, allowing developers to build and test applications locally [(Pinecone, 2024)](https://docs.pinecone.io/release-notes/2024).\n- **Enhanced Security Features**: Introduction of customer-managed encryption keys and private endpoints, with audit logs in early access [(Pinecone, 2024)](https://docs.pinecone.io/release-notes/2024).\n- **Pinecone Assistant Updates**: Updates include the ability to retrieve context snippets and structured data files [(Pinecone, 2024)](https://docs.pinecone.io/release-notes/2024).\n- **New SDK Releases**: Major updates for SDKs in Python, Node.js, Go, and Java, enhancing reranking and query capabilities [(Pinecone, 2024)](https://docs.pinecone.io/release-notes/2024).\n\n### Research and Development\n\nPinecone is developing \"Luna,\" an LLM designed to eliminate hallucinations in AI responses using a novel training technique called information-free training [(Pinecone, 2024)](https://www.pinecone.io/blog/hallucination-free-llm/).\n\n## Financial Information\n\nPinecone remains a private company, with its stock available through private markets for accredited investors. The company is actively seeking investment opportunities as it prepares for potential future growth [(Nasdaq Private Market, 2024)](https://www.nasdaqprivatemarket.com/company/pinecone/).\n\n## Market Position and Competitors\n\nPinecone competes with other vector database providers like Weaviate and DataStax Astra DB. Despite the competitive landscape, Pinecone's focus on seamless AI integration and robust features positions it favorably in the market [(Y Combinator, 2024)](https://news.ycombinator.com/item?id=42315364).\n\n## User Feedback and Community Sentiment\n\nFeedback on platforms like Reddit shows a polarized view of Pinecone's offerings. While praised for its semantic search capabilities, some users express concerns about setup complexity and scaling costs [(Reddit, 2024)](https://community.pinecone.io/t/401-unauthorized-exception-calling-the-api/5096).\n\n# Conclusion\n\nPinecone is a prominent player in the vector database market, offering innovative solutions for AI applications. With recent product launches, enhanced security features, and ongoing research, the company is well-positioned for future growth. However, potential investors and candidates should consider the competitive landscape and user feedback when evaluating Pinecone's long-term viability and market strategy. For more detailed information, please refer to the original sources cited throughout this report.",
  "target": [
    "Pinecone",
    "Pinecone",
    "pinecone.io",
    null,
    false,
    false,
    null,
    [
      false,
      false
    ]
  ],
  "webpage_result": {
    "summary_markdown": "# Pinecone Company Overview\n\n## Company History\nPinecone was founded in 2019 by Edo Liberty, who previously served as a research director at AWS and Yahoo!. Recognizing the potential of combining AI models with vector search to enhance applications like spam detection and recommendation systems, he identified a gap in the market for a packaged solution. This led to the creation of Pinecone, aimed at providing the necessary infrastructure for building and running advanced AI applications, making it accessible to engineering teams of all sizes and expertise levels.\n\n## Services and Products\nPinecone offers a vector database designed for storing and querying high-dimensional vectors, enabling efficient semantic search and retrieval. Key features include:\n\n- **Embeddings**: Users can choose from hosted models or bring their own vectors.\n- **Filters**: Retrieve vectors that match specific metadata filters.\n- **Real-time Indexing**: Dynamically index upserted and updated vectors for fresh reads.\n- **Full-text Search**: Exact keyword matches with sparse indexes.\n- **Rerankers**: Enhance precision in search results.\n- **Namespaces**: Create data partitions for tenant isolation.\n- **Security**: Data encryption at rest and in transit, private networking, and customer-managed encryption keys.\n\nPinecone also provides a dedicated plan that allows users to deploy a private Pinecone region for enhanced security and control.\n\n## Customers\nPinecone serves a diverse range of customers, including:\n\n- **1up**: Achieved 10x faster response generation for RFPs.\n- **Hyperleap**: Increased click-through rates by 50% for job seekers.\n- **DISCO**: Revolutionized legal technology with Pinecone.\n- **Chipper Cash**: Enhanced real-time fraud detection.\n- **InpharmD**: Redefined evidence-based healthcare.\n\nCustomer testimonials highlight Pinecone's scalability, ease of use, and commitment to success.\n\n## Leadership Team\nEdo Liberty, the founder, leads the company with a vision to democratize access to AI technology. The leadership team is composed of experienced professionals from various tech backgrounds, focusing on innovation and customer success.\n\n## Culture\nPinecone fosters a community-oriented culture, engaging with engineers and data scientists through forums and community events. The company emphasizes collaboration and support, aiming to empower developers in the AI and machine learning space.\n\n## Partners\nPinecone partners with organizations in AI/ML, search, and cloud infrastructure to enhance the development of AI applications. The Pinecone Partner Program facilitates integration and collaboration with technology partners.\n\n## Community Engagement\nPinecone encourages community engagement through a developer forum and regular updates on research and product developments. This platform allows users to connect, share experiences, and seek support.\n\nFor more information, visit [Pinecone's official website](https://www.pinecone.io/).",
    "page_markdowns": [
      "# [The vector database to build knowledgeable AI](https://www.pinecone.io/)\nTrusted in production\n\nThe world's most innovative companies are already in production with Pinecone.\n\nEmbeddings\n\nChoose from our leading hosted models or bring your own vectors.\n\nFilters\n\nRetrieve only the vectors that match your metadata filters.\n\nReal-time indexing\n\nUpserted and updated vectors are dynamically indexed in real-time to ensure fresh reads.\n\nFull-text search\n\nGet an exact keyword match with sparse indexes when semantic search isn't enough.\n\nRerankers\n\nAdd an extra layer of precision with rerankers to boost the most relevant matches.\n\nNamespaces\n\nCreate partitions of your data with namespaces to ensure tenant isolation.\n\nWorks where you do\n\nUse Pinecone with your favorite cloud provider, data sources, models, frameworks, and more.\n\nSecure\n\nWith encryption at rest and in transit, hierarchical encryption keys, private networking, and more, your data is secure. Contact us to deploy a privately managed Pinecone region within your cloud.\n\nReliable\n\nPowering mission-critical applications of all sizes, with uptime SLAs, support SLAs, and observability.",
      "# [Pinecone Status](https://status.pinecone.io/)\nSubscribe to updates for [GCP-Starter] Increase in write operations errors and an increase in freshness lag. via email and/or text message. You'll receive email notifications when incidents are updated, and text message notifications whenever Pinecone creates or resolves an incident.",
      "# [Incident History](https://status.pinecone.io/history)\n",
      "# [Pinecone](https://www.pinecone.io/contact/)\nContact\n\nContact our team\n\nWe’d love to hear from you. Send us your questions about Pinecone and we’ll schedule a time to learn and share more with you.",
      "# [Pinecone](https://www.pinecone.io/company/)\nPinecone was founded in 2019 by Edo Liberty. As a research director at AWS and at Yahoo! before that, Edo saw the tremendous power of combining AI models and vector search to dramatically improve applications such as spam detectors and recommendation systems.\n\nWhile he was working on custom vector search systems at enormous scales, he assumed there was already a packaged solution out there for everyone else who didn’t have the same engineering and data-science resources available. To his surprise, there wasn’t. Thus Pinecone and the vector database category of solutions was born.\n\nPinecone was created to provide the critical storage and retrieval infrastructure needed for building and running state-of-the-art AI applications. The founding principle was to make the solution accessible to engineering teams of all sizes and levels of AI expertise, which led to the fully managed service and ease of use that Pinecone is known for today.",
      "# [Trust and Security](https://www.pinecone.io/security/)\nEncryption\n\nSecure your data at rest and in transit with enterprise-grade protection.\n\nPrivate endpoints\n\nSecurely connect to Pinecone without exposing traffic to the public internet.\n\nCustomer Managed Encryption Keys\n\nEncrypt data using your own cloud provider KMS for enhanced control.\n\nAPI key roles\n\nGranular access control that grants each application only the permissions it needs.\n\nUser RBAC\n\nAssign roles and permissions to ensure secure, role-based access to Pinecone.\n\nComing Soon\n\nMFA\n\nAdd an extra layer of security with two-step verification for your Pinecone account.",
      "# [Customers](https://www.pinecone.io/customers/)\nRAG\n\n1up achieves 10x faster response generation for RFPs and compliance questionnaires with Pinecone\n\nAgents\n\n“No other vector database matches Pinecone's scalability and production readiness.\"\n\nBryan McCann\n\nCTO & Co-Founder\n\nSearch, Recommendations\n\nHyperleap increased click-through rates by 50% and provided job seekers customized matches with Pinecone for their AI-powered job board\n\nSearch\n\n“We wanted sub-second vector search across millions of alerts, and we didn’t want to have to worry about database architecture or maintenance.”\n\nPeter Silberman\n\nChief Technology Officer\n\nAgents\n\n“Pinecone’s scalable, serverless architecture is crucial for powering AI innovation and delighting customers.”\n\nLuis Morales\n\nVP of Engineering\n\nRecommendations\n\n“Our choice to work with Pinecone wasn’t just based on technology; it was rooted in their commitment to our success. They listened, understood, and delivered beyond our expectations.”\n\nJacob Eckel\n\nVP, R&D Division Manager\n\nAgents\n\n“Pinecone aligns with our vision to democratize data accessibility for all engineers, and we're excited to uncover more potential with its new serverless architecture.”\n\nPeter Pezaris\n\nChief Strategy and Design Officer\n\nRAG\n\nDISCO Revolutionizes Legal Technology with Pinecone\n\nAgents\n\nShortwave Bets Big on AI with Pinecone\n\nSearch, RAG\n\n\"Pinecone Serverless on multiple regions and cloud providers, especially GCP, has been incredible for us.\"\n\nNick Gomez\n\nFounder and CEO\n\nSearch\n\nChipper Cash thwarts fraudsters in real-time with Pinecone\n\nAgents\n\n“Pinecone was incredibly easy to use, allowing us to quickly achieve success. We chose it to fulfill the promises we made to our clients with the products we were building.”\n\nPhilipp Grothaus\n\nCTO\n\nRAG\n\nInpharmD Redefines Evidence-Based Healthcare with Pinecone\n\nRecommendations\n\n“Thanks to Pinecone’s performance and scalability we can provide accurate, reliable, and real-time threat detection to our customers.”\n\nOded Kalev\n\nR&D Group Lead\n\nSearch\n\n“We got Pinecone up and running in production in just one day. It's so efficient and easy to use that it lets us focus on building and improving our AI features without the usual vector database headaches.”\n\nAlex Danilowicz\n\nCo-Founder\n\nRAG\n\nSixfold's Transformation of Insurance Underwriting with Pinecone\n\nRAG\n\n\"Pinecone was a no-brainer for us. We needed to move quickly, and Pinecone was the leader in the vector database space\"\n\nJohn Wang\n\nCo-founder and CTO\n\nSearch\n\nGlasp achieves 5X cost savings in knowledge access for millions of users with Pinecone\n\nRAG, Recommendations\n\n“Pinecone has transformed our customer service operations, enabling us to achieve unprecedented levels of efficiency and customer satisfaction”\n\nManish Pandya\n\nSVP of Digital Transformation\n\nRAG\n\nMyAsk AI uses Pinecone to help customer support teams save hours of reading, searching, and waiting with their own AI assistants.\n\nRAG, Search\n\nWe evaluated our semantic search system’s recall performance both with and without the Pinecone reranker and saw substantial improvements. Using a reranker has from that point onwards been a no-brainer. By centralizing our vector database interactions to one system, it will be easier for other teams to adopt this technology too.\n\nViktor Karlsson\n\nSoftware Engineer",
      "# [Community](https://www.pinecone.io/community/)\nCommunity Center\n\nEngage with the Pinecone Community\n\nThe Pinecone Community is for engineers, data scientists, and anyone else involved in the new frontier of (vector) search.\n\nDeveloper forum\n\nJoin a community of 1000+ developers for easy access to support and engineers who build with Pinecone daily.\n\nFollow us around\n\nKeep up to date with the latest updates and research from Pinecone",
      "# [Pinecone](https://www.pinecone.io/pricing/)\nThe Dedicated plan offers Bring-your-own-cloud (BYOC), allowing you to deploy a private Pinecone region, giving you the security and control of a self-hosted solution while retaining the seamless experience of our fully managed SaaS product. Contact us for more information.",
      "# [Partners](https://www.pinecone.io/partners/)\nPinecone Partner Program\n\nPinecone partners with exceptional organizations in AI/ML, search, and cloud infrastructure to help developers build remarkably better GenAI applications.\n\nTechnology Partners\n\nConnect and integrate with Pinecone. The Pinecone Partner Program is designed to streamline the experience for developers who are building knowledgeable AI applications.\n\nIntegration Attribution\n\nGain visibility on the impact of the integration through an updated SDK\n\nPinecone Verified\n\nEnsure joint value to developers by verifying your integration with Pinecone\n\nJoin us\n\nBecome a Partner\n\nWe’d love to hear from you. Send us your questions about Pinecone and we’ll schedule a time to learn and share more with you.",
      "# [Pinecone Console](https://app.pinecone.io/organizations)\n",
      "# [Model Gallery](https://docs.pinecone.io/models/overview)\nPinecone integrations enable you to build and deploy AI applications faster and more efficiently. Integrate Pinecone with your favorite frameworks, data sources, and infrastructure providers.",
      "# [Pinecone Docs](https://docs.pinecone.io/integrations/voyage)\n0.59: Your content Some of our services give you the opportunity to make your content publicly available for example, you might post a product or restaurant review that you wrote, or you might upload a blog post that you created. See the Permission to use your content section for more about your rights in your content, and how your content is used in our services See the Removing your content section to learn why and how we might remove user-generated content from our services If you think that someone is infringing your intellectual property rights, you can send us notice of the infringement and well take appropriate action. For example, we suspend or close the Google Accounts of repeat copyright infringers as described in our Copyright Help Centre. 0.47: Google content Some of our services include content that belongs to Google for example, many of the visual illustrations that you see in Google Maps. You may use Googles content as allowed by these terms and any service-specific additional terms, but we retain any intellectual property rights that we have in our content. Dont remove, obscure or alter any of our branding, logos or legal notices. If you want to use our branding or logos, please see the Google Brand Permissions page. Other content Finally, some of our services gives you access to content that belongs to other people or organisations for example, a store owners description of their own business, or a newspaper article displayed in Google News. You may not use this content without that person or organisations permission, or as otherwise allowed by law. The views expressed in the content of other people or organisations are their own, and dont necessarily reflect Googles views. 0.45: Taking action in case of problems Before taking action as described below, well provide you with advance notice when reasonably possible, describe the reason for our action and give you an opportunity to fix the problem, unless we reasonably believe that doing so would: cause harm or liability to a user, third party or Google violate the law or a legal enforcement authoritys order compromise an investigation compromise the operation, integrity or security of our services Removing your content If we reasonably believe that any of your content (1) breaches these terms, service-specific additional terms or policies, (2) violates applicable law, or (3) could harm our users, third parties or Google, then we reserve the right to take down some or all of that content in accordance with applicable law. Examples include child pornography, content that facilitates human trafficking or harassment, and content that infringes someone elses intellectual property rights. Suspending or terminating your access to Google services Google reserves the right to suspend or terminate your access to the services or delete your Google Account if any of these things happen: you materially or repeatedly breach these terms, service-specific additional terms or policies were required to do so to comply with a legal requirement or a court order we reasonably believe that your conduct causes harm or liability to a user, third party or Google for example, by hacking, phishing, harassing, spamming, misleading others or scraping content that doesnt belong to you If you believe that your Google Account has been suspended or terminated in error, you can appeal. Of course, youre always free to stop using our services at any time. If you do stop using a service, wed appreciate knowing why so that we can continue improving our services.",
      "# [Pinecone Docs](https://docs.pinecone.io/integrations/openai)\nOpenAI’s large language models (LLMs) enhance semantic search or “long-term memory” for LLMs. This combo utilizes LLMs’ embedding and completion (or generation) endpoints alongside Pinecone’s vector search capabilities for nuanced information retrieval.\n\nBy integrating OpenAI’s LLMs with Pinecone, you can combine deep learning capabilities for embedding generation with efficient vector storage and retrieval. This approach surpasses traditional keyword-based search, offering contextually-aware, precise results.\n\nSetup guide\n\nView source\n\nOpen in Colab\n\nThis guide covers the integration of OpenAI’s Large Language Models (LLMs) with Pinecone (referred to as the OP stack), enhancing semantic search or ‘long-term memory’ for LLMs. This combo utilizes LLMs’ embedding and completion (or generation) endpoints alongside Pinecone’s vector search capabilities for nuanced information retrieval.\n\nLLMs like OpenAI’s text-embedding-ada-002 generate vector embeddings, i.e., numerical representations of text semantics. These embeddings facilitate semantic-based rather than literal textual matches. Additionally, LLMs like gpt-4 or gpt-3.5-turbo can predict text completions based on information provided from these contexts.\n\nPinecone is a vector database designed for storing and querying high-dimensional vectors. It provides fast, efficient semantic search over these vector embeddings.\n\nBy integrating OpenAI’s LLMs with Pinecone, we combine deep learning capabilities for embedding generation with efficient vector storage and retrieval. This approach surpasses traditional keyword-based search, offering contextually-aware, precise results.\n\nThere are many ways of integrating these two tools and we have several guides focusing on specific use-cases. If you already know what you’d like to do you can jump to these specific materials:\n\nChatGPT Plugins Walkthrough\n\nAsk Lex ChatGPT Plugin\n\nGenerative Question-Answering\n\nRetrieval Augmentation using LangChain\n\nIntroduction to Embeddings\n\nAt the core of the OP stack we have embeddings which are supported via the OpenAI Embedding API. We index those embeddings in the Pinecone vector database for fast and scalable retrieval augmentation of our LLMs or other information retrieval use-cases.\n\nThis example demonstrates the core OP stack. It is the simplest workflow and is present in each of the other workflows, but is not the only way to use the stack. Please see the links above for more advanced usage.\n\nThe OP stack is built for semantic search, question-answering, threat-detection, and other applications that rely on language models and a large corpus of text data.\n\nThe basic workflow looks like this:\n\nEmbed and index\n\nUse the OpenAI Embedding API to generate vector embeddings of your documents (or any text data).\n\nUpload those vector embeddings into Pinecone, which can store and index millions/billions of these vector embeddings, and search through them at ultra-low latencies.\n\nSearch\n\nPass your query text or document through the OpenAI Embedding API again.\n\nTake the resulting vector embedding and send it as a query to Pinecone.\n\nGet back semantically similar documents, even if they don’t share any keywords with the query.\n\nLet’s get started…\n\nEnvironment Setup\n\nWe start by installing the OpenAI and Pinecone clients, we will also need HuggingFace Datasets for downloading the TREC dataset that we will use in this guide.\n\nCreating Embeddings\n\nTo create embeddings we must first initialize our connection to OpenAI Embeddings, we sign up for an API key at OpenAI.\n\nWe can now create embeddings with the OpenAI v3 small embedding model like so:\n\nIn res we should find a JSON-like object containing two 1536-dimensional embeddings, these are the vector representations of the two inputs provided above. To access the embeddings directly we can write:\n\nWe will use this logic when creating our embeddings for the Text REtrieval Conference (TREC) question classification dataset later.\n\nInitializing a Pinecone Index\n\nNext, we initialize an index to store the vector embeddings. For this we need a Pinecone API key, sign up for one here.\n\nPopulating the Index\n\nWith both OpenAI and Pinecone connections initialized, we can move onto populating the index. For this, we need the TREC dataset.\n\nThen we create a vector embedding for each question using OpenAI (as demonstrated earlier), and upsert the ID, vector embedding, and original text for each phrase to Pinecone.\n\nQuerying\n\nWith our data indexed, we’re now ready to move onto performing searches. This follows a similar process to indexing. We start with a text query, that we would like to use to find similar sentences. As before we encode this with OpenAI’s text similarity Babbage model to create a query vector xq. We then use xq to query the Pinecone index.\n\nNow we query.\n\nThe response from Pinecone includes our original text in the metadata field, let’s print out the top_k most similar questions and their respective similarity scores.\n\nLooks good, let’s make it harder and replace “depression” with the incorrect term “recession”.\n\nLet’s perform one final search using the definition of depression rather than the word or related words.\n\nIt’s clear from this example that the semantic search pipeline is clearly able to identify the meaning between each of our queries. Using these embeddings with Pinecone allows us to return the most semantically similar questions from the already indexed TREC dataset.\n\nOnce we’re finished with the index we delete it to save resources."
    ],
    "search_results": [
      {
        "title": "Pinecone Trust and Security Center | Powered by SafeBase",
        "link": "https://security.pinecone.io/",
        "snippet": "ISO ... We're thrilled to announce that Pinecone has achieved ISO 27001:2022 certification! This milestone underscores our unwavering commitment to providing the ...",
        "formattedUrl": "https://security.pinecone.io/"
      },
      {
        "title": "Pinecone: The vector database to build knowledgeable AI",
        "link": "https://www.pinecone.io/",
        "snippet": "Trusted in production. The world's most innovative companies are already in production with Pinecone. ... Gong achieves efficient vector searches, empowering ...",
        "formattedUrl": "https://www.pinecone.io/"
      },
      {
        "title": "Pinecone Status",
        "link": "https://status.pinecone.io/",
        "snippet": "Past Incidents ... No incidents reported today. ... No incidents reported. ... No incidents reported. ... Resolved - This incident has been resolved. ... Monitoring - A ...",
        "formattedUrl": "https://status.pinecone.io/"
      },
      {
        "title": "Incident History - Pinecone Status",
        "link": "https://status.pinecone.io/history",
        "snippet": "March 2025 · [Serverless][AWS][us-east-1] Newly created indexes failing to initialize. This incident has been resolved. · API requests to records (Pinecone ...",
        "formattedUrl": "https://status.pinecone.io/history"
      },
      {
        "title": "Contact | Pinecone",
        "link": "https://www.pinecone.io/contact/",
        "snippet": "Contact our team. We'd love to hear from you. Send us your questions about Pinecone and we'll schedule a time to learn and share more with you. ... By submitting ...",
        "formattedUrl": "https://www.pinecone.io/contact/"
      },
      {
        "title": "Company | Pinecone",
        "link": "https://www.pinecone.io/company/",
        "snippet": "Our origin story. Pinecone was founded in 2019 by Edo Liberty. As a research director at AWS and at Yahoo! before that, Edo saw the tremendous power of ...",
        "formattedUrl": "https://www.pinecone.io/company/"
      },
      {
        "title": "Trust and Security | Pinecone",
        "link": "https://www.pinecone.io/security/",
        "snippet": "Private endpoints. Securely connect to Pinecone without exposing traffic to the public internet. Customer Managed Encryption Keys. Encrypt data using your own ...",
        "formattedUrl": "https://www.pinecone.io/security/"
      },
      {
        "title": "Customers | Pinecone",
        "link": "https://www.pinecone.io/customers/",
        "snippet": "Pinecone supercharges AI for the world's leading companies. See how thousands of Pinecone customers are building easily scalable, high performance AI-powered ...",
        "formattedUrl": "https://www.pinecone.io/customers/"
      },
      {
        "title": "Community | Pinecone",
        "link": "https://www.pinecone.io/community/",
        "snippet": "The Pinecone Community is for engineers, data scientists, and anyone else involved in the new frontier of (vector) search. View EventsJoin the discussion.",
        "formattedUrl": "https://www.pinecone.io/community/"
      },
      {
        "title": "Pricing | Pinecone",
        "link": "https://www.pinecone.io/pricing/",
        "snippet": "Dedicated ... The Dedicated plan offers Bring-your-own-cloud (BYOC), allowing you to deploy a private Pinecone region, giving you the security and control of a ...",
        "formattedUrl": "https://www.pinecone.io/pricing/"
      },
      {
        "title": "Partners | Pinecone",
        "link": "https://www.pinecone.io/partners/",
        "snippet": "Pinecone Partner Program. Pinecone partners with exceptional organizations in AI/ML, search, and cloud infrastructure to help developers build remarkably better ...",
        "formattedUrl": "https://www.pinecone.io/partners/"
      },
      {
        "title": "Pinecone Database | Pinecone",
        "link": "https://www.pinecone.io/product/",
        "snippet": "Reliable. With a 99.95% uptime SLA, Pinecone is trusted by leading startups and enterprises for their production AI applications. Secure. Data is encrypted at ...",
        "formattedUrl": "https://www.pinecone.io/product/"
      },
      {
        "title": "Cookies | Pinecone",
        "link": "https://www.pinecone.io/cookies/",
        "snippet": "Dec 3, 2020 ... Our Websites may use both session cookies (which expire once you close your web browser) and persistent cookies (which stay on your computer or ...",
        "formattedUrl": "https://www.pinecone.io/cookies/"
      },
      {
        "title": "InpharmD Redefines Evidence-Based Healthcare with Pinecone ...",
        "link": "https://www.pinecone.io/customers/inpharmd/",
        "snippet": "InpharmD Redefines Evidence-Based Healthcare with Pinecone · Navigating the Complex Landscape of Medical Data · Enabling Healthcare Professionals with Sherlock ...",
        "formattedUrl": "https://www.pinecone.io/customers/inpharmd/"
      },
      {
        "title": "Pinecone Console",
        "link": "https://app.pinecone.io/organizations",
        "snippet": "Long-term memory for AI.",
        "formattedUrl": "https://app.pinecone.io/organizations"
      },
      {
        "title": "Model Gallery - Pinecone Docs",
        "link": "https://docs.pinecone.io/models/overview",
        "snippet": "llama-text-embed-v2 · multilingual-e5-large · cohere-rerank-3.5 · pinecone-sparse-english-v0 · bge-reranker-v2-m3.",
        "formattedUrl": "https://docs.pinecone.io/models/overview"
      },
      {
        "title": "Semantic Search | Pinecone",
        "link": "https://www.pinecone.io/solutions/semantic/",
        "snippet": "Semantic search allows your applications to search by meaning to find relevant results even if the exact words don't match. The Pinecone vector database lets ...",
        "formattedUrl": "https://www.pinecone.io/solutions/semantic/"
      },
      {
        "title": "DISCO Revolutionizes Legal Technology with Pinecone | Pinecone",
        "link": "https://www.pinecone.io/customers/disco/",
        "snippet": "DISCO Revolutionizes Legal Technology with Pinecone ... DISCO provides a cloud-native legal solution that simplifies legal hold, legal requests, ediscovery, legal ...",
        "formattedUrl": "https://www.pinecone.io/customers/disco/"
      },
      {
        "title": "Glasp achieves 5X cost savings in knowledge access for millions of ...",
        "link": "https://www.pinecone.io/blog/glasp/",
        "snippet": "Jun 5, 2024 ... In a recent conversation with Kazuki Nakayashiki, CEO and co-founder at Glasp, we explored the pivotal role Pinecone plays in supporting Glasp's ...",
        "formattedUrl": "https://www.pinecone.io/blog/glasp/"
      },
      {
        "title": "How 1up turns sales reps into product experts with Pinecone ...",
        "link": "https://www.pinecone.io/customers/1up/",
        "snippet": "1up solves this problem by automating knowledge for sales teams, enabling them to answer sales questions, RFPs, and compliance questionnaires 10x times faster.",
        "formattedUrl": "https://www.pinecone.io/customers/1up/"
      },
      {
        "title": "Transformers Are All You Need | Pinecone",
        "link": "https://www.pinecone.io/learn/transformers/",
        "snippet": "Using self-attention mechanisms, Transformers can capture the context of a word from distant parts of a sentence, both before and after the appearance of that ...",
        "formattedUrl": "https://www.pinecone.io/learn/transformers/"
      },
      {
        "title": "Voyage AI - Pinecone Docs",
        "link": "https://docs.pinecone.io/integrations/voyage",
        "snippet": "Voyage AI ... Voyage AI provides cutting-edge embedding and rerankers. Voyage AI's generalist embedding models continually top the MTEB leaderboard, and the ...",
        "formattedUrl": "https://docs.pinecone.io/integrations/voyage"
      },
      {
        "title": "TaskUs Partners with Pinecone to Enhance Customer Service ...",
        "link": "https://www.pinecone.io/customers/taskus/",
        "snippet": "TaskUs selected Pinecone as the vector database solution for the TaskGPT platform to enable semantic searches and Retrieval Augmented Generation (RAG) for ...",
        "formattedUrl": "https://www.pinecone.io/customers/taskus/"
      },
      {
        "title": "Revolutionizing Revenue Intelligence: Gong's Strategic Partnership ...",
        "link": "https://www.pinecone.io/customers/gong/",
        "snippet": "Gong anonymizes the data by embedding each sentence in a 768-size vector, along with metadata. The resulting information is encrypted and then stored in ...",
        "formattedUrl": "https://www.pinecone.io/customers/gong/"
      },
      {
        "title": "OpenAI - Pinecone Docs",
        "link": "https://docs.pinecone.io/integrations/openai",
        "snippet": "This guide covers the integration of OpenAI's Large Language Models (LLMs) with Pinecone (referred to as the OP stack), enhancing semantic search or 'long-term ...",
        "formattedUrl": "https://docs.pinecone.io/integrations/openai"
      },
      {
        "title": "Pinecone & AWS: Ship scalable and cost-effective Gen AI | Pinecone",
        "link": "https://www.pinecone.io/partners/aws/",
        "snippet": "Pinecone on AWS Marketplace. Develop highly scalable Gen AI apps with Pinecone on the AWS marketplace. Pinecone offers a usage-based pricing model with no ...",
        "formattedUrl": "https://www.pinecone.io/partners/aws/"
      },
      {
        "title": "The vector database to build knowledgeable AI | Pinecone",
        "link": "https://www.pinecone.io/newsroom/news/",
        "snippet": "Pinecone Assistant is now generally available for all users. Expanded functionality makes it even easier to build knowledgeable AI assistants in just minutes.",
        "formattedUrl": "https://www.pinecone.io/newsroom/news/"
      },
      {
        "title": "Pinecone & GCP: Power production-ready Gen AI | Pinecone",
        "link": "https://www.pinecone.io/partners/gcp/",
        "snippet": "Pinecone on GCP Marketplace. Develop and launch Gen AI apps quickly with Pinecone in the GCP marketplace. Pinecone offers a usage-based pricing model with no ...",
        "formattedUrl": "https://www.pinecone.io/partners/gcp/"
      },
      {
        "title": "Launch Week | Pinecone",
        "link": "https://www.pinecone.io/launch-week/",
        "snippet": "We've optimized our serverless architecture to meet the growing demand for large-scale agentic workloads and improved performance for search and recommendation ...",
        "formattedUrl": "https://www.pinecone.io/launch-week/"
      }
    ]
  },
  "general_search_markdown": "# Official social media\n- [Pinecone | LinkedIn](https://fr.linkedin.com/company/pinecone-io?trk=ppro_cprof) - Dec 1, 2023\n- [Pinecone | LinkedIn](https://www.linkedin.com/company/pinecone-io) - Dec 1, 2023\n\n# Job boards\n(No unique job board pages found)\n\n# App stores\n(No app store links found)\n\n# Product reviews\n(No detailed product reviews found)\n\n# News articles (most recent first, grouped by event)\n### Funding and Valuation\n- [Pinecone Now Valued at $750M, Arguably the Most Important ...](https://menlovc.com/perspective/pinecone-now-valued-at-750m/) - Apr 27, 2023\n- [Pinecone Hits $750M Valuation As AI Heats Up Vector Database ...](https://news.crunchbase.com/ai-robotics/startup-venture-funding-database-pinecone/) - Apr 27, 2023\n- [Pinecone raises $100M Series B | Hacker News](https://news.ycombinator.com/item?id=35729816) - Apr 27, 2023\n\n### Product Development\n- [Pinecone Revamps Vector Database Architecture for AI Apps - The ...](https://thenewstack.io/pinecone-revamps-vector-database-architecture-for-ai-apps/) - Feb 27, 2025\n- [Pinecone integrates AI inferencing with vector database | Hacker ...](https://news.ycombinator.com/item?id=42315364) - Dec 4, 2024\n\n### Recognition\n- [Pinecone Named to Fast Company's Annual List of the World's Most ...](https://www.fox21news.com/business/press-releases/cision/20250318LN43765/pinecone-named-to-fast-companys-annual-list-of-the-worlds-most-innovative-companies-of-2025) - Mar 18, 2025\n\n# Key employees (grouped by employee)\n(No profiles or articles related to key employees found)\n\n# Other pages on the company website\n- [Pinecone Console](https://app.pinecone.io/organizations)\n- [Company | Pinecone](https://www.pinecone.io/company/)\n- [Contact | Pinecone](https://www.pinecone.io/contact/)\n- [Customers | Pinecone](https://www.pinecone.io/customers/)\n- [Pricing | Pinecone](https://www.pinecone.io/pricing/)\n- [Trust and Security | Pinecone](https://www.pinecone.io/security/)\n- [Semantic Search | Pinecone](https://www.pinecone.io/solutions/semantic/)\n\n# Other\n- [Pinecone - Crunchbase Company Profile & Funding](https://www.crunchbase.com/organization/pinecone) - Provides an overview of Pinecone's funding and business model.\n- [Understanding Embeddings with Pinecone | by juliuscecilia33 ...](https://medium.com/@juliuscecilia33/understanding-embeddings-with-pinecone-9f765b490387) - Nov 12, 2024 - A detailed exploration of Pinecone's capabilities.\n- [What is Pinecone AI? A Guide to the Craze Behind Vector ...](https://estuary.dev/blog/what-is-pinecone-ai/) - Jun 18, 2023 - An informative guide on vector databases and Pinecone's role.\n- [How to Get More from Your Pinecone Vector Database – Vectorize](https://vectorize.io/how-to-get-more-from-your-pinecone-vector-database/) - Apr 11, 2024 - Tips for optimizing the use of Pinecone's database.",
  "crunchbase_markdown": "# Pinecone, founded 2019-01-01 [(Crunchbase, 2025)](https://www.crunchbase.com/organization/pinecone)\nPinecone develops a vector database that makes it easy to connect company data with generative AI models. It combines vector search libraries, features such as filtering, and distribution infrastructure to provide reliability at any scale.\n\n- [Website](https://www.pinecone.io)\n- [LinkedIn](https://www.linkedin.com/company/pinecone-io)\n- [Twitter](https://www.twitter.com/pinecone)\n\n## Funding (138M USD total)\n\n- 100M USD on 2023-04-26\n- 28M USD on 2022-03-29\n- 10M USD on 2021-01-27\n\n## News\n\n- Data Management News for the Week of March 21; Updates from Confluent, DataStax, dbt Labs & More ([Tim King, 2025-03-21](https://solutionsreview.com/data-management/data-management-news-for-the-week-of-march-21-updates-from-confluent-datastax-dbt-labs-more/))\n- Pinecone Named to Fast Company's Annual List of the World's Most Innovative Companies of 2025 ([Pinecone Systems Inc, 2025-03-18](https://www.prnewswire.com/news-releases/pinecone-named-to-fast-companys-annual-list-of-the-worlds-most-innovative-companies-of-2025-302404493.html))\n- Vector Database Market to Reach USD 10.6 Billion by 2032| SNS Insider ([SNS Insider pvt ltd, 2025-03-07](https://www.globenewswire.com/news-release/2025/03/07/3039040/0/en/Vector-Database-Market-to-Reach-USD-10-6-Billion-by-2032-SNS-Insider.html))\n- Pinecone Revamps Vector Database Architecture for AI Apps ([Loraine Lawson, 2025-02-27](https://thenewstack.io/pinecone-revamps-vector-database-architecture-for-ai-apps/))\n- AI-Powered Professor Rating Assistant With RAG and Pinecone ([Vaibhavi Tiwari, 2025-02-26](https://dzone.com/articles/building-an-ai-powered-professor-rating-assistant))\n- Menlo Ventures Looks To Add To Growing AI Portfolio With New Anthropic Partnership ([Gené Teare, 2024-08-29](https://news.crunchbase.com/ai/menlo-ventures-portfolio-anthropic-anthology/))\n- Pinecone serverless goes multicloud as vector database market heats up ([Sean Michael Kerner, 2024-08-27](https://venturebeat.com/data-infrastructure/pinecone-serverless-goes-multicloud-as-vector-database-market-heats-up/))\n- Pinecone previews new bulk import feature for its serverless offering ([Jenna Barron, 2024-08-27](https://sdtimes.com/data/pinecone-previews-new-bulk-import-feature-for-its-serverless-offering/))\n- Make Pgvector Faster Than Pinecone and 75% Cheaper With This New Open Source Extension ([Avthar Sewrathan, 2024-07-14](https://thenewstack.io/make-pgvector-faster-than-pinecone-and-75-cheaper-with-this-new-open-source-extension/))\n- Robust Intelligence Partners With Pinecone to Secure Retrieval-Augmented Generation (RAG) AI Applications ([Newswire, 2024-06-26](https://www.newswire.com/news/robust-intelligence-partners-with-pinecone-to-secure-retrieval-22369728))\n\n",
  "customer_experience_result": {
    "output_text": "# COMPANY Pinecone\n\n## Performance Issues\n- \"Pinecone has shit latency and the API randomly craps out.\" [(Christosconst, Reddit, 2023-10-05)](https://www.reddit.com/r/vectordatabase/comments/170j6zd/my_strategy_for_picking_a_vector_database_a/k3m1n6h/)\n- \"We started out with Pinecone, but it wasn't super stable. As our data grew, we switched to zilliz cloud. now everything works pretty well.\" [(one-punch-G, Reddit, 2024-07-01)](https://www.reddit.com/r/vectordatabase/comments/170j6zd/my_strategy_for_picking_a_vector_database_a/lb6rfbi/)\n- \"In my experience, Pinecone is good for basics but you hit a roof very quickly if you want to support normal query.\" [(Temporary-Koala-7370, Reddit, 2023-07-14)](https://www.reddit.com/r/MachineLearning/comments/12m9pg0/alternatives_to_pinecone_vector_databases_d/jrzzh3o/)\n- \"I have liked using the service, very simple to set up and use, but now running into a roadblock in production because of the performance being not just bad, but unusable.\" [(LeastIntroduction366, Reddit, 2024-03-26)](https://www.reddit.com/r/MachineLearning/comments/12m9pg0/alternatives_to_pinecone_vector_databases_d/kwn4ngu/)\n\n## Pricing Concerns\n- \"I just got hit for 190$ for 1 pod 86k vector representations. Does anyone else feel like they're grifting?\" [(clavelnotes, Reddit, 2023-05-31)](https://www.reddit.com/r/MachineLearning/comments/12m9pg0/alternatives_to_pinecone_vector_databases_d/jmdn6u4/)\n- \"Same issue here. I have the $70 plan and got a bill for $123 for one index with 3000 products and only made 9 queries for testing.\" [(CapitalAngle8580, Reddit, 2023-07-06)](https://www.reddit.com/r/MachineLearning/comments/12m9pg0/alternatives_to_pinecone_vector_databases_d/jqwnm3j/)\n\n## User Experience\n- \"I've tried Pinecone's examples in Google Colab but am hitting some issues so am still trying...\" [(vap0rtranz, Reddit, 2023-08-03)](https://www.reddit.com/r/LangChain/comments/15h3c5e/document_query_solution_for_small_business/jum5jsr/)\n- \"One reason I was hesitant to use pinecone in the past for our production needs was such a heavy reliance on python. Now I will take another look.\" [(gigapiksel, Reddit, 2022-08-24)](https://www.reddit.com/r/rust/comments/wwe589/pinecone_rust_a_hard_decision_pays_off/ilkn4d8/)\n\n# PRODUCT Pinecone\n\n## General Information\n- \"Pinecone is a vector database.\" [(None, Reddit, 2023-04-23)](https://www.reddit.com/r/ChatGPTCoding/comments/12wn63m/i_have_made_some_easy_tools_to_rip_webpages_clean/jhfy6b0/)\n- \"The key is that using a vector DB makes it more efficient to do vector lookups in the face of tons of data.\" [(tyliggity, Reddit, 2023-04-24)](https://www.reddit.com/r/ChatGPTCoding/comments/12wn63m/i_have_made_some_easy_tools_to_rip_webpages_clean/jhj6vcz/)\n\n## Use Cases and Advantages\n- \"If you have billions of documents and want to make a semantic search engine, where the user inputs a question and the database immediately tells you the top 20 document chunks most similar in topic to the question, a vector database can do that while scaling horizontally and doing map-reduce.\" [(None, Reddit, 2023-05-05)](https://www.reddit.com/r/SoftwareEngineering/comments/107vhoq/vector_databases_like_pinecone_or_weaviate_are/jiy9zgs/)\n- \"Pinecone & Weaviate have the latest tech built into products for doc search and/or Q&A right now: Hybrid search.\" [(vap0rtranz, Reddit, 2023-08-03)](https://www.reddit.com/r/LangChain/comments/15h3c5e/document_query_solution_for_small_business/jum5jsr/)\n- \"Pinecone also saw better performance of models trained to Q&A with hybrid compared to lexical or semantic searches.\" [(vap0rtranz, Reddit, 2023-08-03)](https://www.reddit.com/r/LangChain/comments/15h3c5e/document_query_solution_for_small_business/jum5jsr/)\n\n## Limitations and Disadvantages\n- \"I don't think so. Vector databases have a handful of disadvantages.\" [(Just_CurioussSss, Reddit, 2023-01-13)](https://www.reddit.com/r/SoftwareEngineering/comments/107vhoq/vector_databases_like_pinecone_or_weaviate_are/j45p8qq/)\n- \"Vector databases can be less flexible in terms of data management and querying than traditional databases.\" [(Just_CurioussSss, Reddit, 2023-01-13)](https://www.reddit.com/r/SoftwareEngineering/comments/107vhoq/vector_databases_like_pinecone_or_weaviate_are/j45p8qq/)\n- \"Vector databases might have a higher latency than other databases, especially when the data size is large, or the queries are complex.\" [(Just_CurioussSss, Reddit, 2023-01-13)](https://www.reddit.com/r/SoftwareEngineering/comments/107vhoq/vector_databases_like_pinecone_or_weaviate_are/j45p8qq/)\n\n## Integration with AI and Embeddings\n- \"I believe what you are looking for is called embedding. I just did this for an in-house document that was roughly 200~ pages. You can then do math against these vectors to pull the top X results to feed in with your question.\" [(Zulugod94, Reddit, 2023-04-12)](https://www.reddit.com/r/OpenAI/comments/12jyes5/what_are_good_techniques_for_feeding_extremely/jg0l716/)\n- \"From what I've seen it works exceptionally well with gpt4. Both because of the additional intelligence of 4 but also the 8k token limit it a huge help.\" [(Zulugod94, Reddit, 2023-04-12)](https://www.reddit.com/r/OpenAI/comments/12jyes5/what_are_good_techniques_for_feeding_extremely/jg0w4ok/)\n- \"You can then store those embeddings in a vector db (like pinecone). When you have a query you want to run against the large document, you take your question, calculate the embedding of it and then ask the vector db which of the many 1000 character chunks are most relevant to your question.\" [(Icanteven______, Reddit, 2023-04-12)](https://www.reddit.com/r/OpenAI/comments/12jyes5/what_are_good_techniques_for_feeding_extremely/jg11jjm/)\n- \"It varies by AI, but OpenAI solutions are premised on embeddings. Large text searches are actually cosine-similarity searches on split chunks, using the most similar chunks in the completion prompt.\" [(MatchaGaucho, Reddit, 2023-04-12)](https://www.reddit.com/r/OpenAI/comments/12jyes5/what_are_good_techniques_for_feeding_extremely/jg0peys/)\n- \"You can use LangChain to assist, but essentially you’re calling the OpenAI embeddings API to map your large document into a bunch of numbered vector documents that 'embed' their semantic meaning.\" [(Icanteven______, Reddit, 2023-04-12)](https://www.reddit.com/r/OpenAI/comments/12jyes5/what_are_good_techniques_for_feeding_extremely/jg11jjm/)\n- \"The embeddings themselves are just a static database of vectors, they simply represent the 'value' of chunks of text so you can search for related chunks quickly.\" [(Zulugod94, Reddit, 2023-04-13)](https://www.reddit.com/r/OpenAI/comments/12jyes5/what_are_good_techniques_for_feeding_extremely/jg2skiy/)\n\n## User Feedback and Experiences\n- \"I’m just using gpt3.5 and pinecone, since there’s so much info on using them and they’re super straight forward.\" [(BlandUnicorn, Reddit, 2023-07-10)](https://www.reddit.com/r/LocalLLaMA/comments/14vnfh2/my_experience_on_starting_with_fine_tuning_llms/jreg8tn/)\n- \"I have been working on improving the data to work better with a vector db, and plain chunked text isn’t great.\" [(BlandUnicorn, Reddit, 2023-07-10)](https://www.reddit.com/r/LocalLLaMA/comments/14vnfh2/my_experience_on_starting_with_fine_tuning_llms/jreg8tn/)\n- \"I do plan on switching to a local vector db later when I’ve worked out the best data format to feed it.\" [(BlandUnicorn, Reddit, 2023-07-10)](https://www.reddit.com/r/LocalLLaMA/comments/14vnfh2/my_experience_on_starting_with_fine_tuning_llms/jreg8tn/)",
    "intermediate_steps": [
      "- \"The use case for a vector database is usually super fast search over vectors, k-nearest neighbors and approximate nearest neighbors or something related and specific to vectors like that.\" [(None, Reddit, 2023-05-05)](cache://reddit/12)\n- \"No, a pure vector database cannot replace a traditional DB. Because the only way to retrieve data is via a vector search.\" [(lundren10, Reddit, 2023-09-27)](cache://reddit/10)\n- \"If you have billions of documents and want to make a semantic search engine, where the user inputs a question and the database immediately tells you the top 20 document chunks most similar in topic to the question, a vector database can do that while scaling horizontally and doing map-reduce.\" [(None, Reddit, 2023-05-05)](cache://reddit/12)\n- \"I don't think so. Vector databases have a handful of disadvantages.\" [(Just_CurioussSss, Reddit, 2023-01-13)](cache://reddit/5)\n- \"Vector databases can be less flexible in terms of data management and querying than traditional databases.\" [(Just_CurioussSss, Reddit, 2023-01-13)](cache://reddit/5)\n- \"Vector databases might have a higher latency than other databases, especially when the data size is large, or the queries are complex.\" [(Just_CurioussSss, Reddit, 2023-01-13)](cache://reddit/5)",
      "- \"Pinecone has shit latency and the API randomly craps out.\" [(Christosconst, Reddit, 2023-10-05)](cache://reddit/37)\n- \"We started out with Pinecone, but it wasn't super stable. As our data grew, we switched to zilliz cloud. now everything works pretty well.\" [(one-punch-G, Reddit, 2024-07-01)](cache://reddit/43)\n- \"Pinecone is a vector database.\" [(None, Reddit, 2023-04-23)](cache://reddit/76)\n- \"The key is that using a vector DB makes it more efficient to do vector lookups in the face of tons of data.\" [(tyliggity, Reddit, 2023-04-24)](cache://reddit/79)",
      "- \"Pinecone & Weaviate have the latest tech built into products for doc search and/or Q&A right now: Hybrid search.\" [(vap0rtranz, Reddit, 2023-08-03)](cache://reddit/126)\n- \"Pinecone also saw better performance of models trained to Q&A with hybrid compared to lexical or semantic searches.\" [(vap0rtranz, Reddit, 2023-08-03)](cache://reddit/126)\n- \"I've tried Pinecone's examples in Google Colab but am hitting some issues so am still trying...\" [(vap0rtranz, Reddit, 2023-08-03)](cache://reddit/126)\n- \"One reason I was hesitant to use pinecone in the past for our production needs was such a heavy reliance on python. Now I will take another look.\" [(gigapiksel, Reddit, 2022-08-24)](cache://reddit/154)",
      "- \"I believe what you are looking for is called embedding. I just did this for an in-house document that was roughly 200~ pages. You can then do math against these vectors to pull the top X results to feed in with your question.\" [(Zulugod94, Reddit, 2023-04-12)](cache://reddit/187)\n- \"From what I've seen it works exceptionally well with gpt4. Both because of the additional intelligence of 4 but also the 8k token limit it a huge help.\" [(Zulugod94, Reddit, 2023-04-12)](cache://reddit/189)\n- \"You can then store those embeddings in a vector db (like pinecone). When you have a query you want to run against the large document, you take your question, calculate the embedding of it and then ask the vector db which of the many 1000 character chunks are most relevant to your question.\" [(Icanteven______, Reddit, 2023-04-12)](cache://reddit/219)\n- \"It varies by AI, but OpenAI solutions are premised on embeddings. Large text searches are actually cosine-similarity searches on split chunks, using the most similar chunks in the completion prompt.\" [(MatchaGaucho, Reddit, 2023-04-12)](cache://reddit/217)\n- \"You can use LangChain to assist, but essentially you’re calling the OpenAI embeddings API to map your large document into a bunch of numbered vector documents that 'embed' their semantic meaning.\" [(Icanteven______, Reddit, 2023-04-12)](cache://reddit/219)\n- \"The embeddings themselves are just a static database of vectors, they simply represent the 'value' of chunks of text so you can search for related chunks quickly.\" [(Zulugod94, Reddit, 2023-04-13)](cache://reddit/198)\n- \"There are lots of answers here and they all fit different use cases. I built multiple variations to satisfy specific needs and found some tool and setting combinations, while work great for one use case may end up being totally unworkable for another use case.\" [(justdoitanddont, Reddit, 2023-04-13)](cache://reddit/242)\n- \"I have not seen any reasonably good implementation of embedding with AI besides simple QA, no extended conversations or actual memory. Just a nice customized search engine for your own documents.\" [(Robo_Rascal, Reddit, 2023-04-13)](cache://reddit/256)",
      "- \"In my experience, Pinecone is good for basics but you hit a roof very quickly if you want to support normal query.\" [(Temporary-Koala-7370, Reddit, 2023-07-14)](cache://reddit/306)\n- \"I have liked using the service, very simple to set up and use, but now running into a roadblock in production because of the performance being not just bad, but unusable.\" [(LeastIntroduction366, Reddit, 2024-03-26)](cache://reddit/347)\n- \"I just got hit for 190$ for 1 pod 86k vector representations. Does anyone else feel like they're grifting?\" [(clavelnotes, Reddit, 2023-05-31)](cache://reddit/375)\n- \"Same issue here. I have the $70 plan and got a bill for $123 for one index with 3000 products and only made 9 queries for testing.\" [(CapitalAngle8580, Reddit, 2023-07-06)](cache://reddit/376)\n- \"You can search through 100k vectors in less than a second on an M1 MacBook Pro with a for loop.\" [(johnnydaggers, Reddit, 2023-04-15)](cache://reddit/294)",
      "- \"I’m just using gpt3.5 and pinecone, since there’s so much info on using them and they’re super straight forward.\" [(BlandUnicorn, Reddit, 2023-07-10)](cache://reddit/409)\n- \"I have been working on improving the data to work better with a vector db, and plain chunked text isn’t great.\" [(BlandUnicorn, Reddit, 2023-07-10)](cache://reddit/409)\n- \"I do plan on switching to a local vector db later when I’ve worked out the best data format to feed it.\" [(BlandUnicorn, Reddit, 2023-07-10)](cache://reddit/409)"
    ],
    "url_to_review": {},
    "review_markdowns": [
      "# Post ID 12m9pg0: Alternatives to Pinecone? (Vector databases) [D] with +114 score by [(AlexisMAndrade, Reddit, 2023-04-14)](https://www.reddit.com/r/MachineLearning/comments/12m9pg0/alternatives_to_pinecone_vector_databases_d/)\nPinecone is experiencing a large wave of signups, and it's overloading their ability to add new indexes (14/04/2023, [https://status.pinecone.io/](https://status.pinecone.io/)). What are some other good vector databases?\n\n## Comment ID jg9nfkd with +48 score by [(light24bulbs, Reddit, 2023-04-14)](https://www.reddit.com/r/MachineLearning/comments/12m9pg0/alternatives_to_pinecone_vector_databases_d/jg9nfkd/) (in reply to ID 12m9pg0):\nWe've played with these a lot and we are about to create an \"awesome list\" on github. In our blog post we at least list the different ones. \n\nhttps://lunabrain.com/blog/riding-the-ai-wave-with-vector-databases-how-they-work-and-why-vcs-love-them/\n\nWe've honestly gotten pretty far with pg-vector, the postgres extention. If you're integrating into an existing product and would like to keep all of your existing infra and relations and stuff, its pretty great. Honestly the way pinecone works is kind of janky anyway. \n\nWeaviate seems good although we haven't used it at scale, we've talked with others who have and its fine.\n\n### Comment ID jgcdcds with +8 score by [(vade, Reddit, 2023-04-15)](https://www.reddit.com/r/MachineLearning/comments/12m9pg0/alternatives_to_pinecone_vector_databases_d/jgcdcds/) (in reply to ID jg9nfkd):\nI’ve been benchmarking weaviate and PGVector - and I’ve been getting really wildly different results in terms of perf (weavaiate being 10-30x faster with faceted search than Postgres + PGVector ) and PGVector indexing (even with the heuristic of how to build index based on size of embeddings). \n\nI’m curious if you’ve seen a really solid guide on maximizing PGVector perf (both in terms of speed and accuracy). \n\nThanks in advance!\n\n#### Comment ID jgg1t2u with +1 score by [(pricklyplant, Reddit, 2023-04-16)](https://www.reddit.com/r/MachineLearning/comments/12m9pg0/alternatives_to_pinecone_vector_databases_d/jgg1t2u/) (in reply to ID jgcdcds):\nWhat hardware have you been trying this on?\n\n#### Comment ID k3zjcsc with +1 score by [(WAHNFRIEDEN, Reddit, 2023-10-08)](https://www.reddit.com/r/MachineLearning/comments/12m9pg0/alternatives_to_pinecone_vector_databases_d/k3zjcsc/) (in reply to ID jgcdcds):\nWhat’d you settle on?\n\n### Comment ID jgedb7g with +7 score by [(pricklyplant, Reddit, 2023-04-15)](https://www.reddit.com/r/MachineLearning/comments/12m9pg0/alternatives_to_pinecone_vector_databases_d/jgedb7g/) (in reply to ID jg9nfkd):\nElasticsearch itself is capable of indexing and searching across vector embeddings: https://www.elastic.co/guide/en/elasticsearch/reference/8.6/knn-search.html had you looked at this as an option?\n\n### Comment ID jgbta97 with +3 score by [(ZenDragon, Reddit, 2023-04-15)](https://www.reddit.com/r/MachineLearning/comments/12m9pg0/alternatives_to_pinecone_vector_databases_d/jgbta97/) (in reply to ID jg9nfkd):\nWhat's a good solution if your needs are modest and you just want to store the db on your local machine?\n\n#### Comment ID jgcak9f with +4 score by [(cletch2, Reddit, 2023-04-15)](https://www.reddit.com/r/MachineLearning/comments/12m9pg0/alternatives_to_pinecone_vector_databases_d/jgcak9f/) (in reply to ID jgbta97):\nWeaviate is in my opinion the most easy to implement and play around, so I would advice checking it out for a modest use case.\n\n#### Comment ID jgdb3or with +3 score by [(light24bulbs, Reddit, 2023-04-15)](https://www.reddit.com/r/MachineLearning/comments/12m9pg0/alternatives_to_pinecone_vector_databases_d/jgdb3or/) (in reply to ID jgbta97):\nHonestly if your needs are REALLY modest you might want to look at llama-index (horrible name, it's unrelated to facebooks llama).  Assuming you're just using chatgpt. \n\nJust an in-memory setup with JSON file backend\n\n#### Comment ID jgse27n with +1 score by [(Sufficient-Builder42, Reddit, 2023-04-18)](https://www.reddit.com/r/MachineLearning/comments/12m9pg0/alternatives_to_pinecone_vector_databases_d/jgse27n/) (in reply to ID jgbta97):\nchromadb is not bad as far as I can tell - used it with just a file storage solution then had to go to a local docker container to run it as a service when the file got > 500mb. Seems relatively performant, and was pretty trivial to set up.\n\n#### Comment ID jgcgbb2 with +1 score by [(TrulyMaximumDude, Reddit, 2023-04-15)](https://www.reddit.com/r/MachineLearning/comments/12m9pg0/alternatives_to_pinecone_vector_databases_d/jgcgbb2/) (in reply to ID jgbta97):\nlook up llama index or chromadb\n\n#### Comment ID jgcma8w with +1 score by [(fozziethebeat, Reddit, 2023-04-15)](https://www.reddit.com/r/MachineLearning/comments/12m9pg0/alternatives_to_pinecone_vector_databases_d/jgcma8w/) (in reply to ID jgbta97):\nWith python I’ve been using faiss for a simple in memory setup\n\n### Comment ID jgi1aj9 with +3 score by [(SatoshiNotMe, Reddit, 2023-04-16)](https://www.reddit.com/r/MachineLearning/comments/12m9pg0/alternatives_to_pinecone_vector_databases_d/jgi1aj9/) (in reply to ID jg9nfkd):\nCurious why you thought pinecone is janky. I’m trying to decide among vecDbs and would appreciate any elaboration on this.\n\n#### Comment ID jgi2pxz with +5 score by [(light24bulbs, Reddit, 2023-04-16)](https://www.reddit.com/r/MachineLearning/comments/12m9pg0/alternatives_to_pinecone_vector_databases_d/jgi2pxz/) (in reply to ID jgi1aj9):\nWell, what I saw is from working with it in frameworks like langchain and llama-index.  The worst weird problem I saw was that pinecone doesn't appear to support storing documents alongside your vectors so what people do is actually cram snippets of the document into the metadata, but the metadata is limited to something really really small, so the maximum document length gets constrained. Go look at the llama-index code and you will see the jank.\n\nIf you're using another database alongside pinecone and just want to retrieve uuids or something, it's fine, but it struck me as a very weird omission in their design. I believe weaviate treats documents as first class citizens.\n\n### Comment ID js2rwv9 with +2 score by [(InterestingKnee3541, Reddit, 2023-07-15)](https://www.reddit.com/r/MachineLearning/comments/12m9pg0/alternatives_to_pinecone_vector_databases_d/js2rwv9/) (in reply to ID jg9nfkd):\n>I’m curious if you’ve seen a really solid guide on maximizing PGVector perf (both in terms of speed and accuracy).\n\nthis is amazing. Thank you for sharing this!\n\n### Comment ID l26sy3u with +1 score by [(Jade_Lauren, Reddit, 2024-05-02)](https://www.reddit.com/r/MachineLearning/comments/12m9pg0/alternatives_to_pinecone_vector_databases_d/l26sy3u/) (in reply to ID jg9nfkd):\nWould love to get an update on the \"awesome list\" by luna brain as well\n\n#### Comment ID l26u4y5 with +1 score by [(light24bulbs, Reddit, 2024-05-02)](https://www.reddit.com/r/MachineLearning/comments/12m9pg0/alternatives_to_pinecone_vector_databases_d/l26u4y5/) (in reply to ID l26sy3u):\nCompany ded\n\n### Comment ID jwhj04n with +1 score by [(UncleSammmm, Reddit, 2023-08-16)](https://www.reddit.com/r/MachineLearning/comments/12m9pg0/alternatives_to_pinecone_vector_databases_d/jwhj04n/) (in reply to ID jg9nfkd):\nWhat's the link to this awesome github list?\n\n#### Comment ID k9ssovy with +1 score by [(whatismynamepops, Reddit, 2023-11-18)](https://www.reddit.com/r/MachineLearning/comments/12m9pg0/alternatives_to_pinecone_vector_databases_d/k9ssovy/) (in reply to ID jwhj04n):\nnowhere\n\n## Comment ID jgc0vki with +12 score by [(johnnydaggers, Reddit, 2023-04-15)](https://www.reddit.com/r/MachineLearning/comments/12m9pg0/alternatives_to_pinecone_vector_databases_d/jgc0vki/) (in reply to ID 12m9pg0):\nHow many documents do you have? You can search through 100k vectors in less than a second on an M1 MacBook Pro with a for loop.\n\n### Comment ID jgcuzrj with +8 score by [(CacheMeUp, Reddit, 2023-04-15)](https://www.reddit.com/r/MachineLearning/comments/12m9pg0/alternatives_to_pinecone_vector_databases_d/jgcuzrj/) (in reply to ID jgc0vki):\nI second that. Numpy can easily do brute-force similarity on \\~1M vectors in far less than a second.\n\n#### Comment ID jgczkug with +5 score by [(fets-12345c, Reddit, 2023-04-15)](https://www.reddit.com/r/MachineLearning/comments/12m9pg0/alternatives_to_pinecone_vector_databases_d/jgczkug/) (in reply to ID jgcuzrj):\nAgreed, and save them to disk using the pickle module\n\n#### Comment ID jrd5s4m with +1 score by [(BKKBangers, Reddit, 2023-07-10)](https://www.reddit.com/r/MachineLearning/comments/12m9pg0/alternatives_to_pinecone_vector_databases_d/jrd5s4m/) (in reply to ID jgcuzrj):\nNOOB here. Please can you expand on this? Would you suggest writing a loop yourself or are you referring to a library to seek documents. Many thanks.\n\n### Comment ID jjq966n with +1 score by [(scaleup-123, Reddit, 2023-05-11)](https://www.reddit.com/r/MachineLearning/comments/12m9pg0/alternatives_to_pinecone_vector_databases_d/jjq966n/) (in reply to ID jgc0vki):\nWhich program / API are you using to interact with the files on your computer?\n\n## Comment ID jg9n3l7 with +24 score by [(hasan_za, Reddit, 2023-04-14)](https://www.reddit.com/r/MachineLearning/comments/12m9pg0/alternatives_to_pinecone_vector_databases_d/jg9n3l7/) (in reply to ID 12m9pg0):\nA good open-source alternative that also offers cloud hosting is [Weaviate](https://weaviate.io/).\n\n### Comment ID jgavmjm with +8 score by [(Warhouse512, Reddit, 2023-04-15)](https://www.reddit.com/r/MachineLearning/comments/12m9pg0/alternatives_to_pinecone_vector_databases_d/jgavmjm/) (in reply to ID jg9n3l7):\nAgreed. Weaviate is fire.\n\n### Comment ID jgcdy1y with +3 score by [(thd-ai, Reddit, 2023-04-15)](https://www.reddit.com/r/MachineLearning/comments/12m9pg0/alternatives_to_pinecone_vector_databases_d/jgcdy1y/) (in reply to ID jg9n3l7):\nTheir cloud hosting seemed a bit expensive. Try to have a look at qdrant\n\n#### Comment ID jlecpxk with +1 score by [(d3c3ptr0n, Reddit, 2023-05-24)](https://www.reddit.com/r/MachineLearning/comments/12m9pg0/alternatives_to_pinecone_vector_databases_d/jlecpxk/) (in reply to ID jgcdy1y):\nhttps://www.reddit.com/r/MachineLearning/comments/12m9pg0/comment/jlecoyv/?utm\\_source=share&utm\\_medium=web2x&context=3\n\n### Comment ID ji6hg1v with +3 score by [(Hinged31, Reddit, 2023-04-29)](https://www.reddit.com/r/MachineLearning/comments/12m9pg0/alternatives_to_pinecone_vector_databases_d/ji6hg1v/) (in reply to ID jg9n3l7):\nDumb question. I have like 3000 PDFs I want to be able query and ideally use to generate text from. Is that even possible or is that way too many documents (each is about 20 pages). And/or, just wildly expensive?\n\n#### Comment ID jk32epx with +3 score by [(None, Reddit, 2023-05-14)](https://www.reddit.com/r/MachineLearning/comments/12m9pg0/alternatives_to_pinecone_vector_databases_d/jk32epx/) (in reply to ID ji6hg1v):\nI paid $200 to store the Bible for 30 days as a test\n\n#### Comment ID jrzzh3o with +2 score by [(Temporary-Koala-7370, Reddit, 2023-07-14)](https://www.reddit.com/r/MachineLearning/comments/12m9pg0/alternatives_to_pinecone_vector_databases_d/jrzzh3o/) (in reply to ID ji6hg1v):\nI have implemented pinecone so far, and I just finished implementing elastic. In pinecone you have 130000 vectors in the free version with 1536 dim. A 300 page pdf ocupied 960ish vectors at 400chars per vector.\n\nIn other words, free version of pinecone can hold 39.000 pdf pages at 400chars each vector. This is without using metadata. The number goes down a little bit with metadata.  \n\n\nIn my experience, Pinecone is good for basics but you hit a roof very quickly if you want to support normal query. Elastic is the way to go though documentation is tricky. You need to use the Elasticsearch Enterprise search, not the AppSearch.\n\n### Comment ID jlecoyv with +3 score by [(d3c3ptr0n, Reddit, 2023-05-24)](https://www.reddit.com/r/MachineLearning/comments/12m9pg0/alternatives_to_pinecone_vector_databases_d/jlecoyv/) (in reply to ID jg9n3l7):\ntotal noob question: can i use weaviate on my local machine and for remote purpose i can spin up ec2 or equivalent instances and run weaviate on that? i am just asking what if i don't want to use their cloud services and deploy them on my own system, is that possible?\n\n#### Comment ID jled4yu with +2 score by [(thd-ai, Reddit, 2023-05-24)](https://www.reddit.com/r/MachineLearning/comments/12m9pg0/alternatives_to_pinecone_vector_databases_d/jled4yu/) (in reply to ID jlecoyv):\nHave a look at qdrant. They have an option for a local db\n\n#### Comment ID jmgi2jq with +2 score by [(RobstaDaLobstaa, Reddit, 2023-06-01)](https://www.reddit.com/r/MachineLearning/comments/12m9pg0/alternatives_to_pinecone_vector_databases_d/jmgi2jq/) (in reply to ID jlecoyv):\nYes, here's an example repo that runs Weaviate locally using docker-compose  \nhttps://github.com/laura-ham/HM-Fashion-image-neural-search\n\n## Comment ID jgawkj3 with +14 score by [(naccib, Reddit, 2023-04-15)](https://www.reddit.com/r/MachineLearning/comments/12m9pg0/alternatives_to_pinecone_vector_databases_d/jgawkj3/) (in reply to ID 12m9pg0):\nI would describe [Qdrant](https://qdrant.tech) as an beautifully simple vector database. Definitely worth a try, it has an forever-free tier as well.\n\n## Comment ID jgam7um with +19 score by [(Hackerjurassicpark, Reddit, 2023-04-14)](https://www.reddit.com/r/MachineLearning/comments/12m9pg0/alternatives_to_pinecone_vector_databases_d/jgam7um/) (in reply to ID 12m9pg0):\nMilvus is the only open source vector database I’ve seen running in production serving thousands of rps with ms latencies on a billion vector index\n\n### Comment ID jgapphq with +3 score by [(dandv, Reddit, 2023-04-14)](https://www.reddit.com/r/MachineLearning/comments/12m9pg0/alternatives_to_pinecone_vector_databases_d/jgapphq/) (in reply to ID jgam7um):\n[Weaviate benchmarks](https://weaviate.io/developers/weaviate/benchmarks) are also worth looking at.\n\n#### Comment ID jgb12a5 with +13 score by [(None, Reddit, 2023-04-15)](https://www.reddit.com/r/MachineLearning/comments/12m9pg0/alternatives_to_pinecone_vector_databases_d/jgb12a5/) (in reply to ID jgapphq):\n[deleted]\n\n### Comment ID jgdgcun with +1 score by [(johnnydaggers, Reddit, 2023-04-15)](https://www.reddit.com/r/MachineLearning/comments/12m9pg0/alternatives_to_pinecone_vector_databases_d/jgdgcun/) (in reply to ID jgam7um):\nThen you haven’t looked that hard? I know of others that have been around for years such as Vespa.ai. Yahoo uses that in production.\n\n#### Comment ID jgf13y3 with +1 score by [(Hackerjurassicpark, Reddit, 2023-04-15)](https://www.reddit.com/r/MachineLearning/comments/12m9pg0/alternatives_to_pinecone_vector_databases_d/jgf13y3/) (in reply to ID jgdgcun):\nOh yeah I’ve heard good things about Vespa and Faiss but they were a pain to setup on multiple nodes. Hence we chose milvus\n\n## Comment ID jgbcmit with +7 score by [(gregory_k, Reddit, 2023-04-15)](https://www.reddit.com/r/MachineLearning/comments/12m9pg0/alternatives_to_pinecone_vector_databases_d/jgbcmit/) (in reply to ID 12m9pg0):\nWe’re adding additional capacity on a rolling basis to support over 10k signups per day. Thanks for your patience!\n\nhttps://www.pinecone.io/learn/free-plan-update/\n\n## Comment ID jgazr5h with +7 score by [(yoshiwaan, Reddit, 2023-04-15)](https://www.reddit.com/r/MachineLearning/comments/12m9pg0/alternatives_to_pinecone_vector_databases_d/jgazr5h/) (in reply to ID 12m9pg0):\nThere’s a pretty good list in Langchain, including basic implementation code: https://github.com/hwchase17/langchain/tree/master/langchain/vectorstores\n\n## Comment ID jg9pm6m with +6 score by [(None, Reddit, 2023-04-14)](https://www.reddit.com/r/MachineLearning/comments/12m9pg0/alternatives_to_pinecone_vector_databases_d/jg9pm6m/) (in reply to ID 12m9pg0):\nDepending on what you're doing, there's plugins for sqlite, postgres and elasticsearch. Redis can also do it.\n\n## Comment ID jgbiufn with +3 score by [(rhillbh, Reddit, 2023-04-15)](https://www.reddit.com/r/MachineLearning/comments/12m9pg0/alternatives_to_pinecone_vector_databases_d/jgbiufn/) (in reply to ID 12m9pg0):\nVector Database Index from fall/2022  https://gradientflow.com/the-vector-database-index/\n\n### Comment ID jgftabv with +1 score by [(dandv, Reddit, 2023-04-16)](https://www.reddit.com/r/MachineLearning/comments/12m9pg0/alternatives_to_pinecone_vector_databases_d/jgftabv/) (in reply to ID jgbiufn):\nFAISS is a [vector library rather than a database](https://weaviate.io/blog/vector-library-vs-vector-database).\n\n## Comment ID jy8tpvv with +3 score by [(Signal-Additional, Reddit, 2023-08-29)](https://www.reddit.com/r/MachineLearning/comments/12m9pg0/alternatives_to_pinecone_vector_databases_d/jy8tpvv/) (in reply to ID 12m9pg0):\nZilliz Cloud (also known as Hosted Milvus) is a good alternative and offers a [free plan](https://zilliz.com/free) that includes up to 2 free collections (each holds 500,000 vectors of 768 dimensions). Of course, you can also choose [open source Milvus.](https://zilliz.com/what-is-milvus)\n\n## Comment ID jgam92a with +5 score by [(MiuraDude, Reddit, 2023-04-14)](https://www.reddit.com/r/MachineLearning/comments/12m9pg0/alternatives_to_pinecone_vector_databases_d/jgam92a/) (in reply to ID 12m9pg0):\nQdrant is my favourite. It's also open source.\n\n## Comment ID jgaqr5w with +5 score by [(SDusterwald, Reddit, 2023-04-15)](https://www.reddit.com/r/MachineLearning/comments/12m9pg0/alternatives_to_pinecone_vector_databases_d/jgaqr5w/) (in reply to ID 12m9pg0):\nI use a Weaviate instance hosted on DigitalOcean. Cheaper than using the official cloud services offering, and works well enough for me (I'm only using light loads though, not sure how well it will scale).\n\n## Comment ID jganldd with +6 score by [(fujiitora, Reddit, 2023-04-14)](https://www.reddit.com/r/MachineLearning/comments/12m9pg0/alternatives_to_pinecone_vector_databases_d/jganldd/) (in reply to ID 12m9pg0):\nChroma\n\n## Comment ID jgai1bh with +2 score by [(None, Reddit, 2023-04-14)](https://www.reddit.com/r/MachineLearning/comments/12m9pg0/alternatives_to_pinecone_vector_databases_d/jgai1bh/) (in reply to ID 12m9pg0):\n[removed]\n\n### Comment ID jgapepz with +10 score by [(dandv, Reddit, 2023-04-14)](https://www.reddit.com/r/MachineLearning/comments/12m9pg0/alternatives_to_pinecone_vector_databases_d/jgapepz/) (in reply to ID jgai1bh):\nFAISS is a vector library. A vector database has C(R)UD support for adding, updating and deleting objects and their embeddings without reindexing the entire data set. For more on this, a good post is [Vector Library versus Vector Database](https://weaviate.io/blog/vector-library-vs-vector-database).\n\n## Comment ID jgb8mes with +2 score by [(davidmezzetti, Reddit, 2023-04-15)](https://www.reddit.com/r/MachineLearning/comments/12m9pg0/alternatives_to_pinecone_vector_databases_d/jgb8mes/) (in reply to ID 12m9pg0):\nTake a look at txtai: https://github.com/neuml/txtai\n\n## Comment ID jgccoab with +2 score by [(the_egotist, Reddit, 2023-04-15)](https://www.reddit.com/r/MachineLearning/comments/12m9pg0/alternatives_to_pinecone_vector_databases_d/jgccoab/) (in reply to ID 12m9pg0):\nWe use elastic search vector db indexes on aws, and they work and scale just fine. Super easy to get going too\n\n### Comment ID jgccrhb with +2 score by [(the_egotist, Reddit, 2023-04-15)](https://www.reddit.com/r/MachineLearning/comments/12m9pg0/alternatives_to_pinecone_vector_databases_d/jgccrhb/) (in reply to ID jgccoab):\nhttps://opensearch.org/docs/2.0/search-plugins/knn/knn-index/\n\n## Comment ID jglb1v9 with +2 score by [(BobDang00, Reddit, 2023-04-17)](https://www.reddit.com/r/MachineLearning/comments/12m9pg0/alternatives_to_pinecone_vector_databases_d/jglb1v9/) (in reply to ID 12m9pg0):\nI'm curious if anyone has discovered a vector database that is compatible with the ScaNN method? (https://github.com/google-research/google-research/tree/master/scann)\n\n### Comment ID jy90wat with +1 score by [(Signal-Additional, Reddit, 2023-08-29)](https://www.reddit.com/r/MachineLearning/comments/12m9pg0/alternatives_to_pinecone_vector_databases_d/jy90wat/) (in reply to ID jglb1v9):\nMilvus support ScaNN and 10 others. https://zilliz.com/comparison/milvus-vs-elastic\n\n## Comment ID jgak8ga with +2 score by [(wind_dude, Reddit, 2023-04-14)](https://www.reddit.com/r/MachineLearning/comments/12m9pg0/alternatives_to_pinecone_vector_databases_d/jgak8ga/) (in reply to ID 12m9pg0):\nwow!! I've recently started experimenting with pgvector.\n\n## Comment ID jgb9v8x with +2 score by [(rabbie17, Reddit, 2023-04-15)](https://www.reddit.com/r/MachineLearning/comments/12m9pg0/alternatives_to_pinecone_vector_databases_d/jgb9v8x/) (in reply to ID 12m9pg0):\nChromadb?\n\n## Comment ID kwn4ngu with +1 score by [(LeastIntroduction366, Reddit, 2024-03-26)](https://www.reddit.com/r/MachineLearning/comments/12m9pg0/alternatives_to_pinecone_vector_databases_d/kwn4ngu/) (in reply to ID 12m9pg0):\nIs anyone here because their Pinecone similarity searches are unusably slow?\n\n&#x200B;\n\nI'm using Vertex AI multimodal embeddings, and querying for matches takes too long to be useable.  \n\n\nI have liked using the service, very simple to set up and use, but now running into a roadblock in production because of the performance being not just bad, but unusable.\n\n## Comment ID l28rczp with +1 score by [(fullyautomatedlefty, Reddit, 2024-05-02)](https://www.reddit.com/r/MachineLearning/comments/12m9pg0/alternatives_to_pinecone_vector_databases_d/l28rczp/) (in reply to ID 12m9pg0):\nApertureDB is newer but they're like next-gen, impressed by how fast it is. They have a free docker and community edition with pre-loaded datasets to easily try it out. It's a vector database as well as a graph database, which allows it to speed up projects that use multimodal datasets\n\n## Comment ID l7b11wy with +1 score by [(R53_is_a_database, Reddit, 2024-06-06)](https://www.reddit.com/r/MachineLearning/comments/12m9pg0/alternatives_to_pinecone_vector_databases_d/l7b11wy/) (in reply to ID 12m9pg0):\nThere's my service SvectorDB, if you're a fan of serverless or an AWS user it's made for you\n\n* It has support for CloudFormation / CDK\n* Pricing is transparent, $5 / million queries instead of some opaque \"data scanned \\* vector dimension\" units\n* Scales to 0\n* Supports real-time updates instead of eventual consistency\n* It's also much cheaper than Pinecone\n\n[https://svectordb.com](https://svectordb.com/?utm_source=reddit&utm_medium=organic&utm_campaign=MachineLearning&utm_id=12m9pg0)\n\n## Comment ID lflovq4 with +1 score by [(Search_anything, Reddit, 2024-07-30)](https://www.reddit.com/r/MachineLearning/comments/12m9pg0/alternatives_to_pinecone_vector_databases_d/lflovq4/) (in reply to ID 12m9pg0):\nHere I found a paper about Pinecone side-by-side testing with Table-Search: [https://medium.com/@pavlohrechko/showdown-of-smart-search-systems-pinecone-vs-ai-search-4bd00acc23ad](https://medium.com/@pavlohrechko/showdown-of-smart-search-systems-pinecone-vs-ai-search-4bd00acc23ad)\n\n  \nAlso, Elastic Search showed rather good results for vector databases: [https://medium.com/@artem.mykytyshyn/how-good-is-elastic-for-semantic-search-really-4bcb7719919b](https://medium.com/@artem.mykytyshyn/how-good-is-elastic-for-semantic-search-really-4bcb7719919b)\n\n  \nBut if you want drop your data and it works, you should use solutions like [https://www.table-search.com/](https://www.table-search.com/)\n\nthey have much more advanced and automated ETL\n\n## Comment ID lrgd4la with +1 score by [(alsargent, Reddit, 2024-10-11)](https://www.reddit.com/r/MachineLearning/comments/12m9pg0/alternatives_to_pinecone_vector_databases_d/lrgd4la/) (in reply to ID 12m9pg0):\nDB-Engines has a good list of vector databases, ranked by popularity: [https://db-engines.com/en/ranking/vector+dbms](https://db-engines.com/en/ranking/vector+dbms)\n\n## Comment ID jg9vs4q with +1 score by [(morph3v5, Reddit, 2023-04-14)](https://www.reddit.com/r/MachineLearning/comments/12m9pg0/alternatives_to_pinecone_vector_databases_d/jg9vs4q/) (in reply to ID 12m9pg0):\nNucliaDB\nhttps://github.com/nuclia/nucliadb\n\n## Comment ID jga2q9v with +1 score by [(Mbando, Reddit, 2023-04-14)](https://www.reddit.com/r/MachineLearning/comments/12m9pg0/alternatives_to_pinecone_vector_databases_d/jga2q9v/) (in reply to ID 12m9pg0):\nhttps://github.com/whitead/paper-qa\n\n## Comment ID jgblreo with +1 score by [(ginger_turmeric, Reddit, 2023-04-15)](https://www.reddit.com/r/MachineLearning/comments/12m9pg0/alternatives_to_pinecone_vector_databases_d/jgblreo/) (in reply to ID 12m9pg0):\nI'm trying to use opensearch/elastic search in AWS\n\n### Comment ID k43k0ir with +1 score by [(weez09, Reddit, 2023-10-09)](https://www.reddit.com/r/MachineLearning/comments/12m9pg0/alternatives_to_pinecone_vector_databases_d/k43k0ir/) (in reply to ID jgblreo):\ncurious how this has been for you? I'm also looking to do the same.\n\n#### Comment ID k43sj94 with +1 score by [(ginger_turmeric, Reddit, 2023-10-09)](https://www.reddit.com/r/MachineLearning/comments/12m9pg0/alternatives_to_pinecone_vector_databases_d/k43sj94/) (in reply to ID k43k0ir):\nworked pretty well: https://opensearch.org/docs/latest/search-plugins/knn/index/\n\n## Comment ID jgc6gfn with +1 score by [(mister_chucklez, Reddit, 2023-04-15)](https://www.reddit.com/r/MachineLearning/comments/12m9pg0/alternatives_to_pinecone_vector_databases_d/jgc6gfn/) (in reply to ID 12m9pg0):\nhttps://python.langchain.com/en/latest/modules/indexes/vectorstores.html\n\n## Comment ID jgcdgf5 with +1 score by [(lppier2, Reddit, 2023-04-15)](https://www.reddit.com/r/MachineLearning/comments/12m9pg0/alternatives_to_pinecone_vector_databases_d/jgcdgf5/) (in reply to ID 12m9pg0):\nCan msft cognitive search do this?\n\n## Comment ID jgcs2b8 with +1 score by [(rmyeid, Reddit, 2023-04-15)](https://www.reddit.com/r/MachineLearning/comments/12m9pg0/alternatives_to_pinecone_vector_databases_d/jgcs2b8/) (in reply to ID 12m9pg0):\nCheck out vectara.com, they support vector databases and have friendly api\n\n## Comment ID jgdckdn with +1 score by [(georgeApuiu, Reddit, 2023-04-15)](https://www.reddit.com/r/MachineLearning/comments/12m9pg0/alternatives_to_pinecone_vector_databases_d/jgdckdn/) (in reply to ID 12m9pg0):\n[https://kx.com/](https://kx.com/)\n\n## Comment ID jm8sgtv with +1 score by [(software38, Reddit, 2023-05-30)](https://www.reddit.com/r/MachineLearning/comments/12m9pg0/alternatives_to_pinecone_vector_databases_d/jm8sgtv/) (in reply to ID 12m9pg0):\nAlternatively, for semantic search, semantic similarity, or clustering, you might want to encode your own model based on [Sentence Transformers](https://github.com/UKPLab/sentence-transformers) and deploy it on a CPU or even a GPU for very fast response times.\n\nThis is what NLP Cloud are doing with their [semantic search endpoint](https://docs.nlpcloud.com/#semantic-search) and it works really well.\n\n## Comment ID jmdn6u4 with +1 score by [(clavelnotes, Reddit, 2023-05-31)](https://www.reddit.com/r/MachineLearning/comments/12m9pg0/alternatives_to_pinecone_vector_databases_d/jmdn6u4/) (in reply to ID 12m9pg0):\nPinecone might work great and all but they’re pricey. I just got hit for 190$ for 1 pod 86k vector representations. Does anyone else feel like they're grifting?\n\n### Comment ID jqwnm3j with +2 score by [(CapitalAngle8580, Reddit, 2023-07-06)](https://www.reddit.com/r/MachineLearning/comments/12m9pg0/alternatives_to_pinecone_vector_databases_d/jqwnm3j/) (in reply to ID jmdn6u4):\nSame issue here. I have the $70 plan and got a bill for $123 for one index with 3000 products and only made 9 queries for testing. Seriously! No joke. \n\nJun 1st - Jun 30th 2023\r  \nTotal Cost $123.31\r  \nDaily Average $4.11 WTF? no no no\n\n### Comment ID jqwn6cg with +1 score by [(CapitalAngle8580, Reddit, 2023-07-06)](https://www.reddit.com/r/MachineLearning/comments/12m9pg0/alternatives_to_pinecone_vector_databases_d/jqwn6cg/) (in reply to ID jmdn6u4):\nWeaviate (Open Source)\r  \nMilvus (Open Source)\r  \nFAISS (Open Source)\r  \nPinecone (Cloud Only)\r  \nChroma (Open Source)\r  \nQdrant (Open Source)\n\n## Comment ID jrdkwzo with +1 score by [(Defchaima, Reddit, 2023-07-10)](https://www.reddit.com/r/MachineLearning/comments/12m9pg0/alternatives_to_pinecone_vector_databases_d/jrdkwzo/) (in reply to ID 12m9pg0):\nTry Marqo : https://github.com/marqo-ai/marqo\r  \nhttps://www.marqo.ai/blog/from-iron-manual-to-ironman-augmenting-gpt-with-marqo-for-fast-editable-memory-to-enable-context-aware-question-answering\n\n## Comment ID ju0fjod with +1 score by [(divaaan_technology, Reddit, 2023-07-30)](https://www.reddit.com/r/MachineLearning/comments/12m9pg0/alternatives_to_pinecone_vector_databases_d/ju0fjod/) (in reply to ID 12m9pg0):\nThere is comparison here: https://navidre.medium.com/which-vector-database-should-i-use-a-comparison-cheatsheet-cb330e55fca\n\n## Comment ID junacag with +1 score by [(songrenchu, Reddit, 2023-08-03)](https://www.reddit.com/r/MachineLearning/comments/12m9pg0/alternatives_to_pinecone_vector_databases_d/junacag/) (in reply to ID 12m9pg0):\nWe built an open source vector database leveraging parallel graph traversal indexing, which results in a lower latency. Check it out at https://github.com/epsilla-cloud/vectordb\n\n## Comment ID jwmw5gw with +1 score by [(Feeling-Cow-1848, Reddit, 2023-08-17)](https://www.reddit.com/r/MachineLearning/comments/12m9pg0/alternatives_to_pinecone_vector_databases_d/jwmw5gw/) (in reply to ID 12m9pg0):\nAstraDB https://docs.datastax.com/en/astra-serverless/docs/index.html , it’s nice to see Cassandra database as alternative available now.\n\n## Comment ID jxevazt with +1 score by [(PavanBelagatti, Reddit, 2023-08-23)](https://www.reddit.com/r/MachineLearning/comments/12m9pg0/alternatives_to_pinecone_vector_databases_d/jxevazt/) (in reply to ID 12m9pg0):\nSingleStore can act as a vector database with added capabilities.\n\n## Comment ID jyqp0of with +1 score by [(TopReport133, Reddit, 2023-09-02)](https://www.reddit.com/r/MachineLearning/comments/12m9pg0/alternatives_to_pinecone_vector_databases_d/jyqp0of/) (in reply to ID 12m9pg0):\nAstra db has worked REALLY well on my project love that it’s Cassandra too https://docs.datastax.com/en/astra-serverless/docs/index.html",
      "# Post ID 131kdrx: How I Used Bubble.io with GPT-4 & Pinecone to Unlock Text Similarity Searches - And You Can Too! 🚀🔍 with +3 score by [(nocodeblackbox, Reddit, 2023-04-28)](https://www.reddit.com/r/nocode/comments/131kdrx/how_i_used_bubbleio_with_gpt4_pinecone_to_unlock/)\nHey fellow No-coders,  \n\n\nI recently dove into the world of text similarity searches, combining Pinecone and OpenAI's GPT-4. This powerful combo can be used for a multitude of things when powered with LLMs, like utilizing your own data, querying huge PDFs, and making it less cost-prohibitive. If you're curious about finding relevant text matches for user queries, here's a casual step-by-step guide that I put together. Let's get started!  \n\n\n1. **Turning Text into Numbers with OpenAI Embeddings model** 🧮 First up, we need to convert our text into something computers can understand - embeddings (one-dimensional vector representations). That's where GPT-4 comes in handy. Just send the text to OpenAI's API, and it'll give you the embeddings. Easy peasy!\n2. **Storing the Goods in Pinecone** 🌲 Now we'll stash our embeddings and their metadata (the original text) in Pinecone, a super cool vector search and storage service. We'll create a unique ID string and a namespace to keep things organized. It's like a tidy little drawer for our data.\n3. **Asking Pinecone to Find the Closest Buddies** 🔍 Time for the fun part! With our embeddings and metadata safely stored in Pinecone, we can ask it to find the most relevant matches for any question or prompt. We'll convert the user's query into embeddings using GPT-4 and then search for the closest buddy in Pinecone.\n4. **Showing Off the Results** 🎉 Once Pinecone finds the best matching metadata, we can show it off to the user as the top response to their query. Voilà!  \n\n\n With this knowledge, you can build more complex apps, handle large documents, and even generate responses using GPT-4 and no code tools such as bubble, make, etc.  \n\n\nIf you want a step-by-step tutorial on this check out my youtube video that will be going live here on my channel soon!  \n[https://www.youtube.com/@nocodeblackbox](https://www.youtube.com/@nocodeblackbox)  \n\n\nLet me know what ya'll think and if you would like to see a more complex chain where I combine this with another LLM on top of it\n\n## Comment ID jie4uit with +2 score by [(seed_startup_journey, Reddit, 2023-05-01)](https://www.reddit.com/r/nocode/comments/131kdrx/how_i_used_bubbleio_with_gpt4_pinecone_to_unlock/jie4uit/) (in reply to ID 131kdrx):\nCool! I was following along Pinecone's docs as well:\n\nhttps://docs.pinecone.io/docs/openai\n\nHow much did it cost you to create the embeddings?\n\n### Comment ID jih8lgx with +1 score by [(nocodeblackbox, Reddit, 2023-05-01)](https://www.reddit.com/r/nocode/comments/131kdrx/how_i_used_bubbleio_with_gpt4_pinecone_to_unlock/jih8lgx/) (in reply to ID jie4uit):\nOpenAIs embeddings models are really cheap, you can refer to their pricing here for the ada model  \n\n\n[https://openai.com/pricing](https://openai.com/pricing)\n\n## Comment ID ji27zc4 with +1 score by [(wargio, Reddit, 2023-04-28)](https://www.reddit.com/r/nocode/comments/131kdrx/how_i_used_bubbleio_with_gpt4_pinecone_to_unlock/ji27zc4/) (in reply to ID 131kdrx):\nEasy. Just as I expected 🤓\n\n## Comment ID jip9wc5 with +1 score by [(woodaran, Reddit, 2023-05-03)](https://www.reddit.com/r/nocode/comments/131kdrx/how_i_used_bubbleio_with_gpt4_pinecone_to_unlock/jip9wc5/) (in reply to ID 131kdrx):\nIs it possible to provide Pinecone some data for GPT-4 to reference, but then have GPT-4 generate new data based on the new inputs? \n\n&#x200B;\n\nI.e I provide Pinecone a list of recipes all related to cooking with an air-fryer. Can I then query GPT-4 to search my vector database for these recipes to help it understand the style/way I cook, but to generate a new recipe based on the ingredients I provide in my new query?\n\n### Comment ID jirimdx with +1 score by [(notimeforarcs, Reddit, 2023-05-03)](https://www.reddit.com/r/nocode/comments/131kdrx/how_i_used_bubbleio_with_gpt4_pinecone_to_unlock/jirimdx/) (in reply to ID jip9wc5):\nHey - just wanted to pop in & say that you can do this with Weaviate (an open-source vector DB) using what we call generative search ([https://weaviate.io/developers/weaviate/modules/reader-generator-modules/generative-openai](https://weaviate.io/developers/weaviate/modules/reader-generator-modules/generative-openai)).  \n\n\nDisclaimer - I work for Weaviate! But I am not making anything up obv haha.",
      "# Post ID 107vhoq: Vector databases like Pinecone or Weaviate are all the rage now. Does it make sense to use a vector database as a replacement for a more traditional database like Postgres or Mongo? Why or why not? with +21 score by [(andric, Reddit, 2023-01-10)](https://www.reddit.com/r/SoftwareEngineering/comments/107vhoq/vector_databases_like_pinecone_or_weaviate_are/)\n\n\n## Comment ID j3vi70h with +3 score by [(Kacper-Lukawski, Reddit, 2023-01-11)](https://www.reddit.com/r/SoftwareEngineering/comments/107vhoq/vector_databases_like_pinecone_or_weaviate_are/j3vi70h/) (in reply to ID 107vhoq):\nInsider insights here, as I work for Qdrant ([https://qdrant.tech/](https://qdrant.tech/)). We encourage our users NOT to use their vector database as the primary data source (\"source of truth\"). They are designed to serve the purpose of fast neighbours retrieval based on spatial proximity but do not guarantee data consistency as transactional databases do. There is not even such a concept as a transaction. Moreover, relational data might be hard to model with the vector and corresponding document payload, also, due to the fact we usually vectorize just some of the entities but not all of them (in e-commerce, users and products would typically get vectorized, but not the transactions).\n\nIt is, of course, possible to run even complex queries with built-in filtering mechanisms, but that's not designed to work as OLTP. Vector databases serve a specific purpose, and so they should be used. Full-text search engines have been used for quite a long time, but they are fed with data using an external source to be queried. However, in reality, nobody uses them as a primary database. It's always an additional layer.\n\n### Comment ID mbd7my9 with +1 score by [(makelefani, Reddit, 2025-02-06)](https://www.reddit.com/r/SoftwareEngineering/comments/107vhoq/vector_databases_like_pinecone_or_weaviate_are/mbd7my9/) (in reply to ID j3vi70h):\ninteresting\n\n## Comment ID j45p8qq with +4 score by [(Just_CurioussSss, Reddit, 2023-01-13)](https://www.reddit.com/r/SoftwareEngineering/comments/107vhoq/vector_databases_like_pinecone_or_weaviate_are/j45p8qq/) (in reply to ID 107vhoq):\nI don't think so. Vector databases have a handful of disadvantages. \n\n1. Data structure: Vector databases are optimized for handling high-dimensional vector data, which means they may not be the best choice for data structures that don't fit well into a vector format. For example, data with a large number of categorical variables or data with missing values may not be well-suited for a vector database.\n2. Data management: Vector databases are relatively new, and may lack the same level of robust data management capabilities as more mature databases like Postgres or Mongo. This can make it harder to ensure data integrity and consistency, and can make it more difficult to manage and scale the database over time.\n3. Flexibility: Vector databases can be less flexible in terms of data management and querying than traditional databases. They may not be able to handle a wide range of data types and are not as easily integrated with other systems.\n4. Complexity: Vector databases may require specialized knowledge and additional computational resources to set up and maintain, which can be a barrier for some users.\n5. Scalability: Vector databases, while they are optimized for large-scale similarity search, they might not scale well for very large datasets or high-throughput workloads.\n6. Latency: Vector databases might have a higher latency than other databases, especially when the data size is large, or the queries are complex.\n\nFor disadvantage 3, 4, 5, and 6, Tensor search is an advanced approach for search and retrieval of high-dimensional data that can be an effective solution to some of the disadvantages of using vector databases like Weaviate and Pinecone. Tensor search can handle high-dimensional data with complex relationships and unstructured data, be scalable and provide low-latency search, and can perform similarity search on large datasets without sacrificing performance.\n\n### Comment ID j45xmqm with +1 score by [(Kacper-Lukawski, Reddit, 2023-01-13)](https://www.reddit.com/r/SoftwareEngineering/comments/107vhoq/vector_databases_like_pinecone_or_weaviate_are/j45xmqm/) (in reply to ID j45p8qq):\nCould you please elaborate a bit on how, in our opinion, tensor search solves those disadvantages? That's a bold statement, but tensor search doesn't seem to be much different from vector search, so I don't get why you think it solves anything regarding flexibility, complexity, scalability or latency. To be honest, I don't know if there is any mature enough tensor search tool.\n\nThere are vector databases, like Qdrant, which are scalable and support various data types. Using them requires some knowledge, but that's true for any tool in your stack. Still, these databases are designed to solve a specific problem, and they should be used for those purposes, similarly to graph databases like Neo4j. When it comes to latency, we need to compare apples to apples. I bet none of the RDBMS would beat a vector database for the nearest neighbours search.\n\n## Comment ID jb0v2y2 with +2 score by [(None, Reddit, 2023-03-05)](https://www.reddit.com/r/SoftwareEngineering/comments/107vhoq/vector_databases_like_pinecone_or_weaviate_are/jb0v2y2/) (in reply to ID 107vhoq):\nHow much does it cost to generate embeddings? That’s the question you should care about most. If it’s a lot, then you probably want that in your VPC for now.\n\n### Comment ID jy2nelp with +1 score by [(elie2222, Reddit, 2023-08-28)](https://www.reddit.com/r/SoftwareEngineering/comments/107vhoq/vector_databases_like_pinecone_or_weaviate_are/jy2nelp/) (in reply to ID jb0v2y2):\nHow much you pay OpenAI to generate an embedding is unrelated to the database in which you store the embedding.\n\n#### Comment ID jyyq1nq with +1 score by [(None, Reddit, 2023-09-03)](https://www.reddit.com/r/SoftwareEngineering/comments/107vhoq/vector_databases_like_pinecone_or_weaviate_are/jyyq1nq/) (in reply to ID jy2nelp):\nHaha. That’s not true when some databases lose your data. Weaviate is the only way.\n\n## Comment ID k2gcysa with +2 score by [(lundren10, Reddit, 2023-09-27)](https://www.reddit.com/r/SoftwareEngineering/comments/107vhoq/vector_databases_like_pinecone_or_weaviate_are/k2gcysa/) (in reply to ID 107vhoq):\nNo, a pure vector database cannot replace a traditional DB.  \n\nWhy? Because the only way to retrieve data is via a vector search.  Say your vector search returns an ID to another table where you need to fetch more data. You can't do this with a Pinecone or Weaviate.\n\nUsing a traditional database that has a good vector search capability that can also power the rest of your application reduces the number of production databases you need to run.\n\nMongo, Postgres, and Cassandra are all good options for this.\n\nHere's a good article on the challenges of vector search, and also why you don't need a pure vector DB to solve them.\n\nhttps://thenewstack.io/5-hard-problems-in-vector-search-and-how-cassandra-solves-them/\n\n\n\n## Comment ID jiy9zgs with +1 score by [(None, Reddit, 2023-05-05)](https://www.reddit.com/r/SoftwareEngineering/comments/107vhoq/vector_databases_like_pinecone_or_weaviate_are/jiy9zgs/) (in reply to ID 107vhoq):\nThe use case for a vector database is usually super fast search over vectors, k-nearest neighbors and approximate nearest neighbors or something related and specific to vectors like that.\n\nIf you have billions of documents and want to make a semantic search engine, where the user inputs a question and the database immediately tells you the top 20 document chunks most similar in topic to the question, a vector database can do that while scaling horizontally and doing map-reduce ... for that particular use case relational databases will be hard pressed to outperform.\n\nhttps://www.pinecone.io/learn/vector-database/\n\n## Comment ID jk780zb with +1 score by [(tomhamer5, Reddit, 2023-05-15)](https://www.reddit.com/r/SoftwareEngineering/comments/107vhoq/vector_databases_like_pinecone_or_weaviate_are/jk780zb/) (in reply to ID 107vhoq):\nMarqo (marqo.ai) handles vector search end-to-end (ie. computes the embeddings for you), and you can store metadata in there too like longer text fields. That said, its still not ideal yet to use Marqo as the primary store. Traditional DBs also allow you to do a wider range of different operations like aggregation queries, range queries etc.",
      "# Post ID wwe589: Pinecone: Rust -- A hard decision pays off with +450 score by [(dochtman, Reddit, 2022-08-24)](https://www.reddit.com/r/rust/comments/wwe589/pinecone_rust_a_hard_decision_pays_off/)\n\n\n## Comment ID ilkq2wa with +280 score by [(erlend_sh, Reddit, 2022-08-24)](https://www.reddit.com/r/rust/comments/wwe589/pinecone_rust_a_hard_decision_pays_off/ilkq2wa/) (in reply to ID wwe589):\n>Nevertheless, we reached a tipping point. We decided to move our entire codebase to Rust (and Go for the k8s control plane). Rust seemed to give us all the capabilities we needed, however, there was still one minor problem - **no one on the team knew Rust**.\n\nThis is a pretty remarkable endorsement of Rust. A large-scale rewrite was also its own learn-by-doing project.\n\n### Comment ID illaejd with +56 score by [(ErichDonGubler, Reddit, 2022-08-24)](https://www.reddit.com/r/rust/comments/wwe589/pinecone_rust_a_hard_decision_pays_off/illaejd/) (in reply to ID ilkq2wa):\nAbsolutely. Seems like excellent material for a Quote of the Week for This Week in Rust…so I [submitted](https://users.rust-lang.org/t/twir-quote-of-the-week/328/1283?u=erichdongubler) it! :)\n\n#### Comment ID ilnsj97 with +2 score by [(Overlorde159, Reddit, 2022-08-24)](https://www.reddit.com/r/rust/comments/wwe589/pinecone_rust_a_hard_decision_pays_off/ilnsj97/) (in reply to ID illaejd):\nQuote of the week is a thing?\nWhere might I find a feed of it?\n\n### Comment ID illl3si with +41 score by [(None, Reddit, 2022-08-24)](https://www.reddit.com/r/rust/comments/wwe589/pinecone_rust_a_hard_decision_pays_off/illl3si/) (in reply to ID ilkq2wa):\n[deleted]\n\n#### Comment ID ilm8l0f with +6 score by [(CommunismDoesntWork, Reddit, 2022-08-24)](https://www.reddit.com/r/rust/comments/wwe589/pinecone_rust_a_hard_decision_pays_off/ilm8l0f/) (in reply to ID illl3si):\nIf it's the right technology, it's the right technology. Marketing has nothing to do with it\n\n### Comment ID illdaqc with +3 score by [(MakeWay4Doodles, Reddit, 2022-08-24)](https://www.reddit.com/r/rust/comments/wwe589/pinecone_rust_a_hard_decision_pays_off/illdaqc/) (in reply to ID ilkq2wa):\nOr of the team itself.\n\n## Comment ID illc4al with +80 score by [(U007D, Reddit, 2022-08-24)](https://www.reddit.com/r/rust/comments/wwe589/pinecone_rust_a_hard_decision_pays_off/illc4al/) (in reply to ID wwe589):\nThis was an excellent read.  I would love to hear more detail about how Pinecone got from:\n>  I personally vehemently resisted the idea. Rewrites are notoriously dangerous...\n\n(which is true in my experience also) to:\n\n> Nevertheless, we reached a tipping point. We decided to move our entire codebase to Rust...\n\nToo often, sunk-cost fallacy, risk aversion, fear of the unknown and falling into a \"just one more fix will get us out of this bind\" trap prevail.  How did you avoid this?  (Or maybe you didn't, but learned quickly and reassessed the situation?)\n\nI think many readers who are facing similar situations would love to understand how you navigated this dilemma so successfully.  Kudos to you and your team!\n\nThe confidence with which your team is now able to make code changes sounds like more than just Rust is at play here.  It smells like other best-practices such as good testing practices, SOLID, ports & adapters patterns, etc. are all in use.  I would love to hear more about your take on the sources for the improved velocity and confidence.\n\nAs a leader of a Rust shop myself, I know how good it feels to see the team triumph like this.  Again, congratulations!\n\n### Comment ID ilmuslm with +11 score by [(angelicosphosphoros, Reddit, 2022-08-24)](https://www.reddit.com/r/rust/comments/wwe589/pinecone_rust_a_hard_decision_pays_off/ilmuslm/) (in reply to ID illc4al):\n>good testing practices\n\nBtw, Rust is really good for testing because unit testing in Rust doesn't exclude encapsulation unlike any other industrial language.\n\n#### Comment ID im85pd7 with +1 score by [(None, Reddit, 2022-08-29)](https://www.reddit.com/r/rust/comments/wwe589/pinecone_rust_a_hard_decision_pays_off/im85pd7/) (in reply to ID ilmuslm):\n[deleted]\n\n## Comment ID ill4vy1 with +56 score by [(gregory_k, Reddit, 2022-08-24)](https://www.reddit.com/r/rust/comments/wwe589/pinecone_rust_a_hard_decision_pays_off/ill4vy1/) (in reply to ID wwe589):\nFor anyone in the NYC area, an Engineering Manager from Pinecone will give an in-depth [talk about the Rust rewrite next week](https://www.meetup.com/rust-nyc/events/287821884/).\n\n### Comment ID ilmz08i with +13 score by [(misplaced_my_pants, Reddit, 2022-08-24)](https://www.reddit.com/r/rust/comments/wwe589/pinecone_rust_a_hard_decision_pays_off/ilmz08i/) (in reply to ID ill4vy1):\nWill it be recorded and posted online?\n\n#### Comment ID ilo782i with +4 score by [(gregory_k, Reddit, 2022-08-25)](https://www.reddit.com/r/rust/comments/wwe589/pinecone_rust_a_hard_decision_pays_off/ilo782i/) (in reply to ID ilmz08i):\nI'm not sure, sorry. Maybe you could ask the organizers through Meetup.\n\nI know we'll post a writeup about it on our site some time after.\n\n## Comment ID ilkn4d8 with +61 score by [(gigapiksel, Reddit, 2022-08-24)](https://www.reddit.com/r/rust/comments/wwe589/pinecone_rust_a_hard_decision_pays_off/ilkn4d8/) (in reply to ID wwe589):\nVector similarity search seems like a killer app for rust. You basically need people familiar with the machine learning ecosystem to write low level code. And either you can get the best C++ developers who can handle all of your concurrency thorns, or you can teach python developers rust which guarantees they won’t shoot themselves (and your clients) in the foot. One reason I was hesitant to use pinecone in the past for our production needs was such a heavy reliance on python. Now I will take another look. (Also looking at [qdrant](https://qdrant.tech/)\n\n### Comment ID ilm3m8y with +20 score by [(devzaya, Reddit, 2022-08-24)](https://www.reddit.com/r/rust/comments/wwe589/pinecone_rust_a_hard_decision_pays_off/ilm3m8y/) (in reply to ID ilkn4d8):\nGreetings from [Qdrant](https://github.com/qdrant/qdrant) team. Thanks for mentioning. We made the right decision for Rust from the very beginning. And it pays off not only regarding stability but also performance wise [https://qdrant.tech/benchmarks/](https://qdrant.tech/benchmarks/)\n\n## Comment ID ilmbbwf with +10 score by [(bunoso, Reddit, 2022-08-24)](https://www.reddit.com/r/rust/comments/wwe589/pinecone_rust_a_hard_decision_pays_off/ilmbbwf/) (in reply to ID wwe589):\nTaking a step back here… what is a storage engine for vectors? I’m a little lost at the idea and context for what pine cone would be used for.\n\n### Comment ID ilmlf6e with +30 score by [(gigapiksel, Reddit, 2022-08-24)](https://www.reddit.com/r/rust/comments/wwe589/pinecone_rust_a_hard_decision_pays_off/ilmlf6e/) (in reply to ID ilmbbwf):\nThe basic use case is storing and querying the encodings of your data by neural networks into so called dense vector representations. You can encode data (pictures, text, molecular structures, and so on) in ways that allow you to retrieve that data semantically, e.g. “find pictures best described by this text snippet”, “find solutions to this question”, “find all molecules that might interact with this binding site”. With a vector db you will have to encode your data when you load it but you only have to do it once for each entry. This is easier to set up as a bath process or scheduled job, whereby you can leverage more efficient compute resources to encode. Then you only have to encode the single query datum, but after that querying even large datasets can be extremely fast, eg milliseconds for millions of items. When encoding a datum can take up to a second on a single core, trying to encode both the query and all entries in the database would be comically infeasible. \n\nThese representations are floating point arrays of rank/dimension usually in the range 100-2000, and you query them geometrically, e.g. find me the nearest 20 vectors to this query vector. Using certain approximate nearest neighbour algorithms you can get impressive performance even on a single core with a few gigs of ram.\n\n#### Comment ID ils1jo6 with +1 score by [(privatepublicaccount, Reddit, 2022-08-25)](https://www.reddit.com/r/rust/comments/wwe589/pinecone_rust_a_hard_decision_pays_off/ils1jo6/) (in reply to ID ilmlf6e):\nHow would this compare to e.g. pre-encoding the vectors and storing in a MySQL or Postgres DB? I see the value of vector search, but curious at which point running a custom database/hiring a custom service is necessary/beneficial.\n\n## Comment ID illh0uk with +23 score by [(strangepostinghabits, Reddit, 2022-08-24)](https://www.reddit.com/r/rust/comments/wwe589/pinecone_rust_a_hard_decision_pays_off/illh0uk/) (in reply to ID wwe589):\nSo many developers think learning new languages is going to be as hard as learning your first, but it's not.\n\n### Comment ID illxzy8 with +24 score by [(PM_ME_UR_OBSIDIAN, Reddit, 2022-08-24)](https://www.reddit.com/r/rust/comments/wwe589/pinecone_rust_a_hard_decision_pays_off/illxzy8/) (in reply to ID illh0uk):\nLearning your first functional language is about as hard as learning your first imperative language, and hard on the ego to boot (\"I already know how to code, I don't need this\")\n\nAnd Rust has been described as an ML language in sheep's clothing, so the learning curve can be steep.\n\n#### Comment ID illzkwj with +23 score by [(None, Reddit, 2022-08-24)](https://www.reddit.com/r/rust/comments/wwe589/pinecone_rust_a_hard_decision_pays_off/illzkwj/) (in reply to ID illxzy8):\nThere’s a 3 part Programming languages course on the OSSU curriculum, I always recommend people take it even experienced devs (if they’ve never taken a course like it.)\n\nIt’s starts off in Standard ML, then Racket, then Ruby covering a good amount of theory and practice in languages you’ve most likely never written before.\n\nEver since I took it learning new languages has been pretty trivial. Rust has been pretty easy to learn because of it.\n\n#### Comment ID ilmy47r with +13 score by [(QualitySoftwareGuy, Reddit, 2022-08-24)](https://www.reddit.com/r/rust/comments/wwe589/pinecone_rust_a_hard_decision_pays_off/ilmy47r/) (in reply to ID illxzy8):\nFrom what I've seen, the difficulty of learning Rust isn't really about the functional aspects of it, but more about the system language features such as ownership, borrowing, and lifetimes.\n\n#### Comment ID ilmue5s with +7 score by [(Lich_Hegemon, Reddit, 2022-08-24)](https://www.reddit.com/r/rust/comments/wwe589/pinecone_rust_a_hard_decision_pays_off/ilmue5s/) (in reply to ID illxzy8):\nYup, you learn paradigms, not langauges.\n\n### Comment ID ilmicpi with +7 score by [(iamsaitam, Reddit, 2022-08-24)](https://www.reddit.com/r/rust/comments/wwe589/pinecone_rust_a_hard_decision_pays_off/ilmicpi/) (in reply to ID illh0uk):\nExactly.. unless it’s Rust\n\n## Comment ID ilo9hu1 with +1 score by [(amlunita, Reddit, 2022-08-25)](https://www.reddit.com/r/rust/comments/wwe589/pinecone_rust_a_hard_decision_pays_off/ilo9hu1/) (in reply to ID wwe589):\nOh, I imagine it: C/C++ and Python together bring me remembrances about: \"the slower in the network is the speed limit of connection\". Maybe your positive experience proves it.",
      "# Post ID 12jyes5: What are good techniques for feeding extremely large documents (1000+ pages) into ChatGPT? with +366 score by [(somethingstrang, Reddit, 2023-04-12)](https://www.reddit.com/r/OpenAI/comments/12jyes5/what_are_good_techniques_for_feeding_extremely/)\nI have a use case where I have an extremely large document, let's say 1000+ pages of text (PDF).\n\nI want to be able to search for information by asking ChatGPT to read through the entire corpus and locating that information for me.\n\nSome challenges I see are:\n\n1. ChatGPT / GPT4 have a character limit\n2. Prompt splitting could work, but I worry that ChatGPT may not have enough memory to remember very early information that could provide the necessary context for later information downstream.\n3. It is expensive and time consuming to go through each section one by one after text splitting.\n\nIs there an intelligent way to achieve this with efficiency and scale? Perhaps one way is to use an in-house cheaper and faster LLM to do rough searching and bubble up candidates, and then ask ChatGPT to do the last mile?\n\n&#x200B;\n\n&#x200B;\n\nEDIT: Thanks guys - lots of good suggestions here. Copy pasting some of the info that caught my attention:\n\nLangChain – framework for scalable Generative AI applications- [https://www.pinecone.io/learn/langchain-intro/](https://www.pinecone.io/learn/langchain-intro/)\n\nPineCone – Vector database - [https://www.pinecone.io/](https://www.pinecone.io/)\n\nWeviate – another vector database - [https://weaviate.io/](https://weaviate.io/)\n\nWhat are embeddings?: [https://platform.openai.com/docs/guides/embeddings/what-are-embeddings](https://platform.openai.com/docs/guides/embeddings/what-are-embeddings)\n\nBuilding Q&A system on data tutorial: [https://platform.openai.com/docs/tutorials/web-qa-embeddings](https://platform.openai.com/docs/tutorials/web-qa-embeddings)\n\nAnother tutorial: [https://github.com/openai/openai-cookbook/blob/main/examples/Question\\_answering\\_using\\_embeddings.ipynb](https://github.com/openai/openai-cookbook/blob/main/examples/Question_answering_using_embeddings.ipynb)\n\nYoutube Tutorial on 300-page PDF with LangChain and OpenAI: [https://www.youtube.com/watch?v=h0DHDp1FbmQ](https://www.youtube.com/watch?v=h0DHDp1FbmQ)\n\nCombining all the techniques in one open-source library: [https://github.com/mayooear/gpt4-pdf-chatbot-langchain](https://github.com/mayooear/gpt4-pdf-chatbot-langchain)\n\n&#x200B;\n\nThe steps are (can be supported via LangChain framework):\n\n1.\tTurn the document into embeddings (maybe using Ada)\n\n2.\tStore those embeddings in a vector db (like pinecone)\n\n3.\tWhen user makes a question, use Ada to turn their question into embeddings\n\n4.\tUse those embeddings to search the vector db (via cosine similarity)\n\n5.\tReturn all the relevant strings from the vector db\n\n6.\tConstruct a prompt gpt 3-4 to answer the original user question using info contained in the returned strings from the vector db\n\n7.\tSend result to user\n\n&#x200B;\n\n## Comment ID jg118b6 with +46 score by [(ExtremelyQualified, Reddit, 2023-04-12)](https://www.reddit.com/r/OpenAI/comments/12jyes5/what_are_good_techniques_for_feeding_extremely/jg118b6/) (in reply to ID 12jyes5):\nBasically:\n\nTurn the document into embeddings (maybe using Ada) \n\nStore those embeddings in a vector db (like pinecone)\n\nWhen your user makes a question, use Ada to turn their question into embeddings\n\nUse those embeddings to search the vector db \n\nReturn all the relevant strings from the vector db\n\nConstruct a prompt asking big daddy gpt 3-4 to answer the original user question using info contained in the returned strings from the vector db \n\nSend result to user\n\n### Comment ID jg17iyf with +6 score by [(somethingstrang, Reddit, 2023-04-13)](https://www.reddit.com/r/OpenAI/comments/12jyes5/what_are_good_techniques_for_feeding_extremely/jg17iyf/) (in reply to ID jg118b6):\nThanks, I have a follow on question. Does this mean that the cosine similarity search against the vector database will be the biggest source of accuracy error? You aren’t using GPT4’s ability to deduce the logic of the question at this step, correct? \n\nFor example if my question was “find me examples of real time strategy video games”, the embedding match won’t necessary return “StarCraft”\n\nCurious about your thoughts here\n\n#### Comment ID jg1a8tb with +8 score by [(majaha95, Reddit, 2023-04-13)](https://www.reddit.com/r/OpenAI/comments/12jyes5/what_are_good_techniques_for_feeding_extremely/jg1a8tb/) (in reply to ID jg17iyf):\nYou weren't responding to me, and I'm no expert, but I've grappled with similar situations as well.\n\nI suppose the hope is that either StarCraft will be placed into the vector space similarly to \"real time strategy games,\" and so it would actually be returned by the embedding. And if that wasn't the case, then the text context around where \"StarCraft\" shows up would hopefully push the embeddings in the right direction.\n\nSo I think the bigger question is how big the window ought to be for the embeddings. And I think that boils down to the nature of your data and priorities.\n\n#### Comment ID jg51agw with +1 score by [(justdoitanddont, Reddit, 2023-04-13)](https://www.reddit.com/r/OpenAI/comments/12jyes5/what_are_good_techniques_for_feeding_extremely/jg51agw/) (in reply to ID jg17iyf):\nI dont know about this specific example but suspect that starcraft will be similar to strategy video games on many dimensions (vector attributes). So the match should be significantly better than just keyword match. I have tried other examples and have seen this concept work.\n\n### Comment ID jyu55bf with +3 score by [(OptimBro, Reddit, 2023-09-02)](https://www.reddit.com/r/OpenAI/comments/12jyes5/what_are_good_techniques_for_feeding_extremely/jyu55bf/) (in reply to ID jg118b6):\n>Turn the document into embeddings (maybe using Ada)  \n>  \n>Store those embeddings in a vector db (like pinecone)  \n>  \n>When your user makes a question, use Ada to turn their question into embeddings  \n>  \n>Use those embeddings to search the vector db  \n>  \n>Return all the relevant strings from the vector db  \n>  \n>Construct a prompt asking big daddy gpt 3-4 to answer the original user question using info contained in the returned strings from the vector db  \n>  \n>Send result to user\n\nIt may sound silly but I have gone through many tutorials on YouTube, and this comment taught me how to actually use and implement embeddins, vectors. Thank you.\n\n#### Comment ID jyulfoo with +1 score by [(ExtremelyQualified, Reddit, 2023-09-02)](https://www.reddit.com/r/OpenAI/comments/12jyes5/what_are_good_techniques_for_feeding_extremely/jyulfoo/) (in reply to ID jyu55bf):\nAw thanks!\n\n### Comment ID jg7adxp with +1 score by [(kiropolo, Reddit, 2023-04-14)](https://www.reddit.com/r/OpenAI/comments/12jyes5/what_are_good_techniques_for_feeding_extremely/jg7adxp/) (in reply to ID jg118b6):\nBut what if it’s reasoning related question, and not necessarily containing the same words?\n\n#### Comment ID jg7str3 with +2 score by [(ExtremelyQualified, Reddit, 2023-04-14)](https://www.reddit.com/r/OpenAI/comments/12jyes5/what_are_good_techniques_for_feeding_extremely/jg7str3/) (in reply to ID jg7adxp):\nIn theory, the embeddings are about placing text in multidimensional conceptual space in a similar way that the LLM itself has lots of text arranged in multidimensional conceptual space. So a good amount of the reasoning ends up coming from that. You’re searching for fuzzy concept matches more than searching for word matches. But tbh I don’t know enough to really answer the question of what kinds of reasoning you’re missing out on by doing it this way.\n\n### Comment ID keglwzv with +1 score by [(piratekid79, Reddit, 2023-12-22)](https://www.reddit.com/r/OpenAI/comments/12jyes5/what_are_good_techniques_for_feeding_extremely/keglwzv/) (in reply to ID jg118b6):\nhi, **in retrievalQa from langchain,we have a retriever that retrieves docs from a vector db and provides a context to the llm,lets say im using gpt3.5 whose max tokens is 4096... how do i handle huge context to be sent to it ?**\n\n## Comment ID jg0l716 with +71 score by [(Zulugod94, Reddit, 2023-04-12)](https://www.reddit.com/r/OpenAI/comments/12jyes5/what_are_good_techniques_for_feeding_extremely/jg0l716/) (in reply to ID 12jyes5):\nI believe what you are lookin for is called embedding. I just did this for an in house document that was roughly 200~ pages. You embedd the text, turning chunks of text into vector values. You can then do math against these vectors to pull the top X results to feed in with your question. In this instance I can ask GPT questions about things that only exist within my document, find the top 5 sections related to the question, and feed those into the prompt so it has context to work from. You will be limited by gpt 3.5s token limit, but it does work well.\n\n### Comment ID jg0vvwy with +17 score by [(somethingstrang, Reddit, 2023-04-12)](https://www.reddit.com/r/OpenAI/comments/12jyes5/what_are_good_techniques_for_feeding_extremely/jg0vvwy/) (in reply to ID jg0l716):\nDoes it work well with GPT4? For my use case GPT4 seems to be the only one with acceptable performance out the box\n\n#### Comment ID jg0w4ok with +23 score by [(Zulugod94, Reddit, 2023-04-12)](https://www.reddit.com/r/OpenAI/comments/12jyes5/what_are_good_techniques_for_feeding_extremely/jg0w4ok/) (in reply to ID jg0vvwy):\nFrom what I've seen it works exceptionally well with gpt4. Both because of the additional intelligence of 4 but also the 8k token limit it a huge help.\n\n### Comment ID jg1ie4a with +5 score by [(Formal_Afternoon8263, Reddit, 2023-04-13)](https://www.reddit.com/r/OpenAI/comments/12jyes5/what_are_good_techniques_for_feeding_extremely/jg1ie4a/) (in reply to ID jg0l716):\nGot any material on this? Sounds super interesting. And do you feed gpt the embeddings or the result of the embeddings decoded?\n\n#### Comment ID jg2t82u with +6 score by [(Zulugod94, Reddit, 2023-04-13)](https://www.reddit.com/r/OpenAI/comments/12jyes5/what_are_good_techniques_for_feeding_extremely/jg2t82u/) (in reply to ID jg1ie4a):\nOn openais website they have a link to their \"OpenAICookbook\" that has a nice step by step breakdown of the process. I will try and link it when I get to work!\n\nThe embeddings are just a value of \"context\" of the text, so I like to think of the embedded value as simply the database key. If I ask the questions \"What are sales operations?\" It embedds that question (let's say a value of 0.2 just as an example) now I can search the database for the closest values to 0.2, and grab the database row number with that text. I hope that kinda explains a little better!\n\n### Comment ID jg0ybt4 with +3 score by [(7ewis, Reddit, 2023-04-12)](https://www.reddit.com/r/OpenAI/comments/12jyes5/what_are_good_techniques_for_feeding_extremely/jg0ybt4/) (in reply to ID jg0l716):\nFrom what I've read it can't add context to the things in your document though can it? Using its knowledge from elsewhere in addition to what you've provided\n\n#### Comment ID jg0zp1s with +3 score by [(Zulugod94, Reddit, 2023-04-12)](https://www.reddit.com/r/OpenAI/comments/12jyes5/what_are_good_techniques_for_feeding_extremely/jg0zp1s/) (in reply to ID jg0ybt4):\nI have noticed on the non-chat api calls it does seem to just reformat the exact information that's in the pdf. For my use case, even that works really well because it formats the information in a much more human readable way. Using the 3.5 chat api, it will add in additional knowledge, sometime to a fault! Lol with chat you are just including the information in the prompt itself so it's no different then giving the AI some information as you talk to it. The format goes as roughly like\n\n \"Please use the following information to help answer any questions if you are not confident you have the knowledge already. \n\nContext: (data pulled from embedding query)\n\n(Your conversational input here)\"\n\nThe biggest limitation for chat for me, is I have to feed it almost 3,300 tokens of documentation at a time, as it's annoying big chunks of text for working with an ERP system. I have used it to help me work on my programs by embedding all my classes and methods (100-200 tokens a pop) and that has worked incredibly well. I can't imagine gpt 4 + the 8k limit, that would make a massive difference, nevermind the 32k limit.\n\n### Comment ID jg17rrq with +2 score by [(whoiskjl, Reddit, 2023-04-13)](https://www.reddit.com/r/OpenAI/comments/12jyes5/what_are_good_techniques_for_feeding_extremely/jg17rrq/) (in reply to ID jg0l716):\nWow thank you for this!\n\n### Comment ID jg1iot2 with +2 score by [(hryipcdxeoyqufcc, Reddit, 2023-04-13)](https://www.reddit.com/r/OpenAI/comments/12jyes5/what_are_good_techniques_for_feeding_extremely/jg1iot2/) (in reply to ID jg0l716):\nWhy doesn't GPT just do this automatically?\n\n#### Comment ID jg3st4v with +2 score by [(synystar, Reddit, 2023-04-13)](https://www.reddit.com/r/OpenAI/comments/12jyes5/what_are_good_techniques_for_feeding_extremely/jg3st4v/) (in reply to ID jg1iot2):\nIt's not designed to store additional knowledge to what it was trained on.  That has been left up to other tools. It is simply an LLM trained on a very large amount of data and if you wanted it to be trained on additional data you would have to retrain the entire model everytime you added more data.  These solutions allow you to feed it current data for a particular purpose and store the output of the conversation. It wouldn't necessarily know where to find the correct data if you didn't know so allowing it to just go find it could be dangerous to your purposes anyway.\n\n### Comment ID jg2h8zd with +2 score by [(None, Reddit, 2023-04-13)](https://www.reddit.com/r/OpenAI/comments/12jyes5/what_are_good_techniques_for_feeding_extremely/jg2h8zd/) (in reply to ID jg0l716):\nI thought embeddings didn’t work with 3.5 or 4\n\n#### Comment ID jg2skiy with +3 score by [(Zulugod94, Reddit, 2023-04-13)](https://www.reddit.com/r/OpenAI/comments/12jyes5/what_are_good_techniques_for_feeding_extremely/jg2skiy/) (in reply to ID jg2h8zd):\nThe embeddings themselves are just a static database of vectors, they simply represent the 'value' of chunks of text so you can search for related chunks quickly. There is only one model that produces the actual embeddings text-embedding-ada-002. Once you have the embedding, you are only feeding back text so it can work theoretically with any of the llm models, assuming you can fit it the text within the token limits~\n\n### Comment ID jg2mqnk with +2 score by [(transplantedRedneck, Reddit, 2023-04-13)](https://www.reddit.com/r/OpenAI/comments/12jyes5/what_are_good_techniques_for_feeding_extremely/jg2mqnk/) (in reply to ID jg0l716):\nEmbedding only supported on GPT3 series models\n\n#### Comment ID jg2tit9 with +2 score by [(Zulugod94, Reddit, 2023-04-13)](https://www.reddit.com/r/OpenAI/comments/12jyes5/what_are_good_techniques_for_feeding_extremely/jg2tit9/) (in reply to ID jg2mqnk):\nThe only models capable of producing the actual embeddings are gpt 3 based if that's what you mean, text-embedding-ada-002. All models are capable of 'interacting' with the embeddings as its all done in your own code prior to the query being sent to the API\n\n#### Comment ID jg4m53o with +2 score by [(huffalump1, Reddit, 2023-04-13)](https://www.reddit.com/r/OpenAI/comments/12jyes5/what_are_good_techniques_for_feeding_extremely/jg4m53o/) (in reply to ID jg2mqnk):\nThis isn't embeddings for the gpt-3.5 model though. \n\nIt's \"augmented retrieval\", an efficient way to search and add context to your prompt. \n\nThe easy way would be just to search your documents for the prompt words, but that can miss things. \n\nSo, you tokenize the documents (aka generate embeddings), store the tokens in a vector db, tokenize the prompt, and search the db for the prompt tokens. \n\nThen, take the top few matching clumps of tokens, convert it back to text, and insert THAT into your prompt. \n\nIt's a good way (for now) to easily allow gpt-3.5/4 to \"read\" a large amount of documents. Yes, it doesn't \"read\" the entire thing for each prompt, but returning relevant information this quickly is still very very helpful. \n\nFor example, if you ask a specific technical question, this method could append the relevant parts of your docs to the manual - without having to copy/paste it yourself, and without having to finetune a whole model. (which is not possible yet)\n\n### Comment ID jg33izf with +2 score by [(None, Reddit, 2023-04-13)](https://www.reddit.com/r/OpenAI/comments/12jyes5/what_are_good_techniques_for_feeding_extremely/jg33izf/) (in reply to ID jg0l716):\nAny way for a nontechnical person to do it ?\n\n#### Comment ID jg50wi7 with +2 score by [(justdoitanddont, Reddit, 2023-04-13)](https://www.reddit.com/r/OpenAI/comments/12jyes5/what_are_good_techniques_for_feeding_extremely/jg50wi7/) (in reply to ID jg33izf):\nI can host my code on cloud if you like.\n\n#### Comment ID jg5gip7 with +1 score by [(sorcerykid, Reddit, 2023-04-13)](https://www.reddit.com/r/OpenAI/comments/12jyes5/what_are_good_techniques_for_feeding_extremely/jg5gip7/) (in reply to ID jg33izf):\nI'm even tech savvy, yet this all sounds so complicated. There has to be some kind of boilerplate that people can adapt, at least I certainly hope :)\n\n## Comment ID jg0r4ys with +14 score by [(jameshines10, Reddit, 2023-04-12)](https://www.reddit.com/r/OpenAI/comments/12jyes5/what_are_good_techniques_for_feeding_extremely/jg0r4ys/) (in reply to ID 12jyes5):\nThe Data Independent YouTube channel had a video on using OpenAI and LangChain to \"chat\" with a 300-page PDF.\nhttps://youtu.be/h0DHDp1FbmQ\n\n### Comment ID jg0w3es with +4 score by [(somethingstrang, Reddit, 2023-04-12)](https://www.reddit.com/r/OpenAI/comments/12jyes5/what_are_good_techniques_for_feeding_extremely/jg0w3es/) (in reply to ID jg0r4ys):\nWould this be ok with arbitrarily large documents like 1000+ pages?\n\n#### Comment ID jg0xg4n with +3 score by [(jameshines10, Reddit, 2023-04-12)](https://www.reddit.com/r/OpenAI/comments/12jyes5/what_are_good_techniques_for_feeding_extremely/jg0xg4n/) (in reply to ID jg0w3es):\nIt should be. The only real limits would be budget. I don't think you'd be able to process a data set that size using the free tiers on the various platforms you'd be using. Makes me think we'll quickly become a society where you get access to better and better models as you pay a premium.\n\n### Comment ID jg3f3s7 with +1 score by [(StrongBoyTwoFive, Reddit, 2023-04-13)](https://www.reddit.com/r/OpenAI/comments/12jyes5/what_are_good_techniques_for_feeding_extremely/jg3f3s7/) (in reply to ID jg0r4ys):\ni wonder about bigger documents like 10,000 pages\n\n#### Comment ID jg51suf with +1 score by [(justdoitanddont, Reddit, 2023-04-13)](https://www.reddit.com/r/OpenAI/comments/12jyes5/what_are_good_techniques_for_feeding_extremely/jg51suf/) (in reply to ID jg3f3s7):\nThe way to do this varies between 1000 page documents vs 100 gig document folder. In my experience, the same architecture works great on 1000 page doc but fails flat on the 100 gig use case (and ironically vice versa).\n\n## Comment ID jg0g2te with +41 score by [(fictioninquire, Reddit, 2023-04-12)](https://www.reddit.com/r/OpenAI/comments/12jyes5/what_are_good_techniques_for_feeding_extremely/jg0g2te/) (in reply to ID 12jyes5):\nGPT-4 & LangChain Tutorial: How to Chat With A 56-Page PDF Document (w/Pinecone)\r  \n[https://twitter.com/attractfunding/status/1642940285587013646](https://twitter.com/attractfunding/status/1642940285587013646)\n\n### Comment ID jg0z8ve with +55 score by [(None, Reddit, 2023-04-12)](https://www.reddit.com/r/OpenAI/comments/12jyes5/what_are_good_techniques_for_feeding_extremely/jg0z8ve/) (in reply to ID jg0g2te):\na link to a twitter post that links to a youtube video.  Here, enjoy an elon-free way to get it: https://www.youtube.com/watch?v=ih9PBGVVOO4\n\n#### Comment ID jg3f923 with +2 score by [(fictioninquire, Reddit, 2023-04-13)](https://www.reddit.com/r/OpenAI/comments/12jyes5/what_are_good_techniques_for_feeding_extremely/jg3f923/) (in reply to ID jg0z8ve):\nHaha thought it was the same thread I saw earlier, but it's only a link and some introduction text\n\n### Comment ID jg0vodz with +4 score by [(somethingstrang, Reddit, 2023-04-12)](https://www.reddit.com/r/OpenAI/comments/12jyes5/what_are_good_techniques_for_feeding_extremely/jg0vodz/) (in reply to ID jg0g2te):\nThanks I will check it out. Seems to have potential\n\n## Comment ID jg0peys with +7 score by [(MatchaGaucho, Reddit, 2023-04-12)](https://www.reddit.com/r/OpenAI/comments/12jyes5/what_are_good_techniques_for_feeding_extremely/jg0peys/) (in reply to ID 12jyes5):\nIt varies by AI, but OpenAI solutions are premised on embeddings [https://platform.openai.com/docs/guides/embeddings](https://platform.openai.com/docs/guides/embeddings) \n\nLarge text searches are actually cosine-similarity searches on split chunks, using the most similar chunks in the completion prompt.\n\n## Comment ID jg11jjm with +9 score by [(Icanteven______, Reddit, 2023-04-12)](https://www.reddit.com/r/OpenAI/comments/12jyes5/what_are_good_techniques_for_feeding_extremely/jg11jjm/) (in reply to ID 12jyes5):\nOther people have said, but you want to use embeddings for something that big.\n\nYou can use LangChain to assist, but essentially you’re calling the OpenAI embeddings api to map your large document into a bunch of numbered vector documents that “embed” their semantic meaning. Each document can represent a chunk of the document with like 1000 characters or something.\n\nYou can then store those embeddings in a vectorDB like chroma or pinecone.\n\nThen when you have a query you want to run against the large document, you take your question, calculate the embedding of it (ie vectorize it) and then ask the vector db which of the many 1000 character chunks are most relevant to your question, and it’ll return like 4 or 5 of them along with confidence values of how relevant they are (note all these numbers are configurable).\n\nYou can then feed those chunks into the context of your gpt query using whichever model you like and ask it questions on it.\n\n### Comment ID jg1fog2 with +7 score by [(Strel0k, Reddit, 2023-04-13)](https://www.reddit.com/r/OpenAI/comments/12jyes5/what_are_good_techniques_for_feeding_extremely/jg1fog2/) (in reply to ID jg11jjm):\nA big problem that bothers me is because you are breaking up a doc into chunks - what happens when your answer lies somewhere between two chunks? I feel like this is an invitation for the LLM to hallucinate to fill the gap.\n\nFor example, if the answer to your question is 70% in Chunk A and 30% in chunk B, you're basically at the mercy of the semantic search to decide that both chunks are highly relevant. Or at the mercy of the LLM to fill in the missing 30%.\n\nYes I know you can do some chunk overlap but you're still at the mercy of the semantic search deciding the end section of some explanation is related enough to be included in the top 3-5 ranked chunks.\n\n#### Comment ID jg1jjnb with +3 score by [(Icanteven______, Reddit, 2023-04-13)](https://www.reddit.com/r/OpenAI/comments/12jyes5/what_are_good_techniques_for_feeding_extremely/jg1jjnb/) (in reply to ID jg1fog2):\nThat’s a great point.\n\nI wonder if there’s wiggle room to take advantage of the confidence values that are returned back? Like…if the confidence values don’t meet some threshold, you can assume that situation may have occurred and use a different embedding that has overlap. In fact…why not just use several embeddings that use phase shifted chunks? It depends on the size of the thing you’re embedding of course as it will cost more space to store. For something like a 1000 page document though? You could do it. Then you could map/reduce query all the embeddings and get the chunks that are most relevant across all the embeddings, and hopefully you get more relevant answers that aren’t spliced across the chunks.\n\n#### Comment ID jg1r3hu with +1 score by [(None, Reddit, 2023-04-13)](https://www.reddit.com/r/OpenAI/comments/12jyes5/what_are_good_techniques_for_feeding_extremely/jg1r3hu/) (in reply to ID jg1fog2):\n[deleted]\n\n#### Comment ID jg1jgug with +1 score by [(PussPussMcSquishy, Reddit, 2023-04-13)](https://www.reddit.com/r/OpenAI/comments/12jyes5/what_are_good_techniques_for_feeding_extremely/jg1jgug/) (in reply to ID jg1fog2):\nIs that what chunk overlap means? I’m building a chat bot like this and I see that prop from something like my text splitter or tokenizer or when I create the embedding. Can’t remember.\n\n#### Comment ID jg520eq with +1 score by [(justdoitanddont, Reddit, 2023-04-13)](https://www.reddit.com/r/OpenAI/comments/12jyes5/what_are_good_techniques_for_feeding_extremely/jg520eq/) (in reply to ID jg1fog2):\nChunking text has to be smart in my experience.\n\n## Comment ID jg1p2xn with +5 score by [(SHKEVE, Reddit, 2023-04-13)](https://www.reddit.com/r/OpenAI/comments/12jyes5/what_are_good_techniques_for_feeding_extremely/jg1p2xn/) (in reply to ID 12jyes5):\nI’ve used [LlamaIndex](https://gpt-index.readthedocs.io/en/latest/index.html) to be able to load hundreds of pages pretty easily. I was just doing this for my own education and to use up my free API credits so while the documentation does have sections on improving efficiency, I can’t speak of how scalable this is if you’re looking to productize this or something.\n\n## Comment ID jg1swjp with +5 score by [(notimeforarcs, Reddit, 2023-04-13)](https://www.reddit.com/r/OpenAI/comments/12jyes5/what_are_good_techniques_for_feeding_extremely/jg1swjp/) (in reply to ID 12jyes5):\nI see people have already mentioned turning the data into embeddings. \n\nThere is an easy way to do this, with ChatGPT allowing plugins now. There are plugins to hook up ChatGPT with a vector database like Weaviate so that it ‘remembers’ stuff. \n\n(I think ChatGPT plugins are still in beta or some sort of limited availability though but I imagine it’ll be widely available soon. \n\nIf you want to read more: https://weaviate.io/blog/weaviate-retrieval-plugin\n\nDisclaimer: I work at Weaviate. (Edit: spelling)\n\n### Comment ID k4bdoc8 with +1 score by [(bvjz, Reddit, 2023-10-10)](https://www.reddit.com/r/OpenAI/comments/12jyes5/what_are_good_techniques_for_feeding_extremely/k4bdoc8/) (in reply to ID jg1swjp):\nHello! Does this plugin work with a local LLM alongside Langchain?\nI don't like the idea of my data being exposed to OpenAI\n\n## Comment ID jg1f1n5 with +6 score by [(brainhack3r, Reddit, 2023-04-13)](https://www.reddit.com/r/OpenAI/comments/12jyes5/what_are_good_techniques_for_feeding_extremely/jg1f1n5/) (in reply to ID 12jyes5):\nHere... totally hooking you up here.\n\nhttps://www.youtube.com/watch?v=ih9PBGVVOO4&ab_channel=Chatwithdata\n\nhttps://github.com/mayooear/gpt4-pdf-chatbot-langchain\n\nI was looking at building something similar and already started but will probably land some of my changes there.\n\nBasically, you need to create a vector index of the underlying documents, then insert the into the prompt for every question.\n\nThis will do it for you though.  It will index the data into Pinecone and then they have a chat UI for it.  \n\nI think if one created a context injector bot for OpenAI that you could easily make it work with ChatGPT.\n\nI'm going to be setting this up and dumping like 10GB of PDFs into it :). They're all going to be on math and AI so I was actually gonna hook you guys up and share it here!\n\n### Comment ID jg1jgpa with +1 score by [(somethingstrang, Reddit, 2023-04-13)](https://www.reddit.com/r/OpenAI/comments/12jyes5/what_are_good_techniques_for_feeding_extremely/jg1jgpa/) (in reply to ID jg1f1n5):\nSick man hook me up once you got it\n\n### Comment ID jg2cion with +1 score by [(garb-aholic-, Reddit, 2023-04-13)](https://www.reddit.com/r/OpenAI/comments/12jyes5/what_are_good_techniques_for_feeding_extremely/jg2cion/) (in reply to ID jg1f1n5):\nThank my man. Pls respond once you got it.\n\n### Comment ID jiom9yy with +1 score by [(Apatrickegan, Reddit, 2023-05-03)](https://www.reddit.com/r/OpenAI/comments/12jyes5/what_are_good_techniques_for_feeding_extremely/jiom9yy/) (in reply to ID jg1f1n5):\nHey dude, how’s it going?  I’m trying to do the same thing primarily with an extremely large number of files.  Ironically, I got hit with a pretty substantial bill for a very small number of files from PineCone.  The vector and bedding seems to be just a repository of the numbers. Are there less expensive options such as weaviate?  Or even a local Json DB?\n\nI literally just want to catalogue my own hard drive.\n\n#### Comment ID jipezn8 with +1 score by [(brainhack3r, Reddit, 2023-05-03)](https://www.reddit.com/r/OpenAI/comments/12jyes5/what_are_good_techniques_for_feeding_extremely/jipezn8/) (in reply to ID jiom9yy):\nJust set up a vector DB on your local machine.\n\n#### Comment ID jiri93d with +1 score by [(notimeforarcs, Reddit, 2023-05-03)](https://www.reddit.com/r/OpenAI/comments/12jyes5/what_are_good_techniques_for_feeding_extremely/jiri93d/) (in reply to ID jiom9yy):\nYeah you can run Weaviate on your own machine which is free! \n\nIf you have lots of pages, I would not recommend a local JSON DB or even a numpy array, because one of the things that vector DBs like Weaviate lets you do is to speed up queries through vector indexing. Without it, queries will become too slow to be viable at any kind of medium/large dataset size.\n\n## Comment ID jg0hrph with +3 score by [(tgaume, Reddit, 2023-04-12)](https://www.reddit.com/r/OpenAI/comments/12jyes5/what_are_good_techniques_for_feeding_extremely/jg0hrph/) (in reply to ID 12jyes5):\nI need this for meeting agendas that are several hundred pages and include drawings.\n\n### Comment ID jg0okeu with +3 score by [(McTech0911, Reddit, 2023-04-12)](https://www.reddit.com/r/OpenAI/comments/12jyes5/what_are_good_techniques_for_feeding_extremely/jg0okeu/) (in reply to ID jg0hrph):\nWtf meetings need hundreds of pages for an agenda!?\n\n#### Comment ID jg0s55u with +9 score by [(tgaume, Reddit, 2023-04-12)](https://www.reddit.com/r/OpenAI/comments/12jyes5/what_are_good_techniques_for_feeding_extremely/jg0s55u/) (in reply to ID jg0okeu):\nCity Council Agendas.  They contain all the legal descriptions, site plans, and other supporting documentation for any item before the City Council.\n\n## Comment ID jg1spqc with +2 score by [(plantsnotevolution, Reddit, 2023-04-13)](https://www.reddit.com/r/OpenAI/comments/12jyes5/what_are_good_techniques_for_feeding_extremely/jg1spqc/) (in reply to ID 12jyes5):\nThey say a picture is worth a thousand words…..\n\nOne of the challenges of using GPT-4 for text analysis is that it has a token limit on text inputs, which means that it cannot process very long documents or texts. However, there may be a way to overcome this limitation by converting the text into JPEGs or other image formats. For example, one could use Photoshop or a similar software to create a super high resolution image that contains 100 pages of text reduced in size but still visible upon zooming in. This way, one could fit more information into one image and feed it to GPT-4 as an input. GPT-4 can accept images as prompts and extract text from them using optical character recognition (OCR) or other techniques. This might enable GPT-4 to analyze large documents or texts without surpassing the token limit. However, this idea is not tested and may have some drawbacks, such as loss of quality or accuracy. I don’t have access to GPT-4 plus, so I cannot verify if this works. Maybe someone else who has access can try this idea and share their results.\n\n*full transparency, this is my idea but I used Bing chat mode to express my thought a little more coherently.*\n\n### Comment ID jg52lmh with +1 score by [(justdoitanddont, Reddit, 2023-04-13)](https://www.reddit.com/r/OpenAI/comments/12jyes5/what_are_good_techniques_for_feeding_extremely/jg52lmh/) (in reply to ID jg1spqc):\nText based vectors are going to be lot smaller while being higher fidelity than image based vectors\n\n## Comment ID jg4v343 with +2 score by [(justdoitanddont, Reddit, 2023-04-13)](https://www.reddit.com/r/OpenAI/comments/12jyes5/what_are_good_techniques_for_feeding_extremely/jg4v343/) (in reply to ID 12jyes5):\nThere are lots of answers here and they all fit different use cases. I built multiple variations to satisfy specific needs and found some tool and setting combinations, while work great for one use case may end up being totally unworkable for another use case (ex: marketing vs legal). Hqving said that, Langchain or llama index (or pinecone or some other vector db) to store the vector (embedding) version of your text. Then get vector representation of your query, do a search on the vector store, then send the subset to chat gpt (very broad stroke but this is the gist of it). There are techniques to improve the quality of the results but they are all dependent on the use case / needs and constraints. Happy to provide a more specific path if I understand more of the problem specifics.\n\n## Comment ID jg140d4 with +2 score by [(SexOtter, Reddit, 2023-04-13)](https://www.reddit.com/r/OpenAI/comments/12jyes5/what_are_good_techniques_for_feeding_extremely/jg140d4/) (in reply to ID 12jyes5):\nSomeone feed in the book good with words by Patrick Barry so we can have incredible grammar output by chatgpt\n\n### Comment ID jgcvvrv with +1 score by [(justdoitanddont, Reddit, 2023-04-15)](https://www.reddit.com/r/OpenAI/comments/12jyes5/what_are_good_techniques_for_feeding_extremely/jgcvvrv/) (in reply to ID jg140d4):\nWell this is a scenario where I would fine-tune a open ai model with the contents of this book. This approach makes it reusable than using embedding. But this approach costs more money (to fine tune and also per token cost although you will need less tokens than embedding approach)\n\n## Comment ID jg185bh with +1 score by [(ChiaraStellata, Reddit, 2023-04-13)](https://www.reddit.com/r/OpenAI/comments/12jyes5/what_are_good_techniques_for_feeding_extremely/jg185bh/) (in reply to ID 12jyes5):\nMy approach: I break it into chunks, summarize first chunk, then take that summary and feed it back in when summarizing the next chunk, to provide context, then feed that summary in when summarizing the next chunk, and so on. Then concatenate the results. I overlap the chunks a bit at the boundaries for some additional context. Then I repeat the entire process on the new summarized doc, until I get down to the length I want. Once it all fits in the context buffer, I usually ask for a detailed long summary so I can get a nice final summary of one page or so.\n\nFor most purposes this is adequate, but to address super long-distance references you might want to keep a running summary of all previous sections in the context buffer (a running summary is where you take each new summary and then ask it to combine it with the previous running summary). So then your query would look like this:\n\n(Running summary of sections 1 through k-2)\n\n(Summary of section k-1)\n\nPlease summarize the following section given the above context:\n\nsection k\n\nOr you might want to use some other search algorithm to identify related bits of the document and pull them in (based on similar keyword terms or whatever).\n\n(I'm aware that this is all time-consuming, but it can be automated. If your document is very long and you need a fast response, then I think your idea of using GPT-4 only for the highest levels of the hierarchy seems prudent.)\n\n## Comment ID jg0eudw with +1 score by [(llabusch93, Reddit, 2023-04-12)](https://www.reddit.com/r/OpenAI/comments/12jyes5/what_are_good_techniques_for_feeding_extremely/jg0eudw/) (in reply to ID 12jyes5):\nThere are already products for it like https://www.humata.ai/\n\n### Comment ID jg0i9hj with +2 score by [(pianoceo, Reddit, 2023-04-12)](https://www.reddit.com/r/OpenAI/comments/12jyes5/what_are_good_techniques_for_feeding_extremely/jg0i9hj/) (in reply to ID jg0eudw):\nThis isn't clear form the site. But maybe you will know:  \nCould I upload a book that I have personally written, have it understand my voice, and then write another book for me after I coach it through?\n\n#### Comment ID jg0q0kb with +7 score by [(Langdon_St_Ives, Reddit, 2023-04-12)](https://www.reddit.com/r/OpenAI/comments/12jyes5/what_are_good_techniques_for_feeding_extremely/jg0q0kb/) (in reply to ID jg0i9hj):\nFound GRRM!\n\n#### Comment ID jg15swb with +1 score by [(AntonGemini, Reddit, 2023-04-13)](https://www.reddit.com/r/OpenAI/comments/12jyes5/what_are_good_techniques_for_feeding_extremely/jg15swb/) (in reply to ID jg0i9hj):\nA paragraph or two should be enough for it to ‘understand your voice’\n\n## Comment ID jg0j378 with +1 score by [(jameshines10, Reddit, 2023-04-12)](https://www.reddit.com/r/OpenAI/comments/12jyes5/what_are_good_techniques_for_feeding_extremely/jg0j378/) (in reply to ID 12jyes5):\nI think this would be a good use case for a tool/framework like LangChain. You'd essentially be building a custom LLM trained on the data in your documents. I can see these kinds of hyperfocused models becoming more and more common.\n\n\n\n\n\n\n\n\n\n## Comment ID jg2bc5k with +1 score by [(None, Reddit, 2023-04-13)](https://www.reddit.com/r/OpenAI/comments/12jyes5/what_are_good_techniques_for_feeding_extremely/jg2bc5k/) (in reply to ID 12jyes5):\nThere's a business for those that can make this easy for non-engineers.\n\n### Comment ID jg2nvko with +2 score by [(Robo_Rascal, Reddit, 2023-04-13)](https://www.reddit.com/r/OpenAI/comments/12jyes5/what_are_good_techniques_for_feeding_extremely/jg2nvko/) (in reply to ID jg2bc5k):\nCopied from my comment\n \" How most of these apps/sites use is that after your documents are uploaded, it uses a semantic search ( more advanced then just keyword matching) Then it pulls the relevant information from your documents, pastes it into the conversation for the AI to read, and just regurgitate that information to you as a response to your question. \n\nI have not seen any reasonably good implementation of embedding with AI besides simple QA, no extended conversations or actual memory. Just a nice customized search engine for your own documents. \"\n\nWould this be useful to you if it were setup to basically be:\nPress this button to upload your document(s).\nType your question here : \n\n\nAnd then you get a response based on your question and relevant document details?\n\nit's possible to keep asking further questions and have a conversation that the AI remembers untill you hit the max token limit.\n\n### Comment ID jg53djx with +1 score by [(justdoitanddont, Reddit, 2023-04-13)](https://www.reddit.com/r/OpenAI/comments/12jyes5/what_are_good_techniques_for_feeding_extremely/jg53djx/) (in reply to ID jg2bc5k):\nIf you find one, please ping me. Happy to host an app on cloud.\n\n## Comment ID jg2ka7c with +1 score by [(HumanServitor, Reddit, 2023-04-13)](https://www.reddit.com/r/OpenAI/comments/12jyes5/what_are_good_techniques_for_feeding_extremely/jg2ka7c/) (in reply to ID 12jyes5):\nI'm having trouble understanding embedding.  I mean i think I understand the idea of the distributions it creates, but I'm not sure how to apply them.  For instance, could I use embedding to help teach GPT to write more like me? By comparing my writing samples against vanilla gpt output, for example, and drawing conclusions from that comparison.  I write fiction, and I don't so much want to automate my writing as turn GPT into an orchestra I can conduct that produces music aligned with my vision of the symphony.\n\n### Comment ID jg2mpk3 with +2 score by [(Robo_Rascal, Reddit, 2023-04-13)](https://www.reddit.com/r/OpenAI/comments/12jyes5/what_are_good_techniques_for_feeding_extremely/jg2mpk3/) (in reply to ID jg2ka7c):\nYou can't really teach gpt anything, all embeddings are is a way to retrieve data from your own documents if it's too difficult to just search for a keyword or two. \n\nHow most of these apps/sites use is that after your documents are uploaded, it uses a semantic search ( more advanced then just keyword matching) Then it pulls the relevant information from your documents, pastes it into the conversation for the AI to read, and just regurgitate that information to you as a response to your question. \n\nI have not seen any reasonably good implementation of embedding with AI besides simple QA, no extended conversations or actual memory. Just a nice customized search engine for your own documents.\n\n#### Comment ID jg3mr62 with +1 score by [(HumanServitor, Reddit, 2023-04-13)](https://www.reddit.com/r/OpenAI/comments/12jyes5/what_are_good_techniques_for_feeding_extremely/jg3mr62/) (in reply to ID jg2mpk3):\ninteresting.  Thanks!\n\n## Comment ID jg2orli with +1 score by [(electric_hotdog2k, Reddit, 2023-04-13)](https://www.reddit.com/r/OpenAI/comments/12jyes5/what_are_good_techniques_for_feeding_extremely/jg2orli/) (in reply to ID 12jyes5):\nthis is super easy to do with something like this [https://github.com/marqo-ai/marqo](https://github.com/marqo-ai/marqo) see here [https://github.com/marqo-ai/marqo/blob/mainline/examples/GPT-examples/article/article.md](https://github.com/marqo-ai/marqo/blob/mainline/examples/GPT-examples/article/article.md).\n\n## Comment ID jg2rw0h with +1 score by [(Unixwzrd, Reddit, 2023-04-13)](https://www.reddit.com/r/OpenAI/comments/12jyes5/what_are_good_techniques_for_feeding_extremely/jg2rw0h/) (in reply to ID 12jyes5):\n[Check out this Firefox extension.](https://github.com/NicoleFaye/Chat-Gpt-Long-Text-Input)  It seems to work rather well, it's called \"ChatGPT Long Text Input.  It breaks things into smaller chunks.  If you do PDF's you might want to extract the text from the PDF first, there's a lot of other crap in the PDF you don't need.\n\n## Comment ID jg33z1r with +1 score by [(WhiteNoiseAudio, Reddit, 2023-04-13)](https://www.reddit.com/r/OpenAI/comments/12jyes5/what_are_good_techniques_for_feeding_extremely/jg33z1r/) (in reply to ID 12jyes5):\nCheck out this project, it does exactly what you are asking: https://github.com/mayooear/gpt4-pdf-chatbot-langchain\n\n## Comment ID jg490dv with +1 score by [(flippy_flops, Reddit, 2023-04-13)](https://www.reddit.com/r/OpenAI/comments/12jyes5/what_are_good_techniques_for_feeding_extremely/jg490dv/) (in reply to ID 12jyes5):\nLate reply that no one will see, but you don't need vector databases/pinecove/LangChain, etc to do all of this if you're just messing around.\n\nEssentially it boils down to:\n\n* Convert your document to plain text\n* Split the text into chunks of 500 words or so (e.g. paragraphs)\n* Use OpenAI's createEmbedding API to convert each chunk into an embedding (which is just a long array of numbers)\n* Convert your question to an embedding too\n* Find a \\`cosine similarity\\` algorithm for the language you're using, and compare your question embedding with each chunk. Each score will be 0 - 1 where 1 is very similar.\n* The best 2-4 chunks probably have the answer to your question\n* Create a prompt like: \\`\\[TOP\\_4\\_CHUNKS\\] \\\\n\\\\n \\[QUESTION\\]\\`\n* Send that prompt to GPT3 (or whatever) through OpenAI\n\nThe point is, you can't feed 1,000 pages into a GPT prompt. So the embeddings let you find the relevant paragraphs.\n\n### Comment ID jgogx0f with +1 score by [(Cogitarius, Reddit, 2023-04-17)](https://www.reddit.com/r/OpenAI/comments/12jyes5/what_are_good_techniques_for_feeding_extremely/jgogx0f/) (in reply to ID jg490dv):\nWould this approach (saving chunks locally into text files) be reasonable for hundreds of PDF pages, potentially into the low thousands?\n\n#### Comment ID jgoysvd with +1 score by [(flippy_flops, Reddit, 2023-04-18)](https://www.reddit.com/r/OpenAI/comments/12jyes5/what_are_good_techniques_for_feeding_extremely/jgoysvd/) (in reply to ID jgogx0f):\nSay you have 1,000 pages, 250 words per page. That's maybe a 2MB plain text file. If you used 2 pages per chunk, that'd be 500 API calls to create the embeddings. The embeddings might take up 8MB if you stored them in plain text json. So you'd be running a cosine similarity against 10MB of data which is nothing. You'd take the best matches and create a prompt with them. Even at 100k pages, that's maybe 200MB to save and search which still isn't much (if my math is right)\n\n## Comment ID jg5t5tj with +1 score by [(None, Reddit, 2023-04-13)](https://www.reddit.com/r/OpenAI/comments/12jyes5/what_are_good_techniques_for_feeding_extremely/jg5t5tj/) (in reply to ID 12jyes5):\nYou could reduce the character count. You'll still exceed with your example, but this may help others. \n\n\"As an AI language model, I have been trained to understand and interpret natural language in various forms, including spoken and written language. However, to answer your question, certain types of words can be omitted from a sentence without affecting the overall meaning and comprehension of the sentence. These include:\n\nArticles - such as \"a,\" \"an,\" and \"the\"\nConjunctions - such as \"and,\" \"but,\" and \"or\"\nPrepositions - such as \"in,\" \"on,\" \"at,\" and \"with\"\nPronouns - such as \"he,\" \"she,\" \"it,\" and \"they\"\nSome adjectives and adverbs, although this will depend on the context.\nHowever, please note that omitting certain words from a sentence can alter the intended meaning, so it's essential to use your discretion when deciding which words to omit.\"\n\n## Comment ID jgxcrfq with +1 score by [(janimator0, Reddit, 2023-04-19)](https://www.reddit.com/r/OpenAI/comments/12jyes5/what_are_good_techniques_for_feeding_extremely/jgxcrfq/) (in reply to ID 12jyes5):\nExcellent post! Thanks for the roundup!\n\n## Comment ID jj63bn8 with +1 score by [(yachty66, Reddit, 2023-05-07)](https://www.reddit.com/r/OpenAI/comments/12jyes5/what_are_good_techniques_for_feeding_extremely/jj63bn8/) (in reply to ID 12jyes5):\nAre there any GUI's which provide this service? I found here in the comments [https://www.humata.ai/](https://www.humata.ai/).  Anything else?\n\n## Comment ID jydakzg with +1 score by [(Own_Investigator2904, Reddit, 2023-08-30)](https://www.reddit.com/r/OpenAI/comments/12jyes5/what_are_good_techniques_for_feeding_extremely/jydakzg/) (in reply to ID 12jyes5):\nCheck [app.spectrumx.info](https://app.spectrumx.info). You can train your model there with pdf, text, csv files.  Try your model on the sandbox. If you are happy, then put the model into your own website by inserting 2 lines of js code. Fairly easy to use.",
      "# Post ID 12wn63m: I have made some easy tools to rip webpages, clean the data, and vectorize it for a Pinecone DB. Great if you want your AI to consult a webpage. with +85 score by [(None, Reddit, 2023-04-23)](https://www.reddit.com/r/ChatGPTCoding/comments/12wn63m/i_have_made_some_easy_tools_to_rip_webpages_clean/)\n[https://github.com/Sstobo/Site-Sn33k](https://github.com/Sstobo/Site-Sn33k)\n\n&#x200B;\n\nTake a look :)\n\nIts been helpful for me\n\n## Comment ID jhftxds with +7 score by [(Aware-Communication4, Reddit, 2023-04-23)](https://www.reddit.com/r/ChatGPTCoding/comments/12wn63m/i_have_made_some_easy_tools_to_rip_webpages_clean/jhftxds/) (in reply to ID 12wn63m):\nWhat is pinecone? I'm very new to this world. I could look it up, but I'd rather hear from you if possible\n\n### Comment ID jhfy6b0 with +23 score by [(None, Reddit, 2023-04-23)](https://www.reddit.com/r/ChatGPTCoding/comments/12wn63m/i_have_made_some_easy_tools_to_rip_webpages_clean/jhfy6b0/) (in reply to ID jhftxds):\nPinecone is a vector database. \n\n[https://app.pinecone.io/](https://app.pinecone.io/)\n\nA vector database is essentially  way to make large amounts of data searchable. Since we cant upload a 1000 page document to chatGPT, we use vector databases to selectively deliver the content relevant to our GPT query.\n\nWe take the documents, break them into chunks, serialize the chunks, embed them into a vector database.\n\nLets say we upload a few hundred pages of text about turtles.\n\nNow, every time we ask our LLM about turtles, it will receive the most relevant pieces of the vectorized turtle PDF. So it effectively 'knows it', without having to pass the entire PDF and going impossibly far over your token limits. This way we can have more specific knowledge pools for the ai to draw on.\n\nHope this helps :)\n\n#### Comment ID jhg10wy with +7 score by [(Aware-Communication4, Reddit, 2023-04-23)](https://www.reddit.com/r/ChatGPTCoding/comments/12wn63m/i_have_made_some_easy_tools_to_rip_webpages_clean/jhg10wy/) (in reply to ID jhfy6b0):\nHoly shit. That's amazing. What other types of stuff like this exist? This world is so new. How far behind am I? I've got some general information about AI and prompting saved, but what other resources, like Pinecone, exist?\n\n#### Comment ID jhj6vcz with +3 score by [(tyliggity, Reddit, 2023-04-24)](https://www.reddit.com/r/ChatGPTCoding/comments/12wn63m/i_have_made_some_easy_tools_to_rip_webpages_clean/jhj6vcz/) (in reply to ID jhfy6b0):\nThe key is that using a vector DB makes it more efficient to do vector lookups in the face of tons of data.  Also, the vectors - which are mathematical constructs - allow you to do a proximity search so that you can pull the closest n vectors to the input vector.  This comes into play with Open AI's embedding endpoints where you turn language into a vector as it allows you to do a proximity search in terms of linguistic meaning.  Gone are the days of searching by keywords or other methods.  This is the search technique of the future.\n\n#### Comment ID jhhrgfy with +2 score by [(RMCPhoto, Reddit, 2023-04-24)](https://www.reddit.com/r/ChatGPTCoding/comments/12wn63m/i_have_made_some_easy_tools_to_rip_webpages_clean/jhhrgfy/) (in reply to ID jhfy6b0):\nAt what point in scaling / data size etc should you switch to an online vector store like pinecone vs a local option like FAISS / Chromadb / JSON file?\n\n#### Comment ID jhg19ms with +1 score by [(Aware-Communication4, Reddit, 2023-04-23)](https://www.reddit.com/r/ChatGPTCoding/comments/12wn63m/i_have_made_some_easy_tools_to_rip_webpages_clean/jhg19ms/) (in reply to ID jhfy6b0):\nLike, when the hell would I use this. Where do I start learning about this stuff?\n\n## Comment ID jhgmes5 with +5 score by [(fallenKlNG, Reddit, 2023-04-24)](https://www.reddit.com/r/ChatGPTCoding/comments/12wn63m/i_have_made_some_easy_tools_to_rip_webpages_clean/jhgmes5/) (in reply to ID 12wn63m):\nSounds useful, will have a look. I’m making my own open source OpenAI+Pinecone project that could use something like this! So if I give it a website like LangChain, will it get every page linked on the front page, or would it only get the front page?\n\n### Comment ID jhgplor with +2 score by [(None, Reddit, 2023-04-24)](https://www.reddit.com/r/ChatGPTCoding/comments/12wn63m/i_have_made_some_easy_tools_to_rip_webpages_clean/jhgplor/) (in reply to ID jhgmes5):\nIt tries to get every page. Ive found that starting at the main index works well.\n\n## Comment ID jhg0p6k with +3 score by [(ChoiceOwn555, Reddit, 2023-04-23)](https://www.reddit.com/r/ChatGPTCoding/comments/12wn63m/i_have_made_some_easy_tools_to_rip_webpages_clean/jhg0p6k/) (in reply to ID 12wn63m):\nI'd suggest to replace \"rtdocs\" with \"https://\" to retain the original source in the JSON file.\n\n### Comment ID jhgpit7 with +2 score by [(None, Reddit, 2023-04-24)](https://www.reddit.com/r/ChatGPTCoding/comments/12wn63m/i_have_made_some_easy_tools_to_rip_webpages_clean/jhgpit7/) (in reply to ID jhg0p6k):\nGreat thought, thank you!\n\n## Comment ID jhl3429 with +1 score by [(None, Reddit, 2023-04-24)](https://www.reddit.com/r/ChatGPTCoding/comments/12wn63m/i_have_made_some_easy_tools_to_rip_webpages_clean/jhl3429/) (in reply to ID 12wn63m):\nI have added a PDF consumer to the repo. Tested on multiple docs at once. Adds to your training data, instead of overwriting. Now we can combine webpages, pdfs and more.\n\n## Comment ID jhgpjz1 with +1 score by [(Chris_in_Lijiang, Reddit, 2023-04-24)](https://www.reddit.com/r/ChatGPTCoding/comments/12wn63m/i_have_made_some_easy_tools_to_rip_webpages_clean/jhgpjz1/) (in reply to ID 12wn63m):\nDo these tools crawl websites. If so, what effect will their actions have on the site?\n\nI am member of some private forum sites with huge amounts of valuable information. I wonder what impact, if any, a crawler has on such sites, when they are able to suck up info at such a rapid rate?\n\nWill this kind of behaviour soon be considered anti-social?\n\n### Comment ID jhgpugg with +1 score by [(None, Reddit, 2023-04-24)](https://www.reddit.com/r/ChatGPTCoding/comments/12wn63m/i_have_made_some_easy_tools_to_rip_webpages_clean/jhgpugg/) (in reply to ID jhgpjz1):\nThe crawler is the wget python library. It doesnt crawl so much as just direct download the html.\n\nSome sites (like wikipedia) have seperate instances designed to be crawled / ripped. This is because of exactly what you have said, it can effect the sites performance due to the huge data transfer.\n\n#### Comment ID jhgq8gf with +1 score by [(Chris_in_Lijiang, Reddit, 2023-04-24)](https://www.reddit.com/r/ChatGPTCoding/comments/12wn63m/i_have_made_some_easy_tools_to_rip_webpages_clean/jhgq8gf/) (in reply to ID jhgpugg):\nAm I going to get angry mods banning me from their sites if I start using this?\n\n#### Comment ID jld8q68 with +1 score by [(None, Reddit, 2023-05-24)](https://www.reddit.com/r/ChatGPTCoding/comments/12wn63m/i_have_made_some_easy_tools_to_rip_webpages_clean/jld8q68/) (in reply to ID jhgpugg):\njust wondering if you looked at scrapy?\n\ni've mostly used beautifulsoup, but my sense was, if I wanted to point at a web server and download everything, scrapy would handle the spidering, queuing, multithreading, throttling, and I think it has selectors so you could say, save only text.\n\n## Comment ID jhgpq61 with +1 score by [(Chris_in_Lijiang, Reddit, 2023-04-24)](https://www.reddit.com/r/ChatGPTCoding/comments/12wn63m/i_have_made_some_easy_tools_to_rip_webpages_clean/jhgpq61/) (in reply to ID 12wn63m):\nHow much info can pine cone store?\n\nCan we feed it the entire accumulated D&D archive, so that it can become the ultimate DM?\n\n### Comment ID jhgq49u with +3 score by [(None, Reddit, 2023-04-24)](https://www.reddit.com/r/ChatGPTCoding/comments/12wn63m/i_have_made_some_easy_tools_to_rip_webpages_clean/jhgq49u/) (in reply to ID jhgpq61):\nThats something like what im working on.\n\nI have uploaded the monster manual in PDF form. \n\nNow when my DMbot needs monster information, we query the actual manual.\n\nNext step up will be an agent whos whole role is to be a monster expert.\n\nThis pattern can be applied to anything. Goal would be multiple agents, all responsible for one set of documents, and one function. Think -MonsterDM: an agent whose responsibility is to query the Monster Manual, get accurate information, and generate / control / role play monsters.\n\n#### Comment ID jhgt0rv with +3 score by [(Chris_in_Lijiang, Reddit, 2023-04-24)](https://www.reddit.com/r/ChatGPTCoding/comments/12wn63m/i_have_made_some_easy_tools_to_rip_webpages_clean/jhgt0rv/) (in reply to ID jhgq49u):\nSounds great.\n\nCan you make sure that the MM expert presents itself as a holographic version of Alan Moore for extra effect?\n\n## Comment ID jhh81h4 with +1 score by [(nekrut, Reddit, 2023-04-24)](https://www.reddit.com/r/ChatGPTCoding/comments/12wn63m/i_have_made_some_easy_tools_to_rip_webpages_clean/jhh81h4/) (in reply to ID 12wn63m):\nAwesome work. Does the crawling/pinecone part generate any cost for openai api or is it only when searching the content in the database?\n\nI was thinking of maybe crawling a large site like Microsoft docs (or a large part of it), is the crawling smart enough to only download changed sites the second time you run scripts or will it always grab everything it finds and store it again?\n\n### Comment ID jhigu5k with +1 score by [(None, Reddit, 2023-04-24)](https://www.reddit.com/r/ChatGPTCoding/comments/12wn63m/i_have_made_some_easy_tools_to_rip_webpages_clean/jhigu5k/) (in reply to ID jhh81h4):\nIts only the vectorizing that hits the api, but its very insignificant (we use the old ada model, not gpt4)\n\n## Comment ID jhha1m1 with +1 score by [(tvmaly, Reddit, 2023-04-24)](https://www.reddit.com/r/ChatGPTCoding/comments/12wn63m/i_have_made_some_easy_tools_to_rip_webpages_clean/jhha1m1/) (in reply to ID 12wn63m):\nHow have you gotten around website filled with JavaScript?   \n\nIs there a good algorithm for splitting the text into some optimal sized chunks?\n\n### Comment ID jhigwu4 with +2 score by [(None, Reddit, 2023-04-24)](https://www.reddit.com/r/ChatGPTCoding/comments/12wn63m/i_have_made_some_easy_tools_to_rip_webpages_clean/jhigwu4/) (in reply to ID jhha1m1):\nWe could scrape js, but im only getting HTML at the moment, then removing everything EXCEPT for p tags, headings lists and spans (likely text content)\n\n## Comment ID jhi05qe with +1 score by [(Alarming-Recipe2857, Reddit, 2023-04-24)](https://www.reddit.com/r/ChatGPTCoding/comments/12wn63m/i_have_made_some_easy_tools_to_rip_webpages_clean/jhi05qe/) (in reply to ID 12wn63m):\nthanks. will check it out now\n\n## Comment ID jhkelh8 with +1 score by [(None, Reddit, 2023-04-24)](https://www.reddit.com/r/ChatGPTCoding/comments/12wn63m/i_have_made_some_easy_tools_to_rip_webpages_clean/jhkelh8/) (in reply to ID 12wn63m):\nRemindMe! 1 day\n\n### Comment ID jhkes76 with +1 score by [(RemindMeBot, Reddit, 2023-04-24)](https://www.reddit.com/r/ChatGPTCoding/comments/12wn63m/i_have_made_some_easy_tools_to_rip_webpages_clean/jhkes76/) (in reply to ID jhkelh8):\nI will be messaging you in 1 day on [**2023-04-25 20:37:19 UTC**](http://www.wolframalpha.com/input/?i=2023-04-25%2020:37:19%20UTC%20To%20Local%20Time) to remind you of [**this link**](https://www.reddit.com/r/ChatGPTCoding/comments/12wn63m/i_have_made_some_easy_tools_to_rip_webpages_clean/jhkelh8/?context=3)\n\n[**CLICK THIS LINK**](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5Bhttps%3A%2F%2Fwww.reddit.com%2Fr%2FChatGPTCoding%2Fcomments%2F12wn63m%2Fi_have_made_some_easy_tools_to_rip_webpages_clean%2Fjhkelh8%2F%5D%0A%0ARemindMe%21%202023-04-25%2020%3A37%3A19%20UTC) to send a PM to also be reminded and to reduce spam.\n\n^(Parent commenter can ) [^(delete this message to hide from others.)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Delete%20Comment&message=Delete%21%2012wn63m)\n\n*****\n\n|[^(Info)](https://www.reddit.com/r/RemindMeBot/comments/e1bko7/remindmebot_info_v21/)|[^(Custom)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5BLink%20or%20message%20inside%20square%20brackets%5D%0A%0ARemindMe%21%20Time%20period%20here)|[^(Your Reminders)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=List%20Of%20Reminders&message=MyReminders%21)|[^(Feedback)](https://www.reddit.com/message/compose/?to=Watchful1&subject=RemindMeBot%20Feedback)|\n|-|-|-|-|\n\n## Comment ID jhkjsoi with +1 score by [(magnitudearhole, Reddit, 2023-04-24)](https://www.reddit.com/r/ChatGPTCoding/comments/12wn63m/i_have_made_some_easy_tools_to_rip_webpages_clean/jhkjsoi/) (in reply to ID 12wn63m):\nA cheeky bookmark here\n\n## Comment ID jhl0shr with +1 score by [(Doomtrain86, Reddit, 2023-04-24)](https://www.reddit.com/r/ChatGPTCoding/comments/12wn63m/i_have_made_some_easy_tools_to_rip_webpages_clean/jhl0shr/) (in reply to ID 12wn63m):\nThis looks amazing. I want to learn how to use this. Do you have an example walkthrough? Like the turtle example you mentioned, from atart to finish. If not no worries! Thank you for making this available.\n\n### Comment ID jhl36yx with +2 score by [(None, Reddit, 2023-04-24)](https://www.reddit.com/r/ChatGPTCoding/comments/12wn63m/i_have_made_some_easy_tools_to_rip_webpages_clean/jhl36yx/) (in reply to ID jhl0shr):\nCheck out the repo, ive added some more instructions, pictures, and a PDF eater :)\n\n## Comment ID kj252b0 with +1 score by [(datmyfukingbiz, Reddit, 2024-01-22)](https://www.reddit.com/r/ChatGPTCoding/comments/12wn63m/i_have_made_some_easy_tools_to_rip_webpages_clean/kj252b0/) (in reply to ID 12wn63m):\ncan you make an update, openai and pinecode are depricated. \n\nand may be you could add an interface to speak to your data?\n\n### Comment ID kjvxoz1 with +1 score by [(None, Reddit, 2024-01-28)](https://www.reddit.com/r/ChatGPTCoding/comments/12wn63m/i_have_made_some_easy_tools_to_rip_webpages_clean/kjvxoz1/) (in reply to ID kj252b0):\nI pushed some new changes - hope they help :)",
      "# Post ID 15h3c5e: Document query solution for small business with +4 score by [(liamgwallace, Reddit, 2023-08-03)](https://www.reddit.com/r/LangChain/comments/15h3c5e/document_query_solution_for_small_business/)\nAre there any easy to deploy software solutions for a small business to query its documents using vector search and AI? Either locally stored documents or in OneDrive?\n\n## Comment ID jum5jsr with +8 score by [(vap0rtranz, Reddit, 2023-08-03)](https://www.reddit.com/r/LangChain/comments/15h3c5e/document_query_solution_for_small_business/jum5jsr/) (in reply to ID 15h3c5e):\nPinecone & Weaviate have the latest tech built into products for doc search and/or Q&A right now: Hybrid search.\n\nAzure does have Cognitive Search that is just backed by vector embeddings.  And 365 Copilot appears to have it too but nobody knows for sure except Microsoft chosen Beta testers.\n\nI've been digging around on this use case of Q&A with a chatbot backed by private docs.  It comes up almost daily now in Subreddits about AI, a lot of folks want it, and I have a selfish interest :)\n\nPinecone & Weaviate have products to sell, sure; but they're getting smart folks to look at this use case.  \n\nJust 2 weeks ago, Pinecone added their findings to the academic research that stuffing these general purpose LLM models with more context makes the accuracy of answers WORSE: https://www.pinecone.io/blog/why-use-retrieval-instead-of-larger-context/\n\nPinecone also saw better performance of models trained to Q&A with hybrid compared to lexical or semantic searches: https://www.pinecone.io/blog/hybrid-search/.  It's probably safe to assume general purpose LLMs fare worse at accurately answering a question with just a semantic search of docs stuffed into vectorDB embeddings.  \n\nWeaviate gave a decent synopsis of how hybrid search works on docs too: https://weaviate.io/blog/hybrid-search-explained; and cited academic work for the use case as well: https://weaviate.io/blog/ranking-models-for-better-search.  Basically they come to the same conclusion: Q&A for out-of-domain content, so like private docs, need something more than just stuffing the private docs into vectorDB embeddings and fronting the DB with an LLM chatbot.\n\nI've tried Pinecone's examples in Google Colab but am hitting some issues so am still trying...  \n\nIn the meantime, a point-n-click test is to use GPT4All's LocalDocs plugin.  It does use a vector DB against local docs, you can sync OneDrive docs locally to use it, but it's not hybrid search.  \n\nIf you're already running llamacpp or similar, Langchain has integration with both Weaviate and Pinecone's hybrid search.\n\n### Comment ID jumgnec with +3 score by [(liamgwallace, Reddit, 2023-08-03)](https://www.reddit.com/r/LangChain/comments/15h3c5e/document_query_solution_for_small_business/jumgnec/) (in reply to ID jum5jsr):\nOh wow, that is quite the comprehensive response. I don't have time to review all that right now but I will definitely work my way through it. Thanks\n\nI have played around with langchain and querying documents. Yes it can definitely be hit or miss with retrieving snippets of the document with a vector search. You have to hope you get the right snippets and the model focuses on the right bits for your answer. \n\nBut if the task is simple I have had mostly successful responses. The use case I am looking for here is a friend wants to query technical reference manuals. So I see this as falling in the simple category. Like \"what are the speed restrictions on laden African swallows\". And have the answer reference the document section in the response.\n\nI am only a hobby programmer at best and it would be unsafe for my code to run on his company machine. So I was hoping there was a company out there that had a solution.\n\n#### Comment ID jumkah3 with +2 score by [(vap0rtranz, Reddit, 2023-08-03)](https://www.reddit.com/r/LangChain/comments/15h3c5e/document_query_solution_for_small_business/jumkah3/) (in reply to ID jumgnec):\nOK.\n\nSo your friend can actually install this on their work machine?!\n\nThe simplist, point-n-click solution right now is GPT4All + LocalDocs plugin.  It has a GUI so your friend won't be scared by a CLI.  And it will cite the doc source in its reply so a human can verify accuracy.  Your friend could install this themself ... if allowed.  https://docs.gpt4all.io/gpt4all_chat.html#localdocs-beta-plugin-chat-with-your-data\n\nSince you have tech skills, you could setup LocalGPT or PrivateGPT w/ Langchain.  PromptEngineering, creator of LocalGPT, has a step-by-step with a doc upload button via local web service: https://www.youtube.com/watch?v=RIWbalZ7sTo&t=45s\nBut I don't see how this could work on your friend's work laptop/desktop.  Most company IT departments won't allow a managed system to run a local website without an exception.  \n\nI wouldn't let my friend trust the myriad of websites that have popped up with 'upload your PDF and ask the AI questions'.  Even though no install is required.\n\nPerhaps others can chime in with a UI frontend with doc upload/injest button for non-techies that uses Langchain.\n\n#### Comment ID jun76eu with +2 score by [(basilbowman, Reddit, 2023-08-03)](https://www.reddit.com/r/LangChain/comments/15h3c5e/document_query_solution_for_small_business/jun76eu/) (in reply to ID jumgnec):\nI've used Flowise.ai for a low/no-code version of this (it's just a GUI that sits on top of LangChain)\n\n## Comment ID juq0tuv with +3 score by [(gmarcilhacy, Reddit, 2023-08-04)](https://www.reddit.com/r/LangChain/comments/15h3c5e/document_query_solution_for_small_business/juq0tuv/) (in reply to ID 15h3c5e):\nYou can use PostLLM, it's a desktop application where you manage prompts but also embed and chat with local documents. Everything is stored locally.\n\nI've been working on it for the past few weeks. You can find the project on [Github here](https://github.com/postllm/postllm-app) or download the app on [postllm.com](https://postllm.com/?ref=reddit)",
      "# Post ID 170j6zd: My strategy for picking a vector database: a side-by-side comparison with +88 score by [(kappl, Reddit, 2023-10-05)](https://www.reddit.com/r/vectordatabase/comments/170j6zd/my_strategy_for_picking_a_vector_database_a/)\nI made this table to compare vector databases in order to help me choose the best one for a new project. I spent quite a few hours on it, so I wanted to share it here too in hopes it might help others as well. My main criteria when choosing vector DB were the speed, scalability, developer experinece, community and price.   \n\n\nYou'll find all of the comparison parameters in the article and more details here: [https://benchmark.vectorview.ai/vectordbs.html](https://benchmark.vectorview.ai/vectordbs.html)  \n\n\n[A comparison of vector databases](https://preview.redd.it/95xlf0ug7esb1.png?width=1870&format=png&auto=webp&s=38d452b4209ae20a5c6ef45b5f6c11d269aff49d)\n\n  \n\n\n## Comment ID k3m1n6h with +7 score by [(Christosconst, Reddit, 2023-10-05)](https://www.reddit.com/r/vectordatabase/comments/170j6zd/my_strategy_for_picking_a_vector_database_a/k3m1n6h/) (in reply to ID 170j6zd):\nPinecone has shit latency and the API randomly craps out. I’m very happy with Qdrant\n\n### Comment ID lb94wp4 with +1 score by [(alexrada, Reddit, 2024-07-02)](https://www.reddit.com/r/vectordatabase/comments/170j6zd/my_strategy_for_picking_a_vector_database_a/lb94wp4/) (in reply to ID k3m1n6h):\ndid you use it in production? what volumes are you on?\n\n### Comment ID lc8d6lm with +1 score by [(JacobEdmond, Reddit, 2024-07-08)](https://www.reddit.com/r/vectordatabase/comments/170j6zd/my_strategy_for_picking_a_vector_database_a/lc8d6lm/) (in reply to ID k3m1n6h):\nI have been looking into Qdrant. Thank you for your input. I believe I am going with this one.\n\n## Comment ID k4gs1lf with +3 score by [(rue_so, Reddit, 2023-10-11)](https://www.reddit.com/r/vectordatabase/comments/170j6zd/my_strategy_for_picking_a_vector_database_a/k4gs1lf/) (in reply to ID 170j6zd):\nIf you end up choosing Chroma, Pinecone, Weaviate or Qdrant, don't forget to use VectorAdmin (open source) [vectoradmin.com](https://vectoradmin.com).   \n\n\nIt's a frontend and tool suite for vector dbs so that you can easily edit embeddings, migrate data, clone embeddings to save $ and more.\n\n### Comment ID lc8dc76 with +1 score by [(JacobEdmond, Reddit, 2024-07-08)](https://www.reddit.com/r/vectordatabase/comments/170j6zd/my_strategy_for_picking_a_vector_database_a/lc8dc76/) (in reply to ID k4gs1lf):\nI will give this a try. Thank you.\n\n## Comment ID lb6rfbi with +5 score by [(one-punch-G, Reddit, 2024-07-01)](https://www.reddit.com/r/vectordatabase/comments/170j6zd/my_strategy_for_picking_a_vector_database_a/lb6rfbi/) (in reply to ID 170j6zd):\nWe started out with Pinecone, but it wasn't super stable. As our data grew, we switched to zilliz cloud. now everything works pretty well\n\n### Comment ID lb6sjaj with +1 score by [(Hot-Variation-3772, Reddit, 2024-07-01)](https://www.reddit.com/r/vectordatabase/comments/170j6zd/my_strategy_for_picking_a_vector_database_a/lb6sjaj/) (in reply to ID lb6rfbi):\nOpen Source Milvus has worked very well for me including Milvus Lite running on some edge devices.\n\n## Comment ID k4jl4fu with +2 score by [(domlovesai, Reddit, 2023-10-12)](https://www.reddit.com/r/vectordatabase/comments/170j6zd/my_strategy_for_picking_a_vector_database_a/k4jl4fu/) (in reply to ID 170j6zd):\nMight also be worth checking this out - [https://sanjmo.medium.com/vector-data-store-evaluation-criteria-6d7677ef3b60](https://sanjmo.medium.com/vector-data-store-evaluation-criteria-6d7677ef3b60)\n\nI found it a useful way of looking at what's really important\n\n## Comment ID m57iodc with +2 score by [(Existing-Mirror2315, Reddit, 2025-01-03)](https://www.reddit.com/r/vectordatabase/comments/170j6zd/my_strategy_for_picking_a_vector_database_a/m57iodc/) (in reply to ID 170j6zd):\nI switched from chroma to faiss in my langchain ai app .much better performance. Everything worked smoothly. In the other hand, chroma returns some random errors at random times.\n\n### Comment ID m8s6qhh with +1 score by [(DragonflyBrave9816, Reddit, 2025-01-23)](https://www.reddit.com/r/vectordatabase/comments/170j6zd/my_strategy_for_picking_a_vector_database_a/m8s6qhh/) (in reply to ID m57iodc):\nThis. I have found faiss to be super quick in my investigation of different vector databases.\n\n#### Comment ID madf35a with +1 score by [(pknerd, Reddit, 2025-02-01)](https://www.reddit.com/r/vectordatabase/comments/170j6zd/my_strategy_for_picking_a_vector_database_a/madf35a/) (in reply to ID m8s6qhh):\nCan FAISS work for larger Files? Does it support good Emebdding models?\n\n## Comment ID k3kx57n with +3 score by [(Unique_Ad_8574, Reddit, 2023-10-05)](https://www.reddit.com/r/vectordatabase/comments/170j6zd/my_strategy_for_picking_a_vector_database_a/k3kx57n/) (in reply to ID 170j6zd):\nGood criteria list.\n\nThere is no silver bullet, you picked Milvius then ?\n\nPersonnaly,  I am surprised not to see Astra Vector as it is a reference in term of scalability and performance (as based on Cassandra and operation the search with DiskANN). [http://astra.datastax.com](http://astra.datastax.com)\n\n* Price: free up to 80Millions request/month\n* Integrated both with langchain and llama (look for the Cassandra VectorStores\n* [OpenAi cookbooks](https://cookbook.openai.com/examples/vector_databases/cassandra_astradb/readme)\n* Support HBSW and DiskANN indexes\n* Support hybrid search (meta-data filtering) and text search (keywords)\n* RBAC supported (permission of your token per db, then read/write)\n* Dynamic segment placement => using the Cassandra native partitionning\n* Free hosted tier YES\n* Also Check cassio.org\n\nHave a nice day !\n\n### Comment ID k3kymi2 with +2 score by [(kappl, Reddit, 2023-10-05)](https://www.reddit.com/r/vectordatabase/comments/170j6zd/my_strategy_for_picking_a_vector_database_a/k3kymi2/) (in reply to ID k3kx57n):\nI'm actually trying out both Milvius and Pinecone a little bit more before I finally decide. I also heard someone mention that Vespa should be on the list, so I might look a little bit more in to that. Hadn't heard of astra but that also looks interesting!\n\n#### Comment ID k3q163m with +3 score by [(gregory_k, Reddit, 2023-10-06)](https://www.reddit.com/r/vectordatabase/comments/170j6zd/my_strategy_for_picking_a_vector_database_a/k3q163m/) (in reply to ID k3kymi2):\nMessage me if you have Pinecone questions!\n\n#### Comment ID k3q7gib with +2 score by [(help-me-grow, Reddit, 2023-10-06)](https://www.reddit.com/r/vectordatabase/comments/170j6zd/my_strategy_for_picking_a_vector_database_a/k3q7gib/) (in reply to ID k3kymi2):\nVespa is very good! It's just not well supported lol\n\nI work a lot with Milvus so feel free to ping me about it\n\n#### Comment ID k4qwkxd with +1 score by [(rue_so, Reddit, 2023-10-13)](https://www.reddit.com/r/vectordatabase/comments/170j6zd/my_strategy_for_picking_a_vector_database_a/k4qwkxd/) (in reply to ID k3kymi2):\nSide note - if you use Pinecone (or other popular vector dbs), check out VectorAdmin to use as your frontend/management system. It's open source and simplifies the UX.  \n[vectoradmin.com](https://vectoradmin.com)\n\n## Comment ID kwi1610 with +1 score by [(Present_Air_7694, Reddit, 2024-03-25)](https://www.reddit.com/r/vectordatabase/comments/170j6zd/my_strategy_for_picking_a_vector_database_a/kwi1610/) (in reply to ID 170j6zd):\nIs VectorAdmin completely broken? I've wasted a day trying to get it to connect to Weaviate and then to Chroma locally, but whatever I try I just get messages blaming the DBs for not running when they are.  \n\n\nAnyone recommend any alternative front ends, as it seems a worthwhile project.\n\n### Comment ID m5ymafk with +1 score by [(Simple-Fix6566, Reddit, 2025-01-07)](https://www.reddit.com/r/vectordatabase/comments/170j6zd/my_strategy_for_picking_a_vector_database_a/m5ymafk/) (in reply to ID kwi1610):\ntry this fully UI for ChromaDB, but its only for chroma.  \n[https://github.com/coffeecodeconverter/ChromaFlowStudio](https://github.com/coffeecodeconverter/ChromaFlowStudio)  \nable to add, edit, view, delete, copy, clone, import and export in bulk, and visualize your data all in one place.\n\n## Comment ID kzh0qd7 with +1 score by [(ivarpuvar, Reddit, 2024-04-14)](https://www.reddit.com/r/vectordatabase/comments/170j6zd/my_strategy_for_picking_a_vector_database_a/kzh0qd7/) (in reply to ID 170j6zd):\nChroma has 2 features missing - horizontal scaling and payload indexes. Eg you cannot query with \\`where\\`, because it needs to read the whole db. It uses a lot of memory and crashes often.\n\n  \nI think Chroma is not suitable for most of the use cases. After chroma I tried qdrant and it is very performant and works well\n\n### Comment ID m5yn9w3 with +1 score by [(Simple-Fix6566, Reddit, 2025-01-07)](https://www.reddit.com/r/vectordatabase/comments/170j6zd/my_strategy_for_picking_a_vector_database_a/m5yn9w3/) (in reply to ID kzh0qd7):\nchroma does have its faults. i dont suppose you were running on windows? if so, there was a bug in chroma related to its integration with the HNSW library, whereby it would crash python on the 100th document insert, or update, or deletion. The workaround was to simply increase the batch size - as this was the real issue (default batch was 100, when reached it tries to sync and thats where the bug lies, and crashes python)  \napparently, something not experienced in linux environment - according to some posts on github.\n\nyou posted this 9 months ago, they've released a few versions since then  \nits more stable these days - agree it can use lots of memory for large datasets, but its use case i suppose is for small local projects only its very well suited for that, especially as it streamlines the embedding process and the indexing.\n\n## Comment ID lac31kq with +1 score by [(bornforspace, Reddit, 2024-06-26)](https://www.reddit.com/r/vectordatabase/comments/170j6zd/my_strategy_for_picking_a_vector_database_a/lac31kq/) (in reply to ID 170j6zd):\nThanks for sharing! Interestingly enough, I've switched from Weaviate to Qdrant and have been experiencing better latency at scale.\n\n## Comment ID lbp6he0 with +1 score by [(graph-crawler, Reddit, 2024-07-05)](https://www.reddit.com/r/vectordatabase/comments/170j6zd/my_strategy_for_picking_a_vector_database_a/lbp6he0/) (in reply to ID 170j6zd):\nwhich one has async sdk for python ?\n\n### Comment ID lcqftdb with +1 score by [(frank_sleepy, Reddit, 2024-07-11)](https://www.reddit.com/r/vectordatabase/comments/170j6zd/my_strategy_for_picking_a_vector_database_a/lcqftdb/) (in reply to ID lbp6he0):\nQdrant does\n\n## Comment ID lppb236 with +1 score by [(2600_yay, Reddit, 2024-09-30)](https://www.reddit.com/r/vectordatabase/comments/170j6zd/my_strategy_for_picking_a_vector_database_a/lppb236/) (in reply to ID 170j6zd):\nThis `Vector DB Comparison`page - https://superlinked.com/vector-db-comparison - has dozens of new and more established vector DBs and compares them across many different attributes:\n\n- license so FOSS or not\n- does the vector DB offer hybrid search or not\n- geo search\n- sparse, dense, BM25, full-text\n- what type(s) of data: text, pictures, etc.\n- APIs offered: like RAG, recsys, LangChain, LlamaIndex\n- pricing, whether the vector DB offers a managed service version, etc.\n\nIt even mentions the old Apache projects (Cassandra, Solr, etc.) and older vector DB projects - like ElasticSearch, Vespa, Milvius, Pinecone, etc. - all the way up to the 2020's vector DB projects like qdrant, Chroma, etc.\n\n## Comment ID mdycbbf with +1 score by [(robertsilen, Reddit, 2025-02-21)](https://www.reddit.com/r/vectordatabase/comments/170j6zd/my_strategy_for_picking_a_vector_database_a/mdycbbf/) (in reply to ID 170j6zd):\nIt would be interesting to see how MariaDB Vector compares. [https://mariadb.com/kb/en/vector-overview/](https://mariadb.com/kb/en/vector-overview/)\n\n\n\n## Comment ID k81hzxg with +1 score by [(nkanungo_kx, Reddit, 2023-11-06)](https://www.reddit.com/r/vectordatabase/comments/170j6zd/my_strategy_for_picking_a_vector_database_a/k81hzxg/) (in reply to ID 170j6zd):\nGreat guide! There's been many vector databases popping up but I think it's worth also considering [KDB.AI](https://KDB.AI). It's built on 30 year old vectorized processing technology and is ranked #1 on [DB-engines.com](https://DB-engines.com). In particular, it's one of the only vector databases that has data encryption, compression, and sharding. It's great for enterprise scalability.  \n\n\nThanks for the consideration, and in any case, thank you again for such an informative guide :)\n\n### Comment ID lbp7cau with +1 score by [(graph-crawler, Reddit, 2024-07-05)](https://www.reddit.com/r/vectordatabase/comments/170j6zd/my_strategy_for_picking_a_vector_database_a/lbp7cau/) (in reply to ID k81hzxg):\nit doesn't support async on their python sdk, does it ?",
      "# Post ID 14vnfh2: My experience on starting with fine tuning LLMs with custom data with +915 score by [(Ion_GPT, Reddit, 2023-07-10)](https://www.reddit.com/r/LocalLLaMA/comments/14vnfh2/my_experience_on_starting_with_fine_tuning_llms/)\nI keep seeing questions about \"How I make a model to answer based on my data. I have \\[wiki, pdfs, whatever other documents\\]\"\n\nCurrently I am making a living by helping companies built chatbots fine tuned on their custom data.\n\nMost of those are support or Q&A chatbots to answer questions from clients at any hour and day. There are also internal chatbots to be used to train new people joining the company and several other use cases.\n\nSo, I was thinking to share my experience (it might be wrong and I might be doing everything wrong, but it is my experience and based on this I have a dozen chatbots running in production and talking with clients with few dozen more in different stages of testing).\n\nThe actual training / fine-tuning, while it might initially seem like a daunting task due to the plethora of tools available (FastChat, Axolot, Deepspeed, transformers, LoRA, qLoRA, and more), I must tell you - this is actually the easiest part of the whole process! All you need to do is peek into their repositories, grab an example, and tweak it to fit your model and data.\n\nHowever, the real challenge lies in preparing the data. A massive wiki of product documentation, a thousand PDFs of your processes, or even a bustling support forum with countless topics - they all amount to nothing if you don't have your data in the right format. Projects like Dolly and Orca have shown us how enriching data with context or system prompts can significantly improve the final model's quality. Other projects, like Vicuna, use chains of multi-step Q&A with solid results. There are many other datasets formats, depending of the expected result. For example, a dataset for quotes is much simpler, because there will be no actual interaction, the quote is a quote.\n\nPersonally, I mostly utilize the #instruction, #input, #output format for most of my fine-tuning tasks.\n\nSo, shaping your data in the correct format is, without a doubt, the most difficult and time-consuming step when creating a Language Learning Model (LLM) for your company's documentation, processes, support, sales, and so forth.\n\nMany methods can help you tackle this issue. Most choose to employ GPT4 for assistance. Privacy shouldn't be a concern if you're using Azure APIs, though they might be more costly, but offer privacy. However, if your data is incredibly sensitive, refrain from using them. And remember, any data used to train a public-facing chatbot should not contain any sensitive information.\n\nAutomated tools can only do so much; manual work is indispensable and in many cases, difficult to outsource. Those who genuinely understand the product/process/business should scrutinize and cleanse the data. Even if the data is top-notch and GPT4 does a flawless job, the training could still fail. For instance, outdated information or contradictory responses can lead to poor results.\n\nIn many of my projects, we involve a significant portion of the organization in the process. I develop a simple internal tool allowing individuals to review rows of training data and swiftly edit the output or flag the entire row as invalid.\n\nOnce you've curated and correctly formatted your data, the fine-tuning can commence. If you have a vast amount of data, i.e., tens of thousands of instructions, it's best to fine-tune the actual model. To do this, refer to the model repo and mimic their initial training process with your data.\n\nHowever, if you're working with a smaller dataset, a LoRA or qLoRA fine-tuning would be more suitable. For this, start with examples from LoRA or qLoRA repositories, use booga UI, or experiment with different settings. Getting a good LoRA is a trial and error process, but with time, you'll become good at it.\n\nOnce you have your fine-tuned model, don't expose it directly to clients. Instead, run client queries through the model, showcasing the responses internally and inviting internal users to correct the answers. Depending on the percentage of responses modified by users, you might need to execute another fine-tuning with this new data or completely redo the fine-tuning if results were really poor.\n\nOn the hardware front, while it's possible to train a qLoRA on a single 3090, I wouldn't recommend it. There are too many limitations, and even browsing the web while training could lead to OOM. I personally use a cloud A6000 with 48GB VRAM, which costs about 80 cents per hour.\n\nFor anything larger than a 13B model, whether it's LoRA or full fine-tuning, I'd recommend using A100. Depending on the model and dataset size, and parameters, I run 1, 4, or 8 A100s. Most tools are tested and run smoothly on A100, so it's a safe bet. I once got a good deal on H100, but the hassle of adapting the tools was too overwhelming, so I let it go.\n\nLastly, if you're looking for a quick start, try embeddings. This is a cheap, quick, and acceptable solution for internal needs. You just need to throw all internal documents into a vector db, put a model in front for searching, and voila! With no coding required, you can install booga with the superbooga extension to get started.\n\n&#x200B;\n\nUPDATE:\n\nI saw some questions repeating, sorry that I am not able to answer to everyone, but I am updating here, hope that this helps. Here are some answers for the repeated questions:\n\n1. I do not know how to train a pre-trained model with \"raw\" data, like big documents. From what I know, any further training of a pre-trained model is done by feeding data tokenized and padded to maximum context size of the original model, no more.\n2. Before starting, make sure that the problem that needs to be solved and the expectations are fully defined. \"Teaching the model about xyz\" is not a problem, it is a wish. It is hard to solve \"wishes\", but we can solve problems. For example: \"I want to ask the model about xyz and get accurate answers based on abc data\". This is needed to offer non stop answering chat for customers. We expect customer to ask \"example1, 2, 3, .. 10\" and we expect the answers to be in this style \"example answers with example addressation, formal, informal, etc). We do not want the chat to engage in topics not related to xyz. If customer engage in such topics, politely explain that have no knowledge on that. (with example). This is a better description of the problem.\n3. It is important to define the target audience and how the model will be used. There is a big difference of using it internally inside an organisation or directly expose it to the clients. You can get a lot cheaper when it is just an internal helper and the output can be ignored if not good. For example, in this case, full documents can be ingested via vectordb and use the model to answer questions about the data from the vectordb. If you decide to go with the embeddings, this can be really helpful: [https://github.com/HKUNLP/instructor-embedding](https://github.com/HKUNLP/instructor-embedding)\n4. It is important to define what is the expected way to interact with the model. Do you want to chat with it? Should it follow instructions? Do you want to provide a context and get output in the provided context? Do you want to complete your writing (like Github Copilot or Starcoder)? Do you want to perform specific tasks (eg grammar checking, translation, classification of something etc)?\n5. After all the above are decided and clarified and you decided that embeddings are not what you want and want to proceed further with fine tuning, it is the time to decide on the data format.\n   1. \\#instruction,#input,#output is a popular data format and can be used to train for both chat and instruction following. This is an example dataset in this format: [https://huggingface.co/datasets/yahma/alpaca-cleaned](https://huggingface.co/datasets/yahma/alpaca-cleaned) . I am using this format the most because it is the easiest to format unstructured data into, having the optional #input it makes it very flexible\n   2. It was proven that having better structured, with extra information training data will produce better results. Here is Dolly dataset that is using a context to enrich the data: [https://huggingface.co/datasets/databricks/databricks-dolly-15k](https://huggingface.co/datasets/databricks/databricks-dolly-15k)\n   3. A newer dataset that further proved that data format and quality is the most important in the output is Orca format. It is using a series of system prompts to categorize each data row (similar with a tagging system). [https://huggingface.co/datasets/Open-Orca/OpenOrca](https://huggingface.co/datasets/Open-Orca/OpenOrca)\n   4. We don't need complicated data structure always. For example, if the expecation is that we prompt the model \"Who wrote this quote: \\[famous quote content\\]?\" and we expect to only get name of the author, then a simple format is enough, like it is here: [https://huggingface.co/datasets/Abirate/english\\_quotes](https://huggingface.co/datasets/Abirate/english_quotes)\n   5. For a more fluid conversation, there is the Vicuna format, an Array of Q&A. Here is an example: [https://huggingface.co/datasets/ehartford/wizard\\_vicuna\\_70k\\_unfiltered](https://huggingface.co/datasets/ehartford/wizard_vicuna_70k_unfiltered)\n   6. There are other datasets formats, in some the output is partially masked (for completion suggestion models), but I have not worked and I am not familiar with those formats.\n6. From my experiments, things that can be totally wrong:\n   1. directly train a pre-trained model with less than 50000 data row is more or less useless. I would think of directly train a model when I have more than 100k data rows, for a 13B model and at least 1 mil for a 65B model.\n   2. with smaller datasets, it is efficient to train LoRA of qLoRA. \n   3. I prefer to train a 4 bit qLora 30B model than a fp16 LoRA for a 13B model (about same hw requirements, but the results with the 4bit 30B model are superior to the 13B fp16 model)\n\n&#x200B;\n\n## Comment ID jrg88pv with +49 score by [(cmndr_spanky, Reddit, 2023-07-10)](https://www.reddit.com/r/LocalLLaMA/comments/14vnfh2/my_experience_on_starting_with_fine_tuning_llms/jrg88pv/) (in reply to ID 14vnfh2):\nBy the way, HuggingFace's new \"Supervised Fine-tuning Trainer\" library makes fine tuning stupidly simple, [SFTTrainer() class](https://huggingface.co/docs/trl/main/en/sft_trainer) basically takes care of almost everything, as long as you can supply it a hugging face \"dataset\" that you've prepared for fine tuning. It should work with any model that's published properly to hugging face. Even fine tuning a 1b LLM on my consumer GPU at home, using NO quantization has yielded good results Fine tuning on the dataset that I tried.\n\n### Comment ID jrmqhqc with +8 score by [(Even_Squash5175, Reddit, 2023-07-12)](https://www.reddit.com/r/LocalLLaMA/comments/14vnfh2/my_experience_on_starting_with_fine_tuning_llms/jrmqhqc/) (in reply to ID jrg88pv):\nI'm also working on the finetuning of models for Q&A and I've finetuned llama-7b, falcon-40b, and oasst-pythia-12b using HuggingFace's SFT, [H2OGPT's finetuning script](https://github.com/h2oai/h2ogpt/blob/main/finetune.py) and [lit-gpt](https://github.com/Lightning-AI/lit-gpt).  \n\n\nHuggingFace's SFT is the slowest among them. I can fine tune a 12b model using LoRA for 10 epochs within 20 mins on 8 x A100 but with HF's SFT it takes almost a day. Im not sure if I'm doing something wrong. Do you have the same experience?  \n\n\nI like HF's SFT because the code is very simple and easy to use with HF's transformers library but the finetuning speed is a deterrence.\n\n#### Comment ID jrrct29 with +2 score by [(cmndr_spanky, Reddit, 2023-07-13)](https://www.reddit.com/r/LocalLLaMA/comments/14vnfh2/my_experience_on_starting_with_fine_tuning_llms/jrrct29/) (in reply to ID jrmqhqc):\nI’m seeing huge differences in performance depending on what CUDA PyTorch version is being used. Are you on the latest nightly build 12.1? Also bfloat16 makes a huge difference as well. Huge.\n\nEdit: also I forgot to ask. Are you using Lora / quantized training with SFTT as well? If not, you’re training using the full size / precision so it’s kind of an unfair comparison.\n\n#### Comment ID khxt7oh with +2 score by [(BlueAnyRoy, Reddit, 2024-01-15)](https://www.reddit.com/r/LocalLLaMA/comments/14vnfh2/my_experience_on_starting_with_fine_tuning_llms/khxt7oh/) (in reply to ID jrmqhqc):\nIs there any significant difference in performance besides training speed??\n\n### Comment ID la0mdun with +2 score by [(Infamous_Company_220, Reddit, 2024-06-24)](https://www.reddit.com/r/LocalLLaMA/comments/14vnfh2/my_experience_on_starting_with_fine_tuning_llms/la0mdun/) (in reply to ID jrg88pv):\nI have a doubt, I fine tuned a peft model using llama 2. when I inference , it returns out of the box (previous knowledge/ base knowledge). But I just only want the model to reply only with my private data. How can I achieve it ?\n\n#### Comment ID md3e9oy with +1 score by [(Vast-Knowledge-5758, Reddit, 2025-02-16)](https://www.reddit.com/r/LocalLLaMA/comments/14vnfh2/my_experience_on_starting_with_fine_tuning_llms/md3e9oy/) (in reply to ID la0mdun):\nhave you saved the model and tokenizer?  \ni saved the model to local with weights then uploaded to HF then used the same.  \nwhat i basically do is load base model then on top of that attach my weights/adapters.\n\nIt'll answer according to newly trained data.\n\nP.s. :- I train using lora on mistral-7B for 60k datarows.\n\n## Comment ID jrdshlv with +49 score by [(BlandUnicorn, Reddit, 2023-07-10)](https://www.reddit.com/r/LocalLLaMA/comments/14vnfh2/my_experience_on_starting_with_fine_tuning_llms/jrdshlv/) (in reply to ID 14vnfh2):\nWhen I was looking into fine tuning for a chatbot based on PDF’s, I actually realised that vector db and searching was much more effective to get answers that are straight from the document. Of course that was for this particular use case\n\n### Comment ID jree0eu with +19 score by [(Ion_GPT, Reddit, 2023-07-10)](https://www.reddit.com/r/LocalLLaMA/comments/14vnfh2/my_experience_on_starting_with_fine_tuning_llms/jree0eu/) (in reply to ID jrdshlv):\nIf you like embeddings and vector DB, you should look into this: [https://github.com/HKUNLP/instructor-embedding](https://github.com/HKUNLP/instructor-embedding)\n\n### Comment ID jre62kv with +9 score by [(heswithjesus, Reddit, 2023-07-10)](https://www.reddit.com/r/LocalLLaMA/comments/14vnfh2/my_experience_on_starting_with_fine_tuning_llms/jre62kv/) (in reply to ID jrdshlv):\nTools like that will speed up scientific research. I've been working on it, too. What OSS tools are you using right now? I'm especially curious about vector db's since I don't know much about them.\n\n#### Comment ID jreg8tn with +9 score by [(BlandUnicorn, Reddit, 2023-07-10)](https://www.reddit.com/r/LocalLLaMA/comments/14vnfh2/my_experience_on_starting_with_fine_tuning_llms/jreg8tn/) (in reply to ID jre62kv):\nI’m just using gpt3.5 and pinecone, since there’s so much info on using them and they’re super straight forward. Running through a FastAPI framework backend. I take ‘x’ of the closest vectors (which are just chunked from pdfs, about 350-400 words each) and run them back through the LLM with the original query to get an answer based on that data. \n\nI have been working on improving the data to work better with a vector db, and plain chunked text isn’t great.\n\nI do plan on switching to a local vector db later when I’ve worked out the best data format to feed it. And dream of one day using a local LLM, but the computer power I would need to get the speed/accuracy that 3.5 turbo gives would be insane.\n\nEdit - just for clarity, I will add I’m very new at this and it’s all been a huge learning curve for me.\n\n#### Comment ID jtxqexe with +1 score by [(Hey_You_Asked, Reddit, 2023-07-29)](https://www.reddit.com/r/LocalLLaMA/comments/14vnfh2/my_experience_on_starting_with_fine_tuning_llms/jtxqexe/) (in reply to ID jre62kv):\ncan you please say some more about your process?\n\nit's something I've been incredibly interested in - domain-specific knowledge from primary research/publications - and I'm at a loss how to go about it effectively.\n\nPlease, anything you can impart is super welcome. Thank you!\n\n### Comment ID jrkyadq with +3 score by [(SufficientPie, Reddit, 2023-07-11)](https://www.reddit.com/r/LocalLLaMA/comments/14vnfh2/my_experience_on_starting_with_fine_tuning_llms/jrkyadq/) (in reply to ID jrdshlv):\n> I actually realised that vector db and searching was much more effective to get answers that are straight from the document.\n\nYep, same.  This works decently well: https://github.com/freedmand/semantra\n\n#### Comment ID lk07xav with +1 score by [(kgphantom, Reddit, 2024-08-26)](https://www.reddit.com/r/LocalLLaMA/comments/14vnfh2/my_experience_on_starting_with_fine_tuning_llms/lk07xav/) (in reply to ID jrkyadq):\nwill semantra work over a database of text pulled from pdf files? or only the raw files themselves\n\n#### Comment ID jtz0erz with +1 score by [(Hey_You_Asked, Reddit, 2023-07-29)](https://www.reddit.com/r/LocalLLaMA/comments/14vnfh2/my_experience_on_starting_with_fine_tuning_llms/jtz0erz/) (in reply to ID jrkyadq):\nhave you considered DB-GPT or gpt-academic?\n\n### Comment ID jre8lu3 with +2 score by [(None, Reddit, 2023-07-10)](https://www.reddit.com/r/LocalLLaMA/comments/14vnfh2/my_experience_on_starting_with_fine_tuning_llms/jre8lu3/) (in reply to ID jrdshlv):\n[removed]\n\n#### Comment ID jregkv2 with +1 score by [(BlandUnicorn, Reddit, 2023-07-10)](https://www.reddit.com/r/LocalLLaMA/comments/14vnfh2/my_experience_on_starting_with_fine_tuning_llms/jregkv2/) (in reply to ID jre8lu3):\nYeah that all comes into, I’m working on that atm. Trying various things. The most basic to get around the context length is ‘chunking’ the pdfs into small sizes with overlap. But I’m trying a couple of different things to see if I can do better than that\n\n## Comment ID jre7azh with +40 score by [(killinghurts, Reddit, 2023-07-10)](https://www.reddit.com/r/LocalLLaMA/comments/14vnfh2/my_experience_on_starting_with_fine_tuning_llms/jre7azh/) (in reply to ID 14vnfh2):\nWhomever solves automated data integration from any format will be very rich.\n\n### Comment ID jrg0is3 with +13 score by [(teleprint-me, Reddit, 2023-07-10)](https://www.reddit.com/r/LocalLLaMA/comments/14vnfh2/my_experience_on_starting_with_fine_tuning_llms/jrg0is3/) (in reply to ID jre7azh):\nAfter a few months of research and a few days of attempting to organize data, extract it, and chunk it... \n\nYeah, I could see why.\n\n#### Comment ID kvj12fk with +2 score by [(Medium_Alternative50, Reddit, 2024-03-19)](https://www.reddit.com/r/LocalLLaMA/comments/14vnfh2/my_experience_on_starting_with_fine_tuning_llms/kvj12fk/) (in reply to ID jrg0is3):\ncheckout this video  \n[https://www.youtube.com/watch?v=fYyZiRi6yNE](https://www.youtube.com/watch?v=fYyZiRi6yNE)\n\n#### Comment ID kva5gu1 with +1 score by [(Medium_Alternative50, Reddit, 2024-03-17)](https://www.reddit.com/r/LocalLLaMA/comments/14vnfh2/my_experience_on_starting_with_fine_tuning_llms/kva5gu1/) (in reply to ID jrg0is3):\nwhat type of data have you faced problem in?\n\n### Comment ID kvj0zla with +2 score by [(Medium_Alternative50, Reddit, 2024-03-19)](https://www.reddit.com/r/LocalLLaMA/comments/14vnfh2/my_experience_on_starting_with_fine_tuning_llms/kvj0zla/) (in reply to ID jre7azh):\nI found this video, for creating QnA dataset why not use something like this? \n\n[https://www.youtube.com/watch?v=fYyZiRi6yNE](https://www.youtube.com/watch?v=fYyZiRi6yNE)\n\n### Comment ID jrfs345 with +2 score by [(jacobschauferr, Reddit, 2023-07-10)](https://www.reddit.com/r/LocalLLaMA/comments/14vnfh2/my_experience_on_starting_with_fine_tuning_llms/jrfs345/) (in reply to ID jre7azh):\nwhat do you mean? can you elaborate please?\n\n#### Comment ID jrih0hz with +6 score by [(MINIMAN10001, Reddit, 2023-07-11)](https://www.reddit.com/r/LocalLLaMA/comments/14vnfh2/my_experience_on_starting_with_fine_tuning_llms/jrih0hz/) (in reply to ID jrfs345):\nI mean as he said thousands of pages manually and tediously constructing \"instruction input output.\"\n\nAutomating that process means automating away thousands of pages of manual tedious work.\n\n#### Comment ID jrg3mg5 with +4 score by [(None, Reddit, 2023-07-10)](https://www.reddit.com/r/LocalLLaMA/comments/14vnfh2/my_experience_on_starting_with_fine_tuning_llms/jrg3mg5/) (in reply to ID jrfs345):\ndid you read the original post?\n\n### Comment ID kgiv6qc with +1 score by [(lacooljay02, Reddit, 2024-01-06)](https://www.reddit.com/r/LocalLLaMA/comments/14vnfh2/my_experience_on_starting_with_fine_tuning_llms/kgiv6qc/) (in reply to ID jre7azh):\nWell [chatbase.co](https://chatbase.co) is pretty close\n\nAnd you are correct, he is [swimming](https://x.com/yasser_elsaid_/status/1653411314427985921?s=20) [in cash](https://x.com/yasser_elsaid_/status/1726961060668940782?s=20) (tho i dont know his overhead cost ofc)\n\n## Comment ID jrdztzf with +34 score by [(Paulonemillionand3, Reddit, 2023-07-10)](https://www.reddit.com/r/LocalLLaMA/comments/14vnfh2/my_experience_on_starting_with_fine_tuning_llms/jrdztzf/) (in reply to ID 14vnfh2):\nThis should be pinned/added to the FAQ. Great work, thanks.\n\n## Comment ID jrdrjxg with +8 score by [(Hussei911, Reddit, 2023-07-10)](https://www.reddit.com/r/LocalLLaMA/comments/14vnfh2/my_experience_on_starting_with_fine_tuning_llms/jrdrjxg/) (in reply to ID 14vnfh2):\nis there a way to fine tune on cpu local machine ? , or on ram?\n\n### Comment ID jreik5u with +21 score by [(BlandUnicorn, Reddit, 2023-07-10)](https://www.reddit.com/r/LocalLLaMA/comments/14vnfh2/my_experience_on_starting_with_fine_tuning_llms/jreik5u/) (in reply to ID jrdrjxg):\nI’ve blocked the guy who’s replied to you (newtecture) He’s absolutely toxic and thinks he’s gods gift to r/LocalLLaMA. \n\nEveryone should just report him and hopefully he gets the boot\n\n#### Comment ID jrewaes with +8 score by [(Hussei911, Reddit, 2023-07-10)](https://www.reddit.com/r/LocalLLaMA/comments/14vnfh2/my_experience_on_starting_with_fine_tuning_llms/jrewaes/) (in reply to ID jreik5u):\nI really appreciate you looking out for the community.\n\n### Comment ID kzom2tr with +5 score by [(kurtapyjama, Reddit, 2024-04-15)](https://www.reddit.com/r/LocalLLaMA/comments/14vnfh2/my_experience_on_starting_with_fine_tuning_llms/kzom2tr/) (in reply to ID jrdrjxg):\ni think you can use google colab or kaggle free version for fine tuning and then download the model. Kaggle is pretty decent.\n\n## Comment ID jrea1r4 with +6 score by [(ProlapsedPineal, Reddit, 2023-07-10)](https://www.reddit.com/r/LocalLLaMA/comments/14vnfh2/my_experience_on_starting_with_fine_tuning_llms/jrea1r4/) (in reply to ID 14vnfh2):\nI've been a .net dev since forever, started coding during the .net boom with asp/vb6. For the past 10 years most of the work has been CMS websites, integrations, services etc. I am very interested in what you're talking about.\n\nRight now I'm building my own application with Semantic Kernel and looking into using embeddings as you suggested, but this is my MVP. I think you're on the right track for setting up enterprises with private LLMs.\n\nI assume that enterprises will have all of their data, all of it, integrated into a LLM. Every email, transcribed teams conversation, legal paper, research study, all of it from HR to what you say on Slack. \n\n(Are you seeding the data or also setting up ongoing processes to incorporate new data in batches as time goes on?)\n\nI also assume that  there will be significant room for custom agent / copilots.  An agent could process an email, identify the action items, search active directory for the experts, pull together a new report for the team to discuss, schedule the team meeteing, transcribe the outcome, and then consume the followups as well.\n\nAgents could be researching markets and devising new marketing campaigns, writing the copy, and routing the proposal to human actors for approval and feedback. There's so much that could be done, its all very exciting.\n\nHave you considered hosting training? I'm planning on taking off 3-6 months to work on my application and dig into what can be done with these techs.\n\n### Comment ID jri7nr7 with +5 score by [(Ion_GPT, Reddit, 2023-07-11)](https://www.reddit.com/r/LocalLLaMA/comments/14vnfh2/my_experience_on_starting_with_fine_tuning_llms/jri7nr7/) (in reply to ID jrea1r4):\n>Are you seeding the data or also setting up ongoing processes to incorporate new data in batches as time goes on?\n\nYes and no. This is an expensive process. First, it is important to choose to train an LLM with static data. For example, the user manual for a specific car model it is fully static, not going to change. Each car model is a different LLM.  This is not always possible, there is data that is not static, but it is rarely modifying. In this case, setting a process to accumulate new training data and create a LoRA from time to time and every x months merge the LoRAs into the model.\n\n>I also assume that there will be significant room for custom agent / copilots. An agent could process an email, identify the action items, search active directory for the experts, pull together a new report for the team to discuss, schedule the team meeteing, transcribe the outcome, and then consume the followups as well.\n\nYes, there is a lot of potential. You can check this project for agents: https://github.com/Nuggt-dev/Nuggt/ . Currently I only have \"simple\" projects: mostly 0-shots LLMs to get some responses. Agents are not yet mature enough to be integrated in production environments.\n\n#### Comment ID jrj121r with +1 score by [(ProlapsedPineal, Reddit, 2023-07-11)](https://www.reddit.com/r/LocalLLaMA/comments/14vnfh2/my_experience_on_starting_with_fine_tuning_llms/jrj121r/) (in reply to ID jri7nr7):\nThanks for the reply and the info! \n\nI agree that agents aren't mature. I've been cannibalizing the samples from msft and developing my own patterns. I find that I get improved results using a method where I use the OpenAI  api multiple times for every ask. \n\nFor example, I will give the initial prompt requesting a completion. Then I will prep a new prompt that reiterates what the critical path is for a usable response, send the rules and the openai response back to the api, and ask it to provide feedback on how it could be improved in a bullet format.\n\nThen the initial response, and the editorial comments are sent back in a request to make the suggested changes so that the response is compliant with my rules.\n\nWe confirm that the response is usable, and then can proceed  to the next step of automation.\n\nAsk -> Review -> Edit -> Approve\n\nIs the cycle I have been using in code. I think that this helps when the api drops the ball once in a while, you get a chance to realign the answer if it was off track. Important for a system that is running with hands off the wheel.\n\n## Comment ID jrdw2l9 with +9 score by [(sandys1, Reddit, 2023-07-10)](https://www.reddit.com/r/LocalLLaMA/comments/14vnfh2/my_experience_on_starting_with_fine_tuning_llms/jrdw2l9/) (in reply to ID 14vnfh2):\nHey thanks for this. This is a great intro to fine-tuning.\n\nI have two questions:\n\n1. What is this #instruction, #input, #oytput format for fine-tuning? Do all models accept this input. I know what is input/output...but I don't know what instruction is doing. Is there any example repos u would suggest we study to get a better idea ?\n\n2. If I have a bunch of private documents. Let's say on \"dog health\". These are not input/output...but real documents. Can we fine-tune using this ? Do we have to create the same dataset using the pdf ? How ?\n\n### Comment ID jrefyow with +14 score by [(Ion_GPT, Reddit, 2023-07-10)](https://www.reddit.com/r/LocalLLaMA/comments/14vnfh2/my_experience_on_starting_with_fine_tuning_llms/jrefyow/) (in reply to ID jrdw2l9):\n>What is this #instruction, #input, #oytput format for fine-tuning? Do all models accept this input. I know what is input/output...but I don't know what instruction is doing. Is there any example repos u would suggest we study to get a better idea ?\n\nCheck this dataset, this is a standard #instruction,#input,#output.\n[https://huggingface.co/datasets/yahma/alpaca-cleaned](https://huggingface.co/datasets/yahma/alpaca-cleaned)\nInput is optional. All LLaMA based models accept this format and also some non LLaMA based.\n\n>If I have a bunch of private documents. Let's say on \"dog health\". These are not input/output...but real documents. Can we fine-tune using this ? Do we have to create the same dataset using the pdf ? How ?\n\nFirst, the limitation is in the number of tokens that model can ingest at once. Most of the models are limited at 2048, you can't feed a PDF in that context. Then, if you are going to split it, how are you going to do that? How you decide what is a good place to split it?\n\nNext, training or fine tuning a model means that you show the model how you want to interact with it and hope it will learn and can imitate in the future. This includes also the prompt format. If you only throw PDFs to it, how do you expect to learn to answer questions?\n\nAs I said, preparing data is the hardest part of creating a good chatbot, not the training itself. If you want to just throw raw data, use embeddings, very easy to use with superbooga extension in oobabooga and actually works fine. It is just not a chatbot to be exposed to clients.\n\nIf you are in the embeddings, you might like this project: https://github.com/HKUNLP/instructor-embedding\n\n#### Comment ID jregyo0 with +2 score by [(sandys1, Reddit, 2023-07-10)](https://www.reddit.com/r/LocalLLaMA/comments/14vnfh2/my_experience_on_starting_with_fine_tuning_llms/jregyo0/) (in reply to ID jrefyow):\nSo I didn't understand ur answer about the documents. I hear you when u say \"give it in a question answer format\", but how do people generally do it when they have ...say about 100K PDFs?\n\nI mean base model training is also on documents right ? The world corpus is not in a QA set. So I'm wondering from that perspective ( not debating...but just asking what is the practical way out of this).\n\n### Comment ID jre67qd with +2 score by [(Koliham, Reddit, 2023-07-10)](https://www.reddit.com/r/LocalLLaMA/comments/14vnfh2/my_experience_on_starting_with_fine_tuning_llms/jre67qd/) (in reply to ID jrdw2l9):\nI would also like to know. Making up questions would be more exhausting than having the model \"understand\" the text and be able to answer based on the content of the document\n\n### Comment ID jrgjlo1 with +1 score by [(tronathan, Reddit, 2023-07-10)](https://www.reddit.com/r/LocalLLaMA/comments/14vnfh2/my_experience_on_starting_with_fine_tuning_llms/jrgjlo1/) (in reply to ID jrdw2l9):\n>real documents\n\nEven \"real documents\" have some structure - Are they paragraphs of text? Fiction? Nonfiction? Chat logs? Treasure maps with a big \"X\" marking the spot?\n\n## Comment ID jrdxssp with +3 score by [(a_beautiful_rhind, Reddit, 2023-07-10)](https://www.reddit.com/r/LocalLLaMA/comments/14vnfh2/my_experience_on_starting_with_fine_tuning_llms/jrdxssp/) (in reply to ID 14vnfh2):\nI had luck just using input/output without instruction too. I agree the dataset preparation is the hardest part. Very few dataset tools out there. Everything is a cobbled together python script.\n\nI have not done one way quotes yet but I plan to. Perhaps that one will be better with instruction + quote.\n\n    instruction: Below is a quote written in the style that the person would write.\n    input:\n    output: \"Blah blah blah\"\n\n## Comment ID jrdupcl with +3 score by [(brown2green, Reddit, 2023-07-10)](https://www.reddit.com/r/LocalLLaMA/comments/14vnfh2/my_experience_on_starting_with_fine_tuning_llms/jrdupcl/) (in reply to ID 14vnfh2):\n>On the hardware front, while it's possible to train a qLoRA on a single 3090, I wouldn't recommend it. There are too many limitations, and even browsing the web while training could lead to OOM. I personally use a cloud A6000 with 48GB VRAM, which costs about 80 cents per hour.\n\nYou can use your integrated GPU for browsing and other activities and avoid OOM due to that.\n\n### Comment ID jrdx7gw with +5 score by [(a_beautiful_rhind, Reddit, 2023-07-10)](https://www.reddit.com/r/LocalLLaMA/comments/14vnfh2/my_experience_on_starting_with_fine_tuning_llms/jrdx7gw/) (in reply to ID jrdupcl):\nDefinitely want to have no other things using the GPUs you are training with. Should be a dedicated PC, not something used for browsing. Chrome locks up the entire PC and then your run is done. Hope you can resume after the reboot.\n\nThe real reason to rent A100s is time and to run larger batch sizes.\n\n4bit lora can train a 13b on 50-100k items in like a day or two.  For 30b the time goes up since batch size goes down. The neat thing is you can just use the predicted training time and tweak the context/batches to see how long it will run. \n\nIf it gives you a time of 5 days, A100s start looking way better.\n\n#### Comment ID jre0660 with +2 score by [(hp1337, Reddit, 2023-07-10)](https://www.reddit.com/r/LocalLLaMA/comments/14vnfh2/my_experience_on_starting_with_fine_tuning_llms/jre0660/) (in reply to ID jrdx7gw):\nWhat hardware are you using to train 50k-100k items on 13b model in 1 day? A 4090?\n\n## Comment ID jre8f13 with +3 score by [(Sensitive-Analyst288, Reddit, 2023-07-10)](https://www.reddit.com/r/LocalLLaMA/comments/14vnfh2/my_experience_on_starting_with_fine_tuning_llms/jre8f13/) (in reply to ID 14vnfh2):\nAwsome, what do u think about 13b models are they any good? How long does a typical fine tuning takes in cloud? How did u find clients at first? Elaborate more on structured data  formats that u use, I'm doing fine tuning on functional programming questions which need stuctures and formating ,ur say would be interesting\n\n### Comment ID jri93tl with +2 score by [(Ion_GPT, Reddit, 2023-07-11)](https://www.reddit.com/r/LocalLLaMA/comments/14vnfh2/my_experience_on_starting_with_fine_tuning_llms/jri93tl/) (in reply to ID jre8f13):\nI added an update to the original post. \n\nI always prefer bigger models, when possible. A 30B model with a 4 bit qLoRA outperforms a fp16 fine tuned 13B model.\n\nBut again, all those are tools, there is no generic \"best tool\", but \"best tool for the task\", so there are cases when 7B or even a 3B is the best suited.\n\n## Comment ID jrebb9t with +3 score by [(GreenTeaBD, Reddit, 2023-07-10)](https://www.reddit.com/r/LocalLLaMA/comments/14vnfh2/my_experience_on_starting_with_fine_tuning_llms/jrebb9t/) (in reply to ID 14vnfh2):\nYeah, back when I wrote my guide to it, back before LLaMA existed so it was all GPT-Neo and GPTJ, and before LoRAs were an option so your options were a full fine tune without many tools to do it, even then the actual finetuning was by far the easiest part.\n\nAnd at the time all the documentation to it covered the actual fine tuning, and then data preparation was all \"And then prepare your data, you know, in the way you do it\" with no actual description on how.\n\nI ended up practically building my own toolset for it, which had to be specific to my data and my format because that was literally easier than doing it any other way. And I feel back then everyone had their own way. Though now it's been somewhat standardized. But now the problem is it can cause problems when you need your data to *not* fit the standard way everyone was does it now since the tools out there kinda assume the format.\n\nI still think a lot of the documentation out there glosses over properly formatting your data.\n\n## Comment ID jrg694l with +3 score by [(mmmm_frietjes, Reddit, 2023-07-10)](https://www.reddit.com/r/LocalLLaMA/comments/14vnfh2/my_experience_on_starting_with_fine_tuning_llms/jrg694l/) (in reply to ID 14vnfh2):\nHow did you find clients? Or how did they find you?\n\n### Comment ID jri7w4b with +7 score by [(Ion_GPT, Reddit, 2023-07-11)](https://www.reddit.com/r/LocalLLaMA/comments/14vnfh2/my_experience_on_starting_with_fine_tuning_llms/jri7w4b/) (in reply to ID jrg694l):\nI have 15 years of sw dev freelancing. I contacted my former clients and asked them if they are interested to explore what \"AI can help their business\". \nOutside of former clients, I was not able to find new clients, I have no idea where to search for them. I got few DMs here on Reddit, but nothing started yet, just discovery discussions.\n\n## Comment ID jrg6cao with +3 score by [(None, Reddit, 2023-07-10)](https://www.reddit.com/r/LocalLLaMA/comments/14vnfh2/my_experience_on_starting_with_fine_tuning_llms/jrg6cao/) (in reply to ID 14vnfh2):\nVery cool reading this, I just graduated from uni and I’ve spent the past month getting lots of practice with language models to try to get into your line of work. If you don’t mind, I’d love to hear more about where to find these jobs. I imagine the kind of LLM chatbots you put together for companies are going to become a lot more sophisticated over the next few years, as the models that they’re based on become more multimodal, as context sizes become longer, and as clients become more comfortable doing their work through the interface of a chatbot.\n\n### Comment ID jribip5 with +4 score by [(Ion_GPT, Reddit, 2023-07-11)](https://www.reddit.com/r/LocalLLaMA/comments/14vnfh2/my_experience_on_starting_with_fine_tuning_llms/jribip5/) (in reply to ID jrg6cao):\nNice, I think it is an exciting moment to be young and just starting during those times.\n\nI do not know about jobs, I used to work in a huge corporation that suck my life out of me, I left that and I did freelancing for 15 years. 9 to 6 jobs are not for me.\n\nSince last December I decided to fully focus on LLMs. I got in touch with my former sw dev clients and asked if they would be interested in finding out how LLMs can help their business.\n\n## Comment ID jrj2eoa with +3 score by [(captam_morgan, Reddit, 2023-07-11)](https://www.reddit.com/r/LocalLLaMA/comments/14vnfh2/my_experience_on_starting_with_fine_tuning_llms/jrj2eoa/) (in reply to ID 14vnfh2):\nFantastic write up! You should publish a more detailed version safe for public on Medium to earn a few bucks. \n\nWhat are your thoughts on the top comments on the  post below empirically and anecdotally? They mentioned even top fine-tuned OSS models are still unreasonable vs GPT4. Or that fine-tuning on specific data undoes the instruct transfer learning unless you do it on more instructions. Or that vector search dumbs down the full potential of LLMs.\n\n\n[r/MachineLearning post on LLM implementation](https://www.reddit.com/r/MachineLearning/comments/14tr1te/d_hardest_thing_about_building_with_llms/?utm_source=share&utm_medium=ios_app&utm_name=ioscss&utm_content=1&utm_term=1)\n\n## Comment ID jrkkxb9 with +3 score by [(why_not_zoidberg_82, Reddit, 2023-07-11)](https://www.reddit.com/r/LocalLLaMA/comments/14vnfh2/my_experience_on_starting_with_fine_tuning_llms/jrkkxb9/) (in reply to ID 14vnfh2):\nAwesome content! My question is actually on the business front: how do you compete with those solutions like await.ai or the ones from big companies like chatbots by salesforce?\n\n### Comment ID ltsqyfg with +1 score by [(Zestyclose_Score4262, Reddit, 2024-10-26)](https://www.reddit.com/r/LocalLLaMA/comments/14vnfh2/my_experience_on_starting_with_fine_tuning_llms/ltsqyfg/) (in reply to ID jrkkxb9):\nIt's not necessary to always compete with larget enterprises. In reality, you will find not every customer can exactly get what they want from salesforce. It might be issues of price, service, responding speed...etc. Huge enterprise can get billions dollars but small company can also have opportunity to earn million dollars, so why not?\n\n## Comment ID kd7l5nw with +3 score by [(tiro2000, Reddit, 2023-12-13)](https://www.reddit.com/r/LocalLLaMA/comments/14vnfh2/my_experience_on_starting_with_fine_tuning_llms/kd7l5nw/) (in reply to ID 14vnfh2):\nThanks for the informative post, I have a problem which is after fine-tuning llama-2-7b-HF on a set of 80 French Question and answer records generated from French PDF Report, I even used GPT4 to generate most of them then reviewed for them to be unique, goal to let the model be trained on this report to capture tone, style of report. having same structure \"### Question### Response\" , or whatever tried other templates besides alpaca, <INST> or open Assistant, used Lora , Though outcome of valuation loss is very good, but the model when generating outcomes keeps repeating the question in the answer or template used no matter what template I am using, at least repeating question , I played with generating parameters like penelty = 2 , max\\_tokens , data set seems fine with no repeating pattern for questions. but still same issue, please advise  \nThanks\n\n## Comment ID jrdszqx with +9 score by [(nightlingo, Reddit, 2023-07-10)](https://www.reddit.com/r/LocalLLaMA/comments/14vnfh2/my_experience_on_starting_with_fine_tuning_llms/jrdszqx/) (in reply to ID 14vnfh2):\nThanks for the amazing overview! It is great that you decided to share your professional experience with the community. I've seen many people claim that: fine-tuning is only for teaching the model how to perform tasks , or respond in a certain way, but, for adding new knowledge the only way is to use vector databases. It is interesting that your practical experience is different and that you managed to instill actual new knowledge via fine tuning.\nDid you actually observe the model making use of the new knowledge / facts contained in the finetune dataset?\n\nThanks!\n\n### Comment ID jrei1c6 with +14 score by [(Ion_GPT, Reddit, 2023-07-10)](https://www.reddit.com/r/LocalLLaMA/comments/14vnfh2/my_experience_on_starting_with_fine_tuning_llms/jrei1c6/) (in reply to ID jrdszqx):\n>Did you actually observe the model making use of the new knowledge / facts contained in the finetune dataset?\n\nYes, that is the entire point. Of course, you need to decide what can be fine tuned and what can't. \nI will give you a fictional example for a common thing that I do fine tuning. The user manual of a BMW F30 340i M5 from 2017 (fictional model) has a 1000 pages. However, nothing will change in that manual, ever. You will not get the \"new version\" of the manual.\n\nInstead, you can get a mobile app, or a web link where you can talk with \"your friendly, helpful, digital BMW user manual\" that is able to answer all questions about the content of the manual. With the mobile application, you could even \"talk\" with the manual.\nYou don't even have to select your model, by using the account you already have, it will know what model you own and it will select the right manual. Or, in the worst case you will have to enter the number the car has on the window, behind the wheel.\n\nBehind this, is a custom fine tuned LLM fully trained on that manual. This is something of great success and impress older folks. And older folks in general have more money to spend on expensive, useless stuff.\n\nPlease note the the example above is fictional, I am not training LLMs for BMW (I do it for other companies).\n\nIf your business is a restaurant, it is harder to find something that it is static for longer period to worth doing a model training. You still can train an online ordering chat, combined with embeddings to take in orders.\n\nYou need to understand that all those things are tools. Like any tool, some are good at some things and shit in other situations. There is no universal tool that is good for everything.\n\n#### Comment ID jri1fw5 with +1 score by [(Jian-L, Reddit, 2023-07-11)](https://www.reddit.com/r/LocalLLaMA/comments/14vnfh2/my_experience_on_starting_with_fine_tuning_llms/jri1fw5/) (in reply to ID jrei1c6):\n>If your business is a restaurant, it is harder to find something that it is static for longer period to worth doing a model training. You still can train an online ordering chat, combined with embeddings to take in orders.\n\nThank you, OP. Your examples are truly insightful and align perfectly with what I was hoping to glean from this thread. I've been grappling with the decision of whether to first learn a library like [LlamaIndex](https://github.com/jerryjliu/llama_index), or start with fine-tuning LLM.\n\nIf my understanding is accurate, it seems that LlamaIndex was designed for situations akin to your second example. However, one limitation of libraries like LlamaIndex is the constraint posed by the LLM context — it simply can't accommodate all the nuanced, private knowledge relating to the question.\n\nLooking towards the future, as LLM fine-tuning and training become increasingly mature and cost-effective, do you envision a shift in this limitation? Will we eventually see the removal of the LLM context constraint or is it more likely that tools like LlamaIndex will persist for an extended period due to their specific utility?\n\n#### Comment ID jrw9m0l with +1 score by [(Worldly-Researcher01, Reddit, 2023-07-14)](https://www.reddit.com/r/LocalLLaMA/comments/14vnfh2/my_experience_on_starting_with_fine_tuning_llms/jrw9m0l/) (in reply to ID jrei1c6):\n“Did you actually observe the model making use of the new knowledge / facts contained in the finetune dataset?”\n\nHi OP, thanks so much for your post. To piggyback on the previous post, did you see any sort of emergent knowledge or synthesis of the knowledge? Using your fictional user manual of a BMW for example, would it be able to synthesize answers from two distant parts of the manual? Would you be able to compare and contrast a paragraph from the manual with say a Shakespearean play? Is it able to apply reasoning to ideas that are contained in the user manual? Or perhaps use the ideas in the manual to do some kind of reasoning?\n\nI have always thought fine tuning is only to train the model to following instructions, so your post came as a big surprise.\n\nI am wondering whether it is capable of going beyond just direct regurgitation of facts that is contained in the user manual.\n\n#### Comment ID jsanx7x with +1 score by [(Warm-Interaction-989, Reddit, 2023-07-17)](https://www.reddit.com/r/LocalLLaMA/comments/14vnfh2/my_experience_on_starting_with_fine_tuning_llms/jsanx7x/) (in reply to ID jrei1c6):\nThank you for your previous reply and for sharing your experience on this issue. Nevertheless, I have a few more questions if you don't mind.\n\nWill the BMW manual use a data format such as #instruction, #input, #output? I just need a little confirmation.\n\nAlso, how would you generate the data? Would you simply generate question-answer pairs from the manual? If so, do you think the model would cope with a long conversation, or would it only be able to answer single questions? -> What would your approach be for the model to be able to have a longer conversation?\n\nOne last thing, would the model be able to work well and be useful without being fed some external context such as a suitable piece of manual before answering, or would it just pull answers out of thin air without any context?\n\nYour additional details would be very helpful, thanks!\n\n## Comment ID jre1r0p with +4 score by [(shr1n1, Reddit, 2023-07-10)](https://www.reddit.com/r/LocalLLaMA/comments/14vnfh2/my_experience_on_starting_with_fine_tuning_llms/jre1r0p/) (in reply to ID 14vnfh2):\nGreat write up. I am sure many would also be interested in one walkthrough of entire process. How do you adapt repo example to your particular use case, what is the process of transcribing your data in documents and pdfs to generate training data, iterations and validation process and how do you engage the users to do this process. And also ongoing refinement based on real world usage,how to incorporate that feedback into refining.\n\n## Comment ID jrdy88s with +2 score by [(russianguy, Reddit, 2023-07-10)](https://www.reddit.com/r/LocalLLaMA/comments/14vnfh2/my_experience_on_starting_with_fine_tuning_llms/jrdy88s/) (in reply to ID 14vnfh2):\n> shaping your data in the correct format is, without a doubt, the most difficult and time-consuming step when creating a Language Learning Model (LLM) for your company's documentation, processes, support, sales, and so forth\n\nThis is so true.\n\nCan you give some training data examples? What worked for you, what didn't? \n\nThe issue with GPT4 lies in it's limited context, some of the documentation could be quite large.\n\n### Comment ID jri80jb with +1 score by [(Ion_GPT, Reddit, 2023-07-11)](https://www.reddit.com/r/LocalLLaMA/comments/14vnfh2/my_experience_on_starting_with_fine_tuning_llms/jri80jb/) (in reply to ID jrdy88s):\nI added an update to the original post\n\n#### Comment ID loh3wxc with +1 score by [(THEWIDOWS0N, Reddit, 2024-09-23)](https://www.reddit.com/r/LocalLLaMA/comments/14vnfh2/my_experience_on_starting_with_fine_tuning_llms/loh3wxc/) (in reply to ID jri80jb):\nOh I know its not a mystery the level of my \"honeypot\".\n\n## Comment ID jrez5yy with +2 score by [(Most-Procedure-2201, Reddit, 2023-07-10)](https://www.reddit.com/r/LocalLLaMA/comments/14vnfh2/my_experience_on_starting_with_fine_tuning_llms/jrez5yy/) (in reply to ID 14vnfh2):\nThis is great, thank you for sharing. \n\nI wanted to ask, as it relates to the work you do on this for your clients, how does your team look like in terms of size / expertise? Assuming the timelines are different per project, do you also run your consulting projects in parallel?\n\n### Comment ID jriamrv with +6 score by [(Ion_GPT, Reddit, 2023-07-11)](https://www.reddit.com/r/LocalLLaMA/comments/14vnfh2/my_experience_on_starting_with_fine_tuning_llms/jriamrv/) (in reply to ID jrez5yy):\nI am single, independent, freelance consultant. (Well, I have my wife helping with accounting, insurance, contracts, paperwork in general).\n\nI am responsible for my work, working alone allows me to be involved in all aspects of the project.\n\nI am consulting and charging by hour. I discuss to identify the problem and expectations, I present a plan of action with a clear definition of the expected result and what I will do during the project and what will fall under the client's responsibility.\n\nFor example, I am involved in a project were around 60 people from client's organisation worked to curate data for 3 months. I created a small tool to help with the data curation and their people took care of it. For the duration of this project, I had weekly checkups with the client to see the progress and trained a weekly LoRA based on the curated data to observer progress in knowledge based on the data accumulation.\n\nYes, I am consulting multiple clients in parallel, most clients opt for an initial setup then for few hours per week. My goal is to enable my clients to build, host, manage and maintain own bots.\n\n## Comment ID jrf62yb with +2 score by [(gentlecucumber, Reddit, 2023-07-10)](https://www.reddit.com/r/LocalLLaMA/comments/14vnfh2/my_experience_on_starting_with_fine_tuning_llms/jrf62yb/) (in reply to ID 14vnfh2):\nHave you fine tuned any of the coding bots with lora/qlora? I've been trying to do so with my own dataset for weeks, but I haven't found one lora tuning method that works with any of the tuned starcoder models like starcoderplus or starchat, or even the 3b replit model. What do you recommend?\n\n### Comment ID jri85vw with +2 score by [(Ion_GPT, Reddit, 2023-07-11)](https://www.reddit.com/r/LocalLLaMA/comments/14vnfh2/my_experience_on_starting_with_fine_tuning_llms/jri85vw/) (in reply to ID jrf62yb):\nNo. I am in preliminary discovery phase with a client to train a pre trained coding model with the inhouse codebase, but I have not started anything yet.\n\n#### Comment ID jrkhrea with +1 score by [(gentlecucumber, Reddit, 2023-07-11)](https://www.reddit.com/r/LocalLLaMA/comments/14vnfh2/my_experience_on_starting_with_fine_tuning_llms/jrkhrea/) (in reply to ID jri85vw):\nWanna colab? I'm a junior backend dev and I've been trying to figure this out for like 3 weeks. Maybe I could save you some trouble before you start. I'm trying to find any way to fine tune any version of the starcoder models without breaking my wallet. They don't play nicely with all the standard qlora repos and notebooks because everything is based on llama. MPT looks good as well, but again, very little support from the open source community. Joshdurbin has a hacked version of mpt-30b that's compatible with qlora if you use his repository, but I only got it to start training once, and killed it because it was set to take 150 hours on an A100... Kinda defeats the point of qlora, for me at least\n\n## Comment ID jrfbpef with +2 score by [(insultingconsulting, Reddit, 2023-07-10)](https://www.reddit.com/r/LocalLLaMA/comments/14vnfh2/my_experience_on_starting_with_fine_tuning_llms/jrfbpef/) (in reply to ID 14vnfh2):\nSuper interesting. What would be the average cost and time to finetune a 13B model with a 1K-10K dataset, in your experience? Based on information on this thread, I would imagine it might cost as little as a day and $10 USD, but that sounds too cheap.\n\n### Comment ID jri898i with +4 score by [(Ion_GPT, Reddit, 2023-07-11)](https://www.reddit.com/r/LocalLLaMA/comments/14vnfh2/my_experience_on_starting_with_fine_tuning_llms/jri898i/) (in reply to ID jrfbpef):\nWith this kind of dataset you should train a LoRA. It would cost less than 10$\n\n### Comment ID jrfcsd5 with +1 score by [(mehrdotcom, Reddit, 2023-07-10)](https://www.reddit.com/r/LocalLLaMA/comments/14vnfh2/my_experience_on_starting_with_fine_tuning_llms/jrfcsd5/) (in reply to ID jrfbpef):\nI was under the impression once you fine tune your data, it will not require a significant GPU to run it. I believe a 13b would fit in a 3090. I am also new to this so hoping to learn more about this myself.\n\n#### Comment ID jrfikmu with +1 score by [(insultingconsulting, Reddit, 2023-07-10)](https://www.reddit.com/r/LocalLLaMA/comments/14vnfh2/my_experience_on_starting_with_fine_tuning_llms/jrfikmu/) (in reply to ID jrfcsd5):\nYes, inference would be free and just as fast as your hardware. But for finetuning I previously assumed a very long training time would be needed. OP says you can rent a A6000 for 80 cents/hour, I was wondering how many hours would be needed in such a setup for decent results with a small-ish dataset.\n\n## Comment ID jricaca with +2 score by [(Vaylonn, Reddit, 2023-07-11)](https://www.reddit.com/r/LocalLLaMA/comments/14vnfh2/my_experience_on_starting_with_fine_tuning_llms/jricaca/) (in reply to ID 14vnfh2):\nWhat about [https://gpt-index.readthedocs.io/en/latest/](https://gpt-index.readthedocs.io/en/latest/) that does exactly the job !\n\n## Comment ID jrisnhm with +2 score by [(wensle, Reddit, 2023-07-11)](https://www.reddit.com/r/LocalLLaMA/comments/14vnfh2/my_experience_on_starting_with_fine_tuning_llms/jrisnhm/) (in reply to ID 14vnfh2):\nThank you very much for writing this out. Really useful information!\n\n## Comment ID jrkopyx with +2 score by [(ajibawa-2023, Reddit, 2023-07-11)](https://www.reddit.com/r/LocalLLaMA/comments/14vnfh2/my_experience_on_starting_with_fine_tuning_llms/jrkopyx/) (in reply to ID 14vnfh2):\nHello, Thank you very much for the detailed post! It clarified certain doubts.\n\n## Comment ID jrl5owz with +2 score by [(happyandaligned, Reddit, 2023-07-11)](https://www.reddit.com/r/LocalLLaMA/comments/14vnfh2/my_experience_on_starting_with_fine_tuning_llms/jrl5owz/) (in reply to ID 14vnfh2):\nSharing your personal experience with LLM's is super-useful. Thank you.\n\nHave you ever had a chance to use Reinforcement Learning with Human Feedback (RLHF) in order to align the system responses with human preferences? How are companies currently handling issues like bias, toxicity, sarcasm etc. in the model responses?\n\nFor those interested, you can learn more on hugging face - [https://huggingface.co/blog/rlhf](https://huggingface.co/blog/rlhf)\n\n## Comment ID juozzwu with +2 score by [(vislia, Reddit, 2023-08-03)](https://www.reddit.com/r/LocalLLaMA/comments/14vnfh2/my_experience_on_starting_with_fine_tuning_llms/juozzwu/) (in reply to ID 14vnfh2):\nThanks for sharing the experience! I've been fine tuning with my custom data on llama2. I only used very few rows of custom data, and was hoping to test water with fine tuning. However, it seems the model couldn't learn to adapt to my custom data. Not sure if it was due to too few data. Anything I could do to improve this?\n\n### Comment ID k97jr1x with +1 score by [(ARandomNiceAnimeGuy, Reddit, 2023-11-14)](https://www.reddit.com/r/LocalLLaMA/comments/14vnfh2/my_experience_on_starting_with_fine_tuning_llms/k97jr1x/) (in reply to ID juozzwu):\nLet me know if you got an answer to this. Ive seen that copy pasting the data seems to increase the success rate of a correct answer from the fine tuned llama2, but I dont understand why or how.\n\n## Comment ID k55uog8 with +2 score by [(Medium_Chemist_4032, Reddit, 2023-10-16)](https://www.reddit.com/r/LocalLLaMA/comments/14vnfh2/my_experience_on_starting_with_fine_tuning_llms/k55uog8/) (in reply to ID 14vnfh2):\nAnybody interested in recreating the OP recipe?  \n\n\nI was considering a document reference Q&A chat bot. Maybe about spring boot as a starter.\n\n## Comment ID lj7j87a with +2 score by [(space_monolith, Reddit, 2024-08-21)](https://www.reddit.com/r/LocalLLaMA/comments/14vnfh2/my_experience_on_starting_with_fine_tuning_llms/lj7j87a/) (in reply to ID 14vnfh2):\nu/Ion_GPT, this is such an excellent post. Since it's a year old and there's so much new stuff -- can we get an update?\n\n## Comment ID jrfvh1h with +1 score by [(Bryan-Ferry, Reddit, 2023-07-10)](https://www.reddit.com/r/LocalLLaMA/comments/14vnfh2/my_experience_on_starting_with_fine_tuning_llms/jrfvh1h/) (in reply to ID 14vnfh2):\nDid they change the licence on LLaMA? Building chatbots for companies would certainly seem to constitute commercial use, would it not? I'd love to do something like this at work but that non-commercial licence has always stopped me.\n\n### Comment ID jrfvp9f with +2 score by [(BishBoosh, Reddit, 2023-07-10)](https://www.reddit.com/r/LocalLLaMA/comments/14vnfh2/my_experience_on_starting_with_fine_tuning_llms/jrfvp9f/) (in reply to ID jrfvh1h):\nI have also been wondering this. Are some people/organisations just happy to take the risk?\n\n### Comment ID jrib5fv with +2 score by [(Ion_GPT, Reddit, 2023-07-11)](https://www.reddit.com/r/LocalLLaMA/comments/14vnfh2/my_experience_on_starting_with_fine_tuning_llms/jrib5fv/) (in reply to ID jrfvh1h):\nI never said I am using LLaMA. The model is actually picked up by the client, I am presenting the options, many times I set a booga install with a bunch of models to be tested by the client before choosing.\n\nAlso, definition of commercial user is: \"any activity in which you use a product or service for financial gain\". So, if you create a chatbot on top of LLaMA and ask for payment to access it, you are in direct breach of license.\n\nIf you train a LoRA on top of LLaMA and you are using internally for trining new employees, or you drop some internal processes into a vector DB and use LLaMA to search through your documents, there is not financial gain, it is for \"researching the capabilities of LLaMA\". \n\nAlso we have open LLaMA now https://huggingface.co/openlm-research/open_llama_13b\n\n## Comment ID kwgiatm with +1 score by [(kunkkatechies, Reddit, 2024-03-25)](https://www.reddit.com/r/LocalLLaMA/comments/14vnfh2/my_experience_on_starting_with_fine_tuning_llms/kwgiatm/) (in reply to ID 14vnfh2):\nhow much do you charge for such services ?\n\n## Comment ID kx7plpv with +1 score by [(Overall_Music_2364, Reddit, 2024-03-30)](https://www.reddit.com/r/LocalLLaMA/comments/14vnfh2/my_experience_on_starting_with_fine_tuning_llms/kx7plpv/) (in reply to ID 14vnfh2):\nwow..!! thakyou soo much for this...!! im a student and hv taken  multiple courses on udemy and coursera, trying to learn about LLMs customization.. this is by far the best, most clear and conscise explaination i hv got..\n\n## Comment ID l0ai01q with +1 score by [(distantDuff, Reddit, 2024-04-19)](https://www.reddit.com/r/LocalLLaMA/comments/14vnfh2/my_experience_on_starting_with_fine_tuning_llms/l0ai01q/) (in reply to ID 14vnfh2):\nThis is great info. Thank you for sharing!\n\n## Comment ID l18aex8 with +1 score by [(Dapper_Translator_12, Reddit, 2024-04-25)](https://www.reddit.com/r/LocalLLaMA/comments/14vnfh2/my_experience_on_starting_with_fine_tuning_llms/l18aex8/) (in reply to ID 14vnfh2):\nIs there any free method to fine tune an large language model locall. I have a small workstation with 128GB DDR4 memory, Nvidia RTX A1000 X2 SLI VGA, AMD Threadripper process. I tried AutoTune-Advanced and LLaMA-Factory. They both failed on me. Autotrain say I dont have enough VRAM. LLaMA-Factory say I dont have CUDA. Please help me.\n\n## Comment ID l1pxkxs with +1 score by [(MichaelCompScience, Reddit, 2024-04-29)](https://www.reddit.com/r/LocalLLaMA/comments/14vnfh2/my_experience_on_starting_with_fine_tuning_llms/l1pxkxs/) (in reply to ID 14vnfh2):\nWhat is \"booga UI\"?\n\n## Comment ID l404iw5 with +1 score by [(RanbowPony, Reddit, 2024-05-14)](https://www.reddit.com/r/LocalLLaMA/comments/14vnfh2/my_experience_on_starting_with_fine_tuning_llms/l404iw5/) (in reply to ID 14vnfh2):\nHi, Thanks for sharing your experience,\n\nDo you apply the loss mask to mask out some format, like #instruction,#input,#output, prompt, as these tokens are input, not LLM generated,\n\nIt is reported that model trained with loss mask can have better performance.\n\nWhat is your experience in this issue?\n\n## Comment ID l672h54 with +1 score by [(Southern-Duck1115, Reddit, 2024-05-29)](https://www.reddit.com/r/LocalLLaMA/comments/14vnfh2/my_experience_on_starting_with_fine_tuning_llms/l672h54/) (in reply to ID 14vnfh2):\nThis post really caught my eye.  I've been playing with the idea of training and tweaking a pretrained model with math textbooks so that I can get a better LLM to work with my students.  I am an Algebra/Gemetry teacher and am trying to develop a math LLM and train it to be as good as it can be on helping them pass their end of year exam.  Gemini, and Copilot have been good...but I want better.  Am I crazy?  All I know is python, some basic info on models, and a desire to help my kids out.  Do you think I can go grab some data on huggingface and tune one of these things on my NVIDIA 3070 PC, or am I out of my mind?  What upskills do I need to grab?\n\n## Comment ID l893s8d with +1 score by [(8836eleanor, Reddit, 2024-06-12)](https://www.reddit.com/r/LocalLLaMA/comments/14vnfh2/my_experience_on_starting_with_fine_tuning_llms/l893s8d/) (in reply to ID 14vnfh2):\nGreat thread thank you. You basically have my dream job. How long did it take to train up? Where did you get your experience? Are you self-employed?\n\n## Comment ID lopikhr with +1 score by [(PurpleReign007, Reddit, 2024-09-24)](https://www.reddit.com/r/LocalLLaMA/comments/14vnfh2/my_experience_on_starting_with_fine_tuning_llms/lopikhr/) (in reply to ID 14vnfh2):\nThis is a great post. I'd love to hear how things are going one year later! Any major changes to your approach, tooling, etc?\n\n## Comment ID lv7kj63 with +1 score by [(Plus-Supermarket-546, Reddit, 2024-11-03)](https://www.reddit.com/r/LocalLLaMA/comments/14vnfh2/my_experience_on_starting_with_fine_tuning_llms/lv7kj63/) (in reply to ID 14vnfh2):\nHas anyone able to impart information to an LLM  by fine tuning? based on my experience it learns  in which format to output information. My use case is to fine tune an LLM on company specific data in a way it retains information it is trained on. Also, is full fine tuning possible?\n\n## Comment ID m8dsrrm with +1 score by [(Adorable_Database460, Reddit, 2025-01-21)](https://www.reddit.com/r/LocalLLaMA/comments/14vnfh2/my_experience_on_starting_with_fine_tuning_llms/m8dsrrm/) (in reply to ID 14vnfh2):\nThanks, it's quite helpful.  \nI have a confusion regarding the data format. For a chatbot, Json or txt which format should be preferred in order to get context-aware responses?\n\n## Comment ID mcqnmuo with +1 score by [(Sea_sa, Reddit, 2025-02-14)](https://www.reddit.com/r/LocalLLaMA/comments/14vnfh2/my_experience_on_starting_with_fine_tuning_llms/mcqnmuo/) (in reply to ID 14vnfh2):\nDoes training a model on some data eliminate need to have vector database?\n\n## Comment ID jrdv4ju with +1 score by [(NetTecture, Reddit, 2023-07-10)](https://www.reddit.com/r/LocalLLaMA/comments/14vnfh2/my_experience_on_starting_with_fine_tuning_llms/jrdv4ju/) (in reply to ID 14vnfh2):\nHave you considered using automated pipelines for the tuning? And using tuning for data looks like a bad approach to me.\n\nIn detail:\n\n* I have good success with AI models self-correcting. Write answer, review answer how to make it better, until review passes. This could help with a lot of fine tuning - take the answer, run it through another model to make it better, then put that in as tuning. Stuff like language, lack of examples etc. should be fixable without a human looking at it.\n* I generally dislike the idea of using tuning for what essentially is a database. Would it not be better to work on a better framework for databases (using more than vectorization - there is so much more you can do), then combine that with the language / skill fine tuning in 1. Basically: train it to be a helpful chatbot, then plug in a database. This way changes in data do not require retraining. Now, the AI may not be good enough to get the right data - at a single try, which is where tool use and research -subai can come in handy, taking the request for SOMEHTING, going to the database, making a relevant abstract. Simple embeddings are ridiculous - you basically hope that your snippets hit and are not too large. But a research AI that has larger snippets, gets one, checks validity, extracts info - COULD work (albeit at what performance).\n\nSo, I think the optimal solution is to use both - use tuning to tune the AI to behave acceptable, but use the database approach for... well... the data.\n\n## Comment ID jrduaxe with +1 score by [(None, Reddit, 2023-07-10)](https://www.reddit.com/r/LocalLLaMA/comments/14vnfh2/my_experience_on_starting_with_fine_tuning_llms/jrduaxe/) (in reply to ID 14vnfh2):\n[deleted]\n\n## Comment ID jrdnkox with +1 score by [(exizt, Reddit, 2023-07-10)](https://www.reddit.com/r/LocalLLaMA/comments/14vnfh2/my_experience_on_starting_with_fine_tuning_llms/jrdnkox/) (in reply to ID 14vnfh2):\nHow do you even get access to Azure APIs? We’ve been on the waitlist for months.\n\n### Comment ID jrdzjsd with +2 score by [(SigmaSixShooter, Reddit, 2023-07-10)](https://www.reddit.com/r/LocalLLaMA/comments/14vnfh2/my_experience_on_starting_with_fine_tuning_llms/jrdzjsd/) (in reply to ID jrdnkox):\nIt’s the OpenAI API you want, just google that. No waiting necessary. You can use it to query ChatGPT 3.5 or 4.\n\n#### Comment ID jre99b3 with +1 score by [(exizt, Reddit, 2023-07-10)](https://www.reddit.com/r/LocalLLaMA/comments/14vnfh2/my_experience_on_starting_with_fine_tuning_llms/jre99b3/) (in reply to ID jrdzjsd):\n>Most choose to employ GPT4 for assistance. Privacy shouldn't be a concern if you're using Azure APIs, though they might be more costly, but offer privacy.\n\nI thought OP meant Azure APIs, not OpenAI APIs.\n\n#### Comment ID jrew8zs with +1 score by [(gthing, Reddit, 2023-07-10)](https://www.reddit.com/r/LocalLLaMA/comments/14vnfh2/my_experience_on_starting_with_fine_tuning_llms/jrew8zs/) (in reply to ID jrdzjsd):\nAzure APIs are basically the same thing but set up for big time users.\n\n#### Comment ID jrf7lwt with +1 score by [(Freakin_A, Reddit, 2023-07-10)](https://www.reddit.com/r/LocalLLaMA/comments/14vnfh2/my_experience_on_starting_with_fine_tuning_llms/jrf7lwt/) (in reply to ID jrdzjsd):\nThe Azure OpenAI API has the benefit of knowing where your data are going.  This is why you'd use the Azure APIs, so  that your data can stay in your VPC (or whatever Azure calls a VPC).  \n\nGenerally companies should not be sending private internal company data to the regular OpenAI APIs.\n\n### Comment ID jri9bir with +1 score by [(Ion_GPT, Reddit, 2023-07-11)](https://www.reddit.com/r/LocalLLaMA/comments/14vnfh2/my_experience_on_starting_with_fine_tuning_llms/jri9bir/) (in reply to ID jrdnkox):\nI clicked the \"request\" button, filled the form and in about 6 hours I got an email that I am in. I have few clients that wanted to run all data only through their accounts and did the same and got almost instant access.\n\n#### Comment ID jrie5fl with +1 score by [(exizt, Reddit, 2023-07-11)](https://www.reddit.com/r/LocalLLaMA/comments/14vnfh2/my_experience_on_starting_with_fine_tuning_llms/jrie5fl/) (in reply to ID jri9bir):\nOh wow. I wonder if it’s something in how we filled the form…\n\n## Comment ID jrefglq with +1 score by [(krali_, Reddit, 2023-07-10)](https://www.reddit.com/r/LocalLLaMA/comments/14vnfh2/my_experience_on_starting_with_fine_tuning_llms/jrefglq/) (in reply to ID 14vnfh2):\nI wonder about the training approach for corp knowledge addition to an existing LLM. Common sense dictates the embedding approach would be less prone to error, but you have first-hand experience, that's interesting.\n\n### Comment ID jri9ldz with +2 score by [(Ion_GPT, Reddit, 2023-07-11)](https://www.reddit.com/r/LocalLLaMA/comments/14vnfh2/my_experience_on_starting_with_fine_tuning_llms/jri9ldz/) (in reply to ID jrefglq):\nI added an update to the original post. The answer is always \"it depends\". Embeddings are a very useful and powerful tool. It can ingest raw data and is extremely fast. \n\nI would just install booga, enable superbooga plugin and thrown your raw data there and run some tests. It is the fastest / cheapest way to add some extra knowledge to a model and interact with it.\n\n\n\n\n\n\n\n\n\n## Comment ID jre8d09 with +1 score by [(Wise-Paramedic-4536, Reddit, 2023-07-10)](https://www.reddit.com/r/LocalLLaMA/comments/14vnfh2/my_experience_on_starting_with_fine_tuning_llms/jre8d09/) (in reply to ID 14vnfh2):\nWhat level of error do you wish while training?\n\n## Comment ID jreg6aq with +1 score by [(reggiestered, Reddit, 2023-07-10)](https://www.reddit.com/r/LocalLLaMA/comments/14vnfh2/my_experience_on_starting_with_fine_tuning_llms/jreg6aq/) (in reply to ID 14vnfh2):\nIn my experience, data shaping is always the most daunting task.  \nDecisions concerning method of fill, data fine-tuning, and data type-casting can heavily change the outcome.\n\n## Comment ID jregk3y with +1 score by [(jpasmore, Reddit, 2023-07-10)](https://www.reddit.com/r/LocalLLaMA/comments/14vnfh2/my_experience_on_starting_with_fine_tuning_llms/jregk3y/) (in reply to ID 14vnfh2):\nSuper helpful\n\n### Comment ID jregot5 with +1 score by [(jpasmore, Reddit, 2023-07-10)](https://www.reddit.com/r/LocalLLaMA/comments/14vnfh2/my_experience_on_starting_with_fine_tuning_llms/jregot5/) (in reply to ID jregk3y):\nCan you share a LinkedIn or other contact to: john@very.fm thx (John Pasmore)\n\n## Comment ID jrevfe6 with +1 score by [(kreuzguy, Reddit, 2023-07-10)](https://www.reddit.com/r/LocalLLaMA/comments/14vnfh2/my_experience_on_starting_with_fine_tuning_llms/jrevfe6/) (in reply to ID 14vnfh2):\nDid you test your method on benchmarks? How do you know it's getting better? Because I converted my data to a Q&A format and still it didn't help it to reason over it according to a benchmark I have with multiple answers question.\n\n### Comment ID jrezorh with +1 score by [(mehrdotcom, Reddit, 2023-07-10)](https://www.reddit.com/r/LocalLLaMA/comments/14vnfh2/my_experience_on_starting_with_fine_tuning_llms/jrezorh/) (in reply to ID jrevfe6):\nThanks for doing this. Do you recommend any methods for using the fine tuned version and incorporate it into the existing apps via API calls?\n\n## Comment ID jrf03sl with +1 score by [(Dizzy-Tumbleweeds, Reddit, 2023-07-10)](https://www.reddit.com/r/LocalLLaMA/comments/14vnfh2/my_experience_on_starting_with_fine_tuning_llms/jrf03sl/) (in reply to ID 14vnfh2):\nTrying to understand the benefit of fine tuning instead of serving context through a vector DB to a foundational model\n\n### Comment ID jrhofgn with +1 score by [(BlandUnicorn, Reddit, 2023-07-11)](https://www.reddit.com/r/LocalLLaMA/comments/14vnfh2/my_experience_on_starting_with_fine_tuning_llms/jrhofgn/) (in reply to ID jrf03sl):\nThis is the option I’ve gone with as well. Granted, for best operation you still to spend time to clean your data\n\n## Comment ID jrfit7d with +1 score by [(Serenityprayer69, Reddit, 2023-07-10)](https://www.reddit.com/r/LocalLLaMA/comments/14vnfh2/my_experience_on_starting_with_fine_tuning_llms/jrfit7d/) (in reply to ID 14vnfh2):\nI really appreciate this share buddy. I am curious how people are starting businesses already with the technology changing so fast.  Do you have trouble with clients or are they just excited to see the first signs of life when you show them the demo? \n\nI suppose I mean if one were to start doing this professionally how understanding are clients that this is evolving so fast things might break from time to time.  \n\nIE my ChatGPT api just went down for like 45 minutes.  If you build a service that relys on chatgpt api are clients understanding if it stops working?\n\nOr is it better to just build on the best local model you can find and sacrifice potentially better results for stability?\n\n## Comment ID jrfl6zl with +1 score by [(_Boffin_, Reddit, 2023-07-10)](https://www.reddit.com/r/LocalLLaMA/comments/14vnfh2/my_experience_on_starting_with_fine_tuning_llms/jrfl6zl/) (in reply to ID 14vnfh2):\nHow are you modeling for hardware requirements? Are you going by estimated Tokens/s or some other metric? For the specifications you mentioned in your post, how many Tokens/s are you able to output?\n\n## Comment ID jrfrh63 with +1 score by [(BranNutz, Reddit, 2023-07-10)](https://www.reddit.com/r/LocalLLaMA/comments/14vnfh2/my_experience_on_starting_with_fine_tuning_llms/jrfrh63/) (in reply to ID 14vnfh2):\nGood info 👍\n\n## Comment ID jrs953t with +1 score by [(JoseConseco_, Reddit, 2023-07-13)](https://www.reddit.com/r/LocalLLaMA/comments/14vnfh2/my_experience_on_starting_with_fine_tuning_llms/jrs953t/) (in reply to ID 14vnfh2):\nI just tried to get superbooga but I get this issue:\n\n[https://github.com/oobabooga/text-generation-webui/discussions/3057#discussioncomment-6429929](https://github.com/oobabooga/text-generation-webui/discussions/3057#discussioncomment-6429929)\n\nAbout missing 'zstandard' even though it is installed.  I'm bit new to whole conda, and venv , but I think I have setup everything correctly.   oobabooga was installed from 'One-click installer'\n\n## Comment ID jrwq4nq with +1 score by [(None, Reddit, 2023-07-14)](https://www.reddit.com/r/LocalLLaMA/comments/14vnfh2/my_experience_on_starting_with_fine_tuning_llms/jrwq4nq/) (in reply to ID 14vnfh2):\nCould you add more details to what your internal tooling for review looks like? Given that most of the work lands on cleaning and formatting data, what open source / paid tooling solutions are available today for these tasks?\n\n## Comment ID js6y6hx with +1 score by [(CrimzonGryphon, Reddit, 2023-07-16)](https://www.reddit.com/r/LocalLLaMA/comments/14vnfh2/my_experience_on_starting_with_fine_tuning_llms/js6y6hx/) (in reply to ID 14vnfh2):\nHave you developed any chatbots that are both a fine-tuned model with access to a vector store / embedding?\n\nIt would seem to me that even a finetuned chatbot will struggle with document search, providing references etc.?\n\n## Comment ID jt84xri with +1 score by [(Warm-Interaction-989, Reddit, 2023-07-24)](https://www.reddit.com/r/LocalLLaMA/comments/14vnfh2/my_experience_on_starting_with_fine_tuning_llms/jt84xri/) (in reply to ID 14vnfh2):\nThank you, Ion\\_GPT, for your insightful post! It's incredibly helpful for newcomers!\n\nHowever, I have a query concerning fine-tuning already optimized models, like Llama-2-Chat model. My use case ideally requires leveraging the broad capabilities that Llama-2-Chat already provides, but also incorporating a more specialized knowledge base in certain areas.\n\nIn your opinion, is it feasible to fine-tune a model that's already been fine-tuned, like Llama-2-Chat, without losing a significant portion of its conversational skills, while simultaneously incorporating new specialized knowledge?\n\n## Comment ID juri4vc with +1 score by [(orangeatom, Reddit, 2023-08-04)](https://www.reddit.com/r/LocalLLaMA/comments/14vnfh2/my_experience_on_starting_with_fine_tuning_llms/juri4vc/) (in reply to ID 14vnfh2):\nThanks for sharing, what is your ranked or go to list of fine-tuning repos that you list?\n\n## Comment ID jwthtya with +1 score by [(arthurwolf, Reddit, 2023-08-19)](https://www.reddit.com/r/LocalLLaMA/comments/14vnfh2/my_experience_on_starting_with_fine_tuning_llms/jwthtya/) (in reply to ID 14vnfh2):\n> All you need to do is peek into their repositories, grab an example, and tweak it to fit your model and data.\n\nI've been looking for **hours** for a **straightforward** example I can adapt, just a series of commands that are explained and that I can run.\n\nI **can not** find anything.\n\nWhere did you learn ??\n\n## Comment ID jxacmmx with +1 score by [(orangeatom, Reddit, 2023-08-22)](https://www.reddit.com/r/LocalLLaMA/comments/14vnfh2/my_experience_on_starting_with_fine_tuning_llms/jxacmmx/) (in reply to ID 14vnfh2):\nThanks again, can you share more about finetuning and merging the lora into the pre-trained model and how you do inference for testing and deployment?\n\n## Comment ID jxm2tai with +1 score by [(orangeatom, Reddit, 2023-08-24)](https://www.reddit.com/r/LocalLLaMA/comments/14vnfh2/my_experience_on_starting_with_fine_tuning_llms/jxm2tai/) (in reply to ID 14vnfh2):\nu/ion_GPT can you talk about your approach to inference?\n\n## Comment ID jxxsm73 with +1 score by [(StrictSir8506, Reddit, 2023-08-27)](https://www.reddit.com/r/LocalLLaMA/comments/14vnfh2/my_experience_on_starting_with_fine_tuning_llms/jxxsm73/) (in reply to ID 14vnfh2):\nHi u/Ion_GPT, Thanks for such a detailed and insightful answer.\n\nHow would you deal with data that is ever changing or where you need to recommend something to a user based on his profile etc? Here you need to fetch and pass on the real time and accurate data as a context itself? How do you deal with this and the challenges involved?\n\nSecondly, what about the text data that gets generated while interacting with those chatbots? How to extract further insights out of it and the pipeline to clean and retrain the models?\n\nWould love to learn from your learnings and insights\n\n## Comment ID jy4h9p9 with +1 score by [(therandomalias, Reddit, 2023-08-28)](https://www.reddit.com/r/LocalLLaMA/comments/14vnfh2/my_experience_on_starting_with_fine_tuning_llms/jy4h9p9/) (in reply to ID 14vnfh2):\nHey and thanks so much for the post! Wow I would love to sit down for a coffee and pick your brain more ☕︎\n\nI have lots of questions and I’m sure they’ll all be giving away how little I know about this, but I’m trying to learn :) \n\nI’ll start with one of my very elementary ones…if I’m using Llama2 13B text generation for example, are you using these datasets (i.e. dolly, orca, vicuna) to fine-tune a model like this to improve the quality of the output of answers, and THEN ALSO, once you get a good quality output from these models, fine-tuning them again with private company data?\n\nIn going through a lot of the tutorials in Azure for example, it’s not clear to me if I can fine-tune a model to optimize for multiple things. For example, can i fine-tune a model to optimize how to classify intents in a conversation, AND supplement it with additional healthcare knowledge like hospital codes and their meanings, AND have it learn how to take medical docs and case files and package them into an ‘AI-driven demand packages for injury lawyers’ (referencing the company EvenUp here). I know these aren’t really related, I’m just trying to paint the question with multiple different examples/capabilities. It’s not clear to me when i look at the docs to fine-tune something as the format that is required to ingest the data is very specific for each use case…so do i just fine-tune for classification, then once that’s finished, re-finetune for the other use cases? I’m assuming the answer is yes but I’m not seeing it explicitly stated anywhere…\n\nThanks again for sharing all of this! Always enlightening and super helpful to hear from people who have these in production with customers! Cheers!\n\n## Comment ID jyf465k with +1 score by [(Big-Slide-4906, Reddit, 2023-08-30)](https://www.reddit.com/r/LocalLLaMA/comments/14vnfh2/my_experience_on_starting_with_fine_tuning_llms/jyf465k/) (in reply to ID 14vnfh2):\nI have a question, in all the fine-tune tasks that I have seen, they used a prompt-completion data format to fine-tune an LLM. I mean data is like Q&A type, can we fine-tune on the data which is not Q&A (only documents) or doesn't have any prompt?\n\n## Comment ID jz2j08t with +1 score by [(anuargdeshmukh, Reddit, 2023-09-04)](https://www.reddit.com/r/LocalLLaMA/comments/14vnfh2/my_experience_on_starting_with_fine_tuning_llms/jz2j08t/) (in reply to ID 14vnfh2):\nI have a large document and i'm planning to finetune my model on it. i dont have intruction and ser set but i'm just planning to finetune it for text completion and then use the original \\[INST\\] tags used by trained llama model.  \n have you tried something similar ?\n\n## Comment ID k2ouyqi with +1 score by [(Wrong-Pension7258, Reddit, 2023-09-29)](https://www.reddit.com/r/LocalLLaMA/comments/14vnfh2/my_experience_on_starting_with_fine_tuning_llms/k2ouyqi/) (in reply to ID 14vnfh2):\nI am finetuning [facebook bart base](https://huggingface.co/facebook/bart-base) 139M for 3 tasks - 1) I want it to classify a sentence into one of the 16 classes 2) I want it to extract some entity 3) extract another entity.  \n\n\nHow many datapoints should suffice for good performance? Earlier, I had about 100 points per class (1600 total points) and results were poor. Now I have about 900 per class and results are significantly better. Wondering if increasing the data would lead to even better results?  \nWhat is a good number of data for 139M parameter model?  \n\n\nThanks\n\n## Comment ID k3q86hw with +1 score by [(RE-throwaway2019, Reddit, 2023-10-06)](https://www.reddit.com/r/LocalLLaMA/comments/14vnfh2/my_experience_on_starting_with_fine_tuning_llms/k3q86hw/) (in reply to ID 14vnfh2):\nthis is a great post, thanks for sharing your knowledge and the difficulties you're experiencing today with training open source LLMs\n\n## Comment ID k53bcoc with +1 score by [(Optimal_Original_815, Reddit, 2023-10-16)](https://www.reddit.com/r/LocalLLaMA/comments/14vnfh2/my_experience_on_starting_with_fine_tuning_llms/k53bcoc/) (in reply to ID 14vnfh2):\nWe do have to remember what data we are trying to fine tune the model with. What is the guarantee that the model has not seen any flavor of publicly available data set that we have picked up to fine tune it? The real fun is to choose a domain specific data which belongs to a company's product which model have not seen before. I have been trying hard and had no luck so far. The fine tuning example I was following had 1k records so i prepared my dataset of that size and exactly that format but no luck to see correct answer to even one single question. Model always tends to fallback to its existing knowledge that the new trained data.\n\n## Comment ID k5yezbb with +1 score by [(daniclas, Reddit, 2023-10-22)](https://www.reddit.com/r/LocalLLaMA/comments/14vnfh2/my_experience_on_starting_with_fine_tuning_llms/k5yezbb/) (in reply to ID 14vnfh2):\nThanks a lot for this write-up, I got here because I am trying to use ChatGPT with a OpenAPI specification  (through LangChain) but I'm having a hard time making it understand even the simplest request (for example, search X entity by name after the input: is there a X called name? So it won't even do a simple GET request. \n\nI am trying to train it on understanding what the business domain is, what these different entities are, and how to go about getting them or running other processes through the API, but I am at a loss. Because I am using an agent, not all inputs come from a human (some inputs come from the previous output of a chain), so I also don't understand how to fine-tune that. Do you have any thought on this?\n\n## Comment ID k7goo8n with +1 score by [(datashri, Reddit, 2023-11-02)](https://www.reddit.com/r/LocalLLaMA/comments/14vnfh2/my_experience_on_starting_with_fine_tuning_llms/k7goo8n/) (in reply to ID 14vnfh2):\nHi, sorry for the necro, I'm trying to get to a stage where I can do what you do. May I ask a couple of questions - \n\nTo what depth do I need to understand LLMs and deep learning? Do I need to be familiar/comfortable with the mathematics of it? Or is it more at the application level?\n\n## Comment ID kat5cm9 with +1 score by [(Previous_Giraffe6746, Reddit, 2023-11-26)](https://www.reddit.com/r/LocalLLaMA/comments/14vnfh2/my_experience_on_starting_with_fine_tuning_llms/kat5cm9/) (in reply to ID 14vnfh2):\nWhat clouds do you often use to train your llm? Google collab or others?\n\n## Comment ID kdv4zjq with +1 score by [(beautyofdeduction, Reddit, 2023-12-18)](https://www.reddit.com/r/LocalLLaMA/comments/14vnfh2/my_experience_on_starting_with_fine_tuning_llms/kdv4zjq/) (in reply to ID 14vnfh2):\nThank you for sharing!\n\n## Comment ID ke5mx26 with +1 score by [(sreekanth850, Reddit, 2023-12-20)](https://www.reddit.com/r/LocalLLaMA/comments/14vnfh2/my_experience_on_starting_with_fine_tuning_llms/ke5mx26/) (in reply to ID 14vnfh2):\nDoes H2O GPT does the same ?\n\n## Comment ID ke60cgp with +1 score by [(deeepak143, Reddit, 2023-12-20)](https://www.reddit.com/r/LocalLLaMA/comments/14vnfh2/my_experience_on_starting_with_fine_tuning_llms/ke60cgp/) (in reply to ID 14vnfh2):\nThank you so much for this in-depth explanation of how you fine tune models u/Ion_GPT.\n\nbtw, for privacy focused clients, is there any change in the process of fine tuning, such as masking or anonymising of sensitive data. And how is sensitive data identified when there is too much data to be considered.\n\n## Comment ID khbxtmx with +1 score by [(9090112, Reddit, 2024-01-11)](https://www.reddit.com/r/LocalLLaMA/comments/14vnfh2/my_experience_on_starting_with_fine_tuning_llms/khbxtmx/) (in reply to ID 14vnfh2):\nHi, thanks for this guide. This is extremely helpful for people like me who are just starting out with LLaMA. I have a Q&A chatbot working right now along with a RAG pipeline I'm pretty proud of. But now I want to try my hand at a little training. I probably won't have to resources to fully finetune the 13B model I'm using, but I figure I could try my hand at LoRA. So I had a quick question:\n\n\\* About how large a dataset would I need to LoRA a 7B and 13B Q&A Chatbot?\n\n\\* What does a training dataset for a Q&A Chatbot look like? I see a lot of different terms used to reference training datasets like instruction tuning, prompt datasets, Q&A dataset, it's a little overwhelming. \n\n\\* What are some scalable ways to construct this training dataset? Can I do it all programmatically, or am I going to have do some typing of my own?",
      "# Post ID 13o5m15: Question: Why ChatGPT Plus (GPT-4) answers better than local Langchain + Pinecone Tests? with +8 score by [(ZCAY6, Reddit, 2023-05-21)](https://www.reddit.com/r/LangChain/comments/13o5m15/question_why_chatgpt_plus_gpt4_answers_better/)\nHey everyone, I'm testing out a lot of content related to GPT, Vector Databases, LangChain and so on.  \n\n\nToday I had the idea to embed the book \"The Way of Kings\" which is the first book of the Stormlight Archive series by Brandon Sanderson (strongly recommend it!). Okay, loaded the PDF, Recursively splitted characters in chunks of size 95, resulting in 34733 documents that I upload to Pinecone as Vectors.  \n\n\nPlease do comment about it, since I'm no expert at it, but as I understand, now that the WHOLE book is embed and in a database, shouldn't it be able to be accessed entirely by GPT as an extension of it own \"knowledge\" (LLM model)?   \n\n\nAfter some testing with my code, I've noticed that smaller chunks and a larger k sample in the .similarity\\_search() function result in somewhat more content on the answer, but still far away from what I get in the ChatGPT using GPT-4. \n\nTrying like this:\n\n    query = \"Define and detail what is a highstorm?\"\ndocs = docsearch.similarity_search(query, k=80)\nchain.run(input_documents=docs, question=query)\n\n[Python Code](https://preview.redd.it/ht62xkzfv81b1.png?width=2734&format=png&auto=webp&s=4bde9a4f36c2c2acfdd4131ad3a079d3586f14db)\n\nThe answer was: *' A highstorm is a powerful storm that typically occurs in the world of Roshar. It is characterized by strong winds, heavy rains, and large amounts of Stormlight. Highstorms usually move from east to west, and can cause flooding and destruction. They are predicted by stormwardens and can be temporarily shielded from by nearby rock formations.'*\n\n&#x200B;\n\nBut when I asked for ChatGPT, without giving ANY CONTEXT of what I was talking about:  \n\n\n[Print from ChatGPT](https://preview.redd.it/j3ecnrerv81b1.png?width=1656&format=png&auto=webp&s=975818c0c3877116aa4d2711400372274cd8da92)\n\n&#x200B;\n\nSo, I got some questions:  \n\\- If we need to pass so much content to GPT give a good answer, is it really useful for these cases?  \n\\- If every ***K*** ***sample*** results in ***X*** ***tokens*** passed through the prompt by chaining it, is it really performant or cost effective?  \n\\- Why does it use just the prompt to answer and not the knowledge it already has?\n\n## Comment ID jl32k6h with +6 score by [(Jdonavan, Reddit, 2023-05-21)](https://www.reddit.com/r/LangChain/comments/13o5m15/question_why_chatgpt_plus_gpt4_answers_better/jl32k6h/) (in reply to ID 13o5m15):\nHaving done this sort of exercise on the entire \"The Expanse\" series of  books  I  can give you a few pointer based on my experiences.  Apologies for the all of text...\n\n&#x200B;\n\n1. There can be a big difference in the quality of answers when extracting the information from documents vs the model having been trained on it.\n   1. For something like books, you can help close the gap by also indexing wiki content for the book series.  This makes each chunk more information dense.  This approach has allowed be to get decent answers out of GPT-3.6 which performs sub-par when given raw book content.\n   2. Providing lots of context can help, but that's a double edged sword.  That's where the chunk sizes can play a big role.\n2. How you segment matters a lot and there's no perfect size what works for all types of questions.\n   1. I segment on paragraph boundaries using a token limit to determine how many paragraphs are in each . Each chunk is up to, but never more than, that limit.  This allows me to know EXACTLY how many relevant docs can fit .\n   2. New chapters ALWAYS start a new chunk even if that means a small chunk for the prior chapter.\n   3. Chunks should be stored using original casing however your embedding should all be in lower case.  GPT works better with mixed case while the document retrieval step wants things consistent.\n   4. I've been using two indexes, one using 200 tokens and another at 400.   These aren't particularly rigorous, they're what worked well enough to allow me to pay more attention to other parts of the problem for a bit\n   5. There's a whole rabbit hole to go down on enhancing the quality of the retrieval step by encoding additional information in the metadata. \n3. 80 is way too many docs for most questions.  You're giving it a crapload of \"context\" that isn't context.\n   1. You need to set a minimum relevancy threshold for your relevant documents.\n   2. Fewer, more relevant documents is the key.    If you provide GPT  ten \"relevant\" documents  and nine of them don't contain the answer, it quite possible will miss that the tenth did.\n4. I use a tiered approach where I try to generate answers using very few highly relevant sources and if that fails try more docs and/or less relevancy scores The end of this chain is \"let GTP-4 loose on a crapload of chunks\" which takes longer and costs more.\n   1. I created another index that contains known-good answers from that last tier.\n\n### Comment ID jl5qqeg with +1 score by [(liuweiqing, Reddit, 2023-05-22)](https://www.reddit.com/r/LangChain/comments/13o5m15/question_why_chatgpt_plus_gpt4_answers_better/jl5qqeg/) (in reply to ID jl32k6h):\nexcellent\n\n### Comment ID k7r7l3m with +1 score by [(TheWebbster, Reddit, 2023-11-04)](https://www.reddit.com/r/LangChain/comments/13o5m15/question_why_chatgpt_plus_gpt4_answers_better/k7r7l3m/) (in reply to ID jl32k6h):\nSorry to raise a 6month thread/reply but what do you use to ensure chunking lands at the points you've described? Sorry if a naive question but right now I can't understand how that would be done. I'm looking to do a set of books I own (related to business) and this sounds useful - originally I was just going to chunk based on x tokens but that would mean having some chunks contain paragraph breaks and some chapter breaks.\n\n#### Comment ID k7sbb0f with +1 score by [(Jdonavan, Reddit, 2023-11-04)](https://www.reddit.com/r/LangChain/comments/13o5m15/question_why_chatgpt_plus_gpt4_answers_better/k7sbb0f/) (in reply to ID k7r7l3m):\nDepending on the file format it can be as simple as  loading the document in elements mode. Of all you have is text often two newlines in a row can at least get you paragraphs.\n\n## Comment ID jl2ruab with +3 score by [(gentlecucumber, Reddit, 2023-05-21)](https://www.reddit.com/r/LangChain/comments/13o5m15/question_why_chatgpt_plus_gpt4_answers_better/jl2ruab/) (in reply to ID 13o5m15):\nYour chunky size is too small to provide adequate context. I use chunk sizes of 500-1000 so that the documents being passed to gpt contain enough information around the semantically relevant parts that the model has some idea of what's going on. There are a lot of ways to refine the quality of your answers in langchain. You should also be using a qa chain that uses map reduce on the retriever instead of 'stuff'. It looks like you're running the most basic kind of default retriever. Again, lots of stuff that can be done.\n\n### Comment ID jl31tp4 with +1 score by [(ZCAY6, Reddit, 2023-05-21)](https://www.reddit.com/r/LangChain/comments/13o5m15/question_why_chatgpt_plus_gpt4_answers_better/jl31tp4/) (in reply to ID jl2ruab):\nThanks for the suggestions! Gonna try them soon\n\n### Comment ID jl5lu55 with +1 score by [(liuweiqing, Reddit, 2023-05-22)](https://www.reddit.com/r/LangChain/comments/13o5m15/question_why_chatgpt_plus_gpt4_answers_better/jl5lu55/) (in reply to ID jl2ruab):\ngood\n\n## Comment ID jl2ojrb with +2 score by [(water_bottle_goggles, Reddit, 2023-05-21)](https://www.reddit.com/r/LangChain/comments/13o5m15/question_why_chatgpt_plus_gpt4_answers_better/jl2ojrb/) (in reply to ID 13o5m15):\nBecause of the prompt that your chain is pre-pending. Its being asked to answer based on the documents you gave it (80 lol).   \n\n\nPrompts make a lot of difference in how the model will react. If the chain system-prompt says, only provide relevant answers BASED on the sources you gave it, then that is what it will do.\n\n### Comment ID jl2r4t0 with +1 score by [(ZCAY6, Reddit, 2023-05-21)](https://www.reddit.com/r/LangChain/comments/13o5m15/question_why_chatgpt_plus_gpt4_answers_better/jl2r4t0/) (in reply to ID jl2ojrb):\nHmmmm I see. Is this something pre-configured in langchain functions? Is there a function that blends both?\n\n#### Comment ID jl2t8a9 with +2 score by [(water_bottle_goggles, Reddit, 2023-05-21)](https://www.reddit.com/r/LangChain/comments/13o5m15/question_why_chatgpt_plus_gpt4_answers_better/jl2t8a9/) (in reply to ID jl2r4t0):\nyou can see here how you can use your own prompts instead:   \n[https://python.langchain.com/en/latest/modules/chains/index\\_examples/question\\_answering.html](https://python.langchain.com/en/latest/modules/chains/index_examples/question_answering.html)\n\nHopefully this helps :)\n\n#### Comment ID jl2uo0a with +2 score by [(water_bottle_goggles, Reddit, 2023-05-21)](https://www.reddit.com/r/LangChain/comments/13o5m15/question_why_chatgpt_plus_gpt4_answers_better/jl2uo0a/) (in reply to ID jl2r4t0):\nAlso like what the other dude said, your chunks are probs too small\n\n## Comment ID jleml76 with +2 score by [(Intrepid-Air6525, Reddit, 2023-05-24)](https://www.reddit.com/r/LangChain/comments/13o5m15/question_why_chatgpt_plus_gpt4_answers_better/jleml76/) (in reply to ID 13o5m15):\nIt’s definitely a balance between the right chunking method, and how those chunks are stored and searched as an index\n\nHere is the link to my implementation of this.\n\n\nhttps://satellitecomponent.github.io/Neurite/\n\nhttps://github.com/satellitecomponent/Neurite\n\nr/FractalGPT\n\nIt all runs in the browser without langchain. You will need to set up the proxy servers in the GitHub repo to use the embedded search, wolfram, or wiki."
    ],
    "sources": {
      "steam_url": null,
      "steam_reviews": null,
      "google_play_url": null,
      "google_play_reviews": null,
      "apple_store_url": null,
      "apple_reviews": null,
      "reddit_urls": [
        "https://www.reddit.com/r/MachineLearning/comments/12m9pg0/alternatives_to_pinecone_vector_databases_d/",
        "https://www.reddit.com/r/nocode/comments/131kdrx/how_i_used_bubbleio_with_gpt4_pinecone_to_unlock/",
        "https://www.reddit.com/r/SoftwareEngineering/comments/107vhoq/vector_databases_like_pinecone_or_weaviate_are/",
        "https://www.reddit.com/r/rust/comments/wwe589/pinecone_rust_a_hard_decision_pays_off/",
        "https://www.reddit.com/r/OpenAI/comments/12jyes5/what_are_good_techniques_for_feeding_extremely/",
        "https://www.reddit.com/r/ChatGPTCoding/comments/12wn63m/i_have_made_some_easy_tools_to_rip_webpages_clean/",
        "https://www.reddit.com/r/LangChain/comments/15h3c5e/document_query_solution_for_small_business/",
        "https://www.reddit.com/r/vectordatabase/comments/170j6zd/my_strategy_for_picking_a_vector_database_a/",
        "https://www.reddit.com/r/LocalLLaMA/comments/14vnfh2/my_experience_on_starting_with_fine_tuning_llms/",
        "https://www.reddit.com/r/LangChain/comments/13o5m15/question_why_chatgpt_plus_gpt4_answers_better/"
      ],
      "reddit_search_url": "https://www.google.com/search?q=site%3Areddit.com+%22Pinecone%22+related%3Apinecone.io+"
    }
  },
  "glassdoor_result": {
    "company": [
      "Pinecone",
      "Pinecone",
      "pinecone.io",
      null,
      false,
      false,
      null,
      [
        false,
        false
      ]
    ],
    "review_page": "https://www.glassdoor.com/Reviews/Pinecone-Systems-Reviews-E6085804.htm",
    "raw_reviews": {
      "__typename": "EmployerReviewsRG",
      "allReviewsCount": 9,
      "currentPage": 1,
      "filteredReviewsCount": 9,
      "lastReviewDateTime": "2024-11-11T19:29:27.300",
      "numberOfPages": 1,
      "queryJobTitle": null,
      "queryLocation": null,
      "ratedReviewsCount": 9,
      "ratings": {
        "__typename": "EmployerRatings",
        "businessOutlookRating": 1,
        "careerOpportunitiesRating": 5,
        "ceoRating": 1,
        "compensationAndBenefitsRating": 4.9,
        "cultureAndValuesRating": 5,
        "diversityAndInclusionRating": 4.7,
        "overallRating": 5,
        "ratedCeo": {
          "__typename": "Ceo",
          "id": 1113114,
          "photoUrl({\"size\":\"LARGE\"})": "https://media.glassdoor.com/people/sqll/6085804/pinecone-systems-ceo1680597517812.png",
          "name": "Edo Liberty, Bob Wiederhold",
          "photoUrl({\"size\":\"REGULAR\"})": "https://media.glassdoor.com/people/sql/6085804/pinecone-systems-ceo1680597517812.png",
          "title": "Founder, CEO, President, COO"
        },
        "recommendToFriendRating": 1,
        "reviewCount": 9,
        "seniorManagementRating": 4.8,
        "workLifeBalanceRating": 4.7
      },
      "reviews": [
        {
          "__typename": "EmployerReviewRG",
          "advice": null,
          "adviceOriginal": null,
          "cons": "Blah blah blah blah blah",
          "consOriginal": null,
          "countHelpful": 1,
          "countNotHelpful": 0,
          "employer": {
            "__typename": "Employer",
            "id": 6085804,
            "shortName": "Pinecone Systems",
            "firstNotReassignedEmployerId": 6085804,
            "activeStatus": "INACTIVE",
            "approvalStatus": "APPROVED",
            "headquarters": "San Mateo, CA",
            "size": "201 to 500 Employees",
            "bestProfile": {
              "__typename": "EmployerProfile",
              "id": 5016216,
              "rowProfile": false
            },
            "restOfWorldProfile": {
              "__typename": "EmployerProfile",
              "id": 9476430
            },
            "primaryIndustry": {
              "__typename": "EmployerIndustry",
              "industryId": 200155,
              "industryName": "Software Development",
              "sectorId": 10013,
              "sectorName": "Information Technology"
            },
            "ceo": {
              "__typename": "Ceo",
              "id": 1113114,
              "photoUrl({\"size\":\"LARGE\"})": "https://media.glassdoor.com/people/sqll/6085804/pinecone-systems-ceo1680597517812.png",
              "name": "Edo Liberty, Bob Wiederhold",
              "photoUrl({\"size\":\"REGULAR\"})": "https://media.glassdoor.com/people/sql/6085804/pinecone-systems-ceo1680597517812.png",
              "title": "Founder, CEO, President, COO"
            },
            "legalActionBadges": [],
            "employerManagedContent({\"parameters\":[{\"divisionProfileId\":5016216,\"employerId\":6085804}]})": [
              {
                "__typename": "EmployerManagedContent",
                "isContentPaidForTld": false,
                "employerId": 6085804,
                "divisionProfileId": 5016216,
                "featuredVideoLink": null,
                "profileCoverPhoto": null,
                "diversityContent": {
                  "__typename": "DiversityAndInclusionContent",
                  "programsAndInitiatives": null
                }
              }
            ],
            "squareLogoUrl": "https://media.glassdoor.com/sql/6085804/pinecone-systems-squarelogo-1647610264911.png",
            "website": "http://pinecone.io",
            "squareLogoUrl({\"size\":\"LARGE\"})": "https://media.glassdoor.com/sqll/6085804/pinecone-systems-squarelogo-1647610264911.png",
            "squareLogoUrl({\"size\":\"REGULAR\"})": "https://media.glassdoor.com/sql/6085804/pinecone-systems-squarelogo-1647610264911.png",
            "bestLedCompanies({\"onlyCurrent\":true})": [],
            "bestPlacesToWork({\"onlyCurrent\":true})": [],
            "coverPhoto": {
              "__typename": "EmployerCoverPhoto",
              "hiResUrl": null
            },
            "counts": {
              "__typename": "EmployerCounts",
              "benefitCount": 5,
              "globalJobCount": {
                "__typename": "EmployerJobCountHolder",
                "jobCount": 4
              },
              "interviewCount": 15,
              "photoCount": 5,
              "reviewCount": 9,
              "salaryCount": 25
            },
            "officeAddresses": [
              {
                "__typename": "EmployerOfficeAddressEG",
                "id": 553188
              },
              {
                "__typename": "EmployerOfficeAddressEG",
                "id": 553201
              }
            ],
            "parent": null,
            "ratings": {
              "__typename": "DefaultEmployerRatings",
              "diversityAndInclusionRatingCount": 6,
              "overallRating": 5
            }
          },
          "employerResponses": [],
          "employmentStatus": "REGULAR",
          "flaggingDisabled": null,
          "featured": false,
          "isCurrentJob": false,
          "jobTitle": {
            "__typename": "JobTitle",
            "id": 196749,
            "text": "SWE Intern"
          },
          "languageId": "eng",
          "lengthOfEmployment": 0,
          "location": {
            "__typename": "City",
            "id": 1132348,
            "type": "CITY",
            "name": "New York, NY"
          },
          "originalLanguageId": null,
          "pros": "Startup environment and lot of independence",
          "prosOriginal": null,
          "ratingBusinessOutlook": null,
          "ratingCareerOpportunities": 0,
          "ratingCeo": null,
          "ratingCompensationAndBenefits": 0,
          "ratingCultureAndValues": 0,
          "ratingDiversityAndInclusion": 0,
          "ratingOverall": 5,
          "ratingRecommendToFriend": null,
          "ratingSeniorLeadership": 0,
          "ratingWorkLifeBalance": 0,
          "reviewDateTime": "2024-11-11T19:29:27.300",
          "reviewId": 92753543,
          "relatedStructures": [],
          "summary": "Pinecone Reviews",
          "summaryOriginal": null,
          "translationMethod": null
        },
        {
          "__typename": "EmployerReviewRG",
          "advice": "Keep being transparent with communication. Don't scare people with abrupt/sudden announcements; the tech industry is stressful for employees to be in right now. Focus on solid recurring revenue. Don't lose the lead with our community, market share, and mindshare.",
          "adviceOriginal": null,
          "cons": "- there was a recent individual layoff which does scare people a bit. And it wasn't communicated well. Was very abrupt and sudden. \n- the space is becoming very competitive\n- typical startup growing pains and process improvements \n- expectations are high but do you really want anything else if you're in a startup?",
          "consOriginal": null,
          "countHelpful": 0,
          "countNotHelpful": 0,
          "employer": {
            "__typename": "Employer",
            "id": 6085804,
            "shortName": "Pinecone Systems",
            "firstNotReassignedEmployerId": 6085804,
            "activeStatus": "INACTIVE",
            "approvalStatus": "APPROVED",
            "headquarters": "San Mateo, CA",
            "size": "201 to 500 Employees",
            "bestProfile": {
              "__typename": "EmployerProfile",
              "id": 5016216,
              "rowProfile": false
            },
            "restOfWorldProfile": {
              "__typename": "EmployerProfile",
              "id": 9476430
            },
            "primaryIndustry": {
              "__typename": "EmployerIndustry",
              "industryId": 200155,
              "industryName": "Software Development",
              "sectorId": 10013,
              "sectorName": "Information Technology"
            },
            "ceo": {
              "__typename": "Ceo",
              "id": 1113114,
              "photoUrl({\"size\":\"LARGE\"})": "https://media.glassdoor.com/people/sqll/6085804/pinecone-systems-ceo1680597517812.png",
              "name": "Edo Liberty, Bob Wiederhold",
              "photoUrl({\"size\":\"REGULAR\"})": "https://media.glassdoor.com/people/sql/6085804/pinecone-systems-ceo1680597517812.png",
              "title": "Founder, CEO, President, COO"
            },
            "legalActionBadges": [],
            "employerManagedContent({\"parameters\":[{\"divisionProfileId\":5016216,\"employerId\":6085804}]})": [
              {
                "__typename": "EmployerManagedContent",
                "isContentPaidForTld": false,
                "employerId": 6085804,
                "divisionProfileId": 5016216,
                "featuredVideoLink": null,
                "profileCoverPhoto": null,
                "diversityContent": {
                  "__typename": "DiversityAndInclusionContent",
                  "programsAndInitiatives": null
                }
              }
            ],
            "squareLogoUrl": "https://media.glassdoor.com/sql/6085804/pinecone-systems-squarelogo-1647610264911.png",
            "website": "http://pinecone.io",
            "squareLogoUrl({\"size\":\"LARGE\"})": "https://media.glassdoor.com/sqll/6085804/pinecone-systems-squarelogo-1647610264911.png",
            "squareLogoUrl({\"size\":\"REGULAR\"})": "https://media.glassdoor.com/sql/6085804/pinecone-systems-squarelogo-1647610264911.png",
            "bestLedCompanies({\"onlyCurrent\":true})": [],
            "bestPlacesToWork({\"onlyCurrent\":true})": [],
            "coverPhoto": {
              "__typename": "EmployerCoverPhoto",
              "hiResUrl": null
            },
            "counts": {
              "__typename": "EmployerCounts",
              "benefitCount": 5,
              "globalJobCount": {
                "__typename": "EmployerJobCountHolder",
                "jobCount": 4
              },
              "interviewCount": 15,
              "photoCount": 5,
              "reviewCount": 9,
              "salaryCount": 25
            },
            "officeAddresses": [
              {
                "__typename": "EmployerOfficeAddressEG",
                "id": 553188
              },
              {
                "__typename": "EmployerOfficeAddressEG",
                "id": 553201
              }
            ],
            "parent": null,
            "ratings": {
              "__typename": "DefaultEmployerRatings",
              "diversityAndInclusionRatingCount": 6,
              "overallRating": 5
            }
          },
          "employerResponses": [],
          "employmentStatus": "REGULAR",
          "flaggingDisabled": null,
          "featured": false,
          "isCurrentJob": true,
          "jobTitle": {
            "__typename": "JobTitle",
            "id": 30270,
            "text": "Software Developer"
          },
          "languageId": "eng",
          "lengthOfEmployment": 2,
          "location": {
            "__typename": "City",
            "id": 1132348,
            "type": "CITY",
            "name": "New York, NY"
          },
          "originalLanguageId": null,
          "pros": "- company is on fire. It's THE vector database in the AI space. The developer love is very real, and partners love us \n- the company culture. People are genuinely nice, humble, and smart. Haven't met a bad egg yet. Everyone is so eager to help you in a sincere and genuine way. \n- very open culture. There doesn't feel like any turfs or silos between teams. Communication is great \n- generally transparent leadership\n- really smart people. Very very competent managers. \n- pays above average \n- good benefits \n- truly global office, fully remote\n- extremely community and developer oriented\n- strategy and OKRs are efficiently communicated \n-",
          "prosOriginal": null,
          "ratingBusinessOutlook": "POSITIVE",
          "ratingCareerOpportunities": 5,
          "ratingCeo": "APPROVE",
          "ratingCompensationAndBenefits": 5,
          "ratingCultureAndValues": 5,
          "ratingDiversityAndInclusion": 5,
          "ratingOverall": 5,
          "ratingRecommendToFriend": "POSITIVE",
          "ratingSeniorLeadership": 5,
          "ratingWorkLifeBalance": 5,
          "reviewDateTime": "2024-07-16T12:00:23.313",
          "reviewId": 89202228,
          "relatedStructures": [],
          "summary": "Pinecone is a GREAT company",
          "summaryOriginal": null,
          "translationMethod": null
        },
        {
          "__typename": "EmployerReviewRG",
          "advice": null,
          "adviceOriginal": null,
          "cons": "- With a lot of growth, there are growing pains but I see everyone working towards bridging gaps and feel this is normal in most companies",
          "consOriginal": null,
          "countHelpful": 0,
          "countNotHelpful": 0,
          "employer": {
            "__typename": "Employer",
            "id": 6085804,
            "shortName": "Pinecone Systems",
            "firstNotReassignedEmployerId": 6085804,
            "activeStatus": "INACTIVE",
            "approvalStatus": "APPROVED",
            "headquarters": "San Mateo, CA",
            "size": "201 to 500 Employees",
            "bestProfile": {
              "__typename": "EmployerProfile",
              "id": 5016216,
              "rowProfile": false
            },
            "restOfWorldProfile": {
              "__typename": "EmployerProfile",
              "id": 9476430
            },
            "primaryIndustry": {
              "__typename": "EmployerIndustry",
              "industryId": 200155,
              "industryName": "Software Development",
              "sectorId": 10013,
              "sectorName": "Information Technology"
            },
            "ceo": {
              "__typename": "Ceo",
              "id": 1113114,
              "photoUrl({\"size\":\"LARGE\"})": "https://media.glassdoor.com/people/sqll/6085804/pinecone-systems-ceo1680597517812.png",
              "name": "Edo Liberty, Bob Wiederhold",
              "photoUrl({\"size\":\"REGULAR\"})": "https://media.glassdoor.com/people/sql/6085804/pinecone-systems-ceo1680597517812.png",
              "title": "Founder, CEO, President, COO"
            },
            "legalActionBadges": [],
            "employerManagedContent({\"parameters\":[{\"divisionProfileId\":5016216,\"employerId\":6085804}]})": [
              {
                "__typename": "EmployerManagedContent",
                "isContentPaidForTld": false,
                "employerId": 6085804,
                "divisionProfileId": 5016216,
                "featuredVideoLink": null,
                "profileCoverPhoto": null,
                "diversityContent": {
                  "__typename": "DiversityAndInclusionContent",
                  "programsAndInitiatives": null
                }
              }
            ],
            "squareLogoUrl": "https://media.glassdoor.com/sql/6085804/pinecone-systems-squarelogo-1647610264911.png",
            "website": "http://pinecone.io",
            "squareLogoUrl({\"size\":\"LARGE\"})": "https://media.glassdoor.com/sqll/6085804/pinecone-systems-squarelogo-1647610264911.png",
            "squareLogoUrl({\"size\":\"REGULAR\"})": "https://media.glassdoor.com/sql/6085804/pinecone-systems-squarelogo-1647610264911.png",
            "bestLedCompanies({\"onlyCurrent\":true})": [],
            "bestPlacesToWork({\"onlyCurrent\":true})": [],
            "coverPhoto": {
              "__typename": "EmployerCoverPhoto",
              "hiResUrl": null
            },
            "counts": {
              "__typename": "EmployerCounts",
              "benefitCount": 5,
              "globalJobCount": {
                "__typename": "EmployerJobCountHolder",
                "jobCount": 4
              },
              "interviewCount": 15,
              "photoCount": 5,
              "reviewCount": 9,
              "salaryCount": 25
            },
            "officeAddresses": [
              {
                "__typename": "EmployerOfficeAddressEG",
                "id": 553188
              },
              {
                "__typename": "EmployerOfficeAddressEG",
                "id": 553201
              }
            ],
            "parent": null,
            "ratings": {
              "__typename": "DefaultEmployerRatings",
              "diversityAndInclusionRatingCount": 6,
              "overallRating": 5
            }
          },
          "employerResponses": [],
          "employmentStatus": "REGULAR",
          "flaggingDisabled": null,
          "featured": false,
          "isCurrentJob": true,
          "jobTitle": null,
          "languageId": "eng",
          "lengthOfEmployment": 1,
          "location": null,
          "originalLanguageId": null,
          "pros": "- Given autonomy to work on projects\n- Different ideas and perspectives are heard and considered\n- People are encouraged to question or raise areas of the company that aren't working\n- Open culture where everyone feels accessible and is willing to lend a hand",
          "prosOriginal": null,
          "ratingBusinessOutlook": "POSITIVE",
          "ratingCareerOpportunities": 5,
          "ratingCeo": "APPROVE",
          "ratingCompensationAndBenefits": 5,
          "ratingCultureAndValues": 5,
          "ratingDiversityAndInclusion": 5,
          "ratingOverall": 5,
          "ratingRecommendToFriend": "POSITIVE",
          "ratingSeniorLeadership": 5,
          "ratingWorkLifeBalance": 5,
          "reviewDateTime": "2024-08-08T09:41:04.320",
          "reviewId": 89912330,
          "relatedStructures": [],
          "summary": "Fast growing company with room for individual growth",
          "summaryOriginal": null,
          "translationMethod": null
        },
        {
          "__typename": "EmployerReviewRG",
          "advice": null,
          "adviceOriginal": null,
          "cons": "Typical Startup Growing pains that are being solved",
          "consOriginal": null,
          "countHelpful": 0,
          "countNotHelpful": 0,
          "employer": {
            "__typename": "Employer",
            "id": 6085804,
            "shortName": "Pinecone Systems",
            "firstNotReassignedEmployerId": 6085804,
            "activeStatus": "INACTIVE",
            "approvalStatus": "APPROVED",
            "headquarters": "San Mateo, CA",
            "size": "201 to 500 Employees",
            "bestProfile": {
              "__typename": "EmployerProfile",
              "id": 5016216,
              "rowProfile": false
            },
            "restOfWorldProfile": {
              "__typename": "EmployerProfile",
              "id": 9476430
            },
            "primaryIndustry": {
              "__typename": "EmployerIndustry",
              "industryId": 200155,
              "industryName": "Software Development",
              "sectorId": 10013,
              "sectorName": "Information Technology"
            },
            "ceo": {
              "__typename": "Ceo",
              "id": 1113114,
              "photoUrl({\"size\":\"LARGE\"})": "https://media.glassdoor.com/people/sqll/6085804/pinecone-systems-ceo1680597517812.png",
              "name": "Edo Liberty, Bob Wiederhold",
              "photoUrl({\"size\":\"REGULAR\"})": "https://media.glassdoor.com/people/sql/6085804/pinecone-systems-ceo1680597517812.png",
              "title": "Founder, CEO, President, COO"
            },
            "legalActionBadges": [],
            "employerManagedContent({\"parameters\":[{\"divisionProfileId\":5016216,\"employerId\":6085804}]})": [
              {
                "__typename": "EmployerManagedContent",
                "isContentPaidForTld": false,
                "employerId": 6085804,
                "divisionProfileId": 5016216,
                "featuredVideoLink": null,
                "profileCoverPhoto": null,
                "diversityContent": {
                  "__typename": "DiversityAndInclusionContent",
                  "programsAndInitiatives": null
                }
              }
            ],
            "squareLogoUrl": "https://media.glassdoor.com/sql/6085804/pinecone-systems-squarelogo-1647610264911.png",
            "website": "http://pinecone.io",
            "squareLogoUrl({\"size\":\"LARGE\"})": "https://media.glassdoor.com/sqll/6085804/pinecone-systems-squarelogo-1647610264911.png",
            "squareLogoUrl({\"size\":\"REGULAR\"})": "https://media.glassdoor.com/sql/6085804/pinecone-systems-squarelogo-1647610264911.png",
            "bestLedCompanies({\"onlyCurrent\":true})": [],
            "bestPlacesToWork({\"onlyCurrent\":true})": [],
            "coverPhoto": {
              "__typename": "EmployerCoverPhoto",
              "hiResUrl": null
            },
            "counts": {
              "__typename": "EmployerCounts",
              "benefitCount": 5,
              "globalJobCount": {
                "__typename": "EmployerJobCountHolder",
                "jobCount": 4
              },
              "interviewCount": 15,
              "photoCount": 5,
              "reviewCount": 9,
              "salaryCount": 25
            },
            "officeAddresses": [
              {
                "__typename": "EmployerOfficeAddressEG",
                "id": 553188
              },
              {
                "__typename": "EmployerOfficeAddressEG",
                "id": 553201
              }
            ],
            "parent": null,
            "ratings": {
              "__typename": "DefaultEmployerRatings",
              "diversityAndInclusionRatingCount": 6,
              "overallRating": 5
            }
          },
          "employerResponses": [],
          "employmentStatus": "REGULAR",
          "flaggingDisabled": null,
          "featured": false,
          "isCurrentJob": false,
          "jobTitle": {
            "__typename": "JobTitle",
            "id": 15169,
            "text": "Account Executive"
          },
          "languageId": "eng",
          "lengthOfEmployment": 0,
          "location": {
            "__typename": "City",
            "id": 1147436,
            "type": "CITY",
            "name": "San Jose, CA"
          },
          "originalLanguageId": null,
          "pros": "Creative team, smart products and good folks",
          "prosOriginal": null,
          "ratingBusinessOutlook": null,
          "ratingCareerOpportunities": 0,
          "ratingCeo": null,
          "ratingCompensationAndBenefits": 0,
          "ratingCultureAndValues": 0,
          "ratingDiversityAndInclusion": 0,
          "ratingOverall": 5,
          "ratingRecommendToFriend": null,
          "ratingSeniorLeadership": 0,
          "ratingWorkLifeBalance": 0,
          "reviewDateTime": "2024-09-16T08:54:29.477",
          "reviewId": 91053381,
          "relatedStructures": [],
          "summary": "High Growth, high intensity, good team",
          "summaryOriginal": null,
          "translationMethod": null
        },
        {
          "__typename": "EmployerReviewRG",
          "advice": null,
          "adviceOriginal": null,
          "cons": "* Sometimes it may get busy during launches, deadlines",
          "consOriginal": null,
          "countHelpful": 0,
          "countNotHelpful": 0,
          "employer": {
            "__typename": "Employer",
            "id": 6085804,
            "shortName": "Pinecone Systems",
            "firstNotReassignedEmployerId": 6085804,
            "activeStatus": "INACTIVE",
            "approvalStatus": "APPROVED",
            "headquarters": "San Mateo, CA",
            "size": "201 to 500 Employees",
            "bestProfile": {
              "__typename": "EmployerProfile",
              "id": 5016216,
              "rowProfile": false
            },
            "restOfWorldProfile": {
              "__typename": "EmployerProfile",
              "id": 9476430
            },
            "primaryIndustry": {
              "__typename": "EmployerIndustry",
              "industryId": 200155,
              "industryName": "Software Development",
              "sectorId": 10013,
              "sectorName": "Information Technology"
            },
            "ceo": {
              "__typename": "Ceo",
              "id": 1113114,
              "photoUrl({\"size\":\"LARGE\"})": "https://media.glassdoor.com/people/sqll/6085804/pinecone-systems-ceo1680597517812.png",
              "name": "Edo Liberty, Bob Wiederhold",
              "photoUrl({\"size\":\"REGULAR\"})": "https://media.glassdoor.com/people/sql/6085804/pinecone-systems-ceo1680597517812.png",
              "title": "Founder, CEO, President, COO"
            },
            "legalActionBadges": [],
            "employerManagedContent({\"parameters\":[{\"divisionProfileId\":5016216,\"employerId\":6085804}]})": [
              {
                "__typename": "EmployerManagedContent",
                "isContentPaidForTld": false,
                "employerId": 6085804,
                "divisionProfileId": 5016216,
                "featuredVideoLink": null,
                "profileCoverPhoto": null,
                "diversityContent": {
                  "__typename": "DiversityAndInclusionContent",
                  "programsAndInitiatives": null
                }
              }
            ],
            "squareLogoUrl": "https://media.glassdoor.com/sql/6085804/pinecone-systems-squarelogo-1647610264911.png",
            "website": "http://pinecone.io",
            "squareLogoUrl({\"size\":\"LARGE\"})": "https://media.glassdoor.com/sqll/6085804/pinecone-systems-squarelogo-1647610264911.png",
            "squareLogoUrl({\"size\":\"REGULAR\"})": "https://media.glassdoor.com/sql/6085804/pinecone-systems-squarelogo-1647610264911.png",
            "bestLedCompanies({\"onlyCurrent\":true})": [],
            "bestPlacesToWork({\"onlyCurrent\":true})": [],
            "coverPhoto": {
              "__typename": "EmployerCoverPhoto",
              "hiResUrl": null
            },
            "counts": {
              "__typename": "EmployerCounts",
              "benefitCount": 5,
              "globalJobCount": {
                "__typename": "EmployerJobCountHolder",
                "jobCount": 4
              },
              "interviewCount": 15,
              "photoCount": 5,
              "reviewCount": 9,
              "salaryCount": 25
            },
            "officeAddresses": [
              {
                "__typename": "EmployerOfficeAddressEG",
                "id": 553188
              },
              {
                "__typename": "EmployerOfficeAddressEG",
                "id": 553201
              }
            ],
            "parent": null,
            "ratings": {
              "__typename": "DefaultEmployerRatings",
              "diversityAndInclusionRatingCount": 6,
              "overallRating": 5
            }
          },
          "employerResponses": [],
          "employmentStatus": "REGULAR",
          "flaggingDisabled": null,
          "featured": false,
          "isCurrentJob": true,
          "jobTitle": {
            "__typename": "JobTitle",
            "id": 31350,
            "text": "Staff Engineer"
          },
          "languageId": "eng",
          "lengthOfEmployment": 2,
          "location": {
            "__typename": "City",
            "id": 1132348,
            "type": "CITY",
            "name": "New York, NY"
          },
          "originalLanguageId": null,
          "pros": "* Very fast-growing company in a highly sought-after industry.\n* Numerous growth opportunities.\n* Excellent company culture.\n* Insights into the AI industry.",
          "prosOriginal": null,
          "ratingBusinessOutlook": "POSITIVE",
          "ratingCareerOpportunities": 5,
          "ratingCeo": "APPROVE",
          "ratingCompensationAndBenefits": 5,
          "ratingCultureAndValues": 5,
          "ratingDiversityAndInclusion": 5,
          "ratingOverall": 5,
          "ratingRecommendToFriend": "POSITIVE",
          "ratingSeniorLeadership": 5,
          "ratingWorkLifeBalance": 5,
          "reviewDateTime": "2024-06-12T12:05:04.013",
          "reviewId": 88194885,
          "relatedStructures": [],
          "summary": "Great culture, growth",
          "summaryOriginal": null,
          "translationMethod": null
        },
        {
          "__typename": "EmployerReviewRG",
          "advice": null,
          "adviceOriginal": null,
          "cons": "A bit stressful like any startup",
          "consOriginal": null,
          "countHelpful": 0,
          "countNotHelpful": 0,
          "employer": {
            "__typename": "Employer",
            "id": 6085804,
            "shortName": "Pinecone Systems",
            "firstNotReassignedEmployerId": 6085804,
            "activeStatus": "INACTIVE",
            "approvalStatus": "APPROVED",
            "headquarters": "San Mateo, CA",
            "size": "201 to 500 Employees",
            "bestProfile": {
              "__typename": "EmployerProfile",
              "id": 5016216,
              "rowProfile": false
            },
            "restOfWorldProfile": {
              "__typename": "EmployerProfile",
              "id": 9476430
            },
            "primaryIndustry": {
              "__typename": "EmployerIndustry",
              "industryId": 200155,
              "industryName": "Software Development",
              "sectorId": 10013,
              "sectorName": "Information Technology"
            },
            "ceo": {
              "__typename": "Ceo",
              "id": 1113114,
              "photoUrl({\"size\":\"LARGE\"})": "https://media.glassdoor.com/people/sqll/6085804/pinecone-systems-ceo1680597517812.png",
              "name": "Edo Liberty, Bob Wiederhold",
              "photoUrl({\"size\":\"REGULAR\"})": "https://media.glassdoor.com/people/sql/6085804/pinecone-systems-ceo1680597517812.png",
              "title": "Founder, CEO, President, COO"
            },
            "legalActionBadges": [],
            "employerManagedContent({\"parameters\":[{\"divisionProfileId\":5016216,\"employerId\":6085804}]})": [
              {
                "__typename": "EmployerManagedContent",
                "isContentPaidForTld": false,
                "employerId": 6085804,
                "divisionProfileId": 5016216,
                "featuredVideoLink": null,
                "profileCoverPhoto": null,
                "diversityContent": {
                  "__typename": "DiversityAndInclusionContent",
                  "programsAndInitiatives": null
                }
              }
            ],
            "squareLogoUrl": "https://media.glassdoor.com/sql/6085804/pinecone-systems-squarelogo-1647610264911.png",
            "website": "http://pinecone.io",
            "squareLogoUrl({\"size\":\"LARGE\"})": "https://media.glassdoor.com/sqll/6085804/pinecone-systems-squarelogo-1647610264911.png",
            "squareLogoUrl({\"size\":\"REGULAR\"})": "https://media.glassdoor.com/sql/6085804/pinecone-systems-squarelogo-1647610264911.png",
            "bestLedCompanies({\"onlyCurrent\":true})": [],
            "bestPlacesToWork({\"onlyCurrent\":true})": [],
            "coverPhoto": {
              "__typename": "EmployerCoverPhoto",
              "hiResUrl": null
            },
            "counts": {
              "__typename": "EmployerCounts",
              "benefitCount": 5,
              "globalJobCount": {
                "__typename": "EmployerJobCountHolder",
                "jobCount": 4
              },
              "interviewCount": 15,
              "photoCount": 5,
              "reviewCount": 9,
              "salaryCount": 25
            },
            "officeAddresses": [
              {
                "__typename": "EmployerOfficeAddressEG",
                "id": 553188
              },
              {
                "__typename": "EmployerOfficeAddressEG",
                "id": 553201
              }
            ],
            "parent": null,
            "ratings": {
              "__typename": "DefaultEmployerRatings",
              "diversityAndInclusionRatingCount": 6,
              "overallRating": 5
            }
          },
          "employerResponses": [],
          "employmentStatus": "REGULAR",
          "flaggingDisabled": null,
          "featured": false,
          "isCurrentJob": true,
          "jobTitle": {
            "__typename": "JobTitle",
            "id": 17959,
            "text": "Senior Software Engineer"
          },
          "languageId": "eng",
          "lengthOfEmployment": 2,
          "location": null,
          "originalLanguageId": null,
          "pros": "A lot is going on, a lot of attention from users. VPs are highly reachable and always willing to let you work on new ideas",
          "prosOriginal": null,
          "ratingBusinessOutlook": "POSITIVE",
          "ratingCareerOpportunities": 5,
          "ratingCeo": "APPROVE",
          "ratingCompensationAndBenefits": 5,
          "ratingCultureAndValues": 5,
          "ratingDiversityAndInclusion": 4,
          "ratingOverall": 5,
          "ratingRecommendToFriend": "POSITIVE",
          "ratingSeniorLeadership": 4,
          "ratingWorkLifeBalance": 4,
          "reviewDateTime": "2024-02-24T09:13:51",
          "reviewId": 84703340,
          "relatedStructures": [],
          "summary": "Great company to work on",
          "summaryOriginal": null,
          "translationMethod": null
        },
        {
          "__typename": "EmployerReviewRG",
          "advice": null,
          "adviceOriginal": null,
          "cons": "Employee expectations are pretty high",
          "consOriginal": null,
          "countHelpful": 0,
          "countNotHelpful": 0,
          "employer": {
            "__typename": "Employer",
            "id": 6085804,
            "shortName": "Pinecone Systems",
            "firstNotReassignedEmployerId": 6085804,
            "activeStatus": "INACTIVE",
            "approvalStatus": "APPROVED",
            "headquarters": "San Mateo, CA",
            "size": "201 to 500 Employees",
            "bestProfile": {
              "__typename": "EmployerProfile",
              "id": 5016216,
              "rowProfile": false
            },
            "restOfWorldProfile": {
              "__typename": "EmployerProfile",
              "id": 9476430
            },
            "primaryIndustry": {
              "__typename": "EmployerIndustry",
              "industryId": 200155,
              "industryName": "Software Development",
              "sectorId": 10013,
              "sectorName": "Information Technology"
            },
            "ceo": {
              "__typename": "Ceo",
              "id": 1113114,
              "photoUrl({\"size\":\"LARGE\"})": "https://media.glassdoor.com/people/sqll/6085804/pinecone-systems-ceo1680597517812.png",
              "name": "Edo Liberty, Bob Wiederhold",
              "photoUrl({\"size\":\"REGULAR\"})": "https://media.glassdoor.com/people/sql/6085804/pinecone-systems-ceo1680597517812.png",
              "title": "Founder, CEO, President, COO"
            },
            "legalActionBadges": [],
            "employerManagedContent({\"parameters\":[{\"divisionProfileId\":5016216,\"employerId\":6085804}]})": [
              {
                "__typename": "EmployerManagedContent",
                "isContentPaidForTld": false,
                "employerId": 6085804,
                "divisionProfileId": 5016216,
                "featuredVideoLink": null,
                "profileCoverPhoto": null,
                "diversityContent": {
                  "__typename": "DiversityAndInclusionContent",
                  "programsAndInitiatives": null
                }
              }
            ],
            "squareLogoUrl": "https://media.glassdoor.com/sql/6085804/pinecone-systems-squarelogo-1647610264911.png",
            "website": "http://pinecone.io",
            "squareLogoUrl({\"size\":\"LARGE\"})": "https://media.glassdoor.com/sqll/6085804/pinecone-systems-squarelogo-1647610264911.png",
            "squareLogoUrl({\"size\":\"REGULAR\"})": "https://media.glassdoor.com/sql/6085804/pinecone-systems-squarelogo-1647610264911.png",
            "bestLedCompanies({\"onlyCurrent\":true})": [],
            "bestPlacesToWork({\"onlyCurrent\":true})": [],
            "coverPhoto": {
              "__typename": "EmployerCoverPhoto",
              "hiResUrl": null
            },
            "counts": {
              "__typename": "EmployerCounts",
              "benefitCount": 5,
              "globalJobCount": {
                "__typename": "EmployerJobCountHolder",
                "jobCount": 4
              },
              "interviewCount": 15,
              "photoCount": 5,
              "reviewCount": 9,
              "salaryCount": 25
            },
            "officeAddresses": [
              {
                "__typename": "EmployerOfficeAddressEG",
                "id": 553188
              },
              {
                "__typename": "EmployerOfficeAddressEG",
                "id": 553201
              }
            ],
            "parent": null,
            "ratings": {
              "__typename": "DefaultEmployerRatings",
              "diversityAndInclusionRatingCount": 6,
              "overallRating": 5
            }
          },
          "employerResponses": [],
          "employmentStatus": "REGULAR",
          "flaggingDisabled": null,
          "featured": false,
          "isCurrentJob": true,
          "jobTitle": null,
          "languageId": "eng",
          "lengthOfEmployment": 2,
          "location": null,
          "originalLanguageId": null,
          "pros": "Great people, a professional yet warm atmosphere, and a really exciting product",
          "prosOriginal": null,
          "ratingBusinessOutlook": "POSITIVE",
          "ratingCareerOpportunities": 0,
          "ratingCeo": "APPROVE",
          "ratingCompensationAndBenefits": 0,
          "ratingCultureAndValues": 0,
          "ratingDiversityAndInclusion": 0,
          "ratingOverall": 5,
          "ratingRecommendToFriend": "POSITIVE",
          "ratingSeniorLeadership": 0,
          "ratingWorkLifeBalance": 0,
          "reviewDateTime": "2023-11-24T13:49:24.130",
          "reviewId": 82072203,
          "relatedStructures": [],
          "summary": "Highly Recommended",
          "summaryOriginal": null,
          "translationMethod": null
        },
        {
          "__typename": "EmployerReviewRG",
          "advice": "Thanks",
          "adviceOriginal": null,
          "cons": "- Chaotic - it's a series B company, do you expect something else?",
          "consOriginal": null,
          "countHelpful": 0,
          "countNotHelpful": 0,
          "employer": {
            "__typename": "Employer",
            "id": 6085804,
            "shortName": "Pinecone Systems",
            "firstNotReassignedEmployerId": 6085804,
            "activeStatus": "INACTIVE",
            "approvalStatus": "APPROVED",
            "headquarters": "San Mateo, CA",
            "size": "201 to 500 Employees",
            "bestProfile": {
              "__typename": "EmployerProfile",
              "id": 5016216,
              "rowProfile": false
            },
            "restOfWorldProfile": {
              "__typename": "EmployerProfile",
              "id": 9476430
            },
            "primaryIndustry": {
              "__typename": "EmployerIndustry",
              "industryId": 200155,
              "industryName": "Software Development",
              "sectorId": 10013,
              "sectorName": "Information Technology"
            },
            "ceo": {
              "__typename": "Ceo",
              "id": 1113114,
              "photoUrl({\"size\":\"LARGE\"})": "https://media.glassdoor.com/people/sqll/6085804/pinecone-systems-ceo1680597517812.png",
              "name": "Edo Liberty, Bob Wiederhold",
              "photoUrl({\"size\":\"REGULAR\"})": "https://media.glassdoor.com/people/sql/6085804/pinecone-systems-ceo1680597517812.png",
              "title": "Founder, CEO, President, COO"
            },
            "legalActionBadges": [],
            "employerManagedContent({\"parameters\":[{\"divisionProfileId\":5016216,\"employerId\":6085804}]})": [
              {
                "__typename": "EmployerManagedContent",
                "isContentPaidForTld": false,
                "employerId": 6085804,
                "divisionProfileId": 5016216,
                "featuredVideoLink": null,
                "profileCoverPhoto": null,
                "diversityContent": {
                  "__typename": "DiversityAndInclusionContent",
                  "programsAndInitiatives": null
                }
              }
            ],
            "squareLogoUrl": "https://media.glassdoor.com/sql/6085804/pinecone-systems-squarelogo-1647610264911.png",
            "website": "http://pinecone.io",
            "squareLogoUrl({\"size\":\"LARGE\"})": "https://media.glassdoor.com/sqll/6085804/pinecone-systems-squarelogo-1647610264911.png",
            "squareLogoUrl({\"size\":\"REGULAR\"})": "https://media.glassdoor.com/sql/6085804/pinecone-systems-squarelogo-1647610264911.png",
            "bestLedCompanies({\"onlyCurrent\":true})": [],
            "bestPlacesToWork({\"onlyCurrent\":true})": [],
            "coverPhoto": {
              "__typename": "EmployerCoverPhoto",
              "hiResUrl": null
            },
            "counts": {
              "__typename": "EmployerCounts",
              "benefitCount": 5,
              "globalJobCount": {
                "__typename": "EmployerJobCountHolder",
                "jobCount": 4
              },
              "interviewCount": 15,
              "photoCount": 5,
              "reviewCount": 9,
              "salaryCount": 25
            },
            "officeAddresses": [
              {
                "__typename": "EmployerOfficeAddressEG",
                "id": 553188
              },
              {
                "__typename": "EmployerOfficeAddressEG",
                "id": 553201
              }
            ],
            "parent": null,
            "ratings": {
              "__typename": "DefaultEmployerRatings",
              "diversityAndInclusionRatingCount": 6,
              "overallRating": 5
            }
          },
          "employerResponses": [],
          "employmentStatus": "REGULAR",
          "flaggingDisabled": null,
          "featured": false,
          "isCurrentJob": true,
          "jobTitle": {
            "__typename": "JobTitle",
            "id": 504273,
            "text": "Staff Solutions Engineer"
          },
          "languageId": "eng",
          "lengthOfEmployment": 0,
          "location": null,
          "originalLanguageId": null,
          "pros": "- Leadership - Respect - Creativity - Demands excellence - Good humans",
          "prosOriginal": null,
          "ratingBusinessOutlook": "POSITIVE",
          "ratingCareerOpportunities": 5,
          "ratingCeo": "APPROVE",
          "ratingCompensationAndBenefits": 5,
          "ratingCultureAndValues": 5,
          "ratingDiversityAndInclusion": 5,
          "ratingOverall": 5,
          "ratingRecommendToFriend": "POSITIVE",
          "ratingSeniorLeadership": 5,
          "ratingWorkLifeBalance": 5,
          "reviewDateTime": "2023-09-22T07:38:32.827",
          "reviewId": 80320231,
          "relatedStructures": [],
          "summary": "Pinecone is the most advanced and best AI company to date.",
          "summaryOriginal": null,
          "translationMethod": null
        },
        {
          "__typename": "EmployerReviewRG",
          "advice": null,
          "adviceOriginal": null,
          "cons": "Lots of autonomy, which is not for everyone",
          "consOriginal": null,
          "countHelpful": 0,
          "countNotHelpful": 0,
          "employer": {
            "__typename": "Employer",
            "id": 6085804,
            "shortName": "Pinecone Systems",
            "firstNotReassignedEmployerId": 6085804,
            "activeStatus": "INACTIVE",
            "approvalStatus": "APPROVED",
            "headquarters": "San Mateo, CA",
            "size": "201 to 500 Employees",
            "bestProfile": {
              "__typename": "EmployerProfile",
              "id": 5016216,
              "rowProfile": false
            },
            "restOfWorldProfile": {
              "__typename": "EmployerProfile",
              "id": 9476430
            },
            "primaryIndustry": {
              "__typename": "EmployerIndustry",
              "industryId": 200155,
              "industryName": "Software Development",
              "sectorId": 10013,
              "sectorName": "Information Technology"
            },
            "ceo": {
              "__typename": "Ceo",
              "id": 1113114,
              "photoUrl({\"size\":\"LARGE\"})": "https://media.glassdoor.com/people/sqll/6085804/pinecone-systems-ceo1680597517812.png",
              "name": "Edo Liberty, Bob Wiederhold",
              "photoUrl({\"size\":\"REGULAR\"})": "https://media.glassdoor.com/people/sql/6085804/pinecone-systems-ceo1680597517812.png",
              "title": "Founder, CEO, President, COO"
            },
            "legalActionBadges": [],
            "employerManagedContent({\"parameters\":[{\"divisionProfileId\":5016216,\"employerId\":6085804}]})": [
              {
                "__typename": "EmployerManagedContent",
                "isContentPaidForTld": false,
                "employerId": 6085804,
                "divisionProfileId": 5016216,
                "featuredVideoLink": null,
                "profileCoverPhoto": null,
                "diversityContent": {
                  "__typename": "DiversityAndInclusionContent",
                  "programsAndInitiatives": null
                }
              }
            ],
            "squareLogoUrl": "https://media.glassdoor.com/sql/6085804/pinecone-systems-squarelogo-1647610264911.png",
            "website": "http://pinecone.io",
            "squareLogoUrl({\"size\":\"LARGE\"})": "https://media.glassdoor.com/sqll/6085804/pinecone-systems-squarelogo-1647610264911.png",
            "squareLogoUrl({\"size\":\"REGULAR\"})": "https://media.glassdoor.com/sql/6085804/pinecone-systems-squarelogo-1647610264911.png",
            "bestLedCompanies({\"onlyCurrent\":true})": [],
            "bestPlacesToWork({\"onlyCurrent\":true})": [],
            "coverPhoto": {
              "__typename": "EmployerCoverPhoto",
              "hiResUrl": null
            },
            "counts": {
              "__typename": "EmployerCounts",
              "benefitCount": 5,
              "globalJobCount": {
                "__typename": "EmployerJobCountHolder",
                "jobCount": 4
              },
              "interviewCount": 15,
              "photoCount": 5,
              "reviewCount": 9,
              "salaryCount": 25
            },
            "officeAddresses": [
              {
                "__typename": "EmployerOfficeAddressEG",
                "id": 553188
              },
              {
                "__typename": "EmployerOfficeAddressEG",
                "id": 553201
              }
            ],
            "parent": null,
            "ratings": {
              "__typename": "DefaultEmployerRatings",
              "diversityAndInclusionRatingCount": 6,
              "overallRating": 5
            }
          },
          "employerResponses": [],
          "employmentStatus": "REGULAR",
          "flaggingDisabled": null,
          "featured": false,
          "isCurrentJob": true,
          "jobTitle": null,
          "languageId": "eng",
          "lengthOfEmployment": 0,
          "location": null,
          "originalLanguageId": null,
          "pros": "Deep tech work, transparent, fantastic team members",
          "prosOriginal": null,
          "ratingBusinessOutlook": null,
          "ratingCareerOpportunities": 5,
          "ratingCeo": null,
          "ratingCompensationAndBenefits": 4,
          "ratingCultureAndValues": 5,
          "ratingDiversityAndInclusion": 4,
          "ratingOverall": 5,
          "ratingRecommendToFriend": "POSITIVE",
          "ratingSeniorLeadership": 5,
          "ratingWorkLifeBalance": 4,
          "reviewDateTime": "2021-11-04T00:00:49.030",
          "reviewId": 54897470,
          "relatedStructures": [],
          "summary": "Love every minute in the company",
          "summaryOriginal": null,
          "translationMethod": null
        }
      ],
      "ratingCountDistribution": {
        "__typename": "RatingCountDistribution",
        "overall": {
          "__typename": "FiveStarRatingCountDistribution",
          "_5": 9,
          "_4": 0,
          "_3": 0,
          "_2": 0,
          "_1": 0
        },
        "cultureAndValues": {
          "__typename": "FiveStarRatingCountDistribution",
          "_5": 6,
          "_4": 0,
          "_3": 0,
          "_2": 0,
          "_1": 0
        },
        "careerOpportunities": {
          "__typename": "FiveStarRatingCountDistribution",
          "_5": 6,
          "_4": 0,
          "_3": 0,
          "_2": 0,
          "_1": 0
        },
        "workLifeBalance": {
          "__typename": "FiveStarRatingCountDistribution",
          "_5": 4,
          "_4": 2,
          "_3": 0,
          "_2": 0,
          "_1": 0
        },
        "seniorManagement": {
          "__typename": "FiveStarRatingCountDistribution",
          "_5": 5,
          "_4": 1,
          "_3": 0,
          "_2": 0,
          "_1": 0
        },
        "compensationAndBenefits": {
          "__typename": "FiveStarRatingCountDistribution",
          "_5": 5,
          "_4": 1,
          "_3": 0,
          "_2": 0,
          "_1": 0
        },
        "diversityAndInclusion": {
          "__typename": "FiveStarRatingCountDistribution",
          "_5": 4,
          "_4": 2,
          "_3": 0,
          "_2": 0,
          "_1": 0
        },
        "recommendToFriend": {
          "__typename": "RecommendToFriendRatingCountDistribution",
          "WONT_RECOMMEND": 0,
          "RECOMMEND": 7
        }
      }
    },
    "reviews": [
      {
        "advice": null,
        "cons": "Lots of autonomy, which is not for everyone",
        "lengthOfEmployment": 0,
        "pros": "Deep tech work, transparent, fantastic team members",
        "ratingOverall": 5,
        "reviewId": 54897470,
        "summary": "Love every minute in the company",
        "jobTitle": null,
        "reviewDateTime": "2021-11-04T00:00:49.030000",
        "employer_url_part": "Pinecone-Systems"
      },
      {
        "advice": "Thanks",
        "cons": "- Chaotic - it's a series B company, do you expect something else?",
        "lengthOfEmployment": 0,
        "pros": "- Leadership - Respect - Creativity - Demands excellence - Good humans",
        "ratingOverall": 5,
        "reviewId": 80320231,
        "summary": "Pinecone is the most advanced and best AI company to date.",
        "jobTitle": {
          "id": 504273,
          "text": "Staff Solutions Engineer"
        },
        "reviewDateTime": "2023-09-22T07:38:32.827000",
        "employer_url_part": "Pinecone-Systems"
      },
      {
        "advice": null,
        "cons": "Employee expectations are pretty high",
        "lengthOfEmployment": 2,
        "pros": "Great people, a professional yet warm atmosphere, and a really exciting product",
        "ratingOverall": 5,
        "reviewId": 82072203,
        "summary": "Highly Recommended",
        "jobTitle": null,
        "reviewDateTime": "2023-11-24T13:49:24.130000",
        "employer_url_part": "Pinecone-Systems"
      },
      {
        "advice": null,
        "cons": "A bit stressful like any startup",
        "lengthOfEmployment": 2,
        "pros": "A lot is going on, a lot of attention from users. VPs are highly reachable and always willing to let you work on new ideas",
        "ratingOverall": 5,
        "reviewId": 84703340,
        "summary": "Great company to work on",
        "jobTitle": {
          "id": 17959,
          "text": "Senior Software Engineer"
        },
        "reviewDateTime": "2024-02-24T09:13:51",
        "employer_url_part": "Pinecone-Systems"
      },
      {
        "advice": null,
        "cons": "* Sometimes it may get busy during launches, deadlines",
        "lengthOfEmployment": 2,
        "pros": "* Very fast-growing company in a highly sought-after industry.\n* Numerous growth opportunities.\n* Excellent company culture.\n* Insights into the AI industry.",
        "ratingOverall": 5,
        "reviewId": 88194885,
        "summary": "Great culture, growth",
        "jobTitle": {
          "id": 31350,
          "text": "Staff Engineer"
        },
        "reviewDateTime": "2024-06-12T12:05:04.013000",
        "employer_url_part": "Pinecone-Systems"
      },
      {
        "advice": "Keep being transparent with communication. Don't scare people with abrupt/sudden announcements; the tech industry is stressful for employees to be in right now. Focus on solid recurring revenue. Don't lose the lead with our community, market share, and mindshare.",
        "cons": "- there was a recent individual layoff which does scare people a bit. And it wasn't communicated well. Was very abrupt and sudden. \n- the space is becoming very competitive\n- typical startup growing pains and process improvements \n- expectations are high but do you really want anything else if you're in a startup?",
        "lengthOfEmployment": 2,
        "pros": "- company is on fire. It's THE vector database in the AI space. The developer love is very real, and partners love us \n- the company culture. People are genuinely nice, humble, and smart. Haven't met a bad egg yet. Everyone is so eager to help you in a sincere and genuine way. \n- very open culture. There doesn't feel like any turfs or silos between teams. Communication is great \n- generally transparent leadership\n- really smart people. Very very competent managers. \n- pays above average \n- good benefits \n- truly global office, fully remote\n- extremely community and developer oriented\n- strategy and OKRs are efficiently communicated \n-",
        "ratingOverall": 5,
        "reviewId": 89202228,
        "summary": "Pinecone is a GREAT company",
        "jobTitle": {
          "id": 30270,
          "text": "Software Developer"
        },
        "reviewDateTime": "2024-07-16T12:00:23.313000",
        "employer_url_part": "Pinecone-Systems"
      },
      {
        "advice": null,
        "cons": "- With a lot of growth, there are growing pains but I see everyone working towards bridging gaps and feel this is normal in most companies",
        "lengthOfEmployment": 1,
        "pros": "- Given autonomy to work on projects\n- Different ideas and perspectives are heard and considered\n- People are encouraged to question or raise areas of the company that aren't working\n- Open culture where everyone feels accessible and is willing to lend a hand",
        "ratingOverall": 5,
        "reviewId": 89912330,
        "summary": "Fast growing company with room for individual growth",
        "jobTitle": null,
        "reviewDateTime": "2024-08-08T09:41:04.320000",
        "employer_url_part": "Pinecone-Systems"
      },
      {
        "advice": null,
        "cons": "Typical Startup Growing pains that are being solved",
        "lengthOfEmployment": 0,
        "pros": "Creative team, smart products and good folks",
        "ratingOverall": 5,
        "reviewId": 91053381,
        "summary": "High Growth, high intensity, good team",
        "jobTitle": {
          "id": 15169,
          "text": "Account Executive"
        },
        "reviewDateTime": "2024-09-16T08:54:29.477000",
        "employer_url_part": "Pinecone-Systems"
      },
      {
        "advice": null,
        "cons": "Blah blah blah blah blah",
        "lengthOfEmployment": 0,
        "pros": "Startup environment and lot of independence",
        "ratingOverall": 5,
        "reviewId": 92753543,
        "summary": "Pinecone Reviews",
        "jobTitle": {
          "id": 196749,
          "text": "SWE Intern"
        },
        "reviewDateTime": "2024-11-11T19:29:27.300000",
        "employer_url_part": "Pinecone-Systems"
      }
    ],
    "jobs": [],
    "summary_markdown": "# Employee Sentiments\n\n## Reasons Employees Like Working at Pinecone\n\n### Positive Work Environment\n- \"Love every minute in the company\" [(Anonymous, Glassdoor, 2021-11-04)](https://www.glassdoor.com/Reviews/Employee-Review-Pinecone-Systems-RVW54897470.htm)\n- \"Great people, a professional yet warm atmosphere, and a really exciting product\" [(Anonymous, Glassdoor, 2023-11-24)](https://www.glassdoor.com/Reviews/Employee-Review-Pinecone-Systems-RVW82072203.htm)\n- \"People are genuinely nice, humble, and smart. Haven't met a bad egg yet.\" [(Software Developer, Glassdoor, 2024-07-16)](https://www.glassdoor.com/Reviews/Employee-Review-Pinecone-Systems-RVW89202228.htm)\n\n### Growth Opportunities\n- \"Very fast-growing company in a highly sought-after industry.\" [(Staff Engineer, Glassdoor, 2024-06-12)](https://www.glassdoor.com/Reviews/Employee-Review-Pinecone-Systems-RVW88194885.htm)\n- \"Numerous growth opportunities.\" [(Staff Engineer, Glassdoor, 2024-06-12)](https://www.glassdoor.com/Reviews/Employee-Review-Pinecone-Systems-RVW88194885.htm)\n- \"Given autonomy to work on projects\" [(Anonymous, Glassdoor, 2024-08-08)](https://www.glassdoor.com/Reviews/Employee-Review-Pinecone-Systems-RVW89912330.htm)\n\n### Leadership and Culture\n- \"Leadership - Respect - Creativity - Demands excellence - Good humans\" [(Staff Solutions Engineer, Glassdoor, 2023-09-22)](https://www.glassdoor.com/Reviews/Employee-Review-Pinecone-Systems-RVW80320231.htm)\n- \"Generally transparent leadership\" [(Software Developer, Glassdoor, 2024-07-16)](https://www.glassdoor.com/Reviews/Employee-Review-Pinecone-Systems-RVW89202228.htm)\n- \"Open culture where everyone feels accessible and is willing to lend a hand\" [(Anonymous, Glassdoor, 2024-08-08)](https://www.glassdoor.com/Reviews/Employee-Review-Pinecone-Systems-RVW89912330.htm)\n\n## Reasons Employees Dislike Working at Pinecone\n\n### High Expectations and Stress\n- \"Employee expectations are pretty high\" [(Anonymous, Glassdoor, 2023-11-24)](https://www.glassdoor.com/Reviews/Employee-Review-Pinecone-Systems-RVW82072203.htm)\n- \"A bit stressful like any startup\" [(Senior Software Engineer, Glassdoor, 2024-02-24)](https://www.glassdoor.com/Reviews/Employee-Review-Pinecone-Systems-RVW84703340.htm)\n- \"Typical startup growing pains and process improvements\" [(Software Developer, Glassdoor, 2024-07-16)](https://www.glassdoor.com/Reviews/Employee-Review-Pinecone-Systems-RVW89202228.htm)\n\n### Communication Issues\n- \"There was a recent individual layoff which does scare people a bit. And it wasn't communicated well.\" [(Software Developer, Glassdoor, 2024-07-16)](https://www.glassdoor.com/Reviews/Employee-Review-Pinecone-Systems-RVW89202228.htm)\n- \"With a lot of growth, there are growing pains but I see everyone working towards bridging gaps\" [(Anonymous, Glassdoor, 2024-08-08)](https://www.glassdoor.com/Reviews/Employee-Review-Pinecone-Systems-RVW89912330.htm)\n\n## Key Events or Changes in the Company\n\n- \"It's a series B company, do you expect something else?\" [(Staff Solutions Engineer, Glassdoor, 2023-09-22)](https://www.glassdoor.com/Reviews/Employee-Review-Pinecone-Systems-RVW80320231.htm)\n- \"There was a recent individual layoff which does scare people a bit.\" [(Software Developer, Glassdoor, 2024-07-16)](https://www.glassdoor.com/Reviews/Employee-Review-Pinecone-Systems-RVW89202228.htm)\n\n## Specific Details About Benefits\n\n- \"Pays above average\" [(Software Developer, Glassdoor, 2024-07-16)](https://www.glassdoor.com/Reviews/Employee-Review-Pinecone-Systems-RVW89202228.htm)\n- \"Good benefits\" [(Software Developer, Glassdoor, 2024-07-16)](https://www.glassdoor.com/Reviews/Employee-Review-Pinecone-Systems-RVW89202228.htm)"
  },
  "news_result": [
    [
      "Pinecone",
      "Pinecone",
      "pinecone.io",
      null,
      false,
      false,
      null,
      [
        false,
        false
      ]
    ],
    [
      {
        "title": "2024 releases - Pinecone Docs",
        "link": "https://docs.pinecone.io/release-notes/2024",
        "snippet": "Dec 23, 2024 ... 4.2 of the Pinecone Python SDK. This release adds a required keyword argument, metric , to the query_namespaces method. This change enables the SDK to merge ...",
        "formattedUrl": "https://docs.pinecone.io/release-notes/2024"
      },
      {
        "title": "Pinecone integrates AI inferencing with vector database | Hacker ...",
        "link": "https://news.ycombinator.com/item?id=42315364",
        "snippet": "Dec 4, 2024 ... Weaviate seems to have added a similar capability — kind of wild that they announced on the same day. Looks like Pinecone also includes reranking as part of the ...",
        "formattedUrl": "https://news.ycombinator.com/item?id=42315364"
      },
      {
        "title": "Manage serverless indexes - Pinecone Docs",
        "link": "https://docs.pinecone.io/guides/indexes/manage-indexes",
        "snippet": "Mar 11, 2025 ... ... \"example-sparse-index-govk0nt.svc.aped-4627-b74a.pinecone.io\", \"spec\": { \"serverless\": { \"cloud\": \"aws\", \"region\": \"us-east-1\" } }, \"status\": { \"ready ...",
        "formattedUrl": "https://docs.pinecone.io/guides/indexes/manage-indexes"
      },
      {
        "title": "LangGraph and Research Agents | Pinecone",
        "link": "https://www.pinecone.io/learn/langgraph-research-agent/",
        "snippet": "Jul 11, 2024 ... Subscribe to Pinecone ... Research agents are multi-step AI agents that aim to provide in-depth answers to user queries. These agents differ from typical ...",
        "formattedUrl": "https://www.pinecone.io/learn/langgraph-research-agent/"
      },
      {
        "title": "First-of-its-kind Pinecone Knowledge Platform to Power Best-in ...",
        "link": "https://www.prnewswire.com/news-releases/first-of-its-kind-pinecone-knowledge-platform-to-power-best-in-class-retrieval-for-customers-302320811.html",
        "snippet": "Dec 3, 2024 ... PRNewswire/ -- With its vector database at the core, Pinecone, the leading knowledge platform for building accurate, secure, and scalable artificial...",
        "formattedUrl": "https://www.prnewswire.com/news.../first-of-its-kind-pinecone-knowledge-..."
      },
      {
        "title": "Introducing the First Hallucination-Free LLM | Pinecone",
        "link": "https://www.pinecone.io/blog/hallucination-free-llm/",
        "snippet": "Apr 1, 2024 ... While Pinecone is most known for the vector database which helps reduce hallucinations through Retrieval Augmented Generation, we're also investing in ...",
        "formattedUrl": "https://www.pinecone.io/blog/hallucination-free-llm/"
      },
      {
        "title": "401 Unauthorized exception calling the API - Support - Pinecone ...",
        "link": "https://community.pinecone.io/t/401-unauthorized-exception-calling-the-api/5096",
        "snippet": "Apr 4, 2024 ... The API key you provided was rejected while calling https://xxx-yyy.svc.gcp-starter.pinecone.io/query. Please check your configuration values and try again.",
        "formattedUrl": "https://community.pinecone.io/t/401-unauthorized-exception...the.../5096"
      },
      {
        "title": "The Practitioner's Guide To E5 | Pinecone",
        "link": "https://www.pinecone.io/learn/the-practitioners-guide-to-e5/",
        "snippet": "Jun 24, 2024 ... Learn how to use the Pinecone Inference API along with the multilingual E5 model in this article. The article covers why the E5 model was selected, ...",
        "formattedUrl": "https://www.pinecone.io/learn/the-practitioners-guide-to-e5/"
      },
      {
        "title": "Pinecone pricing: Features and plans explained - Orb",
        "link": "https://www.withorb.com/blog/pinecone-pricing",
        "snippet": "Oct 25, 2024 ... Understand Pinecone pricing and features. Learn how this vector database works, how its pricing model works, and how to build a similar pricing engine.",
        "formattedUrl": "https://www.withorb.com/blog/pinecone-pricing"
      },
      {
        "title": "PineconeNotFoundError: A call to https://api.pinecone.io/indexes ...",
        "link": "https://github.com/FlowiseAI/Flowise/issues/2144",
        "snippet": "Apr 9, 2024 ... PineconeNotFoundError: A call to https://api.pinecone.io/indexes/index1 returned HTTP status 404. #2144. Closed. zbalsara21 opened this issue on Apr 9, 2024 ...",
        "formattedUrl": "https://github.com/FlowiseAI/Flowise/issues/2144"
      },
      {
        "title": "Pinecone Vector Store node documentation | n8n Docs",
        "link": "https://docs.n8n.io/integrations/builtin/cluster-nodes/root-nodes/n8n-nodes-langchain.vectorstorepinecone/",
        "snippet": "Jan 17, 2025 ... On this page, you'll find the node parameters for the Pinecone node, and links to more resources. Credentials. You can find authentication information for this ...",
        "formattedUrl": "https://docs.n8n.io/.../builtin/.../n8n-nodes-langchain.vectorstorepinecone/"
      },
      {
        "title": "Add RAG to Your Flink AI Flow Using Federated Search with Pinecone",
        "link": "https://www.confluent.io/blog/flink-ai-rag-with-federated-search/",
        "snippet": "Mar 4, 2025 ... ... Pinecone subscription with an active API key are required. Step 1: Create a Pinecone vector database index by logging into https://app.pinecone.io.",
        "formattedUrl": "https://www.confluent.io/blog/flink-ai-rag-with-federated-search/"
      },
      {
        "title": "Pinecone - Company Profile - Tracxn",
        "link": "https://tracxn.com/d/companies/pinecone/__0UwanTwdq5VEQ_i9ABozYwVOolyHuunHPhQ2BgGYPG4",
        "snippet": "Jan 23, 2025 ... Pinecone - Provider of a cloud-based vector database for similarity search. Raised a total funding of $138M over 3 rounds from 5 investors.",
        "formattedUrl": "https://tracxn.com/.../pinecone/__0UwanTwdq5VEQ_i9ABozYwVOolyHu..."
      },
      {
        "title": "Sell or Invest in Pinecone Stock Pre-IPO | 1% Fee on Trades",
        "link": "https://www.nasdaqprivatemarket.com/company/pinecone/",
        "snippet": "Oct 30, 2024 ... Pinecone. www.pinecone.io. Hypercube Systems-Logo. Pinecone Stock. Company Profile: Pinecone is a vector database designed to power AI ...",
        "formattedUrl": "https://www.nasdaqprivatemarket.com/company/pinecone/"
      },
      {
        "title": "Pinecone Named to Fast Company's Annual List of the World's Most ...",
        "link": "https://www.fox21news.com/business/press-releases/cision/20250318LN43765/pinecone-named-to-fast-companys-annual-list-of-the-worlds-most-innovative-companies-of-2025",
        "snippet": "Mar 18, 2025 ... Pinecone Named to Fast Company's Annual List of the World's Most Innovative Companies of 2025. Pinecone_Systems_Inc_Logo. News provided by. Pinecone Systems Inc.",
        "formattedUrl": "https://www.fox21news.com/.../pinecone-named-to-fast-companys-annual-l..."
      },
      {
        "title": "ChatGPT not referencing embeddings from Vector Database ...",
        "link": "https://community.n8n.io/t/chatgpt-not-referencing-embeddings-from-vector-database/80569",
        "snippet": "Feb 22, 2025 ... I have a separate workflow which uses ChatGPT to summarise news articles, then pushes the summary plus the article metadata to Pinecone to store as embeddings.",
        "formattedUrl": "https://community.n8n.io/t/chatgpt-not-referencing-embeddings.../80569"
      },
      {
        "title": "Fivetran and Pinecone Assistant make starting with RAG easy",
        "link": "https://www.fivetran.com/blog/fivetran-and-pinecone-assistant-make-starting-with-rag-easy",
        "snippet": "Oct 23, 2024 ... Create the RAG application using the Pinecone Assistant. To set up your first Assistant, go to the Pinecone console at https://app.pinecone.io/. Once logged ...",
        "formattedUrl": "https://www.fivetran.com/.../fivetran-and-pinecone-assistant-make-starting-..."
      },
      {
        "title": "Understanding Embeddings with Pinecone | by juliuscecilia33 ...",
        "link": "https://medium.com/@juliuscecilia33/understanding-embeddings-with-pinecone-9f765b490387",
        "snippet": "Nov 12, 2024 ... Recently, I took a deep dive into Pinecone (https://www.pinecone.io/), a vector database designed for high-dimensional data like embeddings.",
        "formattedUrl": "https://medium.com/.../understanding-embeddings-with-pinecone-9f765b4..."
      },
      {
        "title": "How to Get More from Your Pinecone Vector Database – Vectorize",
        "link": "https://vectorize.io/how-to-get-more-from-your-pinecone-vector-database/",
        "snippet": "Apr 11, 2024 ... This will also include the text that was used to create those vectors. In this way you can find the most similar content that Pinecone has in its search indexes ...",
        "formattedUrl": "https://vectorize.io/how-to-get-more-from-your-pinecone-vector-database/"
      },
      {
        "title": "Pinecone Revamps Vector Database Architecture for AI Apps - The ...",
        "link": "https://thenewstack.io/pinecone-revamps-vector-database-architecture-for-ai-apps/",
        "snippet": "Feb 27, 2025 ... ... linked together to function as a single unit. ... Disk-based Metadata Filtering, which is another new feature in this update of Pinecone's serverless architecture ...",
        "formattedUrl": "https://thenewstack.io/pinecone-revamps-vector-database-architecture-for-ai..."
      }
    ],
    [
      "# [2024 releases](https://docs.pinecone.io/release-notes/2024)\nLaunch week: Pinecone Local\n\nPinecone now offers Pinecone Local, an in-memory database emulator available as a Docker image. You can use Pinecone Local to develop your applications locally, or to test your applications in CI/CD, without connecting to your Pinecone account, affecting production data, or incurring any usage or storage fees. Pinecone Local is in public preview.\n\nLaunch week: Enhanced security and access controls\n\nSupport for customer-managed encryption keys (CMEK) is now in public preview.\n\nYou can now change API key permissions.\n\nPrivate Endpoints are now in general availability. Use Private Endpoints to connect AWS PrivateLink to Pinecone while keeping your VPC private from the public internet.\n\nAudit logs, now in early access, provide a detailed record of user and API actions that occur within the Pinecone platform.\n\nPinecone docs: New workflows and best practices\n\nAdded typical Pinecone Database and Pinecone Assistant workflows to the Docs landing page.\n\nUpdated various examples to use the production best practice of targeting an index by host instead of name.\n\nUpdated the Amazon Bedrock integration setup guide. It now utilizes Bedrock Agents.\n\nPinecone Assistant: Context snippets and structured data files\n\nYou can now retrieve the context snippets that Pinecone Assistant uses to generate its responses. This data includes relevant chunks, relevancy scores, and references.\n\nYou can now upload JSON (.json) and Markdown (.md) files to an assistant.\n\nReleased Python SDK v5.4.0 and v5.4.1\n\nReleased v5.4.0 and v5.4.1 of the Pinecone Python SDK. v5.4.0 adds a query_namespaces utility method to run a query in parallel across multiple namespaces in an index and then merge the result sets into a single ranked result set with the top_k most relevant results. v5.4.1 adds support for the pinecone-plugin-inference package required for some integrated inference operations.\n\nReleased major SDK updates: Node.js, Go, Java, and Python\n\nReleased v4.0.0 of the Pinecone Node.js SDK. This version uses the latest stable API version, 2024-10, and adds support for reranking and import.\n\nReleased v2.0.0 of the Pinecone Go SDK. This version uses the latest stable API version, 2024-10, and adds support for reranking and import.\n\nReleased v3.0.0 of the Pinecone Java SDK. This version uses the latest stable API version, 2024-10, and adds support for embedding, reranking, and import.\n\nv3.0.0 also includes the following breaking change: The control class has been renamed db_control. Before upgrading to this version, be sure to update all relevant import statements to account for this change.\n\nFor example, you would change import org.openapitools.control.client.model.*; to import org.openapitools.db_control.client.model.*;.\n\nv5.3.0 and v5.3.1 of the Pinecone Python SDK use the latest stable API version, 2024-10. These versions were release previously.\n\nChat and update features added to Assistant\n\nAdded the chat_assistant endpoint to the Assistant API. It can be used to chat with your assistant, and get responses and citations back in a structured form.\n\nYou can now add instructions when creating or updating an assistant. Instructions are a short description or directive for the assistant to apply to all of its responses. For example, you can update the instructions to reflect the assistant’s role or purpose.\n\nYou can now update an existing assistant with new instructions or metadata.\n\nReleased v5.3.1 of the Pinecone Python SDK. This version adds a missing python-dateutil dependency.\n\nReleased v1.1.1 of the Pinecone Go SDK. This version adds support for non-secure client connections.\n\nReleased v2.1.0 of the Pinecone Java SDK. This version adds support for non-secure client connections.\n\nAdded the metrics_alignment operation, which provides a way to evaluate the correctness and completeness of a response from a RAG system. This feature is in public preview.\n\nWhen using Pinecone Assistant, you can now choose an LLM for the assistant to use and filter the assistant’s responses by metadata.\n\nReleased v3.0.2 of the Pinecone Node.js SDK. This version removes a native Node utility function that was causing issues for users running in Edge. There are no downstream affects of its removal; existing code should not be impacted.\n\nReleased v5.1.0 of the Pinecone Python SDK. With this version, the SDK can now be installed with pip install pinecone / pip install \"pinecone[grpc]\". This version also includes a has_index() helper function to check if an index exists.\n\nReleased v0.1.0 and v0.1.1 of the Pinecone Rust SDK. The Rust SDK is in “alpha” and is under active development. The SDK should be considered unstable and should not be used in production. Before a 1.0 release, there are no guarantees of backward compatibility between minor versions. See the Rust SDK README for full installation instructions and usage examples.\n\nYou can now back up and restore serverless indexes. This feature is in public preview.\n\nServerless indexes are now in general availability on GCP and Azure for Standard and Enterprise plans.\n\nYou can now deploy serverless indexes in the europe-west1 (Netherlands) region of GCP.\n\nReleased v5.0.0 of the Pinecone Python SDK. This version depends on Pinecone API version 2024-07 and includes the ability to prevent accidental index deletion. Additionally, the pinecone-plugin-inference package required to generate embeddings with Pinecone Inference is now included by default; it is no longer necessary to install the plugin separately.\n\nReleased v3.0.0 of the Pinecone Node.js SDK. This version depends on Pinecone API version 2024-07 and includes the ability to prevent accidental index deletion. Additionally, this version supports generating embeddings via Pinecone Inference.\n\nReleased v2.0.0 of the Pinecone Java SDK. This version depends on Pinecone API version 2024-07 and includes the ability to prevent accidental index deletion. Additionally, this version includes the following breaking changes:\n\ncreateServerlessIndex() now requires a new argument: DeletionProtection.ENABLED or DeletionProtection.DISABLED.\n\nconfigureIndex() has been renamed configurePodsIndex().\n\nFor more details, see the Java SDK v2.0.0 migration guide.\n\nReleased version 1.1.0 of the official Spark connector for Pinecone. In this release, you can now set a source tag. Additionally, you can now upsert records with 40KB of metadata, increased from 5KB.\n\nReleased version 1.2.2 of the Pinecone Java SDK. This release simplifies the proxy configuration process. It also fixes an issue where the user agent string was not correctly setup for gRPC calls. Now, if the source tag is set by the user, it is appended to the custom user agent string.\n\nReleased version 4.1.1 of the Pinecone Python SDK. In this release, you can now use colons inside source tags. Additionally, the gRPC version of the Python SDK now allows retries of up to MAX_MSG_SIZE.\n\nYou can now use the ConnectPopup function to bypass the Connect widget and open the “Connect to Pinecone” flow in a popup. This can be used in an app or website for a seamless Pinecone signup and login process.\n\nReleased version 4.0.0 of the Pinecone Python SDK. In this release, we are upgrading the protobuf dependency in our optional grpc extras from 3.20.3 to 4.25.3. Significant performance improvements have been made with this update. This is a breaking change for users of the optional GRPC addon (installed with pinecone[grpc]).\n\nThe free Starter plan now includes 1 project, 5 serverless indexes in the us-east-1 region of AWS, and up to 2 GB of storage. Although the Starter plan has stricter limits than other plans, you can upgrade whenever you’re ready.\n\nAs announced in January 2024, control plane operations like create_index, describe_index, and list_indexes now use a single global URL, https://api.pinecone.io, regardless of the cloud environment where an index is hosted. This is now in general availability. As a result, the legacy version of the API, which required regional URLs for control plane operations, is deprecated as of April 15, 2024 and will be removed in a future, to be announced, release.\n\nReleased version 3.2.2 of the Pinecone Python SDK. This version fixes a minor issue introduced in v3.2.0 that resulted in a DeprecationWarning being incorrectly shown to users who are not passing in the deprecated openapi_config property. This warning can safely be ignored by anyone who is not preparing to upgrade.\n\nReleased version 2.2.0 of the Pinecone Node.js SDK. This releases adds an optional sourceTag that you can set when constructing a Pinecone client to help Pinecone associate API activity to the specified source.\n\nReleased version 0.4.1 of the Pinecone Go SDK. This version adds an optional SourceTag that you can set when constructing a Pinecone client to help Pinecone associate API activity to the specified source.\n\nReleased version 2.2.0 of the Pinecone Node.js SDK.\n\nReleased version 0.4.1 of the Pinecone Go SDK.\n\nReleased version 3.2.1 of the Pinecone Python SDK. This version adds an optional source_tag that you can set when constructing a Pinecone client to help Pinecone associate API activity to the specified source. See the v3.2.1 release notes in GitHub for more details.\n\nReleased version 0.8.1 of the Canopy SDK. This version includes bug fixes, the removal of an unused field for Cohere chat calls, and added guidance on creating a knowledge base with a specified record encoder when using the core library. See the v0.8.1 release notes in GitHub for more details.\n\nThe Pinecone console has a new look and feel, with a brighter, minimalist design; reorganized menu items for quicker, more intuitive navigation; and easy access to recently viewed indexes in the sidebar.\n\nWhen viewing the list of indexes in a project, you can now search indexes by index name; sort indexes alphabetically, by how recently they were viewed or created, or by status; and filter indexes by index type (serverless, pod-based, or starter).\n\nFixed a bug that caused inaccurate index fullness reporting for some pod-based indexes on GCP.\n\nYou can now deploy serverless indexes in the us-east-1 region of AWS.\n\nReleased version 2.1.0 of the Pinecone Node.js SDK. This version adds support for listing the IDs of records in a serverless index. You can list all records or just those with a common ID prefix. Listing by common ID prefix is especially useful as part of managing RAG documents.\n\nYou can now configure single single-on to manage your teams’ access to Pinecone through any identity management solution with SAML 2.0 support, such as Okta. This feature is available on the Enterprise plan only.\n\nThe latest version of the Canopy SDK (v0.8.0) adds support for Pydantic v2. For applications depending on Pydantic v1, this is a breaking change; review the Pydantic v1 to v2 migration guide and make the necessary changes before upgrading. See the Canopy SDK release notes in GitHub for more details.\n\nThe latest version of Pinecone’s Python SDK (v3.1.0) adds support for listing the IDs of records in a serverless index. You can list all records or just those with a common ID prefix. Listing by common ID prefix is especially useful as part of managing RAG documents. See the Python SDK release notes in GitHub for more details.\n\nIt is now possible to convert a pod-based starter index to a serverless index. For organizations on the Starter plan, this requires upgrading to Standard or Enterprise; however, upgrading comes with $100 in serverless credits, which will cover the cost of a converted index for some time.\n\nThe latest version of the Canopy SDK (v0.6.0) adds support for the new API mentioned above as well as namespaces, LLMs that do not have function calling functionality for query generation, and more. See the release notes in GitHub for more details.\n\nThe latest versions of Pinecone’s Python SDK (v3.0.0) and Node.js SDK (v2.0.0) support the new API. To use the new API, existing users must upgrade to the new client versions and adapt some code. For guidance, see the Python SDK v3 migration guide and Node.js SDK v2 migration guide.\n\nThe Pinecone documentation is now versioned. The default “latest” version reflects the new Pinecone API. The “legacy” version reflects the previous API, which requires regional URLs for control plane operations and does not support serverless indexes.",
      "# [Pinecone integrates AI inferencing with vector database](https://news.ycombinator.com/item?id=42315364)\nThis title was a little misleading to me IMO because (maybe my skill issue) I associated \"inferencing\" with \"generation\".\n\nAfter reading the article, it seems Pinecone just now supports in-DB vectorization, a feature that is shared by:\n\n- DataStax Astra DB: https://www.datastax.com/blog/simplifying-vector-embedding-g... (since May 2024)\n\n- Weaviate: https://weaviate.io/blog/introducing-weaviate-embeddings (as of yesterday)\n\nAstra DB seems to just be a tutorial showing how to generate embeddings using another service.\n\nWeaviate seems to have added a similar capability — kind of wild that they announced on the same day.\n\nLooks like Pinecone also includes reranking as part of the same process — did Weaviate add that as well?\n\nNo doubt, it's technically great that Pinecone trained their own embeddings model—but from a business/customer standpoint I can't help but ask _why?_. This is one of those \"build it or buy it\" cases where teams must decide to either integrate with an existing solution or build their own. I'm not sure I see the advantage (from an end user perspective) of using Pinecone's home-rolled embeddings model other than, say OpenAI's, especially given the cost factor: OpenAI embeddings costs really not much.\n\n> Astra DB seems to just be a tutorial showing how to generate embeddings using another service.\n\nThe link I shared showed how a single request to Astra DB's data API has Astra DB automatically create embeddings behind the scenes, integrating with an embedding service the user chooses when they set their database up. Indeed embeddings are generated by another service and not in-house, but from an end-user perspective, they don't need to generate embeddings themeselves as was the prior art and coordinate requests between:\n\n- get text - generate embeddings - take embeddings and send to DB\n\nAs of May when they announced Vectorize, one request did all that. I believe from an end-user experience, this is really analogous to what Weaviate and Pinecone are offering unless I'm missing something.\n\nCan someone please explain how this works?\n\nI assumed that a specific flavour of LLM was needed, an “embedding model” to generate the vectors. Is this announcement that pinecone is adding their own?\n\nIs it better or worse than the models here: https://ollama.com/search?c=embedding For example?\n\nNormally you take your content and run it through an embedding model, inserting the resulting vectors into the vector DB. On a query, for instance, you run the query through the embedding model and query the vector database for the most similar hits to the resulting embedding vector. Similarly reranking is when get get the broad hits from the embedding similarity search and/or BM25, and then a reranker uses the looked up source material to rank the results more finely.\n\nThis is building it into the vector DB such that you send it the content and it is \"built in\".\n\nSeems silly. It's like bundling a stove with cookware. But cookware fit specific niches and have different life cycles. I get that it might cater to some \"drop in solution\" targets, but seems of no value for most engineered, long-term solutions.\n\nThere's more technical detail here: https://www.pinecone.io/blog/integrated-inference/\n\n> Is this announcement that pinecone is adding their own?\n\nTLDR: they trained their own embeddings model and rely on Cohere for ranking. Pinecone (the database) uses this model automatically to generate and store embeddings.\n\n> I assumed that a specific flavour of LLM was needed, an “embedding model” to generate the vectors.\n\nYou're mostly right, with one caveat: embeddings models aren't really LLMs in that they're not very large: they just map semantic meaning to numerical space.\n\n> Is it better or worse than the models here: https://ollama.com/search?c=embedding For example?\n\nThis is the golden question. As far as I know, there is no appropriate benchmarking/eval data about this. I think the real value is the first-class integration between their model and their service.",
      "# [Manage serverless indexes](https://docs.pinecone.io/guides/indexes/manage-indexes)\nThis page shows you how to manage your existing serverless indexes.\n\nFor guidance on pod-based indexes, see Manage pod-based indexes.\n\nList indexes\n\nUse the list_indexes operation to get a complete description of all indexes in a project:\n\nThe response will look like this:\n\nWith the Python SDK, you can use the .names() helper function to iterate over the index names in the list_indexes() response, for example:\n\nDescribe an index\n\nUse the describe_index endpoint to get a complete description of a specific index:\n\nThe response will look like this:\n\nDelete an index\n\nUse the delete_index operation to delete an index and all of its associated resources.\n\nIf deletion protection is enabled on an index, requests to delete it will fail and return a 403 - FORBIDDEN status with the following error:\n\nBefore you can delete such an index, you must first disable deletion protection.\n\nAssociate an embedding model\n\nIntegrated inference lets you upsert and search without extra steps for embedding data and reranking results.\n\nTo configure an existing serverless index for an embedding model, use the configure_index operation as follows:\n\nSet embed.model to one of Pinecone’s hosted embedding models.\n\nSet embed.field_map to the name of the field in your source document that contains the data for embedding.\n\nConfigure deletion protection\n\nEnable deletion protection\n\nYou can prevent an index and its data from accidental deleting when creating a new index or after its been created. In both cases, you set the deletion_protection parameter to enabled.\n\nTo enable deletion protection when creating a new index:\n\nTo enable deletion protection when configuring an existing index:\n\nWhen deletion protection is enabled on an index, requests to delete the index fail and return a 403 - FORBIDDEN status with the following error:\n\nDisable deletion protection\n\nBefore you can delete an index with deletion protection enabled, you must first disable deletion protection as follows:\n\nTags are key-value pairs that you can use to categorize and identify the index.\n\nTo add tags to an index, use the tags parameter when creating a new index or when configuring an existing index.\n\nTo add tags when creating a new index:\n\nTo add or update tags when configuring an existing index:\n\nTo view the tags of an index, list all indexes in a project or get information about a specific index.\n\nTo remove a tag from an index, configure the index and use the tags parameter to send the tag key with an empty value (\"\").\n\nThe following example removes the example: tag tag from example-index:\n\nView backups for an index\n\nServerless indexes can be backed up. You can view all backups for a specific index, as in the following example:\n\nThe example returns a response like the following:",
      "# [LangGraph and Research Agents](https://www.pinecone.io/learn/langgraph-research-agent/)\nResearch agents are multi-step AI agents that aim to provide in-depth answers to user queries. These agents differ from typical conversational agents, where we typically find fast response times and short AI answers.\n\nThere are several good reasons our conversational agents respond in the way they do. The first reason is that we aim to imitate the style of human conversation. A typical conversation between two people consists of short back-and-forths — it's rare (and often not wanted) that one of those people suddenly breaks out into a multi-page monologue.\n\nAnother typical quality of human conversations is a short wait time between responses. When someone says \"hi,\" it would be odd to wait even a second before getting a response. We expect low-latency responses from people, and the same applies to our conversational AI agents.\n\nNow, let's move away from conversational agents and consider what qualities we might want in a research agent. A longer, more researched, and cited response can be much more helpful if we ask a question in research or study. In this scenario, many might choose to delay their response by a few (or more) seconds if they're getting a more detailed response. In these scenarios, a user prefers to speak to a research agent rather than the typical conversational agent.\n\nBecause we have less time pressure with research agents, we can design them to work through a multi-step research pipeline and synthesize information from different places. Given a user's query, a research agent should be able to search the web, read papers, and access other relevant sources of information for its particular use case.\n\nThat doesn't mean a research agent cannot be conversational. In most cases, users prefer a conversational interface, and we can design agents that do both.\n\nGraph showing the research agent we will build in this article.\n\nWe'll focus on building a research agent and how we can achieve that using a graph-based approach to agent construction.\n\nVideo companion to the article.\n\nGraphs for Agents\n\nAgents are still a relatively new concept. The modern form of agents, primarily LLM-powered, only became popular in early 2022, shortly after the release of ChatGPT.\n\nThese earlier agents focused on providing an LLM with a decision-making framework allowing iterative decision-making and tool-use steps. The most popular of these being the Reason Action (ReAct) method [1].\n\nReAct encourages an LLM to generate steps broken into iterative reasoning and action steps. The reasoning step encourages the LLM to outline what steps it needs to take to answer a question. The agent then follows up with an action where our tool/function call executes a step like searching the internet.\n\nReAct agent process. The reasoning step manifests as the Thought output from the LLM. The reasoning step is followed by an Action like Search[Apple Remote]. Multiple iterations can occur to arrive at a final answer.\n\nAfter these two steps, the agent receives an observation, i.e., the result of our action step. This reasoning-action loop can be repeated with various actions, all culminating in a final answer.\n\nThe ReAct framework is powerful because it allows us to combine LLMs with a wide range of data sources and executable programs.\n\nPopular implementations of ReAct (or a similar method called \"MRKL\") made their way to libraries like LangChain, Llama-Index, and Haystack. These implementations allowed us to built ReAct-like agents using an object oriented approach.\n\nThe object-oriented approach functions well but leaves little flexibility or transparency in our agent. It's very possible to use ReAct agents via LangChain and have no idea how any of it is functioning, given the framework's abstractions.\n\nReAct-like agent constructed as a graph.\n\nAn interesting solution to this is to construct agents as graphs. When building an agent as a graph, you can still produce the iterative ReAct-like loop, but it is much easier to customize that loop. We can add more deterministic flows and hierarchical decision-making and simply build with more flexibility and transparency.\n\nMore deterministic flows in our agent allow us to address two key limitations of ReAct-like agents: interpretability and entropy.\n\nThe interpretability of LLMs can often be hard to understand or measure. LLMs lack hard-coded rules, in contrast to traditional software which is very interpretable as it is constructed solely from hard-coded rules. This lack of deterministic behavior in LLMs is both a blessing and a curse. They can generate a seemingly infinite number of outputs, resulting in seemingly infinite potential — but those outputs are hard to engineer and impossible to fully predict.\n\nTo deploy LLM-based software into the world, we need confidence that it will behave a certain way. However, LLMs are compressed representations of an entire world, making it impossible (for now) to predict every possible output of an LLM.\n\nBecause of the lack of interpretability, LLM-based software needs as much determinstic logic as possible. Anything that does not require an LLM, should not use an LLM.\n\nEntropy is the tendency of a system to become more chaotic over time.When executed iteratively without human guidance, an LLM can hallucinate or simply struggle to keep track of an increasingly long set of prior thoughts, actions, and observations. Due to this, LLMs typically become increasingly divergent from their original purpose as the number of iterative generations increases.\n\nLangGraph\n\nLangGraph is LangChain's graph-based agent framework and one of the most popular frameworks for building graph-based agents. It focuses on providing more \"fine-grained\" control over an agent's flow and state.\n\nState\n\nAt the core of a graph in LangGraph is the agent state. The state is a mutable object where we track the current state of the agent execution as we pass through the graph. We can include different parameters within the state, in our research agent we will use a minimal example containing three parameters:\n\ninput: This is the user's most recent query. Usually, this is a question that we want to answer with our research agent.\n\nchat_history: We are building a conversational agent that can support multiple interactions. To allow previous interactions to provide additional context throughout our agent logic, we include the chat history in the agent state.\n\nintermediate_steps provides a record of all steps the research agent will take between the user asking a question via input and the agent providing a final answer. These can include \"search arxiv\", \"perform general purpose web search,\" etc. These intermediate steps are crucial to allowing the agent to follow a path of coherent actions and ultimately producing an informed final answer.\n\nFollow Along with Code!\n\nThe remainder of this article will be focused on the implementation of a LangGraph research agent using Python. You can find a copy of the code here.\n\nIf following along, make sure to install prerequisite packages first:\n\n!pip install -qU \\ datasets==2.19.1 \\ langchain-pinecone==0.1.1 \\ langchain-openai==0.1.9 \\ langchain==0.2.5 \\ langchain-core==0.2.9 \\ langgraph==0.1.1 \\ semantic-router==0.0.48 \\ serpapi==0.1.5 \\ google-search-results==2.4.2 \\\n\nWe define our minimal agent state object like so:\n\nThe agent graph consists of several nodes, five of which are custom tools we need to create. Those tools are:\n\nArXiv paper fetch: Given an arXiv paper ID, this tool provides our agent with the paper's abstract.\n\nWeb search: This tool provides our agent access to Google search for more generalized queries.\n\nRAG search: We will create a knowledge base containing AI arXiv papers. This tool provides our agent with access to this knowledge.\n\nRAG search with filter: Sometimes, our agent may need more information from a specific paper. This tool allows our agent to do just that.\n\nFinal answer: We create a custom final answer tool that forces our agent to output information in a specific format like:\n\nINTRODUCTION ------------ <some intro to our report> RESEARCH STEPS -------------- <the steps the agent took during research> REPORT ------ <the report main body> CONCLUSION ---------- <the report conclusion> SOURCES ------- <any sources the agent used>\n\nWe'll setup each of these tools, which our LLM and graph-logic will later be able to execute.\n\nArXiv Paper Fetch Tool\n\nThe fetch_arxiv tool will allow our agent to get the summary of a specific paper given an ArXiv paper ID. We will simply send a GET request to arXiv and use regex to extract the paper abstract. That GET request will return something like this:\n\nA lot is going on there. Fortunately, we can a small regex to find the paper abstract.Now we pack all of this logic into a tool for our agent.\n\nfrom langchain_core.tools import tool @tool(\"fetch_arxiv\") def fetch_arxiv(arxiv_id: str): \"\"\"Gets the abstract from an ArXiv paper given the arxiv ID. Useful for finding high-level context about a specific paper.\"\"\" # get paper page in html res = requests.get( f\"https://export.arxiv.org/abs/{arxiv_id}\" ) # search html for abstract re_match = abstract_pattern.search(res.text) # return abstract text return re_match.group(1)\n\nWe can test the tool to confirm it works using the .invoke method like so:\n\nWeb Search Tool\n\nThe web search tool will provide the agent with access to web search. We will instruct our LLM to use this for more general knowledge queries. To implement the search component of the tool, we will be using the SerpAPI. You can get a free API key for the service here. Once you have the API key we run a query for \"coffee\" like so:\n\nWe place all of this logic into another tool:\n\nWe provide two RAG-focused tools for our agent. The rag_search allows the agent to perform a simple RAG search for some information across all indexed research papers. The rag_search_filter also searches, but within a specific paper filtered for via the arxiv_id parameter.\n\nWe also define the format_rag_contexts function to handle the transformation of our Pinecone results from a JSON object to a readble plaintext format. Note: you can find the code for setting up the index we use here.\n\nFinal Answer \"Tool\"\n\nFinally, we define a \"final answer\" tool. This tool isn't a tool in the usual sense, instead we use it to force a particular output format from our LLM via the function/tool calling.\n\n@tool(\"final_answer\") def final_answer( introduction: str, research_steps: str, main_body: str, conclusion: str, sources: str ): \"\"\"Returns a natural language response to the user in the form of a research report. There are several sections to this report, those are: - `introduction`: a short paragraph introducing the user's question and the topic we are researching. - `research_steps`: a few bullet points explaining the steps that were taken to research your report. - `main_body`: this is where the bulk of high quality and concise information that answers the user's question belongs. It is 3-4 paragraphs long in length. - `conclusion`: this is a short single paragraph conclusion providing a concise but sophisticated view on what was found. - `sources`: a bulletpoint list provided detailed sources for all information referenced during the research process \"\"\" if type(research_steps) is list: research_steps = \"\\n\".join([f\"- {r}\" for r in research_steps]) if type(sources) is list: sources = \"\\n\".join([f\"- {s}\" for s in sources]) return \"\"\n\nWe have finished defining all our tools and can move onto our next components.\n\nDefining the Oracle\n\nThe Oracle LLM is our graph's decision maker. It decides which path we should take down our graph. It functions similarly to an agent but is much simpler; consisting of a single prompt, LLM, and tool instructions. When executed the oracle will only run once before we move to another node in our graph.\n\nThe Oracle consists of an LLM provided with a set of potential function calls (i.e., our tools) that it can decide to use. We force it to use at least one of those tools using the tool_choice=\"any\" setting (see below). Our Oracle only makes the decision to use a tool; it doesn't execute the tool code itself (we do that separately in our graph).\n\nOur prompt for the Oracle will emphasize its decision-making ability within the system_prompt. We will also add a placeholder for us to later insert chat_history, and provide a place for us to insert the intermediate steps (scratchpad) and user input.\n\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder system_prompt = \"\"\"You are the oracle, the great AI decision maker. Given the user's query you must decide what to do with it based on the list of tools provided to you. If you see that a tool has been used (in the scratchpad) with a particular query, do NOT use that same tool with the same query again. Also, do NOT use any tool more than twice (ie, if the tool appears in the scratchpad twice, do not use it again). You should aim to collect information from a diverse range of sources before providing the answer to the user. Once you have collected plenty of information to answer the user's question (stored in the scratchpad) use the final_answer tool.\"\"\" prompt = ChatPromptTemplate.from_messages([ (\"system\", system_prompt), MessagesPlaceholder(variable_name=\"chat_history\"), (\"user\", \"{input}\"), (\"assistant\", \"scratchpad: {scratchpad}\"), ])\n\nNext, we must initialize our llm (for this we use gpt-4o) and then create the runnable pipeline of our Oracle.\n\nThe runnable connects our inputs (the user input and chat_history) to our prompt, and our prompt to our llm. It is also where we bind our tools to the LLM and enforce function calling via tool_choice=\"any\".\n\nfrom langchain_core.messages import ToolCall, ToolMessage from langchain_openai import ChatOpenAI llm = ChatOpenAI( model=\"gpt-4o\", openai_api_key=os.environ[\"OPENAI_API_KEY\"], temperature=0 ) tools=[ rag_search_filter, rag_search, fetch_arxiv, web_search, final_answer ] # define a function to transform intermediate_steps from list # of AgentAction to scratchpad string def create_scratchpad(intermediate_steps: list[AgentAction]): research_steps = [] for i, action in enumerate(intermediate_steps): if action.log != \"TBD\": # this was the ToolExecution research_steps.append( f\"Tool: {action.tool}, input: {action.tool_input}\\n\" f\"Output: {action.log}\" ) return \"\\n---\\n\".join(research_steps) oracle = ( { \"input\": lambda x: x[\"input\"], \"chat_history\": lambda x: x[\"chat_history\"], \"scratchpad\": lambda x: create_scratchpad( intermediate_steps=x[\"intermediate_steps\"] ), } | prompt | llm.bind_tools(tools, tool_choice=\"any\") )\n\nWe can quickly test the agent to confirm it is functional:It is running, but we are returning a lot of output here. We can narrow this down to what we need—i.e., the chosen tool name and generated input args for the tool.\n\nWe can see now that our Oracle decided to use the web_search tool with a query of \"interesting facts about dogs\" — a good choice.\n\nPreparing the Graph Components\n\nOur graph needs functions that consume the AgentState we defined earlier, and output intermediate_steps. To do this, we wrap our oracle and tool functions to use the AgentState. We'll also define a router which will route our state to different tool nodes based on the output from our oracle. We get this value from the out.tool_calls[0][\"name\"] value.\n\nfrom typing import TypedDict def run_oracle(state: TypedDict): print(\"run_oracle\") print(f\"intermediate_steps: {state['intermediate_steps']}\") out = oracle.invoke(state) tool_name = out.tool_calls[0][\"name\"] tool_args = out.tool_calls[0][\"args\"] action_out = AgentAction( tool=tool_name, tool_input=tool_args, log=\"TBD\" ) return { \"intermediate_steps\": [action_out] } def router(state: TypedDict): # return the tool name to use if isinstance(state[\"intermediate_steps\"], list): return state[\"intermediate_steps\"][-1].tool else: # if we output bad format go to final answer print(\"Router invalid format\") return \"final_answer\"\n\nOur tools can run using the same function logic we define with run_tool. We add the input parameters to our tool call and the resultant output to our graph state's intermediate_steps parameter.\n\ntool_str_to_func = { \"rag_search_filter\": rag_search_filter, \"rag_search\": rag_search, \"fetch_arxiv\": fetch_arxiv, \"web_search\": web_search, \"final_answer\": final_answer } def run_tool(state: TypedDict): # use this as helper function so we repeat less code tool_name = state[\"intermediate_steps\"][-1].tool tool_args = state[\"intermediate_steps\"][-1].tool_input print(f\"{tool_name}.invoke(input={tool_args})\") # run tool out = tool_str_to_func[tool_name].invoke(input=tool_args) action_out = AgentAction( tool=tool_name, tool_input=tool_args, log=str(out) ) return {\"intermediate_steps\": [action_out]}\n\nConstructing the Graph\n\nThe graph in LangGraph consists of several components, we will be using the following:\n\nNodes: these are the steps in the graph where we execute some logic. These could be an LLM call, tool execution, or our final answer output. We add them via graph.add_node(...).\n\nEdges: these are the connecting flows/paths in our graph. If we connect a node to another via an edge, we can move between those nodes. An edge only allows travel in a single direction, so from node A -> node B we would write graph.add_edge(\"node A\", \"node B\") . However, we can allow bidirectional travel by adding a second edge from node B -> node A with graph.add_edge(\"node B\", \"node A\").\n\nEntry point: this specifies the node from which we would enter the graph, i.e. our starting node. If starting at node A we write graph.set_entry_point(\"node A\").\n\nConditional edge: a conditional edge allows us to specify multiple potential edges from a source node to a set of destination nodes defined by a mapping passed to the path parameter. If adding conditional edges from node B to nodes C, D, and E — all decided using a router function (more on this later), we would write graph.add_conditional_edges(source=\"node A\", path=router).\n\nfrom langgraph.graph import StateGraph, END # initialize the graph with our AgentState graph = StateGraph(AgentState) # add nodes graph.add_node(\"oracle\", run_oracle) graph.add_node(\"rag_search_filter\", run_tool) graph.add_node(\"rag_search\", run_tool) graph.add_node(\"fetch_arxiv\", run_tool) graph.add_node(\"web_search\", run_tool) graph.add_node(\"final_answer\", run_tool) # specify the entry node graph.set_entry_point(\"oracle\") # add the conditional edges which use the router graph.add_conditional_edges( source=\"oracle\", # where in graph to start path=router, # function to determine which node is called ) # create edges from each tool back to the oracle for tool_obj in tools: if tool_obj.name != \"final_answer\": graph.add_edge(tool_obj.name, \"oracle\") # if anything goes to final answer, it must then move to END graph.add_edge(\"final_answer\", END) # finally, we compile our graph runnable = graph.compile()\n\nFrom all of this we will have an agent graph that looks like this:\n\nThe agent graph that we built.\n\nBuilding Research Reports\n\nOur research agent is now ready, and we can begin testing it. First, we'll start with something simple that is out of our agent's scope but allows us to test it quickly.\n\nLet's create a function to consume the agent output and format it into our report. Note: we could add this into the final answer tool logic if preferred.\n\ndef build_report(output: dict): research_steps = output[\"research_steps\"] if type(research_steps) is list: research_steps = \"\\n\".join([f\"- {r}\" for r in research_steps]) sources = output[\"sources\"] if type(sources) is list: sources = \"\\n\".join([f\"- {s}\" for s in sources]) return f\"\"\" INTRODUCTION ------------ {output[\"introduction\"]} RESEARCH STEPS -------------- {research_steps} REPORT ------ {output[\"main_body\"]} CONCLUSION ---------- {output[\"conclusion\"]} SOURCES ------- {sources} \"\"\"\n\nNow we run our build_report function:Now let's try with an on-topic question regarding AI.Let's try one more, we can ask about RAG:\n\nThere we go, we're seeing our research agent use multiple tools such as web search and arxiv search, synthesize all of the information it retrieves, and finally produce a mini-research report!\n\nThat's it for this article covering LangGraph and research agents. We've looked at the essentials behind conversational agents and research agents and where they share similarities and differences—particularly on the engineering front. We've discussed the idea behind graph-based agent frameworks and the additional visibility and flexibility they provide. Finally, we built our own research agent using LangGraph.\n\nGraphs and agents are very promising concepts, and we expect pairing both to become an increasingly popular choice for engineers when building AI applications—particularly as AI becomes an increasingly critical component of reliable and trustworthy software.\n\nReferences\n\n[1] S. Yao, J. Zhao, D. Yu, N. Du, I. Shafran, K. Narasimhan, Y. Cao, ReAct: Synergizing Reasoning and Acting in Language Models (2023), ICLR",
      "# [Introducing the First Hallucination-Free LLM](https://www.pinecone.io/blog/hallucination-free-llm/)\nWhile Pinecone is most known for the vector database which helps reduce hallucinations through Retrieval Augmented Generation, we’re also investing in finding other ways to reduce hallucinations. Today, we’re excited to announce a breakthrough in our research: The first-ever LLM that never hallucinates — ever.\n\nIt’s called Luna, and we will open-source the model eventually, but for now, due to the far-reaching implications of an AI model that never hallucinates, we’re only sharing the model’s source and weights with vetted institutions.\n\nThe motivation: LLMs hallucinate without access to company data\n\nHallucinations are the predominant reason why most AI applications never reach production. While LLMs answer most questions about public information, they don’t have sufficient knowledge to answer questions that require access to private data. While this is already being addressed with RAG — using a vector database to retrieve and feed relevant context to the LLM — we wondered if there was an even easier way.\n\nOur novel approach targets the root issue causing all other LLMs to hallucinate: They don’t know the limits of their knowledge, so they often fail to admit when they don’t know the answer. And so they make something up. And therein lies the key insight: A model will never hallucinate if it always admits what it does not know.\n\nO Light Eternal, in Thyself contained!\n\nThou only know Thyself, and in Thyself\n\nBoth known and knowing, smile on Thyself!\n\nHow it works: Information-free training\n\nThe result of many months of research — conducted in a previously undisclosed satellite Pinecone office in Bowling Green, Kentucky — and many millions of dollars spent on GPUs is a 122B-parameter AI model designed to address hallucinations without access to domain-specific knowledge.\n\nThe model was developed with a novel technique we call information-free training. Just as Alpha-zero made history [1] by becoming the best chess engine in the world merely by playing itself and without knowledge of historical games, our model does the same for factual question-answering tasks. Rather than being trained on public, semi-public, accidentally public, and questionably public data, the model was trained by endlessly asking itself questions and measuring the resulting answer quality. The technique also draws on ideas from Ming-Wei et al.[3] and other work on zero-shot learning.\n\nOur scientists noticed a strong correlation between trying to answer questions factually and hallucination. We define the assumed knowledge factor (AKF) as the confidence level set by the model when it forms factual content. High levels of AKF indicate high confidence that factual sentences contain correct information. Low AKF makes the model more unsure about its answers’ factual contents. Note that AKF correlates positively with hallucinations.\n\nThe key insight with training Luna is to consider the other extreme value of AKF. That is, what happens when you set AKF to zero?\n\nAmazingly enough, slowly adjusting AKF all the way to zero while training Luna reduced hallucinations to precisely 0%. To our knowledge, this is the first LLM to achieve this feat.\n\nBased on our experiments, the equation above gives the best-performing adjustment schedule for AKF (denoted by Zeta). Here, t gives the epoch training index and the values of X are only loosely defined to have some relation to the factualness and correctness of the output. Note that conditional probability over loosely defined variables makes training much more complicated and compute-intensive. We will elaborate on these technical difficulties and consequent solutions in a future technical report.\n\nPerformance: Zero hallucinations... at a cost\n\nLuna is not (yet) the best model in the world on all fronts. Achieving zero hallucinations comes at a steep price of significantly diminished performance on other tasks. When reviewing results, we found Luna tends to answer pretty much all questions with some version of “I don’t know.” Therefore, the results are relatively poor on coding (0%) and task completion (0%), as well as usefulness (0%).\n\nWhile this might diminish the magnitude of the achievements, one must remember that Luna achieved these results without access to any information.\n\nIt is not clear whether these results can be improved.\n\nFuture Research\n\nPinecone is heavily invested in AI research as a whole and knowledgeable AI specifically. We’re deeply committed to advancing the state of the art in this field, and we’re hiring.\n\nThat said, we will probably halt further research on information-free training. If you want to reliably improve the quality, performance, and commercial viability of your AI applications, you can pair any other LLM of your choice with the Pinecone vector database.\n\nReferences\n\n[1] Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm\n\n[2] LLM-Perf Leaderboard",
      "# [401 Unauthorized exception calling the API on 2024-04-04](https://community.pinecone.io/t/401-unauthorized-exception-calling-the-api/5096)\nAll our requests to Pinecone started failing today with a 401 Unauthorized exception. The stacktrace is unhelpful:\n\nFailed to retrieve response: PineconeAuthorizationError: The API key you provided was rejected while calling https://xxx-yyy.svc.gcp-starter.pinecone.io/query. Please check your configuration values and try again. You can find the configuration values for your project in the Pinecone developer console at [https://app.pinecone.io](https://app.pinecone.io/) at mapHttpStatusError (webpack-internal:///(rsc)/./node_modules/@pinecone-database/pinecone/dist/errors/http.js:181:20) at eval (webpack-internal:///(rsc)/./node_modules/@pinecone-database/pinecone/dist/errors/handling.js:170:55) at step (webpack-internal:///(rsc)/./node_modules/@pinecone-database/pinecone/dist/errors/handling.js:107:23) at Object.eval [as next] (webpack-internal:///(rsc)/./node_modules/@pinecone-database/pinecone/dist/errors/handling.js:48:20) at fulfilled (webpack-internal:///(rsc)/./node_modules/@pinecone-database/pinecone/dist/errors/handling.js:11:32) at process.processTicksAndRejections (node:internal/process/task_queues:95:5) { cause: undefined\n\nWe repeated the query with a new API key, and also tried calling the index from Python. Both failed. The Pinecone console also fails to load the index via the ‘Browse’ tab. The describe_index_stats API call on the console fails with a 401 Unauthorized error with the following response headers:\n\nAccess-Control-Allow-Origin: [https://app.pinecone.io](https://app.pinecone.io/) Alt-Svc: h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000 Content-Length: 12 Content-Type: text/plain Date: Thu, 04 Apr 2024 20:35:17 GMT Server: envoy Via: 1.1 google Www-Authenticate: Malformed domain X-Pinecone-Auth-Rejected-Reason: Malformed domain\n\nI can understand if the server is down, but that’s not the exception message I would expect in that case. How can we fix this error?\n\nWe are on a starter index, what about you? It might be an issue impacting only starter indexes, although I don’t see incidents on the status page https://status.pinecone.io/\n\nI created a support ticket, but was basically told they couldn’t help us because we aren’t a paying customer. Well, if the status page or exception message were helpful, I could figure something out, but at the moment we’re pretty stuck!",
      "# [The Practitioner's Guide To E5](https://www.pinecone.io/learn/the-practitioners-guide-to-e5/)\nA good embedding model makes a huge difference in the performance of a search application. When picking the first model to use with Pinecone Inference API, we were looking for a model that would be a good choice for a wide variety of applications. We picked E5 because it’s small, open source, natively multilingual, and performs well on benchmarks across languages.\n\nIn this article, we’ll give you a look under the hood of the E5 model: how it was trained, how to best use it, and a working example that shows off some of the handy properties of a multilingual model.\n\nWhy are embedding models important?\n\nMost of the world’s information is locked up in unstructured data like text, images, and audio.\n\nThese data can’t readily be interpreted by software applications unless they are converted to vectors (also known as embeddings), which are numerical representations of data.\n\nEmbedding models are the engines that do this conversion.\n\nMultilingual models can do this within and across languages, acting as a sort of universal language semantic layer.\n\nWhy host a model in the first place?\n\nBuilding with vector databases involves a lot of moving parts, and one of the hardest ones is choosing a model to embed your data with.\n\nTypically, this required you to:\n\nFind a set of models, open source and third party APIs\n\nLearn how to use the models, best practices and implementation\n\nCompare their performance, and choose one\n\nDeploy it for inference\n\nEmbed your data\n\nUpsert into Pinecone\n\nBuild!\n\nNow with the Pinecone Inference API, you can get started embedding and upserting your data within the Pinecone SDK. It’s as easy as one function call:\n\nThe Pinecone Inference API takes care of hosting best-in-class models and tokenization for you as well.\n\nWhat is E5?\n\nThe E5 model series was developed by Microsoft researchers, and stands for EmbeEddings from bidirEctional Encoder rEpresentations, and is meant to be a performant embedding model for semantic search, retrieval and classification tasks [1]. The specific instantiation of the E5 model hosted on Pinecone is called multilingual-e5-large.\n\nAlthough multilingual embedding models existed prior to E5, E5 itself stands out for its unique training dataset, training processes, and performance.\n\nDataset Creation and Training Process\n\nThe researchers behind E5 pointed out that prior models were developed using two kinds of datasets:\n\nManually labeled datasets, like MS-MARCO or SNLI, which are high quality but expensive to scale upward for better performance\n\nSynthetic dataset tasks like the Inverse Cloze Task, which take advantage of text structure to generated labeled data, but are inherently lower quality [1, 2].\n\nAdditionally, most embedding models are not multilingual, and obtaining data that satisfies the above criteria demands a corresponding increase in dataset size.\n\nSo, they set out to curate a massive multilingual dataset which would allow for the best properties of both manually labeled datasets and synthetic ones [3].\n\nThis dataset was constructed by collecting naturally occurring text pairs across varying sources and languages. A text pair can be thought of as two texts that would normally exist together in the “wild”, such as titles of articles and corresponding sections, translation pairs, and questions/answer pairs.\n\nAnother important quality of these pairs is that they can be symmetric or asymmetric. You might have a pair of texts that are roughly the same length (such as translation pairs) or different lengths (such as document titles and paragraphs). The researchers decided to differentiate these as “query” texts and “passage” texts, so much so that they prefix the corresponding data when training the model, to better perform on certain retrieval tasks [3].\n\nIf we were to train embedding models for semantic search and retrieval, we’d want embeddings of these pairs to be close together in vector space. This would correspond to the closeness of their semantic meaning and relevance to one another, which are desirable properties to encode in vectors.\n\nAnd since they occur across the internet in large numbers, it’s easy to grab and identify these pairs.\n\nBy using these pairs in the training data for embedding models, we can take advantage of the inherent relevance and semantic closeness of these objects, as a signal for embedding models.\n\nAnd how was multilingualism achieved? Multilingual models like E5 leverage clever tokenization schemes that allow for all sorts of languages to be processed during training.\n\nE5’s dataset and tokenization procedures ensure that any of around 100 languages can be embedded seamlessly with the model [4].\n\nA great primer on multilingual training of embedding models is located here.\n\nThe Pinecone Inference API tokenizes input text for you, so there is no need to be concerned with implementation details for different languages; just pass the text to the endpoint.\n\nAfter suitable datasets are gathered, the next step is model training.\n\nLearning with Weakly Supervised Contrastive Pretraining\n\nRemember those natural text pairs we mentioned? Finding labels in this fashion is called “weak supervision”, as they can still be noisy, as opposed to “supervised” datasets with clear labels.\n\nThe E5 training process begins by using a pretrained XLM-Roberta model, and trains additionally on a billion multilingual text pairs. Even though the XLM-Roberta model is already multilingual, this additional pretraining in a pairwise fashion is critical for downstream tasks [5].\n\n“Contrastive pretraining” just means training so that the resulting embedding representations of two neighboring text pairs cluster together in vector space.\n\nHowever, we also need to train with mismatched text pairs too, which we aim to push far away from one another in vector space. These are also known as in-batch negatives, as they are created with the data within a batch.\n\nTogether, this is “weakly supervised contrastive pretraining”, as the model learns on a huge amount of weakly labeled data in a contrastive fashion.\n\nSpecializing with Supervised Finetuning\n\nAfter imbuing the model with the information gained from weakly supervised contrastive pretraining, the supervised finetuning step focuses the model on a curated multilingual labeled dataset spanning topics such as retrieval, semantic similarity, and question answering.\n\nThe dataset being used here is similar to what we’d expect for other manually labeled datasets, with the exception that it also spans several languages. Using human labeled data in this stage ensures the model is learning exactly what it needs to for these useful tasks. This dataset, alongside some clever techniques such as knowledge distillation and mined hard negatives really supercharges the multilingual capabilities of the model.\n\nThe net effect is that E5 provides great performance along with multilingual capability and relatively small size. Even at only 500 million parameters, E5 maintains competitive scores on the Massive Text Embedding Benchmark English leaderboard, and also on MIRACL, a benchmark dataset for multilingual retrieval [3]. This demonstrates that the model doesn’t compromise its English performance due to multilingual training.\n\nTips for building with E5\n\nQuery and Passage Embeddings\n\nUsing the Pinecone Inference API with E5 requires specifying a required input type parameter. This input type parameter allows the API to serve different embeddings for queries and passages.\n\nThis is because during the training process, the researchers behind E5 prompted queries and passages differently to help differentiate the kind of embeddings learned used in different tasks.\n\nYou can enable this prompting by passing the corresponding ‘query’ or ‘passage’ to the input type parameter in Pinecone. Generally, data upserted to Pinecone will need the “passage” input type, and queries will get the “query” input type.\n\nWhen using E5, it would be good to keep in mind the following rules of thumb:\n\nUse Query and Passage for “asymmetric” tasks that involve aligning a search or question with longer passages chunks, such as retrieval augmented generation, semantic search, or question answering\n\nUse Query prefix exclusively for tasks that are “symmetric”, such as bitext mining, paraphrase retrieval, similarity search, etc\n\nUse Query prefix for using embeddings as features, such as for classification or data labeling with Pinecone [4]\n\nSkip tokenizing, but still chunk: the Inference API handles tokenization for you.\n\nMigrate your embeddings: In general, you will need to use the same embedding model to embed your data and the vector you’re using to query. If you have existing embeddings with a different model provider, you will have to re-embed those using E5\n\nKnow E5’s model parameters: Batch size is currently limited to 96, with an input size of 1024 tokens.\n\nStick with cosine similarity: The E5 model was trained using cosine similarity based loss function, so use the “cosine” distance metric when creating your index [1].\n\nA worked out example: Language Learning\n\nWith the launch of Pinecone Inference in public preview, we've provided an example notebook to help you get started using the Inference API and the multilingual E5 model. In the notebook, we add semantic search to a multilingual language learning dataset, to enable faster learning in English and Spanish sentences.\n\nAnd there’s more you can do with E5 and the Inference API:\n\nEmbed your data for your current semantic search or RAG workflow with Pinecone\n\nEnable effortless multilingual search over your monolingual dataset\n\nExpand existing capabilities over your single language search application for global customers\n\nExplore the multilingual data you already have, without being limited by lack of translations\n\nEnjoy the ease of mind with a managed, multilingual API\n\nWe’re excited to put the power of a multilingual embedding model like E5 in your hands. It’s the first of several to come that will be made available through the Inference API. Stay tuned for more!\n\nReferences\n\n[1] L. Wang et al., “Text embeddings by weakly-supervised contrastive pre-training,” arXiv.org, https://arxiv.org/abs/2212.03533 (accessed Jun. 21, 2024).\n\n[2] K. Lee, M.-W. Chang, and K. Toutanova, “Latent retrieval for weakly supervised open domain question answering,” arXiv.org, https://arxiv.org/abs/1906.00300 (accessed Jun. 21, 2024).\n\n[3] L. Wang et al., “Multilingual E5 text embeddings: A technical report,” arXiv.org, https://arxiv.org/abs/2402.05672 (accessed Jun. 21, 2024).\n\n[4] “Multilingual-E5-large Model Card ,” Hugging Face, https://huggingface.co/intfloat/multilingual-e5-large (accessed Jun. 21, 2024).\n\n[5] A. Conneau et al., “Unsupervised cross-lingual representation learning at scale,” arXiv.org, https://arxiv.org/abs/1911.02116 (accessed Jun. 21, 2024).",
      "# [Pinecone pricing: Features and plans explained](https://www.withorb.com/blog/pinecone-pricing)\nPinecone provides a powerful vector database designed for the unique demands of AI applications. This guide explores Pinecone’s pricing structure, features, and the types of companies that can create a similar model.\n\nYou'll also learn:\n\nHow Pinecone's vector database works\n\nKey use cases for Pinecone\n\nA step-by-step guide to building your own pricing model\n\nHow Orb can help you create a flexible and effective pricing strategy\n\nLet’s get started by going a bit more in-depth about what Pinecone actually is.\n\nWhat is Pinecone?\n\nPinecone is a leading provider of vector databases, which are specialized databases designed for storing and searching vectors. These vectors are mathematical representations of data such as images, text, audio, and more.\n\nPinecone's core strength is its ability to perform fast and accurate similarity searches. It can find the most similar vectors to a given query vector, even within a massive dataset. This capability is crucial for AI apps that rely on finding related or relevant information.\n\nKey use cases for Pinecone include:\n\nRecommendation systems: Suggesting products, services, or content based on user preferences\n\nSemantic search: Finding information based on meaning, not just keywords\n\nImage and video retrieval: Searching for visually similar content\n\nAnomaly detection: Identifying unusual patterns or outliers in data\n\nPinecone differentiates itself by offering a fully managed, cloud-native solution via pinecone.io. Because it’s a cloud-native product, they eliminate the need for users to manage infrastructure. Pinecone's service is designed to be scalable and reliable.\n\nHow does Pinecone work?\n\nWe've established that Pinecone is a vector database specializing in similarity search. But how does this process actually unfold? Let's dive deeper:\n\nVector creation\n\nPinecone takes your data and transforms it into numerical representations called vectors. When we say data, we mean images, text, audio, or other formats. These vectors capture the key information of your data, allowing you to draw meaningful comparisons.\n\nUsers interact with the platform and perform these operations through the Pinecone API.\n\nIndexing\n\nPinecone uses sophisticated indexing techniques to structure vectors, allowing for rapid retrieval. It is a highly specialized filing system designed for instant access. With potentially millions of vectors to manage, efficient organization is key.\n\nSimilarity search\n\nSimilarity search is at the heart of Pinecone’s feature offering. When you present a new data point (in vector form) to Pinecone, it quickly identifies the most similar vectors within its database. This process is executed through the Pinecone API, which enables efficient querying and retrieval of similar vectors.\n\nSimilarity search is based on calculating distances between vectors in a multi-dimensional space. Vectors closer together are deemed more alike.\n\nPinecone delivers a powerful platform for managing and querying vector data. Its speed, scalability, and metadata filtering capabilities make it an indispensable tool for building high-performance AI applications.\n\nWhat are Pinecone's pricing models?\n\nPinecone offers a variety of pricing models to cater to different needs and use cases, from experimentation to large-scale production deployments. Let's take a look at the options:\n\nProduct pricing\n\nPinecone's product pricing revolves around three main plans:\n\nStarter: This plan is ideal for those just starting out with Pinecone. It's free and provides resources for exploring the platform and building small applications.\n\n‍\n\nStandard: The Standard plan is designed for production applications of any scale. It follows a pay-as-you-go model, meaning you only pay for the resources you consume. This plan offers unlimited serverless, inference, and assistant usage.\n\nYou can find the detailed pricing on their pricing page, but to give you an idea, serverless index storage starts at $0.33/GB/month.\n\nEnterprise: This plan provides a custom pricing model tailored to your specific needs. It includes features like single sign-on and enterprise-grade support. To get specific pricing for your needs, you'll need to contact their sales team.\n\nSupport pricing\n\nBesides product pricing, Pinecone offers support tiers to provide varying levels of assistance. These are the tiers:\n\nFree: This tier provides access to Pinecone's community forum, AI support bot, and documentation for free.\n\n‍\n\nDeveloper: Ideal for developers who need more personalized guidance. This tier offers email support with a 1-business-day response time, access to their help desk, and all features from the Free tier. Pricing for this tier is $29 per month.\n\n‍\n\nPro: The Pro tier provides email support, 24/7 on-call availability, and features from previous tiers. It's priced at $499 per month.\n\n‍\n\nEnterprise: This tier offers the fastest response times, a dedicated Slack channel, and direct support. Since it's included with the Enterprise plan, support pricing is bundled into the custom pricing.\n\nWhat types of companies is the Pinecone pricing model best for?\n\nThe Pinecone pricing structure, with its tiered plans and pay-as-you-go options, caters to a wide range of companies. Let's explore which types of businesses can particularly benefit from this approach:\n\nStartups experimenting with vector search: The free Starter plan allows them to explore Pinecone's capabilities without any upfront investment. The pay-as-you-go structure of the Standard plan allows them to scale their usage in line with their needs.\n\n‍\n\nRapidly growing businesses: The Standard plan's pay-as-you-go model ensures that they only pay for what they use. Companies avoid large upfront costs and can adapt to fast growth or seasonal fluctuations.\n\n‍\n\nCompanies with varying support needs: Pinecone's tiered support plans cater to businesses with various levels of expertise. Startups might find the free tier sufficient, while growing teams might opt for Developer or Pro support for faster response times.\n\nHow to build a pricing model like Pinecone’s\n\nInspired by Pinecone's approach? We’ll now share a breakdown of how you can create a similarly effective pricing structure for your product or service. You can follow these steps:\n\nStep 1: Understand your costs and value proposition\n\nStart by identifying your key cost drivers. What resources are essential to delivering your product or service? Is it computing power, storage, customer support, or something else?\n\nOnce you have a handle on your costs, define your value proposition. What unique benefits do you offer your customers? How does your offering compare to competitors? Understanding both your costs and value is crucial for setting appropriate prices.\n\nStep 2: Design your pricing tiers\n\nOffer a free tier for experimentation and initial adoption. Potential customers can experience your product's value without any financial commitment. You should then introduce a pay-as-you-go tier for scaling businesses.\n\nConsider a premium tier for enterprise customers. This tier can offer custom pricing, high-availability features, and dedicated support. The goal? To meet the demands of mission-critical applications.\n\nStep 3: Choose your pricing metrics\n\nSelect metrics that accurately reflect resource consumption and value delivered. For a database service, metrics might be linked to storage capacity, query volume, or data transfer.\n\nYou should also guarantee transparency in your pricing model. Communicate how usage translates to costs so customers can understand and predict expenses.\n\nStep 4: Offer flexible support options\n\nProvide a range of support tiers to cater to different needs. Think community support, email support with varying response times, and dedicated account management for enterprise customers.\n\nDon’t forget to price your support tiers appropriately. Consider factors like response times and the level of personalized attention offered.\n\nStep 5: Monitor and adjust continuously\n\nTrack key metrics like customer acquisition cost, lifetime value, and churn rate. This data will help you evaluate the effectiveness of your pricing model and identify areas for improvement.\n\nBe prepared to adjust your pricing as your business evolves. Market conditions, customer feedback, and product updates may require changes to your pricing over time.\n\nHow Orb can support your pricing engine\n\nPinecone pricing models show a deep understanding of the needs of AI-powered businesses. What if you could adopt these principles and build a pricing strategy as powerful and flexible as Pinecone's?\n\nWith Orb, you can.\n\nOrb is a done-for-you billing platform that allows companies to design and manage sophisticated pricing models, just like Pinecone and other leading technology companies. We handle the complexities of usage-based pricing and any pricing engine you need.\n\nHere's how Orb can help you create a Pinecone-like pricing model:\n\nUsage tracking: Orb tracks every billable event, providing the foundation for accurate invoicing. This feature captures the nuances of usage-based pricing for vector databases like Pinecone.\n\n‍\n\nTiered plans: Orb enables you to create and manage many pricing tiers. This feature allows companies to mirror Pinecone's approach and cater to many customer segments.\n\n‍\n\nData-driven insights: Orb transforms usage data into actionable insights. We give you a clear understanding of customer behavior to refine your pricing strategy.\n\n‍\n\nEffortless integrations: Orb integrates with your existing data warehouses and accounting software. We help streamline your financial operations for a smooth billing process.\n\n‍\n\nCustomizable billing: Orb's custom SQL editor gives you the freedom to define your own usage metrics and pricing rules. We aid you in creating a truly bespoke model that's ideal for your niche.",
      "# [PineconeNotFoundError: A call to https://api.pinecone.io/indexes/index1 returned HTTP status 404. · Issue #2144 · FlowiseAI/Flowise](https://github.com/FlowiseAI/Flowise/issues/2144)\nI am using 1.6.3 build and since morning I am not able to connect to my flow. I keep getting this error message when I try to query to my chatbot or upsert.\n\nPineconeNotFoundError: A call to\n\nhttps://api.pinecone.io/indexes/index1\n\nreturned HTTP status 404.\n\nI am not using serverless index. Can someone please help.",
      "# [Pinecone Vector Store node documentation](https://docs.n8n.io/integrations/builtin/cluster-nodes/root-nodes/n8n-nodes-langchain.vectorstorepinecone/)\nPinecone Vector Store node#\n\nUse the Pinecone node to interact with your Pinecone database as vector store. You can insert documents into a vector database, get documents from a vector database, retrieve documents to provide them to a retriever connected to a chain, or connect directly to an agent as a tool.\n\nOn this page, you'll find the node parameters for the Pinecone node, and links to more resources.\n\nNode usage patterns#\n\nYou can use the Pinecone Vector Store node in the following patterns.\n\nYou can use the Pinecone Vector Store as a regular node to insert, update, or get documents. This pattern places the Pinecone Vector Store in the regular connection flow without using an agent.\n\nYou can see an example of this in scenario 1 of this template.\n\nConnect directly to an AI agent as a tool#\n\nYou can connect the Pinecone Vector Store node directly to the tool connector of an AI agent to use vector store as a resource when answering queries.\n\nHere, the connection would be: AI agent (tools connector) -> Pinecone Vector Store node.\n\nUse a retriever to fetch documents#\n\nYou can use the Vector Store Retriever node with the Pinecone Vector Store node to fetch documents from the Pinecone Vector Store node. This is often used with the Question and Answer Chain node to fetch documents from the vector store that match the given chat input.\n\nAn example of the connection flow would be: Question and Answer Chain (Retriever connector) -> Vector Store Retriever (Vector Store connector) -> Pinecone Vector Store.\n\nUse the Vector Store Question Answer Tool to answer questions#\n\nAnother pattern uses the Vector Store Question Answer Tool to summarize results and answer questions from the Pinecone Vector Store node. Rather than connecting the Pinecone Vector Store directly as a tool, this pattern uses a tool specifically designed to summarizes data in the vector store.\n\nThe connections flow in this case would look like this: AI agent (tools connector) -> Vector Store Question Answer Tool (Vector Store connector) -> Pinecone Vector store.\n\nNode parameters#\n\nOperation Mode#\n\nThis Vector Store node has five modes: Get Many, Insert Documents, Retrieve Documents (As Vector Store for Chain/Tool), Retrieve Documents (As Tool for AI Agent), and Update Documents. The mode you select determines the operations you can perform with the node and what inputs and outputs are available.\n\nGet Many#\n\nIn this mode, you can retrieve multiple documents from your vector database by providing a prompt. The prompt will be embedded and used for similarity search. The node will return the documents that are most similar to the prompt with their similarity score. This is useful if you want to retrieve a list of similar documents and pass them to an agent as additional context.\n\nInsert Documents#\n\nUse Insert Documents mode to insert new documents into your vector database.\n\nRetrieve Documents (As Vector Store for Chain/Tool)#\n\nUse Retrieve Documents (As Vector Store for Chain/Tool) mode with a vector-store retriever to retrieve documents from a vector database and provide them to the retriever connected to a chain. In this mode you must connect the node to a retriever node or root node.\n\nRetrieve Documents (As Tool for AI Agent)#\n\nUse Retrieve Documents (As Tool for AI Agent) mode to use the vector store as a tool resource when answering queries. When formulating responses, the agent uses the vector store when the vector store name and description match the question details.\n\nUse Update Documents mode to update documents in a vector database by ID. Fill in the ID with the ID of the embedding entry to update.\n\nGet Many parameters#\n\nPinecone Index: Select or enter the Pinecone Index to use.\n\nPrompt: Enter your search query.\n\nLimit: Enter how many results to retrieve from the vector store. For example, set this to 10 to get the ten best results.\n\nInsert Documents parameters#\n\nPinecone Index: Select or enter the Pinecone Index to use.\n\nRetrieve Documents (As Vector Store for Chain/Tool) parameters#\n\nPinecone Index: Select or enter the Pinecone Index to use.\n\nRetrieve Documents (As Tool for AI Agent) parameters#\n\nName: The name of the vector store.\n\nDescription: Explain to the LLM what this tool does. A good, specific description allows LLMs to produce expected results more often.\n\nPinecone Index: Select or enter the Pinecone Index to use.\n\nLimit: Enter how many results to retrieve from the vector store. For example, set this to 10 to get the ten best results.\n\nNode options#\n\nPinecone Namespace#\n\nAnother segregation option for how to store your data within the index.\n\nMetadata Filter#\n\nAvailable in Get Many mode. When searching for data, use this to match with metadata associated with the document.\n\nThis is an AND query. If you specify more than one metadata filter field, all of them must match.\n\nWhen inserting data, the metadata is set using the document loader. Refer to Default Data Loader for more information on loading documents.\n\nClear Namespace#\n\nAvailable in Insert Documents mode. Deletes all data from the namespace before inserting the new data.\n\nTemplates and examples#\n\nRelated resources#\n\nRefer to LangChain's Pinecone documentation for more information about the service.\n\nView n8n's Advanced AI documentation.\n\nFind your Pinecone index and namespace#\n\nYour Pinecone index and namespace are available in your Pinecone account.\n\nAI glossary#\n\ncompletion: Completions are the responses generated by a model like GPT.\n\nhallucinations: Hallucination in AI is when an LLM (large language model) mistakenly perceives patterns or objects that don't exist.\n\nvector database: A vector database stores mathematical representations of information. Use with embeddings and retrievers to create a database that your AI can access when answering questions.\n\nvector store: A vector store, or vector database, stores mathematical representations of information. Use with embeddings and retrievers to create a database that your AI can access when answering questions.",
      "# [Add RAG to Your Flink AI Flow Using Federated Search with Pinecone](https://www.confluent.io/blog/flink-ai-rag-with-federated-search/)\nWith the advent of modern Large Language Models (LLMs), Retrieval Augmented Generation (RAG) has become a de-facto technology choice, employed to extract insights from a variety of data sources using natural language queries. RAG combined with LLMs presents many new possibilities for integrating Generative AI capabilities within existing business applications, specifically opening up many new use cases within the data streaming and analytics space.\n\nChatbots for business applications, such as those related to customer relationship management (CRM) or health care, have widely adopted RAG techniques to create a question/answer interface for consumers. While LLMs provide the Q&A interface and the capability of understanding and rerouting user queries sent in natural language, RAG provides the capability for chatbots to query grounded knowledgebases—by responding with similarity searches on vector embedding databases.\n\nThe diagram below illustrates a simplified workflow for how such a Q&A interface works with RAG and LLMs. A typical workflow normally consists of the following steps:\n\nText from documents is extracted\n\nText is split into chunks\n\nEmbeddings are generated for the chunks\n\nEmbeddings are stored in a vector database\n\nAn end-user question is encoded as a vector by an embedding model\n\nThe question vector is sent as a query to the vector database\n\nThe vector database returns the nearest neighbor vectors as a similarity search response to the query, as chunks\n\nThe chunks are re-ranked to find the top answer\n\nThe response is sent to an LLM, along with the question, so the LLM has more context to answer the question fully\n\nThe LLM’s answer is returned to the user\n\nRAG with streaming data\n\nRAG works very well on grounded, context-specific, knowledge sources. Within an enterprise, such sources could be business specific files, databases, policy documents, etc. Answers derived by LLMs from the result of similarity searches on vector embedding databases provide true information, and do not hallucinate—unlike information retrieved by LLMs from online sources.\n\nGoing back to the example of a chatbot mentioned above, the knowledgebase might be constructed using policy, FAQ (Frequently Asked Questions), or standard-operating procedure (SOP) documents. As explained earlier, a vector embedding database is the most suited for storing this knowledge, in the form of vector embeddings. This knowledgebase is then used to respond to natural language queries, which returns semantically similar chunks of text as a result. The LLM then contextualizes the result and provides the end user with an answer that solves the business query.\n\nIn our example, creating the knowledgebase and setting up the vector database have been done offline. The natural language query search accessing the knowledgebase to get a semantically similar answer happens in real-time, however. For a chatbot, this means passing the question as a data stream to a Kafka topic, with the vector search performed by Flink SQL on Confluent Cloud. The scenario is shown in the diagram below:\n\nFlink SQL for GenAI use cases\n\nI have written about the capability of invoking remote LLM endpoints from Flink SQL on Confluent Cloud in a previous blog. Please read it to understand how the ML_PREDICT() function of Flink SQL works within Confluent Cloud.\n\nTo summarize, ML_PREDICT() opens up new possibilities for streaming data enrichment using Flink SQL on Confluent Cloud, making it possible to invoke a plethora of LLM endpoints (OpenAI, Azure OpenAI, Amazon Bedrock, Google Vertex AI, etc.). You can execute common tasks such as “classification,” “regression,” “text generation,” “embedding,” “clustering,” etc.—in real time, all from within a Flink SQL job.\n\nRead more about how an AI model can be built using the CREATE MODEL Statement in Confluent Cloud for Apache Flink.\n\nIntroducing FEDERATED_SEARCH() with Flink SQL\n\nWhile ML_PREDICT() enables invoking LLM endpoints, Confluent Cloud for Apache Flink® now supports read-only external tables, enabling vector search with federated query execution on external vector databases.\n\nThe federated query capability is implemented using the FEDERATED_SEARCH() function.\n\nFEDERATED_SEARCH() will soon be in early access (EA) mode, allowing interested engineering teams to test out the functionality on a case-by-case basis, and to provide feedback.\n\nFEDERATED_SEARCH() enables searching through the index of the following vector databases:\n\nElasticsearch\n\nPinecone\n\nMongoDB Atlas\n\nDepending on the kind of embedding model used, all three vector databases possess the capability to store vector embeddings, and provide sophisticated similarity search algorithms like cosine similarity, dot product, and ANN (approximate nearest neighbor) using HNSW (hierarchical navigable small world).\n\nFor the RAG use case, results from an invocation of FEDERATED_SEARCH() fetch similar documents from the vector database, in real time, to complete a search query. Thus, to make Flink FEDERATED_SEARCH() work, a properly hydrated vector database is a prerequisite.\n\nJust like the ML_PREDICT() function, FEDERATED_SEARCH() can be written as part of a Flink SQL SELECT statement.\n\nCombine ML_PREDICT() with FEDERATED_SEARCH()\n\nLet’s put this to use by extending the chatbot use case. We’ll take as an example an Airlines FAQchatbot, which is used with a Q&A interface to take end user queries, and to provide answers to these queries using the airline’s rules and policies documents.\n\nThe preparatory step is to build a vector database, and populate it with embeddings created from the documents, using a modern embedding model. For our example, the knowledgebase is created from the passenger travel FAQ PDF from the Alliance Airlines website.\n\nWe’ll use the Pinecone vector database, as well as OpenAI’s “text-embedding-ada-002” embedding model, to convert the text from the FAQ PDF to vectors. An active OpenAI subscription, an OpenAI API key, and an active Pinecone subscription with an active API key are required.\n\nStep 1:\n\nCreate a Pinecone vector database index by logging into https://app.pinecone.io.\n\nThe index is named “passenger-faq-db,” and ”text-embedding-ada-002” is selected as the embedding model, which automatically populates the “Dimensions” field with “1536,” and the “Metric” as “cosine.”\n\nThis step ensures that the Pinecone vector database index will store vectors with 1536 dimensions, so the query vector will also need to be of dimension 1536.\n\nFor this example, AWS is chosen as the cloud provider with “us-east-1” as the region. It is essential that the Flink AI model is chosen in the same region as the cloud provider.\n\nThe vector database gets created with the configured dimension and the metric for comparing vectors (which is “cosine,” in this case).\n\nNote the “HOST” property, which will be used to create an environment variable “PINECONE_HOST” in a later part of this article.\n\nTo create embeddings for the FAQ docs, first a config.py file is created. This Python script creates the required environment variables and instantiates the Pinecone index.\n\nThe next Python code snippet creates vector embeddings for the FAQ PDF. This process runs offline and populates the Pinecone index. For converting the end user query to a vector embedding, Flink AI’s ML_PREDICT() function is used in Confluent Cloud.\n\nTo generate vector embeddings in real time, the Create Embeddings Action, the newest member of Confluent Cloud for Apache Flink® Actions suite can be used. Read more about the Create Embeddings Action here.\n\nOnce the above script has been run successfully, the Pinecone index is populated with vector embeddings created from the FAQ document, and is ready for a user query.\n\nStep 2:\n\nWith the vector database fully populated, FEDERATED_SEARCH() can be combined with ML_PREDICT() to orchestrate the RAG workflow. The following diagram shows how these invocations are made in a real world environment:\n\nThe explanation of the workflow is as follows:\n\nUser types question into the chatbot\n\nThe chatbot application produces an event to add the question in a Kafka topic on Confluent Cloud\n\nA Flink dynamic table is created automatically, with the user question as one of the rows\n\nFlink on Confluent Cloud invokes ML_PREDICT() with a task type “embedding” and uses the “text-embedding-ada-002” embedding model from OpenAI to convert the user question to a vector embedding representation\n\nFlink invokes FEDERATED_SEARCH() on the Pinecone vector database created in the earlier section and Pinecone returns a “similarity search” response\n\nThe top ranked response is returned to the user, in this example. For a chatbot application in production, the response might be sent to an LLM first, to generate a contextualized response.\n\nLet’s understand how these steps are executed using Flink on Confluent Cloud.\n\nFirst, two ”connection resources” are created, for OpenAI and Pinecone respectively. A connection resource enables users to connect to model providers in a way that protects a user’s secrets, so Flink statements can make calls to these services securely.\n\nConnection resources are an Early Access Program feature in Confluent Cloud. This feature should be used only for evaluation and non-production testing purposes, or to provide feedback to Confluent, particularly as it becomes more widely available in follow-on preview editions. To participate in this Early Access Program, a Confluent account manager should be contacted.\n\nThe connection resource creation is done through Confluent CLI, after ensuring that the CLI version is the latest one.\n\nCreate a connection resource for the OpenAI embedding model as follows:\n\nCreate a connection resource for the Pinecone vector database as follows:\n\nNotice how the endpoint is the “HOST” property copied from the Pinecone web console.\n\nOnce the connection resources are created, an AI Model needs to be created with Confluent Cloud for Apache Flink. This Flink AI model is used to invoke ML_PREDICT(). Also, as explained earlier, this model is used to convert the user question written in natural language (English, in this case), to a vector embedding.\n\nTo create the Flink AI model, a new environment is created on Confluent Cloud and the SQL Workspace is opened, after creating a Flink compute pool.\n\nThe following Flink SQL statements are all run within the Confluent Cloud for Flink SQL Workspace:\n\nThe “WITH” options have been populated with the “openai.model_version” parameter, which in this case is a language embedding model, and the \"openai.connection\" parameter is populated with the newly created “openai-con” connection.\n\nA quick “show models” demonstrates that the model has been created correctly.\n\nNow that these resources have been created, let’s simulate an end user query, by inserting a question into a Flink table. In real world use cases, a Kafka topic would be populated with the question coming from web applications, mobile apps, or CRM system APIs.\n\nLet’s create a Flink dynamic table for inserting the user question, and another Flink dynamic table to convert the question to its vector embedding in real time. This text-to-embedding conversion is done using Flink’s ML_PREDICT() function with the “userqembed” Flink model.\n\nLet’s insert an end user question:\n\nNext, let’s run a Flink continuous query to ensure that the end user question is converted into its vector embedding form, by invoking ML_PREDICT() on the OpenAI embedding model that we just created:\n\nNext, a Pinecone external table is created.\n\nThis table is used to run the query on the already created Pinecone vector database.\n\nFinally, vector search is invoked using FEDERATED_SEARCH():\n\nThe result is the response to the semantic search for the query “Can I bring my pet.”\n\nNotice how a “Top 3” ranked response parameter is inserted into the FEDERATED_SEARCH() method signature, using the number 3. This returns the top three responses from the similarity search on Pinecone.\n\nThe query “Can I bring my pet” gets a response related to “Guide/Service dogs…” from the Pinecone vector database.\n\nIn order to organize the response in a better way, let’s use a Flink CTAS (CREATE TABLE AS SELECT) statement, to create a results table:\n\nThe Flink dynamic table “pinecone_result” now stores the top three responses.\n\nLet’s invoke another Flink SQL query to flatten the result and read the data:\n\nHere’s what the response looks like:\n\nFEDERATED_SEARCH() responds with the perfect top three “pet-related” answers:\n\nThis example illustrates the possibilities you can use to design and build RAG-aware, real-time workloads—using a combination of Kafka and Flink AI features on Confluent Cloud.\n\nHow to experiment on Confluent Cloud\n\nThis article introduces the powerful FEDERATED_SEARCH() feature for Flink on Confluent Cloud. The feature is still in its early days, but if you want to run an experiment with it for real-time streaming use cases, get in touch with your Confluent Sales Representative, and ask to be white listed.\n\nNext steps\n\nIf you already have a data streaming workload on Confluent Cloud using Flink, ML_PREDICT() in conjunction with FEDERATED_SEARCH() will enable you to run RAG use cases for streaming workloads, with quick access to unstructured data. For Flink on Confluent Cloud, these features pave the way for building complex agentic workflows for enterprise use cases. A future article will build such an agentic workflow with Confluent Cloud for Flink. Stay tuned!\n\n‎",
      "# [2025 Company Profile, Funding & Competitors by Tracxn on 2021-08-17](https://tracxn.com/d/companies/pinecone/__0UwanTwdq5VEQ_i9ABozYwVOolyHuunHPhQ2BgGYPG4)\n",
      "# [Sell or Invest in Pinecone Stock Pre-IPO on 2024-10-30](https://www.nasdaqprivatemarket.com/company/pinecone/)\nPinecone stock does not trade on public stock exchanges. Pinecone stock is considered a private security and you need to be an accredited investor to trade shares. To invest in Pinecone stock you can either buy shares directly through the company or work with a secondary trading marketplace like Nasdaq Private Market.\n\nLearn more about the secondary market for private company shares on the investors page.\n\nPinecone Stock does not trade on public stock exchanges. In order to sell Pinecone shares you need to work directly with the company or with a secondary trading marketplace like Nasdaq Private Market. NPM has more than a decade of experience helping sell shares on the private market to our network of institutional investors.\n\nLearn more about NPM services for selling privately held stock on the Employee Shareholders page.\n\nRead more about how to sell private company stock on the private secondary market:\n\nArticles section of NPM.com\n\nSelling Your Private Stock: Understanding the Essentials\n\nHow Employees Sell Private Company Shares in the Secondary Market\n\nThere is no ticker symbol for Pinecone stock because it is a private company. Private companies do not have ticker symbols like public companies, they are identified primarily by using the company ‘street’ name or legal name.\n\nNasdaq Private Market currently works with employees, ex-employees, and other company investors. In order to sell Pinecone stock you need to receive the company’s approval, which Nasdaq Private Market manages on your behalf.\n\nLearn more about selling privately held stock on the Employee Shareholders page.\n\nRead more about how to sell shares of a private company and eligibility for selling privately held stock:\n\nAre You Eligible to Sell Private Company Stock?\n\nCurrently, Nasdaq Private Market only works with accredited entities and institutional investors to purchase private company stock. In order to invest in Pinecone stock, you need to be an accredited entity or institutional investor.\n\nLearn more about the Nasdaq Private Market tool kit for investors on the Investor Trading page.\n\nOnce you buy or sell, stock share ownership must transfer before funds are wired. With Nasdaq Private Market, clients leverage our patent-pending Transfer and Settlement technology which streamlines and manages transfer activity from match to settlement.\n\nTo learn more about private company stock Transfer and Settlement, check out the Transfer and Settlement page.\n\nPinecone is a private company and has not had an IPO. There is currently no Pinecone IPO price. Get started to begin exploring the Nasdaq Private Market Tape D™ database and gain access to actionable data related to Pinecone IPO.\n\nLearn more about Tape D™ private company stock data for institutional investors on the on the Tape D™ Product Page.",
      "# [ChatGPT not referencing embeddings from Vector Database on 2025-02-22](https://community.n8n.io/t/chatgpt-not-referencing-embeddings-from-vector-database/80569)\nDescribe the problem/error/question\n\nI have a separate workflow which uses ChatGPT to summarise news articles, then pushes the summary plus the article metadata to Pinecone to store as embeddings:\n\nAn example of the content pushed to Pinecone:\n\nI created a second workflow which would run at 6am and create a summary of the key points from the last 24 hours’ content in the Vector database:\n\nWhat is the error message (if any)?\n\nThis workflow will frequently respond with something like \"It seems I’m unable to retrieve the latest cybersecurity news summaries at this time. However, you can stay informed about recent incidents and trends by checking reputable cybersecurity news sources like Krebs on Security, Threatpost, or the Cybersecurity & Infrastructure Security Agency (CISA).\n\nIf you have specific incidents or topics in mind, I can assist you with details or provide guidance based on existing knowledge. Let me know how you would like to proceed!\"\n\nMy understanding of the workflow I’ve set up is that it should just be referencing the data sent to Pinecone, but it seems to think it needs to reference outside sources and isn’t using the Vector Store I’ve connected it to.\n\nLooking at the Vector Store Tool2 node, the input (what I presume is what the AI Agent node is providing to the vector store as a query) is very basic (e.g. “Provide a summary of the latest cybersecurity news and incidents over the past 24 hours.”) and the response from the connected Model in the 3 runs it attempted is “I don’t know.”\n\nClicking through to the connected OpenAI Chat Model1 node, it’s clear the content from the Vector Store is being provided along with the question, but the Model still only replies “I don’t know.”\n\nPlease share your workflow\n\nSee above\n\nShare the output returned by the last node\n\nIt seems I’m unable to retrieve the latest cybersecurity news summaries at this time. However, you can stay informed about recent incidents and trends by checking reputable cybersecurity news sources like Krebs on Security, Threatpost, or the Cybersecurity & Infrastructure Security Agency (CISA).\n\nIf you have specific incidents or topics in mind, I can assist you with details or provide guidance based on existing knowledge. Let me know how you would like to proceed!\n\nI expected that workflow should reference the embeddings in the Vector Database and create a summary using ChatGPT of the contents, but it seems to be interpreting the prompt as a request to resolve external resources, despite being provided the content by the Vector Store.\n\nInformation on your n8n setup\n\nn8n version: 1.77.3\n\nDatabase (default: SQLite): Default\n\nn8n EXECUTIONS_PROCESS setting (default: own, main): Default\n\nRunning n8n via (Docker, npm, n8n cloud, desktop app): Docker\n\nOperating system: Windows 11 Pro\n\nPlease note I’m completely new to AI, embeddings, and automation with n8n, I’m figuring it out as I go along.\n\nI’m assuming embeddings are the way to go here, but given my use case, I feel like I might’ve been better off with a traditional relational database, and just storing the ChatGPT summaries as strings?\n\nThe reason for my thinking is that - as you would’ve seen from the content being pushed to Pinecone - it includes the original article metadata like the title, url, date published, etc. The intent of that is that I could ask ChatGPT to provide the summary as well as links to the original articles which were referenced.\n\nLooking at the Pinecone DB, even though those fields like the publish date and url do make it in - there’s no relationship between those objects, or to the original summary.\n\nThat being the case - is it even possible for the AI Agent to even know what the summaries from the last 24 hours were, let alone, what the original title and URL for the article are?\n\nAs I mentioned - not even sure I’m on the right path here, any guidance on how to best address my original use case of creating a daily summary of the key topics in the article summaries would be greatly appreciated!",
      "# [Understanding Embeddings with Pinecone by juliuscecilia33, medium.com on 2024-11-12](https://medium.com/@juliuscecilia33/understanding-embeddings-with-pinecone-9f765b490387)\nWhat Exactly Are Embeddings?\n\nAt their core, embeddings are numerical representations of data that capture the “meaning” or “context” of the data in a way that makes it easy for computers to understand relationships and similarities. Imagine you’re working with words, sentences, or even images. Embeddings allow you to map out these items in a high-dimensional space, like a cloud of points, where the closer points are to each other, the more similar they are in meaning or content.\n\nLet’s say I have words like “king” and “queen.” Even though these words are distinct, they share a close conceptual relationship. On the other hand, “king” and “apple” are wildly different. Embeddings represent this difference by placing “king” and “queen” close together in a multidimensional space, while “king” and “apple” end up far apart. It’s like a smart map that places related items nearby, which is perfect for all kinds of applications in search, recommendations, and more.\n\nHow Do Embeddings Work?\n\nHere’s how embeddings turn complex data into numbers with meaningful relationships:\n\nRepresentation in Space: Embeddings convert each word, sentence, or image into a set of numbers. Imagine these numbers as coordinates of a point floating in high-dimensional space (which can be hundreds or thousands of dimensions, not just 2D or 3D).\n\nCapturing Relationships: Similar items cluster together, while unrelated ones are spaced apart. In practice, this means we can easily measure distances in the embedding space to compare how similar or different things are. This setup is like a built-in similarity detector in AI applications, making it possible for computers to “understand” that “king” is closer to “queen” than it is to “apple.”\n\nPractical Examples of Embeddings\n\nLet’s break down some real-world scenarios where embeddings make life a lot easier (and data a lot smarter):\n\n1. Word Embeddings\n\nSuppose we have words like “king,” “queen,” “man,” and “woman.” A good word embedding model (like Word2Vec or GloVe) would map these words into a space where:\n\n“king” sits near “queen”\n\n“man” is close to “woman”\n\nThe relationship between “king” and “man” mirrors the relationship between “queen” and “woman”\n\nThis isn’t just hypothetical. A model might actually assign numerical vectors to these words, like:\n\nking → [0.7, 1.2, -0.4, 0.9]\n\nqueen → [0.7, 1.2, -0.3, 0.8]\n\nman → [0.5, 0.9, -0.2, 0.7]\n\nwoman → [0.5, 0.9, -0.1, 0.6]\n\nFrom these vectors, you can see that “king” and “queen” are close to each other in the embedding space, showing they’re similar concepts, while “king” and “apple” would end up much farther apart.\n\n2. Sentence Embeddings\n\nBeyond single words, we can use embeddings for sentences. Think about two sentences like:\n\n“The cat is sleeping on the mat.”\n\n“A cat is napping on a mat.”\n\nThough they use different words, these sentences convey the same meaning, and their embeddings would be close in vector space. On the other hand, a sentence like “The dog is running in the park” would produce an embedding that’s farther away, indicating a different idea.\n\n3. Image Embeddings",
      "# [How to Get More from Your Pinecone Vector Database by Chris Latimer on 2024-04-12](https://vectorize.io/how-to-get-more-from-your-pinecone-vector-database/)\nWhy are vector databases so popular right now?\n\nJust a few years ago, vector databases were a niche technology that powered traditional machine learning use cases like recommendation systems and fraud detection algorithms. Today, the explosion in interest around generative AI has elevated vector databases to new heights. Among the most popular options for most developers and companies is Pinecone vector database.\n\nPinecone is a cloud native vector database. Interestingly, it is only available as a cloud service compared to other products which are often times available as self-hosted open source solutions.\n\nPinecone was one of the first vector database vendors to recognize how transformative this technology would be for generative AI use cases. In particular, techniques such as retrieval augmented generation (RAG) are driving a lot of Pinecone adoption by developers who want a highly scalable semantic search solution without the hassle of operating an open source alternative.\n\nHow do most people use Pinecone?\n\nFor a comprehensive explanation of vector databases, head over to our Ultimate Vector Database Guide. In a nutshell, Pinecone is a specialized type of search engine that is purpose build to provide fast, efficient querying of potentially vast amounts of vector data.\n\nVectors in this context are typically generated using a text embedding model like Open AI’s text-embedding-v3 or ada-002 models. Text embedding models are a type of machine learning model that is intended to accept some text as its input and produce a vector.\n\nA vector is just an array of floating point numbers, but those numbers encode the semantic meaning of the input text. This means that when you query Pinecone, you’ll create a text embedding for some string, and Pinecone will hand back a set of the most similar vectors. This will also include the text that was used to create those vectors. In this way you can find the most similar content that Pinecone has in its search indexes.\n\nThis is a powerful technique that lets you connect your private data with your LLM to unlock new use cases.\n\nSo while ChatGPT doesn’t have the customer service handbook that your customer support team uses, Pinecone lets you find the relevant parts of that handbook when a customer asks a question. And combining those relevant results with your LLM integration, you can now improve the performance and accuracy of your gen AI applications.\n\nGetting started with Pinecone and vector data\n\nThere are two ways to get started with Pinecone: the easy way and the hard way.\n\nPinecone the hard way\n\nThis approach involves writing code to populate your vector database with high dimensional vector data. You will typically accomplish this by identifying all the data you want to include in your vector indexes. You will identify the collections of documents from file systems, knowledge bases, traditional databases, and other SaaS tools.\n\nYou’ll usually use either python or javascript along with natural language processing libraries to extract the text from your source data. You’ll then break that text data into smaller pieces, called chunks.\n\nFor each of those chunks, you’ll either leverage an open source embedding model from the MTEB leaderboard, or you’ll use a commercial offering such as OpenAI or Voyage AI to generate text embedding vectors. Each vector corresponds to a chunk from your source data.\n\nYou’ll then use the user friendly api that is provided by Pinecone, often via the python or javascript library, to write that data into your Pinecone database index.\n\nAt that point, you are ready to start searching for similar vectors using the semantic search algorithms provided by Pinecone.\n\nNow comes the hard part. You need to verify that the data stored in your indexes are going to deliver high performance, high accuracy results that are going to work well for your use cases in a real world environment.\n\nThis process involves creating a large number of example records, performing similarity search queries to retrieve your vector search results. You then must perform an assessment of how relevant those results are, and whether or not those values, along with any metadata, provide the necessary context to satisfy your users and their requirements.\n\nPinecone the easy way\n\nThe easy way to get started with Pinecone is to use Vectorize. Vectorize provides a simple step by step formula to ensure you end up with an optimized search index that is custom tailored for your data and your users.\n\nStep 1: Start with a data-driven approach\n\nThere are many different ways to chunk your documents and other source data that you want to turn into vectors. There are also many embedding models you can use to turn those chunks into vectors. A common mistake developers make is to not put enough thought into which approach will work best for their data and their use case.\n\nA better way is to leverage the free experiments feature in Vectorize. With experiments, you can take a representative sample of your data and immediately see how various vectorization strategies will perform for you.\n\nYou don’t even need a Pinecone instance setup to try this out, Vectorize will provide the vector database engine for you so you don’t need to clutter up your real database with experiment data.\n\nYou start by providing a description for your experiment and uploading a representative sample of your data in the form of PDFs, HTML docs, text files, or other document formats:\n\nYou can then start the experiment. Vectorize will tell you immediately the topics and categories of questions your data would be best at answering, then it will tell you which vectorization strategy is best at providing relevant context to help answer those questions.\n\nNow instead of relying on gut feel and optimism, you can make a decision on how to vectorize your data using definitive evidence that shows which option will perform best.\n\nStep 2: Assess the vector search performance directly\n\nUsing the Vectorize RAG Sandbox, you can “chat with your experiment data” to see if the quantitative assessment of your semantic search results match up with your personal experience.\n\nHere you can ask your pinecone vector database any of the questions generated in the experiment or anything you wish. Vectorize will automatically generate a query vector, perform a vector search using the pinecone client, and show you the top results. You’ll be able to see relevancy scores and normalized discounted cumulative gain scores for your results to help further inform your decision.\n\nStep 3: Promote your experiment results into a production-ready vector pipeline\n\nOnce you are ready to populate your vector database with your real production data, you can use Vectorize to create a vector pipeline.\n\nTo do this, you’ll start by configuring access to your Pinecone using a destination connector.\n\nHere you simply supply your API key and a name. For security, your api credentials are stored in an encrypted secret storage.\n\nNext, you’ll configure the source connector where your data lives. Here you can select from the many file systems, knowledge bases, and SaaS platforms that Vectorize supports.\n\nOnce your source system and your Pinecone configurations are set up, you can create a vector pipeline to automatically populate your search indexes.\n\nWithin the pipeline, you will be able to select the embedding model and chunking strategy that worked best for you in your experiment.\n\nOnce this is in place, Vectorize will automatically populate your vector indexes in Pinecone. It will also trigger a watch process that will capture any new data or data changes and perform real time updates as needed. This ensures your Pinecone vector database remains up to date and your customers never .\n\nKey architectural designs for Pinecone vector databases\n\nServerless or Pod Based?\n\nPinecone comes with two deployment options: serverless and pod-based and fortunately you can try both options in their free tier.\n\nThe Pinecone serverless database has a usage based pricing structure that allows you to start small with a price structure that grows with you. A key attribute of serverless is its ability to let you to scale elastically in response to growing or shrinking workloads.\n\nPod-based, on the other hand provides you with fixed capacity. While it would be nice if Pinecone could handle massive spikes in traffic instantly with no instability, they too are subject to the laws of computing.\n\nWith pod-based vector databases, you will have an easier time pre-scaling your clusters for peak loads. And while Pinecone generally can provide a high-performance search index in either deployment option, you have more control over the compute resources assigned to your database when using the pod-based option. Of course the trade off with this is that you will be paying for capacity that goes unused.\n\nWith either option, your costs will scale with the dimension size of the vectors in your index. Each index will require more compute for vectors that have a higher dimension size.\n\nHigh-dimension vectors require large amounts of object storage and are more computationally intensive to search. Each query must compare each dimension and while indexes help make this process more efficient, it does have a cost to perform similarity searches.\n\nNamespaces vs Metadata filtering\n\nUsing filtering\n\nMost vector databases provide metadata filtering to make similarity search more efficient. The idea is to limit the number of values that must be compared in each index to achieve high performance semantic search. This makes vector search more efficient, faster, with better scale.\n\nUsing Namespaces\n\nPinecone extends the standard metadata filtering capabilities by also offering full featured partitioning of your vector data. When you query your data in this approach, you must specify the namespace. Pinecone will limit searching only to the partition within the database that matches the namespace specified. Compared to metadata filtering, this is not as flexible, but is much more scalable and help with use cases such as multi-tenancy or log data.\n\nImplementing semantic search\n\nQuerying with a query vector value\n\nQuerying in Pinecone is very simple using either the JavaScript or Python client.\n\nIn python, you would use this approach:\n\nfrom pinecone import Pinecone pc = Pinecone(api_key=\"YOUR_API_KEY\") index = pc.Index(\"my-index\") index.query( namespace=\"my-namespace\", vector=[0.4, 0.7, -0.3, 0.1, 0.9, 0.4, 0.5, 0.8], top_k=5, include_values=True )\n\nHere, we are performing a similarity search using a query vector specified in the vector field. It’s important that the dimension size of your search vector matches the dimension configured on your index.\n\nQuerying with a metadata filter\n\nMetadata filtering works very similar to the last example, but we can include a filter parameter which will narrow down the values searched and only records with matching metadata will be returned.\n\nfrom pinecone import Pinecone pc = Pinecone(api_key=\"YOUR_API_KEY\") index = pc.Index(\"my-index\") index.query( namespace=\"my-namespace\", vector=[0.4, 0.7, -0.3, 0.1, 0.9, 0.4, 0.5, 0.8], filter={ \"color\": {\"$eq\": \"red\"} }, top_k=5, include_values=True )\n\nConclusion\n\nPinecone makes it easy to accomplish the task of building a similarity search based on vector data. While the Pinecone client offerings are easy to use, Pinecone has many of the same challenges as other vector databases when it comes to ingestion and verifying that your vector indexes will perform well. However, using Vectorize with Pinecone makes this process a breeze.",
      "# [Pinecone Revamps Vector Database Architecture for AI Apps by Loraine Lawson, Felipe Cardeneti Mendes, Lubos Kosco, Jack Wallen, Antoni Olendzki, Franz Knupfer, B. Cameron Gain, Ridge Kimani, Kim McMahon, Vicki Walker on 2025-02-27](https://thenewstack.io/pinecone-revamps-vector-database-architecture-for-ai-apps/)\nPinecone announced Tuesday the next generation version of its serverless architecture, which the company says is designed to better support a wide variety of AI applications.\n\nWith the advent of AI, the cloud-based vector database provider has noticed a shift in how its databases are used, explained chief technology officer Ram Sriharsha. In a recent post announcing the architecture changes, Sriharsha said broader use of AI applications has led to a rise in demand for:\n\nRecommender systems requiring 1000s of queries per second;\n\nSemantic search across billions of documents; and\n\nAI agentic systems that require millions of independent agents operating simultaneously.\n\nIn short, Pinecone is trying to serve diverse and sometimes opposing customer needs. Among the differences is that retrieval-augmented generation (RAG) and agentic AI workflows tend to be more sporadic than semantic search, the company noted.\n\n“They look very different from semantic search use cases,” Sriharsha told The New Stack. “In these emerging use cases, you see that actual workloads are very spiky, so it’s the opposite of predictable workload.”\n\nAlso, the corpus of information might be actually quite small — from a few documents to a few hundred documents. Even larger loads are broken up into what Pinecone calls “namespaces” or “tenants.” Within each tenant, the number of documents might be small, he said.\n\nThat requires a very different sort of system to be able to serve that cost effectively, he added.\n\nA Pod-Based Architecture\n\nAbout four years ago, Pinecone began to ship the public version of its vector database in a pod-based architecture.\n\nA pod-based architecture is a way of organizing computing resources where a “pod” is a group of dedicated computers tightly linked together to function as a single unit. It’s often used for cloud computing, high-performance computing (HPC), and other scenarios where scalability and resource management are the primary concerns.\n\nThat worked because traditionally, recommender systems used a “build once and serve many” form of indexing, Sriharsha explained.\n\n“Often, vector indexes for recommender workloads would be built in batch mode, taking hours,” he wrote in the blog. “This means such indexes will be hours stale, but it also allows for heavy optimization of the serving index since it can be treated as static.”\n\nServerless Architecture\n\nSemantic search workloads bring different requirements, he continued. They generally have a larger corpus and require predictable low latency — even though their throughput isn’t very high. They tend to heavily use metadata filters and their workloads care more about freshness, which is whether the database indexes reflect the most recent inserts and deletes.\n\nAgentic workloads are different still, with a small to moderate sized corpora of fewer than a million vectors, but lots of namespaces or tenants.\n\nHe noted that customers running agentic workloads want:\n\nHighly-accurate vector search out of the box without becoming vector search experts;\n\nFreshness, elasticity, and the ability to ingest data without hitting system limits, resharding, and resizing; and\n\nPredictable, low latencies.\n\nSupporting that requires a serverless architecture, Sriharsha said.\n\n“That has been highly successful for these RAG and agentic use cases and so on, and it’s driven a lot of cost savings to customers, and it’s also allowed people to run things at large scale in a way that they couldn’t do before,” he said.\n\nConvergence on One Approach\n\nBut now Pinecone was supporting two systems: The pod-based architecture and the serverless architecture. The cloud-provider began to look at how it could converge the two in a way that offered customers the best of both.\n\n”They still don’t want to have to deal with sizing all these systems and all of this complexity, so they can benefit from all the niceties of serverless, but they need something that allows them to do massive scale workloads,” Sriharsha said. “That meant we had to figure out how to converge pod architecture into serverless and have all the benefits of serverless, but at the same time do something that allows people to run these very different sort of workloads.”\n\nTuesday’s announcement was the culmination of months of work to create one architecture to serve all needs.\n\nThis next-generation approach allows Pinecone to support cost-effective scaling to 1000+ QPS through provisioned read capacity, high performance sparse indexing for higher retrieval quality, and millions of namespaces per index to support massively multitenant use cases.\n\nIt involves the following key innovations to Pinecone’s vector databases, according to Sriharsha’s post:\n\nLog structured Indexing. Log-structured indexing (LSI) is a data storage technique that prioritizes write speed and efficiency that Pinecone has adapted and applied to their vector database;\n\nA new freshness approach that routes all reads through the memtable (an in-memory structure that holds the most recently written data);\n\nPredictable caching in which the index portion of the file, (Pinecone calls these slabs), is always cached between local SSD and memory, which enables Pinecone “to serve queries immediately, without having to wait for a warm up period for cold queries”;\n\nCost-effective at high QPS; and\n\nDisk-based Metadata Filtering, which is another new feature in this update of Pinecone’s serverless architecture."
    ],
    "# Pinecone Company and Product Report\n\n## Company Overview\n\nPinecone is a leading provider of vector databases, specializing in fast and efficient similarity searches for AI applications. The company has positioned itself as a cloud-native solution, eliminating the need for users to manage infrastructure, which is a significant advantage for businesses looking to leverage AI without the overhead of traditional database management [(Pinecone, 2024)](https://www.withorb.com/blog/pinecone-pricing). \n\nPinecone's architecture supports various AI workloads, including recommendation systems, semantic search, and anomaly detection, making it a versatile choice for companies across different sectors. The company has recently revamped its serverless architecture to better accommodate the diverse needs of AI applications, particularly those requiring high query per second (QPS) capabilities and low latency [(Lawson et al., The New Stack, 2025)](https://thenewstack.io/pinecone-revamps-vector-database-architecture-for-ai-apps/).\n\n## Recent Developments\n\n### Product Launches and Features\n\n1. **Pinecone Local**: Recently launched, Pinecone Local is an in-memory database emulator available as a Docker image. This allows developers to build and test applications locally without affecting production data or incurring costs [(Pinecone, 2024)](https://docs.pinecone.io/release-notes/2024).\n\n2. **Enhanced Security Features**: Pinecone has introduced customer-managed encryption keys (CMEK) and private endpoints for secure connections, which are now in general availability. Additionally, audit logs are in early access, providing detailed records of user and API actions [(Pinecone, 2024)](https://docs.pinecone.io/release-notes/2024).\n\n3. **Pinecone Assistant Updates**: The Pinecone Assistant has received significant updates, including the ability to retrieve context snippets and structured data files, enhancing its functionality for users [(Pinecone, 2024)](https://docs.pinecone.io/release-notes/2024).\n\n4. **New SDK Releases**: Pinecone has released major updates for its SDKs across various programming languages, including Python, Node.js, Go, and Java, with enhancements such as support for reranking and improved query capabilities [(Pinecone, 2024)](https://docs.pinecone.io/release-notes/2024).\n\n### Research and Development\n\nPinecone is also investing in AI research, recently announcing the development of \"Luna,\" an LLM designed to eliminate hallucinations in AI responses. This model is based on a novel training technique called information-free training, which aims to improve the reliability of AI outputs [(Pinecone, 2024)](https://www.pinecone.io/blog/hallucination-free-llm/).\n\n## Financial Information\n\nPinecone is a private company and has not yet gone public. As of now, its stock is available for purchase through private markets, specifically for accredited investors. The company has not disclosed its valuation or specific funding rounds, but it is actively seeking investment opportunities as it prepares for potential future growth [(Nasdaq Private Market, 2024)](https://www.nasdaqprivatemarket.com/company/pinecone/).\n\n## Market Position and Competitors\n\nPinecone competes with other vector database providers such as Weaviate and DataStax Astra DB. The recent introduction of in-database vectorization features by competitors highlights the competitive landscape in which Pinecone operates. However, Pinecone's focus on seamless integration with AI applications and its robust feature set positions it favorably in the market [(Y Combinator, 2024)](https://news.ycombinator.com/item?id=42315364).\n\n## User Feedback and Community Sentiment\n\nUser feedback on platforms like Reddit indicates a polarized view of Pinecone's offerings. While some users praise its capabilities for semantic search and AI integration, others express concerns about the complexity of setup and potential costs associated with scaling. It is essential for prospective candidates and investors to consider these mixed sentiments when evaluating Pinecone's market position [(Reddit, 2024)](https://community.pinecone.io/t/401-unauthorized-exception-calling-the-api/5096).\n\n## Conclusion\n\nPinecone is at the forefront of the vector database market, offering innovative solutions tailored for AI applications. With recent product launches, enhanced security features, and ongoing research initiatives, the company is well-positioned for future growth. However, potential investors and candidates should remain aware of the competitive landscape and user feedback as they assess Pinecone's long-term viability and market strategy. \n\nFor more detailed information, please refer to the original sources cited throughout this report."
  ],
  "lineage": {
    "run_at": "2025-03-28T22:43:16.186727",
    "git_sha": "9e00c41"
  }
}