{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core import Seed, init\n",
    "\n",
    "init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title='Imagine Pediatrics | Virtual Health Care for Children' link='https://www.imaginepediatrics.org/' snippet=\"How it works ... Call us at (833) 208-7770 to see if your child is eligible. 2. If your child is eligible, we'll help you download our app. You can start working\\xa0...\" formattedUrl='https://www.imaginepediatrics.org/'\n",
      "title='Imagine Pediatrics PC: Bilingual Pediatrician' link='https://imaginepediatricsrome.com/' snippet='Imagine Pediatrics provides comprehensive, compassionate, and evidence based medical care for your children. Walk-ins welcome, Now Taking New Patients!' formattedUrl='https://imaginepediatricsrome.com/'\n",
      "title='Welcome to Imagine Pediatric Therapy.' link='https://imaginepediatrictherapy.com/' snippet='We proudly serve Northeast Oklahoma and are conveniently located just off Hwy 169 in the city of Owasso. For driving directions from your location, simply click\\xa0...' formattedUrl='https://imaginepediatrictherapy.com/'\n",
      "title='Imagine Pediatric Dentistry: Imagine Yourself Here' link='https://imaginechildrensdentistry.com/' snippet=\"At Imagine Children's Dentistry and Orthodontics, we know when children have a great dental experience it builds healthy oral habits for an entire lifetime.\" formattedUrl='https://imaginechildrensdentistry.com/'\n",
      "title='Imagine Pediatric Therapy' link='https://www.imaginepeds.com/' snippet='A leading provider of pediatric therapy in Chicagoland. We provide individualized therapy services in a multi-disciplinary setting with a holistic intervention\\xa0...' formattedUrl='https://www.imaginepeds.com/'\n",
      "title='Q&A: Imagine Pediatrics CEO George Boghos | Healthcare Innovation' link='https://www.hcinnovationgroup.com/population-health-management/complex-care/article/53080968/qa-imagine-pediatrics-ceo-george-boghos' snippet=\"Dec 19, 2023 ... Startup company's value-based payment model brings 24/7 medical, behavioral, and social care into the homes of Medicaid-eligible children\\xa0...\" formattedUrl='https://www.hcinnovationgroup.com/.../qa-imagine-pediatrics-ceo-george-b...'\n",
      "title='Imagine Pediatrics - NHCC' link='https://healthcarecouncil.com/members/imagine-pediatrics/' snippet='We help healthcare organizations provide innovative, digitally enabled, value-based care solutions that improve total cost of care outcomes, improved quality,\\xa0...' formattedUrl='https://healthcarecouncil.com/members/imagine-pediatrics/'\n",
      "title='Contact Imagine Pediatric Therapy' link='https://www.imaginepediatrictherapy.com/contact_us/' snippet='12899 E 76th St N #109, Owasso, OK 74055. We proudly serve Northeast Oklahoma and are conveniently located just off Hwy 169 in the city of Owasso. For driving\\xa0...' formattedUrl='https://www.imaginepediatrictherapy.com/contact_us/'\n",
      "title='Jobs at Imagine Pediatrics' link='https://boards.greenhouse.io/imaginepediatrics' snippet='Health Services · Bilingual Pediatric Nurse Care Coordinator · Licensed Vocational Nurse/Licensed Practical Nurse · Medical Assistant/Certified Medical\\xa0...' formattedUrl='https://boards.greenhouse.io/imaginepediatrics'\n",
      "title='Imagine Pediatric Therapies | Bentonville AR' link='https://www.facebook.com/imaginepediatrictherapies/' snippet='Imagine Pediatric Therapies, Bentonville, Arkansas. 613 likes · 17 talking about this · 366 were here. Pediatric therapy clinic serving children &...' formattedUrl='https://www.facebook.com/imaginepediatrictherapies/'\n"
     ]
    }
   ],
   "source": [
    "# # optionally, help find the URL for the seed\n",
    "# from google_search import search\n",
    "\n",
    "# for result in search(\"Imagine Pediatics\"):\n",
    "#     print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-08-31 14:57:03.496\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mcompany_webpage\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m65\u001b[0m - \u001b[1m68,711 -> 4,596 chars (7%) \u001b[0m\n",
      "\u001b[32m2024-08-31 14:57:03.502\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mcompany_webpage\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m65\u001b[0m - \u001b[1mExtractive fraction: 14% \u001b[0m\n",
      "\u001b[32m2024-08-31 14:57:03.503\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mcompany_webpage\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m65\u001b[0m - \u001b[1mPercent of URLs in sources: 89% ❌\u001b[0m\n",
      "\u001b[32m2024-08-31 14:57:03.503\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mcompany_webpage\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m65\u001b[0m - \u001b[1mCitation density: 18.2% (percent of output used by URLs/link syntax) \u001b[0m\n",
      "\u001b[32m2024-08-31 14:57:03.503\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mcompany_webpage\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m65\u001b[0m - \u001b[1mDistinct URLs (summary / input): 18 / 29\u001b[0m\n",
      "\u001b[32m2024-08-31 14:57:03.504\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mcompany_webpage\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m65\u001b[0m - \u001b[1mSuspicious URLs: {'https://www.imaginepediatrics.org/events', 'https://www.imaginepediatrics.org/holiday'}\u001b[0m\n",
      "\u001b[32m2024-08-31 14:57:03.504\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mcompany_webpage\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m65\u001b[0m - \u001b[1mCache mentions: 0 ✅\u001b[0m\n",
      "\u001b[32m2024-08-31 14:57:05.557\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mcore\u001b[0m:\u001b[36mshorten_markdown\u001b[0m:\u001b[36m230\u001b[0m - \u001b[1m13,375 -> 10,906 chars (82% of original)\u001b[0m\n",
      "\u001b[32m2024-08-31 14:57:05.586\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mcore\u001b[0m:\u001b[36munshorten_markdown\u001b[0m:\u001b[36m249\u001b[0m - \u001b[1m2,220 -> 3,611 chars (163% of original)\u001b[0m\n",
      "\u001b[32m2024-08-31 14:57:05.587\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mgeneral_search\u001b[0m:\u001b[36msummarize\u001b[0m:\u001b[36m124\u001b[0m - \u001b[1m13,375 -> 3,611 chars (27%) \u001b[0m\n",
      "\u001b[32m2024-08-31 14:57:05.588\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mgeneral_search\u001b[0m:\u001b[36msummarize\u001b[0m:\u001b[36m124\u001b[0m - \u001b[1mExtractive fraction: 73% \u001b[0m\n",
      "\u001b[32m2024-08-31 14:57:05.588\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mgeneral_search\u001b[0m:\u001b[36msummarize\u001b[0m:\u001b[36m124\u001b[0m - \u001b[1mPercent of URLs in sources: 100% ✅\u001b[0m\n",
      "\u001b[32m2024-08-31 14:57:05.589\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mgeneral_search\u001b[0m:\u001b[36msummarize\u001b[0m:\u001b[36m124\u001b[0m - \u001b[1mCitation density: 54.9% (percent of output used by URLs/link syntax) \u001b[0m\n",
      "\u001b[32m2024-08-31 14:57:05.589\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mgeneral_search\u001b[0m:\u001b[36msummarize\u001b[0m:\u001b[36m124\u001b[0m - \u001b[1mDistinct URLs (summary / input): 20 / 49\u001b[0m\n",
      "\u001b[32m2024-08-31 14:57:05.590\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mgeneral_search\u001b[0m:\u001b[36msummarize\u001b[0m:\u001b[36m124\u001b[0m - \u001b[1mSuspicious URLs: set()\u001b[0m\n",
      "\u001b[32m2024-08-31 14:57:05.590\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mgeneral_search\u001b[0m:\u001b[36msummarize\u001b[0m:\u001b[36m124\u001b[0m - \u001b[1mCache mentions: 0 ✅\u001b[0m\n",
      "\u001b[32m2024-08-31 14:57:05.592\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mapp_stores.apple\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m79\u001b[0m - \u001b[1m0 chars in 0 reviews\u001b[0m\n",
      "\u001b[32m2024-08-31 14:57:05.593\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mapp_stores.google_play\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m164\u001b[0m - \u001b[1m1,163 chars in 6 reviews\u001b[0m\n",
      "\u001b[32m2024-08-31 14:57:05.834\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mscrapfly_scrapers.crunchbase\u001b[0m:\u001b[36mscrape_company\u001b[0m:\u001b[36m62\u001b[0m - \u001b[1mscraping company: https://www.crunchbase.com/organization/imagine-pediatrics-96a9/people\u001b[0m\n"
     ]
    },
    {
     "ename": "ScrapflyAspError",
     "evalue": "<-- 422 | ERR::ASP::SHIELD_PROTECTION_FAILED - The ASP shield failed to solve the challenge against the anti scrapping protection - Unable to bypass www.crunchbase.com. Checkout the related doc: https://scrapfly.io/docs/scrape-api/error/ERR::ASP::SHIELD_PROTECTION_FAILED",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mScrapflyAspError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 9\u001b[0m\n\u001b[1;32m      5\u001b[0m logger\u001b[38;5;241m.\u001b[39madd(sys\u001b[38;5;241m.\u001b[39mstderr, level\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mINFO\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01munified\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m markdown_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m unified\u001b[38;5;241m.\u001b[39mrun(\n\u001b[1;32m     10\u001b[0m     Seed\u001b[38;5;241m.\u001b[39minit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImagine Pediatrics\u001b[39m\u001b[38;5;124m\"\u001b[39m, domain\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimaginepediatrics.org\u001b[39m\u001b[38;5;124m\"\u001b[39m), \n\u001b[1;32m     11\u001b[0m     num_reddit_threads\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, \n\u001b[1;32m     12\u001b[0m     max_glassdoor_review_pages\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, \n\u001b[1;32m     13\u001b[0m     max_glassdoor_job_pages\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m     14\u001b[0m     max_news_articles\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m40\u001b[39m,\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;66;03m# glassdoor_url=\"https://www.glassdoor.com/Reviews/Pomelo-Care-Reviews-E9429297.htm\"\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     )\n",
      "File \u001b[0;32m~/company-detective/src/unified.py:225\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(target, num_reddit_threads, max_glassdoor_review_pages, max_glassdoor_job_pages, max_news_articles, glassdoor_url)\u001b[0m\n\u001b[1;32m    221\u001b[0m     steam_review_content \u001b[38;5;241m=\u001b[39m app_stores\u001b[38;5;241m.\u001b[39msteam\u001b[38;5;241m.\u001b[39mrun(steam_url)\n\u001b[1;32m    222\u001b[0m     dynamic_contexts[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSteam Reviews\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m steam_review_content\n\u001b[0;32m--> 225\u001b[0m crunchbase_markdown \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m process_crunchbase(target)\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m crunchbase_markdown:\n\u001b[1;32m    227\u001b[0m     crunchbase_markdown \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/company-detective/src/crunchbase/__init__.py:43\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(target)\u001b[0m\n\u001b[1;32m     41\u001b[0m crunchbase_raw_response \u001b[38;5;241m=\u001b[39m cache\u001b[38;5;241m.\u001b[39mget(url)\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m crunchbase_raw_response:\n\u001b[0;32m---> 43\u001b[0m     crunchbase_raw_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m scrapfly_scrapers\u001b[38;5;241m.\u001b[39mcrunchbase\u001b[38;5;241m.\u001b[39mscrape_company(url)\n\u001b[1;32m     44\u001b[0m     cache\u001b[38;5;241m.\u001b[39mset(url, crunchbase_raw_response, expire\u001b[38;5;241m=\u001b[39mtimedelta(days\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m14\u001b[39m)\u001b[38;5;241m.\u001b[39mtotal_seconds())\n\u001b[1;32m     46\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpdating cache with Crunchbase response: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, crunchbase_raw_response)\n",
      "File \u001b[0;32m~/company-detective/src/scrapfly_scrapers/crunchbase.py:63\u001b[0m, in \u001b[0;36mscrape_company\u001b[0;34m(url)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# note: we use /people tab because it contains the most data:\u001b[39;00m\n\u001b[1;32m     62\u001b[0m log\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscraping company: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 63\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m SCRAPFLY\u001b[38;5;241m.\u001b[39masync_scrape(ScrapeConfig(url, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mBASE_CONFIG))\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m parse_company(result)\n",
      "File \u001b[0;32m~/company-detective/.venv/lib/python3.10/site-packages/scrapfly/client.py:326\u001b[0m, in \u001b[0;36mScrapflyClient.async_scrape\u001b[0;34m(self, scrape_config, loop)\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m loop \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    324\u001b[0m     loop \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mget_running_loop()\n\u001b[0;32m--> 326\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mrun_in_executor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39masync_executor, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscrape, scrape_config)\n",
      "File \u001b[0;32m/usr/lib/python3.10/concurrent/futures/thread.py:58\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mset_exception(exc)\n",
      "File \u001b[0;32m~/company-detective/.venv/lib/python3.10/site-packages/backoff/_sync.py:105\u001b[0m, in \u001b[0;36mretry_exception.<locals>.retry\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     96\u001b[0m details \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m\"\u001b[39m: target,\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124margs\u001b[39m\u001b[38;5;124m\"\u001b[39m: args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124melapsed\u001b[39m\u001b[38;5;124m\"\u001b[39m: elapsed,\n\u001b[1;32m    102\u001b[0m }\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 105\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mtarget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m exception \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    107\u001b[0m     max_tries_exceeded \u001b[38;5;241m=\u001b[39m (tries \u001b[38;5;241m==\u001b[39m max_tries_value)\n",
      "File \u001b[0;32m~/company-detective/.venv/lib/python3.10/site-packages/scrapfly/client.py:419\u001b[0m, in \u001b[0;36mScrapflyClient.scrape\u001b[0;34m(self, scrape_config, no_raise)\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m no_raise \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, ScrapflyError) \u001b[38;5;129;01mand\u001b[39;00m e\u001b[38;5;241m.\u001b[39mapi_response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    417\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m e\u001b[38;5;241m.\u001b[39mapi_response\n\u001b[0;32m--> 419\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[0;32m~/company-detective/.venv/lib/python3.10/site-packages/scrapfly/client.py:408\u001b[0m, in \u001b[0;36mScrapflyClient.scrape\u001b[0;34m(self, scrape_config, no_raise)\u001b[0m\n\u001b[1;32m    406\u001b[0m request_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_scrape_request(scrape_config\u001b[38;5;241m=\u001b[39mscrape_config)\n\u001b[1;32m    407\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_http_handler(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mrequest_data)\n\u001b[0;32m--> 408\u001b[0m scrape_api_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscrape_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscrape_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreporter\u001b[38;5;241m.\u001b[39mreport(scrape_api_response\u001b[38;5;241m=\u001b[39mscrape_api_response)\n\u001b[1;32m    412\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m scrape_api_response\n",
      "File \u001b[0;32m~/company-detective/.venv/lib/python3.10/site-packages/scrapfly/client.py:423\u001b[0m, in \u001b[0;36mScrapflyClient._handle_response\u001b[0;34m(self, response, scrape_config)\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_handle_response\u001b[39m(\u001b[38;5;28mself\u001b[39m, response:Response, scrape_config:ScrapeConfig) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ScrapeApiResponse:\n\u001b[1;32m    422\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 423\u001b[0m         api_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle_api_response\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    424\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    425\u001b[0m \u001b[43m            \u001b[49m\u001b[43mscrape_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscrape_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    426\u001b[0m \u001b[43m            \u001b[49m\u001b[43mraise_on_upstream_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscrape_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_on_upstream_error\u001b[49m\n\u001b[1;32m    427\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    429\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m scrape_config\u001b[38;5;241m.\u001b[39mmethod \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHEAD\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    430\u001b[0m             logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<-- [\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m] \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m | \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (\n\u001b[1;32m    431\u001b[0m                 api_response\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mstatus_code,\n\u001b[1;32m    432\u001b[0m                 api_response\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mreason,\n\u001b[1;32m    433\u001b[0m                 api_response\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mrequest\u001b[38;5;241m.\u001b[39murl,\n\u001b[1;32m    434\u001b[0m                 \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    435\u001b[0m             ))\n",
      "File \u001b[0;32m~/company-detective/.venv/lib/python3.10/site-packages/scrapfly/client.py:605\u001b[0m, in \u001b[0;36mScrapflyClient._handle_api_response\u001b[0;34m(self, response, scrape_config, raise_on_upstream_error)\u001b[0m\n\u001b[1;32m    596\u001b[0m         body \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mcontent\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    598\u001b[0m api_response:ScrapeApiResponse \u001b[38;5;241m=\u001b[39m ScrapeApiResponse(\n\u001b[1;32m    599\u001b[0m     response\u001b[38;5;241m=\u001b[39mresponse,\n\u001b[1;32m    600\u001b[0m     request\u001b[38;5;241m=\u001b[39mresponse\u001b[38;5;241m.\u001b[39mrequest,\n\u001b[1;32m    601\u001b[0m     api_result\u001b[38;5;241m=\u001b[39mbody,\n\u001b[1;32m    602\u001b[0m     scrape_config\u001b[38;5;241m=\u001b[39mscrape_config\n\u001b[1;32m    603\u001b[0m )\n\u001b[0;32m--> 605\u001b[0m \u001b[43mapi_response\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraise_on_upstream_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mraise_on_upstream_error\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m api_response\n",
      "File \u001b[0;32m~/company-detective/.venv/lib/python3.10/site-packages/scrapfly/api_response.py:435\u001b[0m, in \u001b[0;36mScrapeApiResponse.raise_for_result\u001b[0;34m(self, raise_on_upstream_error)\u001b[0m\n\u001b[1;32m    433\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m error\n\u001b[1;32m    434\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 435\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error\n",
      "\u001b[0;31mScrapflyAspError\u001b[0m: <-- 422 | ERR::ASP::SHIELD_PROTECTION_FAILED - The ASP shield failed to solve the challenge against the anti scrapping protection - Unable to bypass www.crunchbase.com. Checkout the related doc: https://scrapfly.io/docs/scrape-api/error/ERR::ASP::SHIELD_PROTECTION_FAILED"
     ]
    }
   ],
   "source": [
    "# Set the log level\n",
    "import sys\n",
    "from loguru import logger\n",
    "logger.remove()\n",
    "logger.add(sys.stderr, level=\"INFO\")\n",
    "\n",
    "import unified\n",
    "\n",
    "markdown_file = await unified.run(\n",
    "    Seed.init(\"Imagine Pediatrics\", domain=\"imaginepediatrics.org\"), \n",
    "    num_reddit_threads=10, \n",
    "    max_glassdoor_review_pages=5, \n",
    "    max_glassdoor_job_pages=0,\n",
    "    max_news_articles=40,\n",
    "    # glassdoor_url=\"https://www.glassdoor.com/Reviews/Pomelo-Care-Reviews-E9429297.htm\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-08-29 16:59:30.060\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpublish\u001b[0m:\u001b[36mmarkdown_file_to_html\u001b[0m:\u001b[36m26\u001b[0m - \u001b[1mHTML written to /home/keith/company-detective/output/Grammarly/Grammarly_20240829_165800.html\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'http://company-detective.s3-website-us-west-2.amazonaws.com/reports/Grammarly_20240829_165800.html'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from publish import markdown_file_to_html, publish_to_s3\n",
    "\n",
    "html_file = markdown_file_to_html(markdown_file)\n",
    "publish_to_s3(html_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO CATCH: ScrapflyAspError in ht eCrunchbase pipeline"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
