{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unified summary, Version 2!\n",
    "\n",
    "Key changes from version 1:\n",
    "- Organized by topic rather than data source\n",
    "- More data sources: \n",
    "    - Indeed job descriptions\n",
    "    - Crunchbase\n",
    "    - General search results\n",
    "- Technical\n",
    "    - Permalinks in sources and piping them through, rather than each pipeline being different\n",
    "    - Extract, organize, then abstract\n",
    "    - Heavy use of caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core import CompanyProduct, init_langchain_cache, init_requests_cache\n",
    "\n",
    "init_requests_cache()\n",
    "init_langchain_cache()\n",
    "\n",
    "target = CompanyProduct.same(\"98point6\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import reddit.map_reduce_summarizer\n",
    "import reddit.search\n",
    "import reddit.fetch\n",
    "\n",
    "from core import CompanyProduct\n",
    "from search import SearchResult\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Optional\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "@dataclass\n",
    "class RedditSummary:\n",
    "    sources: List[SearchResult]\n",
    "    threads: List[reddit.fetch.Submission]\n",
    "    summary_markdown: str\n",
    "    summary_intermediate_steps: List[str]\n",
    "    summary_input_documents: List[Document]\n",
    "\n",
    "def process_reddit(target: CompanyProduct, num_threads=2, min_comments=2) -> Optional[RedditSummary]:\n",
    "    reddit_client = reddit.fetch.init()\n",
    "\n",
    "    # Search for URLs\n",
    "    search_results = reddit.search.find_submissions(target, num_results=10)\n",
    "\n",
    "    # Fetch the Submissions from Reddit\n",
    "    post_submissions = [reddit_client.submission(url=result.link) for result in search_results]\n",
    "\n",
    "    # Filter Submissions to only those with enough comments\n",
    "    post_submissions = [submission for submission in post_submissions if submission.num_comments >= min_comments]\n",
    "\n",
    "    if len(post_submissions) == 0:\n",
    "        print(f\"No posts with enough comments found for {target}\")\n",
    "        return None\n",
    "\n",
    "    # Limit the number of threads\n",
    "    post_submissions = post_submissions[:num_threads]\n",
    "\n",
    "    # Aggregate the summaries\n",
    "    result = reddit.map_reduce_summarizer.summarize(target, post_submissions)\n",
    "\n",
    "    return RedditSummary(\n",
    "        sources=search_results,\n",
    "        threads=post_submissions,\n",
    "        summary_markdown=result[\"output_text\"],\n",
    "        summary_intermediate_steps=result[\"intermediate_steps\"],\n",
    "        summary_input_documents=result[\"input_documents\"],\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pprint import pprint\n",
    "from typing import List\n",
    "\n",
    "from core import CompanyProduct\n",
    "from search import SearchResult\n",
    "\n",
    "\n",
    "from glassdoor.search import find_review\n",
    "from glassdoor.summarizer import summarize\n",
    "\n",
    "import scrapfly_scrapers.glassdoor\n",
    "from scrapfly_scrapers.glassdoor import scrape_reviews, scrape_jobs\n",
    "from glassdoor.models import UrlBuilder, GlassdoorReview, GlassdoorJob\n",
    "\n",
    "scrapfly_scrapers.glassdoor.BASE_CONFIG[\"cache\"] = True\n",
    "\n",
    "@dataclass\n",
    "class GlassdoorResult:\n",
    "    # inputs\n",
    "    company: CompanyProduct\n",
    "\n",
    "    # intermediate data\n",
    "    review_page: SearchResult\n",
    "    raw_reviews: dict\n",
    "    reviews: List[GlassdoorReview]\n",
    "\n",
    "    # outputs\n",
    "    jobs: List[GlassdoorJob]\n",
    "    summary_markdown: str\n",
    "    \n",
    "    @property\n",
    "    def num_parsed_reviews(self):\n",
    "        return len(self.reviews)\n",
    "    \n",
    "    @property\n",
    "    def num_raw_reviews(self):\n",
    "        return self.raw_reviews.get('allReviewsCount', 0)\n",
    "\n",
    "\n",
    "async def process_glassdoor(target: CompanyProduct, max_review_pages=1, max_job_pages=0, debug=False) -> GlassdoorResult:\n",
    "    review_page = find_review(target)\n",
    "    company, company_id = UrlBuilder.parse_review_url(review_page.link)\n",
    "\n",
    "    # job results, not 100% used yet\n",
    "    jobs = []\n",
    "    if max_job_pages > 0:\n",
    "        job_results = await scrape_jobs(UrlBuilder.jobs(company, company_id), max_pages=max_job_pages)\n",
    "        jobs = [GlassdoorJob(**result) for result in job_results]\n",
    "        jobs = sorted(jobs, key=lambda job: job.jobTitleText)\n",
    "    \n",
    "    response = await scrape_reviews(review_page.link, max_pages=max_review_pages)\n",
    "\n",
    "    if debug:\n",
    "        pprint(response)\n",
    "    reviews = GlassdoorReview.parse_reviews(company, response)\n",
    "    \n",
    "    review_summary = summarize(target, reviews)\n",
    "    \n",
    "    # TODO: Pull out allReviewsCount from glassdoor_results\n",
    "    return GlassdoorResult(\n",
    "        target, \n",
    "        review_page,\n",
    "        response,\n",
    "        reviews, \n",
    "        jobs,\n",
    "        review_summary.content\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple, Mapping, List\n",
    "\n",
    "from core import CompanyProduct\n",
    "from search import SearchResult\n",
    "import news.search\n",
    "import news.scrape\n",
    "import news.summarize\n",
    "\n",
    "class NewsSummary(NamedTuple):\n",
    "    # input\n",
    "    target: CompanyProduct\n",
    "\n",
    "    # intermediates\n",
    "    search_results: List[SearchResult]\n",
    "    article_markdowns: Mapping[str, str]\n",
    "\n",
    "    # output\n",
    "    summary_markdown: str\n",
    "\n",
    "\n",
    "def process_news(target: CompanyProduct, max_results=30) -> NewsSummary:\n",
    "    search_results = news.search.find_news_articles(target, num_results=max_results)\n",
    "\n",
    "    article_markdowns = {result.link: news.scrape.get_article_markdown(result.link) for result in search_results}\n",
    "\n",
    "    article_markdown_list = [article for article in article_markdowns.values() if article]\n",
    "\n",
    "    llm_result = news.summarize.summarize(target, article_markdown_list)\n",
    "\n",
    "    return NewsSummary(\n",
    "        target=target, \n",
    "        search_results=search_results, \n",
    "        article_markdowns=article_markdowns, \n",
    "        summary_markdown=llm_result.content\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "def eval_filename(target: CompanyProduct, create_folder=True, extension=\"html\") -> str:\n",
    "    # Make the output folder\n",
    "    folder_name = re.sub(r\"[^a-zA-Z0-9]\", \"_\", f\"{target.company} {target.product}\")\n",
    "    folder_path = f\"evaluation/{folder_name}\"\n",
    "\n",
    "    if create_folder:\n",
    "        os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "    # Create the filename using the current timestamp\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filename = f\"{folder_path}/{timestamp}.{extension}\"\n",
    "\n",
    "    return filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from crunchbase import find_people_url, parse\n",
    "import scrapfly_scrapers.crunchbase\n",
    "import jinja2\n",
    "\n",
    "scrapfly_scrapers.crunchbase.BASE_CONFIG[\"cache\"] = True\n",
    "templates = jinja2.Environment(loader=jinja2.FileSystemLoader(\"templates\"))\n",
    "\n",
    "_response_cache = {}\n",
    "\n",
    "async def process_crunchbase(target: CompanyProduct, debug=False) -> str:\n",
    "    url = find_people_url(target)\n",
    "\n",
    "    # For whatever reason, Scrapfly doesn't cache all the time\n",
    "    if url not in _response_cache:\n",
    "        _response_cache[url] = await scrapfly_scrapers.crunchbase.scrape_company(url)\n",
    "\n",
    "    crunchbase_raw_response = _response_cache[url]\n",
    "    if debug:\n",
    "        pprint(crunchbase_raw_response)\n",
    "    organization, employees = parse(crunchbase_raw_response)\n",
    "\n",
    "    return templates.get_template(\"crunchbase.md\").render(organization=organization, employees=employees)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jinja2\n",
    "\n",
    "templates = jinja2.Environment(\n",
    "    loader=jinja2.FileSystemLoader(\"templates\"),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-08-12 20:12:20.847\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mscrapfly_scrapers.glassdoor\u001b[0m:\u001b[36mscrape_reviews\u001b[0m:\u001b[36m105\u001b[0m - \u001b[1mscraping reviews from https://www.glassdoor.com/Reviews/98point6-Reviews-E1181484.htm\u001b[0m\n",
      "\u001b[32m2024-08-12 20:12:20.859\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mscrapfly_scrapers.glassdoor\u001b[0m:\u001b[36mscrape_reviews\u001b[0m:\u001b[36m113\u001b[0m - \u001b[1mscraped first page of reviews of https://www.glassdoor.com/Reviews/98point6-Reviews-E1181484.htm, scraping remaining 2 pages\u001b[0m\n",
      "\u001b[32m2024-08-12 20:12:22.389\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mscrapfly_scrapers.glassdoor\u001b[0m:\u001b[36mscrape_reviews\u001b[0m:\u001b[36m123\u001b[0m - \u001b[1mscraped 30 reviews from https://www.glassdoor.com/Reviews/98point6-Reviews-E1181484.htm in 3 pages\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Glassdoor: The prompt context has 16,256 characters in 30 reviews\n",
      "Failed to get article from https://www.reuters.com/article/brief-98point6-inc-files-to-say-raised-2/brief-98point6-inc-files-to-say-raised-29-1-million-in-equity-financing-idUSFWN1M10QV/: 401\n",
      "Failed to get article from https://www.crunchbase.com/organization/mdlive/company_overview/overview_timeline: 403\n",
      "Failed to get article from https://ptdiocese.org/benefits: 403\n",
      "Failed to get article from https://www.fiercehealthcare.com/health-tech/transcarent-launches-direct-employer-solution-partnership-advocate-mount-sinai-other: 403\n",
      "Failed to get article from https://www.360dx.com/business-news/barda-launches-diagnostics-medical-devices-accelerator: 403\n",
      "Failed to get article from https://www.zoominfo.com/c/brightmd-inc/371829919: 403\n",
      "Failed to get article from https://forgeglobal.com/search-private-companies/healthcare/digital-health/: 403\n",
      "Failed to get article from https://www.modernhealthcare.com/digital-health/talkspace-transcarent-partnerships-mergers-glen-tullman-jon-cohen: 403\n",
      "78,352 characters in unified context\n",
      "Written to evaluation/98point6_98point6/20240812_201401.md\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from core import CompanyProduct\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"\n",
    "You're an expert in business and product development.\n",
    "Carefully review all of the following information about a company and its product.\n",
    "Write a comprehensive markdown summary of all information with citations to the original sources for reference.\n",
    "Citations should follow the format [(Author, Source, Date)](url).\n",
    "\n",
    "Follow this template to summarize the most important information about a company and its product. Each markdown section has tips on what information is most critical.\n",
    "\n",
    "# About {company_name}\n",
    "\n",
    "The About section should provide all the essential information about the company.\n",
    "An ideal summary should incorporate the answers to the following questions:\n",
    "- When was the company founded?\n",
    "- Approximately how many employees work at the company?\n",
    "- What products does the company produce? What services does the company offer?\n",
    "- How does the company make money? Who are their customers in general? Is it B2B, B2C? If B2B, include example customers.\n",
    "\n",
    "# Key personnel\n",
    "\n",
    "# News (reverse chronological, grouped by event)\n",
    "\n",
    "# Working at {company_name}\n",
    "\n",
    "Questions that should be answered by this summary:\n",
    "- Is the leadership team good?\n",
    "- What benefits are provided?\n",
    "- Is the company good at DEI?\n",
    "- Whats's the work-life balance like and workload?\n",
    "- How has working at the company changed over time?\n",
    "- How does employee satisfaction vary by job function?\n",
    "- Why do people like working here?\n",
    "- Why do people dislike working here?\n",
    "\n",
    "## Positive sentiments and experiences\n",
    "\n",
    "## Negative sentiments and experiences\n",
    "\n",
    "## Verifyable statements about working at {company_name}\n",
    "\n",
    "# User reviews, sentiments, and feedback about {product_name}\n",
    "\n",
    "## Positive sentiments and experiences\n",
    "\n",
    "## Negative sentiments and experiences\n",
    "\n",
    "## Verifyable statements about using {product_name}\n",
    "\n",
    "# Bibliography\n",
    "\n",
    "The Bibliography should include a list of all the sources used to compile the summary.\n",
    "\n",
    "\n",
    "Feel free to create subheadings or additional sections as needed to capture the most important information about the company and its product.\n",
    "Format the output as a markdown document, preserving any links in the source.\n",
    "            \"\"\",\n",
    "        ),\n",
    "        (\n",
    "            \"human\",\n",
    "            \"\"\"\n",
    "            Company: {company_name}\n",
    "            Product: {product_name}\n",
    "            \n",
    "            Reddit sources: \n",
    "            {reddit_text}\n",
    "\n",
    "            Glassdoor sources:\n",
    "            {glassdoor_text}\n",
    "\n",
    "            News sources:\n",
    "            {news_text}\n",
    "\n",
    "            Crunchbase information:\n",
    "            {crunchbase_text}\n",
    "            \"\"\",\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "async def unified_summary(target: CompanyProduct, num_reddit_threads=2, max_glassdoor_review_pages=1, max_glassdoor_job_pages=1, max_news_articles=10):\n",
    "    crunchbase_markdown = await process_crunchbase(target)\n",
    "    reddit_result = process_reddit(target, num_threads=num_reddit_threads)\n",
    "    glassdoor_result = await process_glassdoor(target, max_review_pages=max_glassdoor_review_pages, max_job_pages=max_glassdoor_job_pages)\n",
    "    news_result = process_news(target, max_results=max_news_articles)\n",
    "\n",
    "\n",
    "    # feed results into LLM for summarization\n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "    runnable = prompt | llm\n",
    "    result = runnable.invoke({\n",
    "        \"company_name\": target.company, \n",
    "        \"product_name\": target.product,\n",
    "        \"reddit_text\": reddit_result.summary_markdown,\n",
    "        \"glassdoor_text\": glassdoor_result.summary_markdown,\n",
    "        \"news_text\": news_result.summary_markdown,\n",
    "        \"crunchbase_text\": crunchbase_markdown,\n",
    "        })\n",
    "    result.content = result.content.strip().strip(\"```markdown\").strip(\"```\")\n",
    "\n",
    "\n",
    "    with open(eval_filename(target, extension=\"md\"), \"w\") as f:\n",
    "        f.write(result.content)\n",
    "\n",
    "        # Write the raw Reddit summary too\n",
    "        f.write(f\"\\n----\\n## Reddit Summary\\n{reddit_result.summary_markdown}\\n\\n\")\n",
    "\n",
    "        # Write the individual Reddit threads\n",
    "        for thread in reddit_result.threads:\n",
    "            f.write(f\"{reddit.fetch.submission_to_markdown(thread)}\\n\\n\")\n",
    "\n",
    "        # Write the raw Glassdoor summary too\n",
    "        f.write(f\"\\n----\\n## Glassdoor Summary\\n{glassdoor_result.summary_markdown}\\n\\n\")\n",
    "\n",
    "        # Write the individual Glassdoor reviews\n",
    "        for review in glassdoor_result.reviews:\n",
    "            review_md = templates.get_template(\"glassdoor_review.md\").render(review=review)\n",
    "            f.write(f\"{review_md}\\n\\n\")\n",
    "\n",
    "        # Write the raw News summary too\n",
    "        f.write(f\"\\n----\\n## News Summary\\n{news_result.summary_markdown}\\n\\n\")\n",
    "\n",
    "        # Write the raw Crunchbase summary too\n",
    "        f.write(f\"\\n----\\n## Crunchbase Summary\\n{crunchbase_markdown}\\n\\n\")\n",
    "\n",
    "        print(f\"Written to {f.name}\")\n",
    "\n",
    "await unified_summary(\n",
    "    CompanyProduct.same(\"98point6\"), \n",
    "    num_reddit_threads=10, \n",
    "    max_glassdoor_review_pages=3, \n",
    "    max_glassdoor_job_pages=0,\n",
    "    max_news_articles=50\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
