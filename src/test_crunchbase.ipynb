{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core import init\n",
    "\n",
    "init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core import Seed\n",
    "\n",
    "target = Seed.init(\"Imagine Pediatrics\", \"imaginepediatrics.org\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google_search import search\n",
    "\n",
    "# results = list(\n",
    "#     search(f'site:crunchbase.com/organization \"Mt. Joy\"', num=10)\n",
    "# )\n",
    "# results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-08-31 15:45:22.032\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mgoogle_search\u001b[0m:\u001b[36msearch\u001b[0m:\u001b[36m58\u001b[0m - \u001b[34m\u001b[1mGoogle search results: {'kind': 'customsearch#search', 'url': {'type': 'application/json', 'template': 'https://www.googleapis.com/customsearch/v1?q={searchTerms}&num={count?}&start={startIndex?}&lr={language?}&safe={safe?}&cx={cx?}&sort={sort?}&filter={filter?}&gl={gl?}&cr={cr?}&googlehost={googleHost?}&c2coff={disableCnTwTranslation?}&hq={hq?}&hl={hl?}&siteSearch={siteSearch?}&siteSearchFilter={siteSearchFilter?}&exactTerms={exactTerms?}&excludeTerms={excludeTerms?}&linkSite={linkSite?}&orTerms={orTerms?}&dateRestrict={dateRestrict?}&lowRange={lowRange?}&highRange={highRange?}&searchType={searchType}&fileType={fileType?}&rights={rights?}&imgSize={imgSize?}&imgType={imgType?}&imgColorType={imgColorType?}&imgDominantColor={imgDominantColor?}&alt=json'}, 'queries': {'request': [{'title': 'Google Custom Search - site:www.crunchbase.com/organization \"Imagine Pediatrics\"', 'totalResults': '4', 'searchTerms': 'site:www.crunchbase.com/organization \"Imagine Pediatrics\"', 'count': 4, 'startIndex': 1, 'language': 'lang_en', 'inputEncoding': 'utf8', 'outputEncoding': 'utf8', 'safe': 'off', 'cx': 'd5705c8967e3c4d63', 'gl': 'us', 'hl': 'en'}]}, 'context': {'title': 'company_detective'}, 'searchInformation': {'searchTime': 0.092036, 'formattedSearchTime': '0.09', 'totalResults': '4', 'formattedTotalResults': '4'}, 'items': [{'kind': 'customsearch#result', 'title': 'Imagine Pediatrics - Crunchbase Company Profile & Funding', 'htmlTitle': '<b>Imagine Pediatrics</b> - Crunchbase Company Profile &amp; Funding', 'link': 'https://www.crunchbase.com/organization/imagine-pediatrics-96a9', 'displayLink': 'www.crunchbase.com', 'snippet': 'Imagine Pediatrics is a virtual healthcare service for children with complex medical conditions.', 'htmlSnippet': '<b>Imagine Pediatrics</b> is a virtual healthcare service for children with complex medical conditions.', 'formattedUrl': 'https://www.crunchbase.com/organization/imagine-pediatrics-96a9', 'htmlFormattedUrl': 'https://www.crunchbase.com/organization/<b>imagine-pediatrics</b>-96a9', 'pagemap': {'cse_thumbnail': [{'src': 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRD02PlVW89b5UwupxbTUtd-B6CVoy1mfaAvdDI-EYCOqSlKZOmu1xbqH6K&s', 'width': '343', 'height': '147'}], 'metatags': [{'og:image': 'https://images.crunchbase.com/image/upload/c_pad,h_256,w_256,f_auto,q_auto:eco,dpr_1/en9jtjiqgswgrxwucruk', 'og:type': 'website', 'og:image:alt': 'Imagine Pediatrics', 'og:image:width': '256', 'og:site_name': 'Crunchbase', 'viewport': 'width=device-width, minimum-scale=1.0, initial-scale=1.0', 'og:title': 'Imagine Pediatrics - Crunchbase Company Profile & Funding', 'og:image:height': '256', 'og:url': 'https://www.crunchbase.com/organization/imagine-pediatrics-96a9', 'og:description': 'Imagine Pediatrics is a virtual healthcare service for children with complex medical conditions.', 'og:image:secure_url': 'https://images.crunchbase.com/image/upload/c_pad,h_256,w_256,f_auto,q_auto:eco,dpr_1/en9jtjiqgswgrxwucruk'}], 'cse_image': [{'src': 'https://images.crunchbase.com/image/upload/c_pad,h_256,w_256,f_auto,q_auto:eco,dpr_1/en9jtjiqgswgrxwucruk'}]}}, {'kind': 'customsearch#result', 'title': 'Imagine Pediatrics - Crunchbase Company Profile & Funding', 'htmlTitle': '<b>Imagine Pediatrics</b> - Crunchbase Company Profile &amp; Funding', 'link': 'https://www.crunchbase.com/organization/imagine-pediatrics', 'displayLink': 'www.crunchbase.com', 'snippet': 'Imagine Pediatrics offers newborn care, acute care, and pediatric care services. They offer breastfeeding basics, same-day sick appointments, and prevention of\\xa0...', 'htmlSnippet': '<b>Imagine Pediatrics</b> offers newborn care, acute care, and pediatric care services. They offer breastfeeding basics, same-day sick appointments, and prevention of&nbsp;...', 'formattedUrl': 'https://www.crunchbase.com/organization/imagine-pediatrics', 'htmlFormattedUrl': 'https://www.crunchbase.com/organization/<b>imagine-pediatrics</b>', 'pagemap': {'cse_thumbnail': [{'src': 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcS5083zbciwrBIMxTpqZMHp3Navu53tGSF0WCFqv-PZO55bfJ2dKteh4dk&s', 'width': '225', 'height': '225'}], 'metatags': [{'og:image': 'https://images.crunchbase.com/image/upload/c_pad,h_256,w_256,f_auto,q_auto:eco,dpr_1/r0wwldhnatfuc9qys6hh', 'og:type': 'website', 'og:image:alt': 'Imagine Pediatrics', 'og:image:width': '256', 'og:site_name': 'Crunchbase', 'viewport': 'width=device-width, minimum-scale=1.0, initial-scale=1.0', 'og:title': 'Imagine Pediatrics - Crunchbase Company Profile & Funding', 'og:image:height': '256', 'og:url': 'https://www.crunchbase.com/organization/imagine-pediatrics', 'og:description': 'Imagine Pediatrics provides newborn care, acute care, and pediatric care services.', 'og:image:secure_url': 'https://images.crunchbase.com/image/upload/c_pad,h_256,w_256,f_auto,q_auto:eco,dpr_1/r0wwldhnatfuc9qys6hh'}], 'cse_image': [{'src': 'https://images.crunchbase.com/image/upload/c_pad,h_256,w_256,f_auto,q_auto:eco,dpr_1/r0wwldhnatfuc9qys6hh'}]}}, {'kind': 'customsearch#result', 'title': 'Danat Al Emarat Hospital - Crunchbase Company Profile & Funding', 'htmlTitle': 'Danat Al Emarat Hospital - Crunchbase Company Profile &amp; Funding', 'link': 'https://www.crunchbase.com/organization/danat-al-emarat-hospital', 'displayLink': 'www.crunchbase.com', 'snippet': 'Alternatives and possible competitors to Danat Al Emarat Hospital may include Imagine Pediatrics , Lakshmi Hospitals , and Milann Fertility . Unlock even\\xa0...', 'htmlSnippet': 'Alternatives and possible competitors to Danat Al Emarat Hospital may include <b>Imagine Pediatrics</b> , Lakshmi Hospitals , and Milann Fertility . Unlock even&nbsp;...', 'formattedUrl': 'https://www.crunchbase.com/organization/danat-al-emarat-hospital', 'htmlFormattedUrl': 'https://www.crunchbase.com/organization/danat-al-emarat-hospital', 'pagemap': {'cse_thumbnail': [{'src': 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQxRGZ9o-9C79Q3x78_l0lRlk6J29vGLFAkx62Un8y1pclo7rwiLYm2ln8&s', 'width': '361', 'height': '140'}], 'metatags': [{'og:image': 'https://images.crunchbase.com/image/upload/c_pad,h_256,w_256,f_auto,q_auto:eco,dpr_1/gllcsj5quakrmjd4qris', 'og:type': 'website', 'og:image:alt': 'Danat Al Emarat Hospital', 'og:image:width': '256', 'og:site_name': 'Crunchbase', 'viewport': 'width=device-width, minimum-scale=1.0, initial-scale=1.0', 'og:title': 'Danat Al Emarat Hospital - Crunchbase Company Profile & Funding', 'og:image:height': '256', 'og:url': 'https://www.crunchbase.com/organization/danat-al-emarat-hospital', 'og:description': 'Danat Al Emarat Hospital provides neonatology, obstetrics, gynecology, and pediatric surgery services.', 'og:image:secure_url': 'https://images.crunchbase.com/image/upload/c_pad,h_256,w_256,f_auto,q_auto:eco,dpr_1/gllcsj5quakrmjd4qris'}], 'cse_image': [{'src': 'https://images.crunchbase.com/image/upload/c_pad,h_256,w_256,f_auto,q_auto:eco,dpr_1/gllcsj5quakrmjd4qris'}]}}, {'kind': 'customsearch#result', 'title': 'Sheikh Shakhbout Medical City - Crunchbase Company Profile ...', 'htmlTitle': 'Sheikh Shakhbout Medical City - Crunchbase Company Profile ...', 'link': 'https://www.crunchbase.com/organization/sheikh-shakhbout-medical-city', 'displayLink': 'www.crunchbase.com', 'snippet': 'Alternatives and possible competitors to Sheikh Shakhbout Medical City may include Imagine Pediatrics , PAD Specialists , and Dallas County Medical Center .', 'htmlSnippet': 'Alternatives and possible competitors to Sheikh Shakhbout Medical City may include <b>Imagine Pediatrics</b> , PAD Specialists , and Dallas County Medical Center .', 'formattedUrl': 'https://www.crunchbase.com/organization/sheikh-shakhbout-medical-city', 'htmlFormattedUrl': 'https://www.crunchbase.com/organization/sheikh-shakhbout-medical-city', 'pagemap': {'cse_thumbnail': [{'src': 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRhpKwm0-lOMi3hio14LbS5hxn7G8y02vtdljVwKfDb0eDqp9eW_tglonjp&s', 'width': '225', 'height': '225'}], 'metatags': [{'og:image': 'https://images.crunchbase.com/image/upload/c_pad,h_256,w_256,f_auto,q_auto:eco,dpr_1/nqzhpurnanwpjofkxp0n', 'og:type': 'website', 'og:image:alt': 'Sheikh Shakhbout Medical City', 'og:image:width': '256', 'og:site_name': 'Crunchbase', 'viewport': 'width=device-width, minimum-scale=1.0, initial-scale=1.0', 'og:title': 'Sheikh Shakhbout Medical City - Crunchbase Company Profile & Funding', 'og:image:height': '256', 'og:url': 'https://www.crunchbase.com/organization/sheikh-shakhbout-medical-city', 'og:description': 'Sheikh Shakhbout Medical City is a health care service center.', 'og:image:secure_url': 'https://images.crunchbase.com/image/upload/c_pad,h_256,w_256,f_auto,q_auto:eco,dpr_1/nqzhpurnanwpjofkxp0n'}], 'cse_image': [{'src': 'https://images.crunchbase.com/image/upload/c_pad,h_256,w_256,f_auto,q_auto:eco,dpr_1/nqzhpurnanwpjofkxp0n'}]}}]}\u001b[0m\n",
      "\u001b[32m2024-08-31 15:45:22.035\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mscrapfly_scrapers.crunchbase\u001b[0m:\u001b[36mscrape_company\u001b[0m:\u001b[36m62\u001b[0m - \u001b[1mscraping company: https://www.crunchbase.com/organization/imagine-pediatrics-96a9/people\u001b[0m\n",
      "CRITICAL:root:<-- 422 | ERR::ASP::SHIELD_PROTECTION_FAILED - The ASP shield failed to solve the challenge against the anti scrapping protection - Unable to bypass www.crunchbase.com. Checkout the related doc: https://scrapfly.io/docs/scrape-api/anti-scraping-protection#maximize_success_rate\n"
     ]
    },
    {
     "ename": "ScrapflyAspError",
     "evalue": "<-- 422 | ERR::ASP::SHIELD_PROTECTION_FAILED - The ASP shield failed to solve the challenge against the anti scrapping protection - Unable to bypass www.crunchbase.com. Checkout the related doc: https://scrapfly.io/docs/scrape-api/anti-scraping-protection#maximize_success_rate",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mScrapflyAspError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcrunchbase\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m overview_md \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m crunchbase\u001b[38;5;241m.\u001b[39mrun(target)\n",
      "File \u001b[0;32m~/company-detective/src/crunchbase/__init__.py:49\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(target)\u001b[0m\n\u001b[1;32m     47\u001b[0m crunchbase_raw_response \u001b[38;5;241m=\u001b[39m cache\u001b[38;5;241m.\u001b[39mget(url)\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m crunchbase_raw_response:\n\u001b[0;32m---> 49\u001b[0m     crunchbase_raw_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m scrapfly_scrapers\u001b[38;5;241m.\u001b[39mcrunchbase\u001b[38;5;241m.\u001b[39mscrape_company(url)\n\u001b[1;32m     50\u001b[0m     cache\u001b[38;5;241m.\u001b[39mset(url, crunchbase_raw_response, expire\u001b[38;5;241m=\u001b[39mtimedelta(days\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m14\u001b[39m)\u001b[38;5;241m.\u001b[39mtotal_seconds())\n\u001b[1;32m     52\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpdating cache with Crunchbase response: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, crunchbase_raw_response)\n",
      "File \u001b[0;32m~/company-detective/src/scrapfly_scrapers/crunchbase.py:63\u001b[0m, in \u001b[0;36mscrape_company\u001b[0;34m(url)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# note: we use /people tab because it contains the most data:\u001b[39;00m\n\u001b[1;32m     62\u001b[0m log\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscraping company: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 63\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m SCRAPFLY\u001b[38;5;241m.\u001b[39masync_scrape(ScrapeConfig(url, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mBASE_CONFIG))\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m parse_company(result)\n",
      "File \u001b[0;32m~/company-detective/.venv/lib/python3.10/site-packages/scrapfly/client.py:326\u001b[0m, in \u001b[0;36mScrapflyClient.async_scrape\u001b[0;34m(self, scrape_config, loop)\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m loop \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    324\u001b[0m     loop \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mget_running_loop()\n\u001b[0;32m--> 326\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mrun_in_executor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39masync_executor, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscrape, scrape_config)\n",
      "File \u001b[0;32m/usr/lib/python3.10/concurrent/futures/thread.py:58\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mset_exception(exc)\n",
      "File \u001b[0;32m~/company-detective/.venv/lib/python3.10/site-packages/backoff/_sync.py:105\u001b[0m, in \u001b[0;36mretry_exception.<locals>.retry\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     96\u001b[0m details \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m\"\u001b[39m: target,\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124margs\u001b[39m\u001b[38;5;124m\"\u001b[39m: args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124melapsed\u001b[39m\u001b[38;5;124m\"\u001b[39m: elapsed,\n\u001b[1;32m    102\u001b[0m }\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 105\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mtarget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m exception \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    107\u001b[0m     max_tries_exceeded \u001b[38;5;241m=\u001b[39m (tries \u001b[38;5;241m==\u001b[39m max_tries_value)\n",
      "File \u001b[0;32m~/company-detective/.venv/lib/python3.10/site-packages/scrapfly/client.py:419\u001b[0m, in \u001b[0;36mScrapflyClient.scrape\u001b[0;34m(self, scrape_config, no_raise)\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m no_raise \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, ScrapflyError) \u001b[38;5;129;01mand\u001b[39;00m e\u001b[38;5;241m.\u001b[39mapi_response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    417\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m e\u001b[38;5;241m.\u001b[39mapi_response\n\u001b[0;32m--> 419\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[0;32m~/company-detective/.venv/lib/python3.10/site-packages/scrapfly/client.py:408\u001b[0m, in \u001b[0;36mScrapflyClient.scrape\u001b[0;34m(self, scrape_config, no_raise)\u001b[0m\n\u001b[1;32m    406\u001b[0m request_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_scrape_request(scrape_config\u001b[38;5;241m=\u001b[39mscrape_config)\n\u001b[1;32m    407\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_http_handler(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mrequest_data)\n\u001b[0;32m--> 408\u001b[0m scrape_api_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscrape_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscrape_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreporter\u001b[38;5;241m.\u001b[39mreport(scrape_api_response\u001b[38;5;241m=\u001b[39mscrape_api_response)\n\u001b[1;32m    412\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m scrape_api_response\n",
      "File \u001b[0;32m~/company-detective/.venv/lib/python3.10/site-packages/scrapfly/client.py:423\u001b[0m, in \u001b[0;36mScrapflyClient._handle_response\u001b[0;34m(self, response, scrape_config)\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_handle_response\u001b[39m(\u001b[38;5;28mself\u001b[39m, response:Response, scrape_config:ScrapeConfig) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ScrapeApiResponse:\n\u001b[1;32m    422\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 423\u001b[0m         api_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle_api_response\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    424\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    425\u001b[0m \u001b[43m            \u001b[49m\u001b[43mscrape_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscrape_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    426\u001b[0m \u001b[43m            \u001b[49m\u001b[43mraise_on_upstream_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscrape_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_on_upstream_error\u001b[49m\n\u001b[1;32m    427\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    429\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m scrape_config\u001b[38;5;241m.\u001b[39mmethod \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHEAD\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    430\u001b[0m             logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<-- [\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m] \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m | \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (\n\u001b[1;32m    431\u001b[0m                 api_response\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mstatus_code,\n\u001b[1;32m    432\u001b[0m                 api_response\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mreason,\n\u001b[1;32m    433\u001b[0m                 api_response\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mrequest\u001b[38;5;241m.\u001b[39murl,\n\u001b[1;32m    434\u001b[0m                 \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    435\u001b[0m             ))\n",
      "File \u001b[0;32m~/company-detective/.venv/lib/python3.10/site-packages/scrapfly/client.py:605\u001b[0m, in \u001b[0;36mScrapflyClient._handle_api_response\u001b[0;34m(self, response, scrape_config, raise_on_upstream_error)\u001b[0m\n\u001b[1;32m    596\u001b[0m         body \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mcontent\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    598\u001b[0m api_response:ScrapeApiResponse \u001b[38;5;241m=\u001b[39m ScrapeApiResponse(\n\u001b[1;32m    599\u001b[0m     response\u001b[38;5;241m=\u001b[39mresponse,\n\u001b[1;32m    600\u001b[0m     request\u001b[38;5;241m=\u001b[39mresponse\u001b[38;5;241m.\u001b[39mrequest,\n\u001b[1;32m    601\u001b[0m     api_result\u001b[38;5;241m=\u001b[39mbody,\n\u001b[1;32m    602\u001b[0m     scrape_config\u001b[38;5;241m=\u001b[39mscrape_config\n\u001b[1;32m    603\u001b[0m )\n\u001b[0;32m--> 605\u001b[0m \u001b[43mapi_response\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraise_on_upstream_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mraise_on_upstream_error\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m api_response\n",
      "File \u001b[0;32m~/company-detective/.venv/lib/python3.10/site-packages/scrapfly/api_response.py:435\u001b[0m, in \u001b[0;36mScrapeApiResponse.raise_for_result\u001b[0;34m(self, raise_on_upstream_error)\u001b[0m\n\u001b[1;32m    433\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m error\n\u001b[1;32m    434\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 435\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error\n",
      "\u001b[0;31mScrapflyAspError\u001b[0m: <-- 422 | ERR::ASP::SHIELD_PROTECTION_FAILED - The ASP shield failed to solve the challenge against the anti scrapping protection - Unable to bypass www.crunchbase.com. Checkout the related doc: https://scrapfly.io/docs/scrape-api/anti-scraping-protection#maximize_success_rate"
     ]
    }
   ],
   "source": [
    "import crunchbase\n",
    "\n",
    "overview_md = await crunchbase.run(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(overview_md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CRITICAL:root:<-- 400 | Language e is not supported. Checkout the related doc: https://scrapfly.io/docs/scrape-api/specification#api_param_lang\n"
     ]
    },
    {
     "ename": "ApiHttpClientError",
     "evalue": "<-- 400 | Language e is not supported. Checkout the related doc: https://scrapfly.io/docs/scrape-api/specification#api_param_lang",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/company-detective/.venv/lib/python3.10/site-packages/scrapfly/api_response.py:402\u001b[0m, in \u001b[0;36mScrapeApiResponse.raise_for_result\u001b[0;34m(self, raise_on_upstream_error)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 402\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/company-detective/.venv/lib/python3.10/site-packages/requests/models.py:1024\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1024\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 400 Client Error: Bad Request for url: https://api.scrapfly.io/scrape?key=scp-live-2ad3a3be49bf44bfbb4dd9bf4e169cad&url=https%3A%2F%2Fwww.crunchbase.com%2Forganization%2F98point6&country=us&asp=true&debug=true&proxy_pool=public_residential_pool&lang=e%2Cn%2C-%2CU%2CS",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mApiHttpClientError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 40\u001b[0m\n\u001b[1;32m      7\u001b[0m BASE_CONFIG \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m# Crunchbase.com requires Anti Scraping Protection bypass feature.\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m# for more: https://scrapfly.io/docs/scrape-api/anti-scraping-protection\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdebug\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     15\u001b[0m }\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# def parse_company(result: ScrapeApiResponse) -> CompanyData:\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m#     \"\"\"parse company page for company and employee data\"\"\"\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m#     # the app cache data can be in one of two places:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m#         # \"employees\": _reduce_employee_dataset(employees),\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m#     }\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m scrapfly_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m SCRAPFLY\u001b[38;5;241m.\u001b[39masync_scrape(ScrapeConfig(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://www.crunchbase.com/organization/98point6\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mBASE_CONFIG))\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpprint\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pprint\n\u001b[1;32m     43\u001b[0m pprint(scrapfly_result)\n",
      "File \u001b[0;32m~/company-detective/.venv/lib/python3.10/site-packages/scrapfly/client.py:326\u001b[0m, in \u001b[0;36mScrapflyClient.async_scrape\u001b[0;34m(self, scrape_config, loop)\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m loop \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    324\u001b[0m     loop \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mget_running_loop()\n\u001b[0;32m--> 326\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mrun_in_executor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39masync_executor, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscrape, scrape_config)\n",
      "File \u001b[0;32m/usr/lib/python3.10/concurrent/futures/thread.py:58\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mset_exception(exc)\n",
      "File \u001b[0;32m~/company-detective/.venv/lib/python3.10/site-packages/backoff/_sync.py:105\u001b[0m, in \u001b[0;36mretry_exception.<locals>.retry\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     96\u001b[0m details \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m\"\u001b[39m: target,\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124margs\u001b[39m\u001b[38;5;124m\"\u001b[39m: args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124melapsed\u001b[39m\u001b[38;5;124m\"\u001b[39m: elapsed,\n\u001b[1;32m    102\u001b[0m }\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 105\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mtarget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m exception \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    107\u001b[0m     max_tries_exceeded \u001b[38;5;241m=\u001b[39m (tries \u001b[38;5;241m==\u001b[39m max_tries_value)\n",
      "File \u001b[0;32m~/company-detective/.venv/lib/python3.10/site-packages/scrapfly/client.py:419\u001b[0m, in \u001b[0;36mScrapflyClient.scrape\u001b[0;34m(self, scrape_config, no_raise)\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m no_raise \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, ScrapflyError) \u001b[38;5;129;01mand\u001b[39;00m e\u001b[38;5;241m.\u001b[39mapi_response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    417\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m e\u001b[38;5;241m.\u001b[39mapi_response\n\u001b[0;32m--> 419\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[0;32m~/company-detective/.venv/lib/python3.10/site-packages/scrapfly/client.py:408\u001b[0m, in \u001b[0;36mScrapflyClient.scrape\u001b[0;34m(self, scrape_config, no_raise)\u001b[0m\n\u001b[1;32m    406\u001b[0m request_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_scrape_request(scrape_config\u001b[38;5;241m=\u001b[39mscrape_config)\n\u001b[1;32m    407\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_http_handler(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mrequest_data)\n\u001b[0;32m--> 408\u001b[0m scrape_api_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscrape_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscrape_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreporter\u001b[38;5;241m.\u001b[39mreport(scrape_api_response\u001b[38;5;241m=\u001b[39mscrape_api_response)\n\u001b[1;32m    412\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m scrape_api_response\n",
      "File \u001b[0;32m~/company-detective/.venv/lib/python3.10/site-packages/scrapfly/client.py:423\u001b[0m, in \u001b[0;36mScrapflyClient._handle_response\u001b[0;34m(self, response, scrape_config)\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_handle_response\u001b[39m(\u001b[38;5;28mself\u001b[39m, response:Response, scrape_config:ScrapeConfig) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ScrapeApiResponse:\n\u001b[1;32m    422\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 423\u001b[0m         api_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle_api_response\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    424\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    425\u001b[0m \u001b[43m            \u001b[49m\u001b[43mscrape_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscrape_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    426\u001b[0m \u001b[43m            \u001b[49m\u001b[43mraise_on_upstream_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscrape_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_on_upstream_error\u001b[49m\n\u001b[1;32m    427\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    429\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m scrape_config\u001b[38;5;241m.\u001b[39mmethod \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHEAD\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    430\u001b[0m             logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<-- [\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m] \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m | \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (\n\u001b[1;32m    431\u001b[0m                 api_response\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mstatus_code,\n\u001b[1;32m    432\u001b[0m                 api_response\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mreason,\n\u001b[1;32m    433\u001b[0m                 api_response\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mrequest\u001b[38;5;241m.\u001b[39murl,\n\u001b[1;32m    434\u001b[0m                 \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    435\u001b[0m             ))\n",
      "File \u001b[0;32m~/company-detective/.venv/lib/python3.10/site-packages/scrapfly/client.py:605\u001b[0m, in \u001b[0;36mScrapflyClient._handle_api_response\u001b[0;34m(self, response, scrape_config, raise_on_upstream_error)\u001b[0m\n\u001b[1;32m    596\u001b[0m         body \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mcontent\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    598\u001b[0m api_response:ScrapeApiResponse \u001b[38;5;241m=\u001b[39m ScrapeApiResponse(\n\u001b[1;32m    599\u001b[0m     response\u001b[38;5;241m=\u001b[39mresponse,\n\u001b[1;32m    600\u001b[0m     request\u001b[38;5;241m=\u001b[39mresponse\u001b[38;5;241m.\u001b[39mrequest,\n\u001b[1;32m    601\u001b[0m     api_result\u001b[38;5;241m=\u001b[39mbody,\n\u001b[1;32m    602\u001b[0m     scrape_config\u001b[38;5;241m=\u001b[39mscrape_config\n\u001b[1;32m    603\u001b[0m )\n\u001b[0;32m--> 605\u001b[0m \u001b[43mapi_response\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraise_on_upstream_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mraise_on_upstream_error\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m api_response\n",
      "File \u001b[0;32m~/company-detective/.venv/lib/python3.10/site-packages/scrapfly/api_response.py:417\u001b[0m, in \u001b[0;36mScrapeApiResponse.raise_for_result\u001b[0;34m(self, raise_on_upstream_error)\u001b[0m\n\u001b[1;32m    406\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m ApiHttpServerError(\n\u001b[1;32m    407\u001b[0m                 request\u001b[38;5;241m=\u001b[39me\u001b[38;5;241m.\u001b[39mrequest,\n\u001b[1;32m    408\u001b[0m                 response\u001b[38;5;241m=\u001b[39me\u001b[38;5;241m.\u001b[39mresponse,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    414\u001b[0m                 api_response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    415\u001b[0m             ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    416\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 417\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m ApiHttpClientError(\n\u001b[1;32m    418\u001b[0m                 request\u001b[38;5;241m=\u001b[39me\u001b[38;5;241m.\u001b[39mrequest,\n\u001b[1;32m    419\u001b[0m                 response\u001b[38;5;241m=\u001b[39me\u001b[38;5;241m.\u001b[39mresponse,\n\u001b[1;32m    420\u001b[0m                 message\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresult[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    421\u001b[0m                 code\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    422\u001b[0m                 resource\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAPI\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    423\u001b[0m                 http_status_code\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresult[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttp_code\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    424\u001b[0m                 documentation_url\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresult\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlinks\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m    425\u001b[0m                 api_response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    426\u001b[0m             ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    427\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresult[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstatus\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDONE\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscrape_success \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m    428\u001b[0m     error \u001b[38;5;241m=\u001b[39m ErrorFactory\u001b[38;5;241m.\u001b[39mcreate(api_response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mApiHttpClientError\u001b[0m: <-- 400 | Language e is not supported. Checkout the related doc: https://scrapfly.io/docs/scrape-api/specification#api_param_lang"
     ]
    }
   ],
   "source": [
    "# Version 2: A rewrite\n",
    "\n",
    "import os\n",
    "from scrapfly import ScrapeApiResponse, ScrapeConfig, ScrapflyClient\n",
    "\n",
    "SCRAPFLY = ScrapflyClient(key=os.environ[\"SCRAPFLY_KEY\"])\n",
    "BASE_CONFIG = {\n",
    "    # Crunchbase.com requires Anti Scraping Protection bypass feature.\n",
    "    # for more: https://scrapfly.io/docs/scrape-api/anti-scraping-protection\n",
    "    \"asp\": True,\n",
    "    \"proxy_pool\": ScrapeConfig.PUBLIC_RESIDENTIAL_POOL,\n",
    "    \"country\": \"US,CA\",\n",
    "    \"lang\": \"en-US\",\n",
    "    \"debug\": True\n",
    "}\n",
    "\n",
    "# def parse_company(result: ScrapeApiResponse) -> CompanyData:\n",
    "#     \"\"\"parse company page for company and employee data\"\"\"\n",
    "#     # the app cache data can be in one of two places:\n",
    "#     app_state_data = result.selector.css(\"script#ng-state::text\").get()\n",
    "#     if not app_state_data:\n",
    "#         app_state_data = _unescape_angular(result.selector.css(\"script#client-app-state::text\").get() or \"\")\n",
    "#     app_state_data = json.loads(app_state_data)\n",
    "#     # there are multiple caches:\n",
    "#     cache_keys = list(app_state_data[\"HttpState\"])\n",
    "#     # Organization data can be found in this cache:\n",
    "#     data_cache_key = next(key for key in cache_keys if \"entities/organizations/\" in key)\n",
    "#     # Some employee/contact data can be found in this key:\n",
    "#     # TODO: Consider refactoring this if we want to include employee data\n",
    "#     # people_cache_key = next(key for key in cache_keys if \"/data/searches/contacts\" in key)\n",
    "\n",
    "#     organization = app_state_data[\"HttpState\"][data_cache_key][\"data\"]\n",
    "#     # employees = app_state_data[\"HttpState\"][people_cache_key][\"data\"]\n",
    "#     return {\n",
    "#         \"organization\": _reduce_organization_dataset(organization),\n",
    "#         \"employees\": [],\n",
    "#         # \"employees\": _reduce_employee_dataset(employees),\n",
    "#     }\n",
    "\n",
    "scrapfly_result = await SCRAPFLY.async_scrape(ScrapeConfig(\"https://www.crunchbase.com/organization/98point6\", **BASE_CONFIG))\n",
    "\n",
    "from pprint import pprint\n",
    "pprint(scrapfly_result)\n",
    "\n",
    "# async def scrape_company(url: str) -> CompanyData:\n",
    "#     \"\"\"scrape crunchbase company page for organization and employee data\"\"\"\n",
    "#     # note: we use /people tab because it contains the most data:\n",
    "#     log.info(f\"scraping company: {url}\")\n",
    "#     result = await SCRAPFLY.async_scrape(ScrapeConfig(url, **BASE_CONFIG))\n",
    "#     return parse_company(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Things to try if it doesn't work\n",
    "# proxy_pool=public_residential_pool\n",
    "# country=us\n",
    "# lang=en"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
