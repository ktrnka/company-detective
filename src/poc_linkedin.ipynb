{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapfly_scrapers.linkedin as linkedin\n",
    "\n",
    "# enable scrapfly cache\n",
    "linkedin.BASE_CONFIG[\"cache\"] = True\n",
    "linkedin.BASE_CONFIG[\"debug\"] = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"running Linkedin scrape and saving results to ./results directory\")\n",
    "\n",
    "# profile_data = await linkedin.scrape_profile(\n",
    "#     urls=[\n",
    "#         \"https://www.linkedin.com/in/williamhgates\"\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "from pprint import pprint\n",
    "# pprint(profile_data)\n",
    "\n",
    "\n",
    "# with open(output.joinpath(\"profile.json\"), \"w\", encoding=\"utf-8\") as file:\n",
    "#     json.dump(profile_data, file, indent=2, ensure_ascii=False)    \n",
    "\n",
    "# company_data = await linkedin.scrape_company(\n",
    "#     urls=[\n",
    "#         \"https://linkedin.com/company/microsoft\",\n",
    "#         \"https://linkedin.com/company/google\",\n",
    "#         \"https://linkedin.com/company/apple\"\n",
    "#     ]\n",
    "# )\n",
    "# with open(output.joinpath(\"company.json\"), \"w\", encoding=\"utf-8\") as file:\n",
    "#     json.dump(company_data, file, indent=2, ensure_ascii=False)    \n",
    "\n",
    "# job_search_data = await linkedin.scrape_job_search(\n",
    "#     # it include other search parameters, refer to the search pages on browser for more details\n",
    "#     keyword=\"Python Developer\",\n",
    "#     location=\"United States\",\n",
    "#     max_pages=3\n",
    "# )\n",
    "# with open(output.joinpath(\"job_search.json\"), \"w\", encoding=\"utf-8\") as file:\n",
    "#     json.dump(job_search_data, file, indent=2, ensure_ascii=False)    \n",
    "\n",
    "# job_data = await linkedin.scrape_jobs(\n",
    "#     urls=[\n",
    "#         \"https://www.linkedin.com/jobs/view/data-center-engineering-operations-engineer-hyd-infinity-dceo-at-amazon-web-services-aws-4017265505\",\n",
    "#         \"https://www.linkedin.com/jobs/view/content-strategist-google-cloud-content-strategy-and-experience-at-google-4015776107\",\n",
    "#         \"https://www.linkedin.com/jobs/view/sr-content-marketing-manager-brand-protection-brand-protection-at-amazon-4007942181\"\n",
    "#     ]\n",
    "# )\n",
    "# with open(output.joinpath(\"jobs.json\"), \"w\", encoding=\"utf-8\") as file:\n",
    "#     json.dump(job_data, file, indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google_search import search, SearchResult\n",
    "\n",
    "linkedin_pages = list(search(\"site:www.linkedin.com/company/ intitle:98point6\"))\n",
    "linkedin_pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Took 26 seconds?!?\n",
    "\n",
    "company_data = await linkedin.scrape_company(\n",
    "    urls=[\n",
    "        linkedin_pages[0].link\n",
    "    ]\n",
    ")\n",
    "\n",
    "pprint(company_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_results = await linkedin.scrape_job_search(\n",
    "    keyword=\"software engineer\",\n",
    "    location=\"United States\",\n",
    "    max_pages=1\n",
    ")\n",
    "\n",
    "pprint(job_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "\n",
    "# def form_urls_params(keyword, location):\n",
    "    #     \"\"\"form the job search URL params\"\"\"\n",
    "    #     params = {\n",
    "    #         \"keywords\": quote_plus(keyword),\n",
    "    #         \"location\": location,\n",
    "    #     }\n",
    "    #     return urlencode(params)\n",
    "\n",
    "    # first_page_url = \"https://www.linkedin.com/jobs/search?\" + form_urls_params(keyword, location)\n",
    "    # first_page = await SCRAPFLY.async_scrape(ScrapeConfig(first_page_url, **BASE_CONFIG))\n",
    "\n",
    "    # selector = response.selector\n",
    "    # total_results = selector.xpath(\"//span[contains(@class, 'job-count')]/text()\").get()\n",
    "    # total_results = int(total_results.replace(\",\", \"\").replace(\"+\", \"\")) if total_results else None\n",
    "    # data = []\n",
    "    # for element in selector.xpath(\"//section[contains(@class, 'results-list')]/ul/li\"):\n",
    "    #     data.append({\n",
    "    #         \"title\": element.xpath(\".//div/a/span/text()\").get().strip(),\n",
    "    #         \"company\": element.xpath(\".//div/div[contains(@class, 'info')]/h4/a/text()\").get().strip(),\n",
    "    #         \"address\": element.xpath(\".//div/div[contains(@class, 'info')]/div/span/text()\").get().strip(),\n",
    "    #         \"timeAdded\": element.xpath(\".//div/div[contains(@class, 'info')]/div/time/@datetime\").get(),\n",
    "    #         \"jobUrl\": element.xpath(\".//div/a/@href\").get().split(\"?\")[0],\n",
    "    #         \"companyUrl\": element.xpath(\".//div/div[contains(@class, 'info')]/h4/a/@href\").get().split(\"?\")[0],\n",
    "    #         \"salary\": strip_text(element.xpath(\".//span[contains(@class, 'salary')]/text()\").get())\n",
    "    #     })\n",
    "    # return {\"data\": data, \"total_results\": total_results}\n",
    "\n",
    "def build_job_search_url(keyword, location):\n",
    "    params = {\n",
    "        \"keywords\": linkedin.quote_plus(keyword),\n",
    "        \"location\": location,\n",
    "    }\n",
    "    return f\"https://www.linkedin.com/jobs/search/?keywords={linkedin.urlencode(params)}\"\n",
    "\n",
    "url = build_job_search_url(\"software engineer\", \"United States\")\n",
    "url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manually scraping the job page so that I can iterate quickly on the xpath\n",
    "\n",
    "from scrapfly import ScrapeConfig, ScrapflyClient\n",
    "import os\n",
    "scrape_config = ScrapeConfig(build_job_search_url(\"software engineer\", \"United States\"), **linkedin.BASE_CONFIG)\n",
    "scrape_config\n",
    "\n",
    "client = ScrapflyClient(key=os.environ[\"SCRAPFLY_KEY\"])\n",
    "response = client.scrape(scrape_config)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector = response.selector\n",
    "total_results = selector.xpath(\"//span[contains(@class, 'job-count')]/text()\").get()\n",
    "total_results = int(total_results.replace(\",\", \"\").replace(\"+\", \"\")) if total_results else None\n",
    "\n",
    "print(f\"Total results: {total_results}\")\n",
    "\n",
    "data = []\n",
    "for element in selector.xpath(\"//section[contains(@class, 'results-list')]/ul/li\"):\n",
    "    data.append({\n",
    "        \"title\": element.xpath(\".//div/a/span/text()\").get().strip(),\n",
    "        \"company\": element.xpath(\".//div/div[contains(@class, 'info')]/h4/a/text()\").get().strip(),\n",
    "        \"address\": element.xpath(\".//div/div[contains(@class, 'info')]/div/span/text()\").get().strip(),\n",
    "        \"timeAdded\": element.xpath(\".//div/div[contains(@class, 'info')]/div/time/@datetime\").get(),\n",
    "        \"jobUrl\": element.xpath(\".//div/a/@href\").get().split(\"?\")[0],\n",
    "        \"companyUrl\": element.xpath(\".//div/div[contains(@class, 'info')]/h4/a/@href\").get().split(\"?\")[0],\n",
    "        \"salary\": linkedin.strip_text(element.xpath(\".//span[contains(@class, 'salary')]/text()\").get())\n",
    "    })\n",
    "    print(data[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, HttpUrl\n",
    "from typing import Optional\n",
    "\n",
    "class JobListing(BaseModel):\n",
    "    title: str\n",
    "    company: str\n",
    "    address: str\n",
    "    timeAdded: str\n",
    "    jobUrl: HttpUrl\n",
    "    companyUrl: HttpUrl\n",
    "    salary: Optional[str]\n",
    "\n",
    "for result in data:\n",
    "    job = JobListing(**result)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
