{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web scraping tests\n",
    "\n",
    "A lot of overall program time is taken in sequential web scraping. If I could parallelize it, that would help tremendously. Similarly, a fair amount of time is spent in content extraction.\n",
    "\n",
    "To test:\n",
    "- Compare the content extractor I'm currently using vs Scrapfly's content extractor on some webpages\n",
    "- Compare the rough time it'd take to parallel scrape \"manually\" vs scrapfly at max concurrency\n",
    "- Estimate costs with scrapfly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core import Seed, init\n",
    "\n",
    "# Comment out to reduce caching\n",
    "# init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seed(company='98point6', deprecated_product='98point6', domain='98point6.com', keywords=None, deprecated_require_news_backlinks=False, deprecated_require_reddit_backlinks=False, primary_product=None, feature_flags=FeatureFlags(require_news_backlinks=False, require_reddit_backlinks=False))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from stored_config import Company\n",
    "\n",
    "target = Company.find_first(\"98point6\").to_core_company()\n",
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_sources.news.search import find_news_articles\n",
    "search_results = find_news_articles(target, num_results=40)\n",
    "urls = [result.link for result in search_results]\n",
    "search_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# efficiency test harness\n",
    "import time\n",
    "\n",
    "def test_scrape(urls, scrape_fn):\n",
    "    start = time.time()\n",
    "    result = scrape_fn(urls)\n",
    "\n",
    "    duration_sec = time.time() - start\n",
    "    sec_per_url = duration_sec / len(urls)\n",
    "    urls_per_sec = len(urls) / duration_sec\n",
    "\n",
    "    print(f\"{scrape_fn.__name__}: {sec_per_url:.1f} sec per URL, {urls_per_sec:,.0f} URLs/s ({duration_sec:.1f} sec total)\")\n",
    "\n",
    "    return result\n",
    "\n",
    "async def test_ascrape(urls, scrape_fn, **kwargs):\n",
    "    start = time.time()\n",
    "    result = await scrape_fn(urls, **kwargs)\n",
    "\n",
    "    duration_sec = time.time() - start\n",
    "    sec_per_url = duration_sec / len(urls)\n",
    "    urls_per_sec = len(urls) / duration_sec\n",
    "\n",
    "    print(f\"{scrape_fn.__name__}: {sec_per_url:.1f} sec per URL, {urls_per_sec:,.0f} URLs/s ({duration_sec:.1f} sec total)\")\n",
    "\n",
    "    return result\n",
    "\n",
    "def dummy_fn(urls):\n",
    "    return urls\n",
    "\n",
    "async def async_dummy_fn(urls):\n",
    "    return urls\n",
    "\n",
    "\n",
    "dummy_result = test_scrape(search_results, dummy_fn)\n",
    "dummy_async_result = await test_ascrape(search_results, async_dummy_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from utils.scrape import request_article\n",
    "\n",
    "def serial_scrape(urls: List[str]):\n",
    "    # Note: For testing, don't run core.init() before so that we don't get a full cache\n",
    "    responses = [request_article(url) for url in urls]\n",
    "\n",
    "    # Show status codes\n",
    "    for response in responses:\n",
    "        print(response.url, response.status_code, type(response))\n",
    "\n",
    "    return responses\n",
    "\n",
    "# baseline_responses = test_scrape(urls, serial_scrape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.async_scrape import scrape\n",
    "from core import cache\n",
    "\n",
    "async_responses = await test_ascrape(urls, scrape, cache=cache)\n",
    "async_responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "Counter([response.status for response in async_responses])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter([response.status_code for response in baseline_responses])\n",
    "# len(baseline_responses), len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core import cache\n",
    "\n",
    "for key in cache.iterkeys():\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
