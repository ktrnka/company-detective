{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "from typing import Iterable, List, Dict, Union\n",
    "import json\n",
    "import newspaper\n",
    "\n",
    "from urllib.parse import urlparse\n",
    "from data_sources.news.scrape import remove_img_tags, article_to_markdown\n",
    "\n",
    "def response_to_article(\n",
    "    response: scrapy.http.Response,\n",
    ") -> newspaper.Article:\n",
    "    \"\"\"Parse the scrapy response into a newspaper Article\"\"\"\n",
    "    article = newspaper.article(\n",
    "        response.url,\n",
    "        language=\"en\",\n",
    "        # Remove images to prevent downloading them, which crashes\n",
    "        input_html=remove_img_tags(response.text),\n",
    "        fetch_images=False,\n",
    "    )\n",
    "    article.parse()\n",
    "    return article\n",
    "\n",
    "\n",
    "class CompanySpider(scrapy.Spider):\n",
    "    name = \"company\"\n",
    "    allowed_domains: List[str]\n",
    "    start_urls: List[str]\n",
    "\n",
    "    def __init__(self, domain: str, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.allowed_domains = [domain]\n",
    "        self.start_urls = [f\"http://{domain}\"]\n",
    "\n",
    "    def parse(self, response: scrapy.http.Response) -> Iterable[Union[Dict, scrapy.http.Request]]:\n",
    "        parsed_article = response_to_article(response)\n",
    "        markdown = article_to_markdown(parsed_article)\n",
    "        yield {\n",
    "            \"url\": response.url,\n",
    "            \"html\": response.text,\n",
    "            \"markdown\": markdown,\n",
    "        }\n",
    "\n",
    "        # Follow links to other pages within the same domain\n",
    "        for href in response.css(\"a::attr(href)\").getall():\n",
    "            if href.startswith(\"/\"):\n",
    "                href = response.urljoin(href)\n",
    "            url_parts = urlparse(href)\n",
    "            if url_parts.netloc.endswith(self.allowed_domains[0]):\n",
    "                yield scrapy.Request(href, callback=self.parse)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_company_webpage(domain: str, depth_limit: int=1):\n",
    "    filename = f\"../output/data/scrapy/{domain}.json\"\n",
    "    process = CrawlerProcess(settings={\n",
    "        \"FEEDS\": {\n",
    "            filename: {\n",
    "                \"format\": \"json\",\n",
    "                \"overwrite\": True,\n",
    "                },\n",
    "        },\n",
    "        \"DOWNLOAD_DELAY\": 1,\n",
    "        # \"DEFAULT_REQUEST_HEADERS\": {\n",
    "        #     \"User-Agent\": \"Mozilla/5.0 (compatible; CompanyBot/1.0; +http://example.com/bot)\",\n",
    "        #     \"Accept-Language\": \"en\",\n",
    "        # },\n",
    "        \"DEPTH_LIMIT\": depth_limit,\n",
    "    })\n",
    "\n",
    "    process.crawl(CompanySpider, domain=domain)\n",
    "\n",
    "    # NOTE: This can only be run once per process, so in a notebook we need to restart the kernel\n",
    "    process.start()\n",
    "\n",
    "    return filename\n",
    "\n",
    "\n",
    "json_file = crawl_company_webpage(\"98point6.com\", depth_limit=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(json_file, \"r\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "total_chars = 0\n",
    "for page in sorted(data, key=lambda p: p[\"url\"]):\n",
    "    truncated_page = page[\"markdown\"][:4000]\n",
    "    print(f\"\"\"\n",
    "# {page[\"url\"]}\n",
    "{len(page[\"markdown\"]):,} chars in markdown\n",
    "\n",
    "{truncated_page}...\n",
    "\"\"\")\n",
    "    total_chars += len(truncated_page)\n",
    "\n",
    "print(f\"Total chars: {total_chars:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
