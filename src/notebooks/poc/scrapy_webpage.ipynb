{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "from typing import Iterable, List, Dict, Union\n",
    "from dataclasses import dataclass\n",
    "import json\n",
    "import newspaper\n",
    "\n",
    "from urllib.parse import urlparse\n",
    "from data_sources.news.scrape import remove_img_tags\n",
    "\n",
    "def response_to_article(\n",
    "    response: scrapy.http.Response,\n",
    ") -> newspaper.Article:\n",
    "    \"\"\"Parse the response from a URL into a newspaper Article\"\"\"\n",
    "    article = newspaper.article(\n",
    "        response.url,\n",
    "        language=\"en\",\n",
    "        # Remove images to prevent downloading them, which crashes\n",
    "        input_html=remove_img_tags(response.text),\n",
    "        fetch_images=False,\n",
    "    )\n",
    "    article.parse()\n",
    "    return article\n",
    "\n",
    "\n",
    "def article_to_markdown(article: newspaper.Article, max_chars=None) -> str:\n",
    "    \"\"\"Format a parsed newspaper Article into Markdown for summarization\"\"\"\n",
    "    header = article.title\n",
    "    if article.authors:\n",
    "        header += f\" by {', '.join(article.authors)}\"\n",
    "    if article.publish_date:\n",
    "        header += f\" on {article.publish_date.strftime('%Y-%m-%d')}\"\n",
    "\n",
    "    text = article.text\n",
    "    if max_chars:\n",
    "        text = text[:max_chars]\n",
    "\n",
    "    header = f\"# [{header}]({article.url})\"\n",
    "\n",
    "    return f\"{header}\\n{text}\"\n",
    "\n",
    "\n",
    "class CompanySpider(scrapy.Spider):\n",
    "    name = \"company\"\n",
    "    allowed_domains: List[str]\n",
    "    start_urls: List[str]\n",
    "\n",
    "    def __init__(self, domain: str, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.allowed_domains = [domain]\n",
    "        self.start_urls = [f\"http://{domain}\"]\n",
    "\n",
    "    def parse(self, response: scrapy.http.Response) -> Iterable[Union[Dict, scrapy.http.Request]]:\n",
    "        # TODO: Replace this with extraction code\n",
    "        # page_content = response.css(\"body\").get()\n",
    "        parsed_article = response_to_article(response)\n",
    "        markdown = article_to_markdown(parsed_article)\n",
    "        yield {\n",
    "            \"url\": response.url,\n",
    "            \"html\": response.text,\n",
    "            \"markdown\": markdown,\n",
    "        }\n",
    "\n",
    "        # Follow links to other pages within the same domain\n",
    "        for href in response.css(\"a::attr(href)\").getall():\n",
    "            if href.startswith(\"/\"):\n",
    "                href = response.urljoin(href)\n",
    "            url_parts = urlparse(href)\n",
    "            # if self.allowed_domains[0] in href:\n",
    "            if url_parts.netloc.endswith(self.allowed_domains[0]):\n",
    "                yield scrapy.Request(href, callback=self.parse)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def crawl_company_webpage(domain: str):\n",
    "    process = CrawlerProcess(settings={\n",
    "        \"FEEDS\": {\n",
    "            \"output.json\": {\n",
    "                \"format\": \"json\",\n",
    "                \"overwrite\": True,\n",
    "                },\n",
    "        },\n",
    "        \"DOWNLOAD_DELAY\": 1,\n",
    "        # \"DEFAULT_REQUEST_HEADERS\": {\n",
    "        #     \"User-Agent\": \"Mozilla/5.0 (compatible; CompanyBot/1.0; +http://example.com/bot)\",\n",
    "        #     \"Accept-Language\": \"en\",\n",
    "        # },\n",
    "        # NOTE: Setting this to 0 will crawl everything\n",
    "        \"DEPTH_LIMIT\": 1,\n",
    "    })\n",
    "\n",
    "    process.crawl(CompanySpider, domain=domain)\n",
    "\n",
    "    # NOTE: This can only be run once per process, so in a notebook we need to restart the kernel\n",
    "    process.start()\n",
    "\n",
    "\n",
    "webpage_results = crawl_company_webpage(\"synthesize.bio\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"output.json\", \"r\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "for page in sorted(data, key=lambda p: p[\"url\"]):\n",
    "    print(f\"\"\"\n",
    "# {page[\"url\"]}\n",
    "{len(page[\"markdown\"]):,} chars in markdown\n",
    "\n",
    "{page[\"markdown\"][:300]}...\n",
    "\"\"\")\n",
    "\n",
    "total_chars = sum(len(page[\"markdown\"]) for page in data)\n",
    "print(f\"Total chars: {total_chars:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
